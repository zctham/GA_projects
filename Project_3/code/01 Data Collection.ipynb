{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "833ab295",
   "metadata": {},
   "source": [
    "# Project 3 : Web APIs and NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4feace3",
   "metadata": {},
   "source": [
    "In this notebook we will be covering the following:\n",
    "\n",
    "1. [Introduction and Problem Statement](#Inital_understanding_of_datasets)\n",
    "2. [Data Dictionary](#Data_Dictionary) \n",
    "3. [Web Scraping](#Web_Scraping)\n",
    "4. [Exporting Data](#Exporting_Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8ae814",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaf1a28",
   "metadata": {},
   "source": [
    "In this project, we will be looking at creating a binary classifier to classify two Reddit posts of choice. Reddit is a social news, content, and discussions website. Posts are organised according to subject into user-created 'subreddits'. Members submit content (such as images, texts, and links) to subreddits, which can be voted up ('upvote') or down ('downvote') by other members. As of June 2021, Reddit ranked among the most popular mobile social apps in the United States with almost 48 million monthly active users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420a2a6a",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d10769",
   "metadata": {},
   "source": [
    "With the increasing popularity of data, technology and their uses in the past decade, it seems that the age of technology is here to stay. Not only is the world learning to harness the power of data, it is also learning to live with it. It comes as no surprise that such a relatively new industry attracts many talents to join in and seek a career from within, be it for the lucrative remuneration, or its challenges. We will be using data from two subreddits, 'datascience' and 'dataengineering' to understand, from a career switcher's point of view, the key differences between the two and if the results will enable him/her to decide which path to pivot to, taking into account his/her skillsets, academic qualification and interests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77286eef",
   "metadata": {},
   "source": [
    "## Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec852f8",
   "metadata": {},
   "source": [
    "|Feature |Type|Dataset|Description|\n",
    "|---|---|---|--|\n",
    "|title|str|datascience_posts/dataengineering_posts|title of reddit post|\n",
    "|selftext|str|datascience_posts/dataengineering_posts|selftext of reddit post|\n",
    "|subreddit|str|datascience_posts/dataengineering_posts|subreddit name|\n",
    "|title lemmatized|str|datascience_posts/dataengineering_posts|title after lemmatizing|\n",
    "|selftext lemmatized|str|datascience_posts/dataengineering_posts|selftext after lemmatizing|\n",
    "|lemma_comb|str|datascience_posts/dataengineering_posts|merge of title lemmatized and selftext lemmatized|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "070b32a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import pandas as pd\n",
    "import requests, time, math\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c2e466",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3934a541",
   "metadata": {},
   "source": [
    "We will be scrapping data from the two subreddits, **'DataScience'** and **'DataEngineering'**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60ae9841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape reddit posts with arguments subreddit, number of counts and epoch number\n",
    "\n",
    "def subreddit_scrapper(subreddit, postcount, before):\n",
    "    # Empty list to append all posts\n",
    "    postlist = []\n",
    "    \n",
    "    # For-loop to get posts    \n",
    "    for num in range((postcount//100)):\n",
    "        base_url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "        params = {\n",
    "        'subreddit': subreddit,\n",
    "        'size': 100,\n",
    "        'before': before\n",
    "        }\n",
    "        \n",
    "        # Requests get using defined parameters\n",
    "        res = requests.get(base_url, params)\n",
    "        \n",
    "        # Convert data to .json and append to empty list\n",
    "        data = res.json()\n",
    "        posts = data['data']\n",
    "        postlist.extend(posts)\n",
    "        \n",
    "        # Set new 'before' to get new 'before' posts\n",
    "        before = posts[-1]['created_utc']\n",
    "        \n",
    "    print(f'{len(postlist)} number of posts scrapped from {subreddit}.')    \n",
    "    return postlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe10937c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 number of posts scrapped from datascience.\n",
      "1000 number of posts scrapped from dataengineering.\n",
      "Wall time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Apply function to scrape data from respective subreddits\n",
    "ds = subreddit_scrapper('datascience', postcount = 1000, before = 1642060546)\n",
    "de = subreddit_scrapper('dataengineering',postcount = 1000, before = 1642060546)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1219bf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting scraped data to DataFrames\n",
    "\n",
    "ds_df = pd.DataFrame(ds)\n",
    "de_df = pd.DataFrame(de)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927b437a",
   "metadata": {},
   "source": [
    "## Exporting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d07fdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting data\n",
    "\n",
    "ds_df.to_csv('../data/datasci.csv', index = False)\n",
    "de_df.to_csv('../data/dataengin.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e07e3d",
   "metadata": {},
   "source": [
    "Exploratory data analysis will be done in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
