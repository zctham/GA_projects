title,selftext,subreddit,title lemmatized,selftext lemmatized
edge regression with edge features in graph neural networks,hey guys working on gnn to do edge regression using both node features and the edge weights of three types of edge it seems to me that when predicting target edge weight the neighbouring edge weights can carry interesting information at the same time including those edge weights as features obviously leads to leakage is there some way of dealing with this or am wrong in my assumptions my current approach is to basically stochastically drop out edges at each training epoch and remove the test edges from the graph entirely which has produced mild underwhelming improvement,1,edge regression with edge feature in graph neural network,hey guy working on gnn to do edge regression using both node feature and the edge weight of three type of edge it seems to me that when predicting target edge weight the neighbouring edge weight can carry interesting information at the same time including those edge weight a feature obviously lead to leakage is there some way of dealing with this or am wrong in my assumption my current approach is to basically stochastically drop out edge at each training epoch and remove the test edge from the graph entirely which ha produced mild underwhelming improvement
aspiring to be data analyst,can someone suggest me how should study data science with better efficiency from engineering background and have some experience with python had watched various youtube channels and tried to learn the skills required to become analyst but now feeling that stucked in tutorial hell understand what concepts tutorials are explaining but as soon as start to do some project on my own back to square one please suggest me some methods to learn data science effectively also in months my campus placements will start don feel that fully prepare for that can someone share some preparation tips,1,aspiring to be data analyst,can someone suggest me how should study data science with better efficiency from engineering background and have some experience with python had watched various youtube channel and tried to learn the skill required to become analyst but now feeling that stucked in tutorial hell understand what concept tutorial are explaining but a soon a start to do some project on my own back to square one please suggest me some method to learn data science effectively also in month my campus placement will start don feel that fully prepare for that can someone share some preparation tip
how to build your own chatbot in python using nltk,how to build your own chatbot in python using nltk,1,how to build your own chatbot in python using nltk,how to build your own chatbot in python using nltk
ds bi graduate certificates vs masters of ds cs degrees,as title says know most people in the ds field look at your experience what have you done also look at certain degrees so coming from non cs ds education would it be better cheaper to do graduate certificates vs going full on ms degree also hiring managers do you put any stock in graduate certificates amp x200b edit have been looking into such program at university of iowa,1,d bi graduate certificate v master of d c degree,a title say know most people in the d field look at your experience what have you done also look at certain degree so coming from non c d education would it be better cheaper to do graduate certificate v going full on m degree also hiring manager do you put any stock in graduate certificate amp x200b edit have been looking into such program at university of iowa
target salary for fresh phd grad in stats,hi all graduating from my phd in few months and am beginning my job search am confused on what good target salary should be since there doesn seem to be that much info on the webs but most alums from my research group got senior ds roles straight out of graduating so that makes me think the median senior ds salary on glassdoor is what should aim for is this even reasonable number phd alums what target comp did you aim for when you were going through the interview process my background if anyone cares few ds internships papers published in progress in bayesian modeling appreciate any advice,1,target salary for fresh phd grad in stats,hi all graduating from my phd in few month and am beginning my job search am confused on what good target salary should be since there doesn seem to be that much info on the web but most alum from my research group got senior d role straight out of graduating so that make me think the median senior d salary on glassdoor is what should aim for is this even reasonable number phd alum what target comp did you aim for when you were going through the interview process my background if anyone care few d internship paper published in progress in bayesian modeling appreciate any advice
hey high school student in mexico and into contest about data science it ll be amazing if maybe you can give me visit and like for the contest ll put the link down and if you can give me doma advices ll be great,hey high school student in mexico and into contest about data science it ll be amazing if maybe you can give me visit and like for the contest ll put the link down and if you can give me doma advices ll be great,1,hey high school student in mexico and into contest about data science it ll be amazing if maybe you can give me visit and like for the contest ll put the link down and if you can give me doma advice ll be great,hey high school student in mexico and into contest about data science it ll be amazing if maybe you can give me visit and like for the contest ll put the link down and if you can give me doma advice ll be great
how much of your workload is assigned to you vs your own how do you find new projects to work on,tldr my questions how much of your work is given requested of you vs projects that you conceived yourself when you do need to find new projects and there nothing obvious who how do you approach finding new work how does being remote and or organizationally isolated affect your approach started new ds position about months ago overall love it pays well good tech resources good people my boss is good guy and well respected in the company but because he important is in meetings of the day and constantly gets pulled into non ds projects which can don want to help with now here my problem most of my initial projects that my manager put me onto during the first few months phase are now completed and don have any more work to do my manager is too busy to go find work for me to do and has essentially told me that it up to me to find something new to work on this would be freeing to some but struggling to find something to do not just few hours of downtime we re talking days weeks here couple of things struggling with work remotely in traditionally non remote company and have very few regular meetings contacts so have basically no working relationships with anyone in the business and it very difficult to build new ones through computer screen my specific team is very small me my manager and other non ds person and we are support team so there not lot of opportunity to help or branch off of other data scientists locally while there are many other data scientists in the company none of them are close to my team so it would be stretching bit to try and insert myself into any of their projects since it not in the scope of my role and don know any of them personally it huge old company with very complicated business processes and org structure so the idea of formulating then solving problem alone as an outsider is likely to be huge waste of time and probably would annoy the people who are responsible maybe this is just rant because bored but wondering if anyone else has been in situation like this and what you did about it that doesn involve finding new company know ds is support role in many places so there probably need to justify yourself and think that the situation in,1,how much of your workload is assigned to you v your own how do you find new project to work on,tldr my question how much of your work is given requested of you v project that you conceived yourself when you do need to find new project and there nothing obvious who how do you approach finding new work how doe being remote and or organizationally isolated affect your approach started new d position about month ago overall love it pay well good tech resource good people my bos is good guy and well respected in the company but because he important is in meeting of the day and constantly get pulled into non d project which can don want to help with now here my problem most of my initial project that my manager put me onto during the first few month phase are now completed and don have any more work to do my manager is too busy to go find work for me to do and ha essentially told me that it up to me to find something new to work on this would be freeing to some but struggling to find something to do not just few hour of downtime we re talking day week here couple of thing struggling with work remotely in traditionally non remote company and have very few regular meeting contact so have basically no working relationship with anyone in the business and it very difficult to build new one through computer screen my specific team is very small me my manager and other non d person and we are support team so there not lot of opportunity to help or branch off of other data scientist locally while there are many other data scientist in the company none of them are close to my team so it would be stretching bit to try and insert myself into any of their project since it not in the scope of my role and don know any of them personally it huge old company with very complicated business process and org structure so the idea of formulating then solving problem alone a an outsider is likely to be huge waste of time and probably would annoy the people who are responsible maybe this is just rant because bored but wondering if anyone else ha been in situation like this and what you did about it that doesn involve finding new company know d is support role in many place so there probably need to justify yourself and think that the situation in
an approachable introduction to the bayesian ordered probit in and stan brms,an approachable introduction to the bayesian ordered probit in and stan brms,1,an approachable introduction to the bayesian ordered probit in and stan brms,an approachable introduction to the bayesian ordered probit in and stan brms
how much of your workload is assigned to you vs your own how do you find new projects to work on,tldr my questions how much of your work is given requested of you vs projects that you conceived yourself when you do need to find new projects and there nothing obvious who how do you approach finding new work how does being remote and or organizationally isolated affect your approach started new ds position about months ago overall love it pays well good tech resources good people my boss is good guy and well respected in the company but because he important is in meetings of the day and constantly gets pulled into non ds projects which can don want to help with now here my problem most of my initial projects that my manager put me onto during the first few months phase are now completed and don have any more work to do my manager is too busy to go find work for me to do and has essentially told me that it up to me to find something new to work on this would be freeing to some but struggling to find something to do not just few hours of downtime we re talking days weeks here couple of things struggling with work remotely in traditionally non remote company and have very few regular meetings contacts so have basically no working relationships with anyone in the business and it very difficult to build new ones through computer screen my specific team is very small me my manager and other non ds person and we are support team so there not lot of opportunity to help or branch off of other data scientists locally while there are many other data scientists in the company none of them are close to my team so it would be stretching bit to try and insert myself into any of their projects since it not in the scope of my role and don know any of them personally it huge old company with very complicated business processes and org structure so the idea of formulating then solving problem alone as an outsider is likely to be huge waste of time and probably would annoy the people who are responsible maybe this is just rant because bored but wondering if anyone else has been in situation like this and what you did about it that doesn involve finding new company know ds is support role in many places so there probably need to justify yourself and think that the situation in,1,how much of your workload is assigned to you v your own how do you find new project to work on,tldr my question how much of your work is given requested of you v project that you conceived yourself when you do need to find new project and there nothing obvious who how do you approach finding new work how doe being remote and or organizationally isolated affect your approach started new d position about month ago overall love it pay well good tech resource good people my bos is good guy and well respected in the company but because he important is in meeting of the day and constantly get pulled into non d project which can don want to help with now here my problem most of my initial project that my manager put me onto during the first few month phase are now completed and don have any more work to do my manager is too busy to go find work for me to do and ha essentially told me that it up to me to find something new to work on this would be freeing to some but struggling to find something to do not just few hour of downtime we re talking day week here couple of thing struggling with work remotely in traditionally non remote company and have very few regular meeting contact so have basically no working relationship with anyone in the business and it very difficult to build new one through computer screen my specific team is very small me my manager and other non d person and we are support team so there not lot of opportunity to help or branch off of other data scientist locally while there are many other data scientist in the company none of them are close to my team so it would be stretching bit to try and insert myself into any of their project since it not in the scope of my role and don know any of them personally it huge old company with very complicated business process and org structure so the idea of formulating then solving problem alone a an outsider is likely to be huge waste of time and probably would annoy the people who are responsible maybe this is just rant because bored but wondering if anyone else ha been in situation like this and what you did about it that doesn involve finding new company know d is support role in many place so there probably need to justify yourself and think that the situation in
merge sort part,merge sort part,1,merge sort part,merge sort part
finding part time ds work,hey guys does anyone know how to find part time non full time roles in data science am recovering from health issue and can handle hour grind but could probably work or hours however don even know where to begin to find this type of work appreciate any thoughts you have,1,finding part time d work,hey guy doe anyone know how to find part time non full time role in data science am recovering from health issue and can handle hour grind but could probably work or hour however don even know where to begin to find this type of work appreciate any thought you have
how to build pdf and html reports in python using zen knit rmarkdown alternative,how to build pdf and html reports in python using zen knit rmarkdown alternative,1,how to build pdf and html report in python using zen knit rmarkdown alternative,how to build pdf and html report in python using zen knit rmarkdown alternative
guidance on searching for job in the uk for foreigner south african,hi guys am looking for any advice on apply for junior mid level data science position in london have masters in advanced analytics statistics data science also have year work experience as grad and year as data scientist current work in credit at large bank in south africa have techincial knowledge of various modelling techniques supervised and unsupervised linear models neural networks etc have experience developing models within the bank and some experience of writing production level code and working with software engineering tools and working on aws azure have aws cloud practioner certification any advice or guidance from hiring managers and others who have walked similar path would be really appreciated thanks so much guys,1,guidance on searching for job in the uk for foreigner south african,hi guy am looking for any advice on apply for junior mid level data science position in london have master in advanced analytics statistic data science also have year work experience a grad and year a data scientist current work in credit at large bank in south africa have techincial knowledge of various modelling technique supervised and unsupervised linear model neural network etc have experience developing model within the bank and some experience of writing production level code and working with software engineering tool and working on aws azure have aws cloud practioner certification any advice or guidance from hiring manager and others who have walked similar path would be really appreciated thanks so much guy
sports analytics company hudl is looking for data scientist applied machine learning,sports analytics company hudl is looking for data scientist applied machine learning,1,sport analytics company hudl is looking for data scientist applied machine learning,sport analytics company hudl is looking for data scientist applied machine learning
interview process,just started interviewing for data analytics roles with various companies extremely nervous as making huge career pivot from head of marketing where also served as the data lead to my first official data analytics role and potentially senior role as well my question is made it to the second round of interviews recently and about to have an assessment sent to me how have you all found them extremely difficult how many of you all have lost out on great role due to an assessment,1,interview process,just started interviewing for data analytics role with various company extremely nervous a making huge career pivot from head of marketing where also served a the data lead to my first official data analytics role and potentially senior role a well my question is made it to the second round of interview recently and about to have an assessment sent to me how have you all found them extremely difficult how many of you all have lost out on great role due to an assessment
newbie here need help finding data,high school student currently studying data science doing research project on printed vs digital on books all things paper like news novels and magazines ve tried to look for data but haven found much except for statista require payment too expensive for just simple research project can anyone who knows where could find other data about the book sales help me out,1,newbie here need help finding data,high school student currently studying data science doing research project on printed v digital on book all thing paper like news novel and magazine ve tried to look for data but haven found much except for statista require payment too expensive for just simple research project can anyone who know where could find other data about the book sale help me out
decision time career choice,hi have recently graduated with masters in computer engineering with majors in machine learning and have around years of experience as data analyst mostly support and year tableau powerbi and sql server reports development and looking to get to data scientist and data analyst roles but after frequent rejections was offered application support engineer role sql server and linux with good company at average compensation ie annum in europe should accept or reject it my only fear is that won be considered for data analyst amp data scientist roles in future if opt for this now,1,decision time career choice,hi have recently graduated with master in computer engineering with major in machine learning and have around year of experience a data analyst mostly support and year tableau powerbi and sql server report development and looking to get to data scientist and data analyst role but after frequent rejection wa offered application support engineer role sql server and linux with good company at average compensation ie annum in europe should accept or reject it my only fear is that won be considered for data analyst amp data scientist role in future if opt for this now
is there anywhere that can find how to use ti for different formulas in elementary statistics,trying to put cheat sheet together and wondering if there is an online resource anywhere that has list of common statistics formulas and how to solve them by calculator appreciate any help have final on friday and nervous,1,is there anywhere that can find how to use ti for different formula in elementary statistic,trying to put cheat sheet together and wondering if there is an online resource anywhere that ha list of common statistic formula and how to solve them by calculator appreciate any help have final on friday and nervous
what is the best data science boot camp for total noob,asking for friend who has only history degree and pretty sure no calc linear algebra or statistics experience hopefully one that gets you job at the end tyvm,1,what is the best data science boot camp for total noob,asking for friend who ha only history degree and pretty sure no calc linear algebra or statistic experience hopefully one that get you job at the end tyvm
what is my main deliverable address data scientist working on ml problems to get my bonus,hello learning data science as know how valuable it is especially with growing data and more competent tools available in the market want to understand what would be my main deliverable and hardest thing to do as data scientist to rise in ranks and get bonus it would be great to understand so can further educate and add focus on building this skill ve browsed through the posts and haven fully understood what is to be the biggest pain skill to have there seems to be loads of tools that automate tasks like model building and stuff so do really need to be that technical would love to hear your thoughts from aspiring data scientist,1,what is my main deliverable address data scientist working on ml problem to get my bonus,hello learning data science a know how valuable it is especially with growing data and more competent tool available in the market want to understand what would be my main deliverable and hardest thing to do a data scientist to rise in rank and get bonus it would be great to understand so can further educate and add focus on building this skill ve browsed through the post and haven fully understood what is to be the biggest pain skill to have there seems to be load of tool that automate task like model building and stuff so do really need to be that technical would love to hear your thought from aspiring data scientist
how to change fields in data science,studied chemistry and then did masters in applied statistics soon it will be two years working in pharmaceutical company in data scientist role my first job amp x200b it been quite journey enjoyed it but also discovered that much more interested in the methodology than the current application would like to keep learning and explore new fields believe have the required skills and knowledge to do good job in other fields however there are two considerations that would prefer to attach feeling more comfortable in case this takes place and for this think the most important for me is to improve my mathematical background improve my chances of getting hired to solve this two at the same time considered study some more but three or four years of bachelors at this stage is too much time investment could do courses by my own in order to learn but this wouldn be reflected in the cv any recommendations opinions,1,how to change field in data science,studied chemistry and then did master in applied statistic soon it will be two year working in pharmaceutical company in data scientist role my first job amp x200b it been quite journey enjoyed it but also discovered that much more interested in the methodology than the current application would like to keep learning and explore new field believe have the required skill and knowledge to do good job in other field however there are two consideration that would prefer to attach feeling more comfortable in case this take place and for this think the most important for me is to improve my mathematical background improve my chance of getting hired to solve this two at the same time considered study some more but three or four year of bachelor at this stage is too much time investment could do course by my own in order to learn but this wouldn be reflected in the cv any recommendation opinion
free live class on deep learning in tensorflow keras by data science youtuber greg hogg,free live class on deep learning in tensorflow keras by data science youtuber greg hogg,1,free live class on deep learning in tensorflow kera by data science youtuber greg hogg,free live class on deep learning in tensorflow kera by data science youtuber greg hogg
learn how to work for american and european companies with our guide,learn how to work for american and european companies with our guide,1,learn how to work for american and european company with our guide,learn how to work for american and european company with our guide
computing categorical feature importance using shap values,so have model with categorical variable encoded with one hot encoding and want to compute the feature importance of the categorical variable the feature importance is defined as the mean absolute value of the shap values then to get the global importance of categorical feature should just aggregate the values of all the components of the one hot encoding,1,computing categorical feature importance using shap value,so have model with categorical variable encoded with one hot encoding and want to compute the feature importance of the categorical variable the feature importance is defined a the mean absolute value of the shap value then to get the global importance of categorical feature should just aggregate the value of all the component of the one hot encoding
where to look for data scientist freelanceres consultants,my company is currently looking into hiring very qualified data scientist as consultant for short term project the type of candidate we are looking for ideally has phd or years of experience as data scientist what resources websites are available for me to look for these kind of candidates so far have looked into toptal upwork and kolabtree are there other websites or platforms can look at for potential candidates,1,where to look for data scientist freelanceres consultant,my company is currently looking into hiring very qualified data scientist a consultant for short term project the type of candidate we are looking for ideally ha phd or year of experience a data scientist what resource website are available for me to look for these kind of candidate so far have looked into toptal upwork and kolabtree are there other website or platform can look at for potential candidate
is there way to update an lda model with more passes,for example run few passes save the model and then run more passes over it want to do this in chunks because my computer shuts down after few passes maybe it heats up idk,1,is there way to update an lda model with more pass,for example run few pass save the model and then run more pass over it want to do this in chunk because my computer shuts down after few pass maybe it heat up idk
how to load partial corpus for gensim lda,is there way to load partial corpus in market matrix format run lda over it save the model and then update the model with the remaining corpus need to do this because of memory issues,1,how to load partial corpus for gensim lda,is there way to load partial corpus in market matrix format run lda over it save the model and then update the model with the remaining corpus need to do this because of memory issue
about cocktail db,hey everyone few days ago asked question about project suggestion in this community got very helpful reply of searching rapid api for datasets did not know rapid api before this searched and found dataset of cocktails yeah love making and having cocktails so that caught my attention as much as am interested in this don understand what kind of technology nlp or something will need if anyone has worked on it before please let me know or else just tell me how to figure these things out and how to go about project,1,about cocktail db,hey everyone few day ago asked question about project suggestion in this community got very helpful reply of searching rapid api for datasets did not know rapid api before this searched and found dataset of cocktail yeah love making and having cocktail so that caught my attention a much a am interested in this don understand what kind of technology nlp or something will need if anyone ha worked on it before please let me know or else just tell me how to figure these thing out and how to go about project
leetcode for faang data science interviews,for anyone with experience with faang interviews for data scientists did you come across lot of leetcode questions currently in the interview process with google and facebook both only at the technical screen stage next week and want to know if should study some general coding questions on leetcode for python ve been using stratasratch so far to prep and all of these questions so far seem more along the lines of query this table and not write me an algorithm for palindromes,1,leetcode for faang data science interview,for anyone with experience with faang interview for data scientist did you come across lot of leetcode question currently in the interview process with google and facebook both only at the technical screen stage next week and want to know if should study some general coding question on leetcode for python ve been using stratasratch so far to prep and all of these question so far seem more along the line of query this table and not write me an algorithm for palindrome
how to do uplift modeling want to optimize revenue customer and incremental new customer,have ab data available test where offers where shown amp control where offer were not shown can create the dataset as required in the research papers problem1 once have made the model how do deploy it or use it to target specific users problem2 understand that rolled up get total uplift how do get that uplift at new customer level increase in revenue level,1,how to do uplift modeling want to optimize revenue customer and incremental new customer,have ab data available test where offer where shown amp control where offer were not shown can create the dataset a required in the research paper problem1 once have made the model how do deploy it or use it to target specific user problem2 understand that rolled up get total uplift how do get that uplift at new customer level increase in revenue level
streamlit app to compare text similarity,streamlit app to compare text similarity,1,streamlit app to compare text similarity,streamlit app to compare text similarity
phd dropouts,what job title do you guys have now do you regret dropping out of your phd program whats the job salary like compared to same experience person but with phd,1,phd dropout,what job title do you guy have now do you regret dropping out of your phd program whats the job salary like compared to same experience person but with phd
guidence,dear all respected data field genius just done my post graduation diploma in data science specialization business analytics and going to pursue master in data science soon can anybody give me guidance,1,guidence,dear all respected data field genius just done my post graduation diploma in data science specialization business analytics and going to pursue master in data science soon can anybody give me guidance
cs software engineer or math stats,hello everyone just to give some background before exposing my question have bs in maths with minor in cs and ms in stats currently attending to ds micromasters while working as research assistant in ds amp x200b even though know the foundations of ml dl know sql and several nosql databases still consider that not able to land relevant job since know nothing about dashboarding datawarehouses data lakes version control docker deploying models cloud tools distributed systems ci cd rest apis and so on if had to hazard guess say that probably of the job of ds or more don know if it happens in your country too but here all ds masters are almost just programming in python in jupyter notebook sure it is good start but that not useful for most companies if could return to the past probably not major in maths stats but in cs sure math is the core of ds but everything we need is already implemented of the time pip install and voila no one cares if know how to make the proof of pca or just naively call it dimension reduction tool unless we ve to implement solution from scratch it seems to me that cs students are far ahead than stats math students when they finished their courses besides research jobs what do you think can you suggest some online courses to get in touch with those swe topics previously mentioned it can be paid like coursera etc regarding math stats students did they accept you and train you in job to get proeficient in recent tech stack how hard was it to follow how do you compare yourselves in job with cs students do you feel any advantage of your math stats knowledge or just worse overall thanks lot on guiding me bit lost,1,c software engineer or math stats,hello everyone just to give some background before exposing my question have b in math with minor in c and m in stats currently attending to d micromasters while working a research assistant in d amp x200b even though know the foundation of ml dl know sql and several nosql database still consider that not able to land relevant job since know nothing about dashboarding datawarehouses data lake version control docker deploying model cloud tool distributed system ci cd rest apis and so on if had to hazard guess say that probably of the job of d or more don know if it happens in your country too but here all d master are almost just programming in python in jupyter notebook sure it is good start but that not useful for most company if could return to the past probably not major in math stats but in c sure math is the core of d but everything we need is already implemented of the time pip install and voila no one care if know how to make the proof of pca or just naively call it dimension reduction tool unless we ve to implement solution from scratch it seems to me that c student are far ahead than stats math student when they finished their course besides research job what do you think can you suggest some online course to get in touch with those swe topic previously mentioned it can be paid like coursera etc regarding math stats student did they accept you and train you in job to get proeficient in recent tech stack how hard wa it to follow how do you compare yourselves in job with c student do you feel any advantage of your math stats knowledge or just worse overall thanks lot on guiding me bit lost
meta code pairing leetcode helpful,hey so have an interview with meta for data scientist product position lined up that will be code pair product sense interview from the eu and these coding assessments are not really thing here even for well paid positions first encountered it when got contacted by doordash couple weeks ago needless to say totally blew it it was about data wrangling and could choose sql python or went with where really quick doing queries with data table because work with it daily felt that it anyway impossible to do these kinds of coding interviews without lot of prep in that sense is it from your experience worth it getting lc premium account asking because saw that they have some specific prep material for facebook and thinking that this may be my shot at catching up with this,1,meta code pairing leetcode helpful,hey so have an interview with meta for data scientist product position lined up that will be code pair product sense interview from the eu and these coding assessment are not really thing here even for well paid position first encountered it when got contacted by doordash couple week ago needle to say totally blew it it wa about data wrangling and could choose sql python or went with where really quick doing query with data table because work with it daily felt that it anyway impossible to do these kind of coding interview without lot of prep in that sense is it from your experience worth it getting lc premium account asking because saw that they have some specific prep material for facebook and thinking that this may be my shot at catching up with this
should quit mobile and start learning data science,ve been learning android development since th grade and really enjoyed this field currently learning software engineering in college year and working as android developer however most of my friends teachers and colleagues tell me that will be good in data science due to my exceptional skill in math and great knowledge in both declarative and functional programming thinking if it is worth to quit my current career and try to learn data science interested if there anyone who moved from software engineering to data science as well really enjoy math much more than programming and my favorite college courses were calculus complex analysis and differential equations what do you think will data science be more enjoyable for me or not,1,should quit mobile and start learning data science,ve been learning android development since th grade and really enjoyed this field currently learning software engineering in college year and working a android developer however most of my friend teacher and colleague tell me that will be good in data science due to my exceptional skill in math and great knowledge in both declarative and functional programming thinking if it is worth to quit my current career and try to learn data science interested if there anyone who moved from software engineering to data science a well really enjoy math much more than programming and my favorite college course were calculus complex analysis and differential equation what do you think will data science be more enjoyable for me or not
how would you calculate expected value of sales leads,simplified situation have leads but can only pursue of them due to limited resources salespeople each of these leads can be pitched up to products each with its own dollar value and estimated probability to convert not all customers have all products available to them they might only qualify for of them customer can convert on any combination of those products how to calculate the expected value of the leads so that can prioritize the top if it were single product could do pconvert expected but how do account for multiple products that can be pitched to the lead,1,how would you calculate expected value of sale lead,simplified situation have lead but can only pursue of them due to limited resource salesperson each of these lead can be pitched up to product each with it own dollar value and estimated probability to convert not all customer have all product available to them they might only qualify for of them customer can convert on any combination of those product how to calculate the expected value of the lead so that can prioritize the top if it were single product could do pconvert expected but how do account for multiple product that can be pitched to the lead
does little sleep affect creativity important questions about sleep,it is said that you need to sleep at least hours day is there difference between sleeping hours or hours does little sleep affect creativity where can really find the most accurate resource why everyone says something else the most successful people got little sleep elon musk sleeps hours claims that he gets by on hours of sleep when he is busy for someone who sleeps hours day suddenly hours of sleep can cause fatigue but after long time he can get his brain used to this sleep time do you think we will experience decrease in our performance after the brain gets used to it,1,doe little sleep affect creativity important question about sleep,it is said that you need to sleep at least hour day is there difference between sleeping hour or hour doe little sleep affect creativity where can really find the most accurate resource why everyone say something else the most successful people got little sleep elon musk sleep hour claim that he get by on hour of sleep when he is busy for someone who sleep hour day suddenly hour of sleep can cause fatigue but after long time he can get his brain used to this sleep time do you think we will experience decrease in our performance after the brain get used to it
concrete numpy data science and machine learning over encrypted data,concrete numpy data science and machine learning over encrypted data,1,concrete numpy data science and machine learning over encrypted data,concrete numpy data science and machine learning over encrypted data
employer wants to pay for bootcamp,hi all was hired as data analyst at fortune with some experience as business data analyst from previous role my knowledge on sql is good not great don have any scripting python experience and my employer is offering to pay for bootcamp to familiarize myself with sql they are asking for my input and mentioned to find some classes that also offer some programming languages in their course because believe it would be beneficial my question to you guys is do you have any recommendations of bootcamp or advice on educating myself thanks,1,employer want to pay for bootcamp,hi all wa hired a data analyst at fortune with some experience a business data analyst from previous role my knowledge on sql is good not great don have any scripting python experience and my employer is offering to pay for bootcamp to familiarize myself with sql they are asking for my input and mentioned to find some class that also offer some programming language in their course because believe it would be beneficial my question to you guy is do you have any recommendation of bootcamp or advice on educating myself thanks
data analyst and data scientist labels,usually introduce myself as data analyst with years experience but have done statistical modeling worked with data engineers on setting up the cloud environment parallel processing time series analysis forecasting data visualization nlp mining unstructured text files and messy structured data supervised and unsupervised learning so do have experience with data science but feel like am overselling myself or an imposter if said am data scientist but could be underselling myself if said am data analyst because have done bit more how define data scientist ve met data scientists with undergrad and graduate degrees in statistics math computer science with years of experience in data science work streams with big data having met these individuals these are the ones respectfully consider data scientists am wrong,1,data analyst and data scientist label,usually introduce myself a data analyst with year experience but have done statistical modeling worked with data engineer on setting up the cloud environment parallel processing time series analysis forecasting data visualization nlp mining unstructured text file and messy structured data supervised and unsupervised learning so do have experience with data science but feel like am overselling myself or an imposter if said am data scientist but could be underselling myself if said am data analyst because have done bit more how define data scientist ve met data scientist with undergrad and graduate degree in statistic math computer science with year of experience in data science work stream with big data having met these individual these are the one respectfully consider data scientist am wrong
create stunning visualization in one line of code,create stunning visualization in one line of code,1,create stunning visualization in one line of code,create stunning visualization in one line of code
free resume builder if you re interested,free resume builder if you re interested,1,free resume builder if you re interested,free resume builder if you re interested
does web scraping look bad on resume,am currently master student and am applying for summer internships right now my previous job was at data analytics firm where one of my main duties was writing new or fixing old web scrapers the company scraped over websites everyday to collect data this data was then used for various projects my concern is that employers might look down upon it because web scraping is sort of grey area is this the case or am worried unnecessarily if it is something that looked down upon how can phrase it to sound better,1,doe web scraping look bad on resume,am currently master student and am applying for summer internship right now my previous job wa at data analytics firm where one of my main duty wa writing new or fixing old web scraper the company scraped over website everyday to collect data this data wa then used for various project my concern is that employer might look down upon it because web scraping is sort of grey area is this the case or am worried unnecessarily if it is something that looked down upon how can phrase it to sound better
multiple time series with exogenous variables the best approach,hi all have many different stores with their own weekly sales data let say stores each store has other time series data for example number of enquiries for that week there about of these time series that want to treat as exogenous variables stores with time series weekly sales data exogenous time series data for each store is there an appropriate time series model for this looked into hierarchical time series but couldn see how you could incorporate exogenous variables looked into your standard time series and it letting arima have exogenous variables is relatively straight forward in python however in this problem there is multiple time series for each store as an add on is it possible to add on to the time series other time independent data for example location of the store type of store etc right now my plan to approach this is by treating it as regression problem using the final week as the outcome variable and the starting weeks to represent growth in the exogenous time series think this would capture some of the relationship but it downside would be losing all the other time data points this isn huge project and the data set is relatively small it would be interested to hear your thoughts cheers,1,multiple time series with exogenous variable the best approach,hi all have many different store with their own weekly sale data let say store each store ha other time series data for example number of enquiry for that week there about of these time series that want to treat a exogenous variable store with time series weekly sale data exogenous time series data for each store is there an appropriate time series model for this looked into hierarchical time series but couldn see how you could incorporate exogenous variable looked into your standard time series and it letting arima have exogenous variable is relatively straight forward in python however in this problem there is multiple time series for each store a an add on is it possible to add on to the time series other time independent data for example location of the store type of store etc right now my plan to approach this is by treating it a regression problem using the final week a the outcome variable and the starting week to represent growth in the exogenous time series think this would capture some of the relationship but it downside would be losing all the other time data point this isn huge project and the data set is relatively small it would be interested to hear your thought cheer
chances as non statistician computer scientist,got my bachelors of engineering in summer and am currently finishing masters in information systems half of it is data science could sadly not apply for pure data science master degree as am writing my thesis and reading more and more into data mining and ai topics did have some basics in my masters realized that there is so much to learn and started wondering if will have chance against others who studied statistics computer science etc since they already covered lot of important topics during their studies that still have to learn how likely is it to get chance as junior data scientist after finish my studies imagine the competition being tough and without job cannot get the experience needed,1,chance a non statistician computer scientist,got my bachelor of engineering in summer and am currently finishing master in information system half of it is data science could sadly not apply for pure data science master degree a am writing my thesis and reading more and more into data mining and ai topic did have some basic in my master realized that there is so much to learn and started wondering if will have chance against others who studied statistic computer science etc since they already covered lot of important topic during their study that still have to learn how likely is it to get chance a junior data scientist after finish my study imagine the competition being tough and without job cannot get the experience needed
how would you recommend go about my ds career path,some background hi guys soon to be data scientist hope lol and about to finish bootcamp by data company in few weeks so far have learned lot mainly coding and ml coming from scientific background was already exposed to all the statistics done some practice projects and will be certified after an exam assessment you get recruited if you perform well enough my possible options are being recruited as an intern finding place to intern on my own in case don get recruited building kaggle online presence by doing few projects taking an advanced course although prefer learning by doing projects what would you do as newbie in industry think bound to make few mistakes what exact path would you recommend take from here in order to be an employee people would want to hire,1,how would you recommend go about my d career path,some background hi guy soon to be data scientist hope lol and about to finish bootcamp by data company in few week so far have learned lot mainly coding and ml coming from scientific background wa already exposed to all the statistic done some practice project and will be certified after an exam assessment you get recruited if you perform well enough my possible option are being recruited a an intern finding place to intern on my own in case don get recruited building kaggle online presence by doing few project taking an advanced course although prefer learning by doing project what would you do a newbie in industry think bound to make few mistake what exact path would you recommend take from here in order to be an employee people would want to hire
help understanding model,will try to be brief and explain the problem for university project have to analyze this dataset of observations and variables inherent in document on physical activity and dated the dependent variable is the number of calories burned during physical activity from each subject the regressors are the level of work that is the number of calories burned per hour and the body mass in kilos of each subject all three variables are quantitative two regression models are provided in the dataset a0 a1 a2 b0 b1 b3 b4 the goal of the study would be to compare the two models with each other while the first is classic multiple regression model for the second model the original document mentions that b0 b1 b3 and b4 are constants obtained through an unspecified graphical method the only thing the document reports is that the model estimated in this way turned out to be have no idea what kind of model this is nor how to adapt it or what to do with it in rstudio since those constants obtained this way please help me understand figure out something about it,1,help understanding model,will try to be brief and explain the problem for university project have to analyze this dataset of observation and variable inherent in document on physical activity and dated the dependent variable is the number of calorie burned during physical activity from each subject the regressors are the level of work that is the number of calorie burned per hour and the body mass in kilo of each subject all three variable are quantitative two regression model are provided in the dataset a0 a1 a2 b0 b1 b3 b4 the goal of the study would be to compare the two model with each other while the first is classic multiple regression model for the second model the original document mention that b0 b1 b3 and b4 are constant obtained through an unspecified graphical method the only thing the document report is that the model estimated in this way turned out to be have no idea what kind of model this is nor how to adapt it or what to do with it in rstudio since those constant obtained this way please help me understand figure out something about it
big data book recommendations,does anyone have good big data recommendations ideally one that goes in to big data architecture design and data engineering techniques and not how to use databricks type of book,1,big data book recommendation,doe anyone have good big data recommendation ideally one that go in to big data architecture design and data engineering technique and not how to use databricks type of book
how to find companies on the internet based on keywords,study artificial intelligence and for project we have to find companies based on given industry and geographic location like us europe asia etc but am bit clueless how ve searched for databases but either they lack company description or they are not that large is there another way to do this,1,how to find company on the internet based on keywords,study artificial intelligence and for project we have to find company based on given industry and geographic location like u europe asia etc but am bit clueless how ve searched for database but either they lack company description or they are not that large is there another way to do this
which udemy course should pick to master python language for data science,which udemy course should pick to master python language for data science,1,which udemy course should pick to master python language for data science,which udemy course should pick to master python language for data science
rejects from everywhere any tips appreciated,am looking for roles in data science and data analytics but have not been able to secure anything till now am dejected and don know what can do to improve my chances should improve my resume should job search better have no way of knowing thought that could get some advice here if you are open to reviewing my resume please let me know and will drop you message this will help me lot thank you for reading,1,reject from everywhere any tip appreciated,am looking for role in data science and data analytics but have not been able to secure anything till now am dejected and don know what can do to improve my chance should improve my resume should job search better have no way of knowing thought that could get some advice here if you are open to reviewing my resume please let me know and will drop you message this will help me lot thank you for reading
has anybody got experience automating generating weekly business and sales reports with python to pdf,work as data analyst in the commercial strategy function of pharma company want program that prepares an end of week sales report that is then distributed to my stakeholders automatically using aws lambda my thinking is that if certain thresholds are met on given metric portion of text will be written if another condition is met then other text will be written you get the idea think it will be lot of if else statements there not too much online about this anybody got some thoughts,1,ha anybody got experience automating generating weekly business and sale report with python to pdf,work a data analyst in the commercial strategy function of pharma company want program that prepares an end of week sale report that is then distributed to my stakeholder automatically using aws lambda my thinking is that if certain threshold are met on given metric portion of text will be written if another condition is met then other text will be written you get the idea think it will be lot of if else statement there not too much online about this anybody got some thought
guidance on data analytics case studies,am prepping up to make switch from being splunk developer to full time data analyst was looking for some case studies two to three to include in my portfolio and some guidance around those am skilled in sql and python,1,guidance on data analytics case study,am prepping up to make switch from being splunk developer to full time data analyst wa looking for some case study two to three to include in my portfolio and some guidance around those am skilled in sql and python
oneflow v0 just came out,oneflow v0 just came out,1,oneflow v0 just came out,oneflow v0 just came out
here is survey for my data viz project please fill the form,here is survey for my data viz project please fill the form,1,here is survey for my data viz project please fill the form,here is survey for my data viz project please fill the form
multi class or one vs rest for classification,hi all have dataset trying to predict which category person falls into have categories the distribution is quite skewed or so samples fall into class another into another and the rest lt but above understand that could fit some multiclass model or could fit model for each category set it the positive class and everything else to the negative class so called one vs rest are there any rules of thumb for which is best in given scenario or is it down to experimentation thinking that one vs rest with lots of categories will inevitably produce lots of models with high class imbalance which can complicate things so perhaps better to go with the multiclass,1,multi class or one v rest for classification,hi all have dataset trying to predict which category person fall into have category the distribution is quite skewed or so sample fall into class another into another and the rest lt but above understand that could fit some multiclass model or could fit model for each category set it the positive class and everything else to the negative class so called one v rest are there any rule of thumb for which is best in given scenario or is it down to experimentation thinking that one v rest with lot of category will inevitably produce lot of model with high class imbalance which can complicate thing so perhaps better to go with the multiclass
employees of faang am doing survey can you help me please,have few questions what is your designation in your company what technologies do you often work interact with what exactly do the big tech faang companies look for in candidates when hiring what are some red flags specific to the faang companies,1,employee of faang am doing survey can you help me please,have few question what is your designation in your company what technology do you often work interact with what exactly do the big tech faang company look for in candidate when hiring what are some red flag specific to the faang company
what are the pros and cons of your data scientist job,thinking of transitioning to data analytics data science and want to see if it the right fit,1,what are the pro and con of your data scientist job,thinking of transitioning to data analytics data science and want to see if it the right fit
what is your process when doing data science,am aware of machine learning algrothims and frameworks such as the basic ones like sci kit learn pytorch and tensorflow if you gave me perfect practice dataset and told me to use model then would know what model to use and would get relatively good results since the data is made for specific algorithm but how do experienced people do data science on real world data that is not pre made for an algorithm in other words what is your process when doing feature engineering so you can have insightful data to plug into model thanks and have good day,1,what is your process when doing data science,am aware of machine learning algrothims and framework such a the basic one like sci kit learn pytorch and tensorflow if you gave me perfect practice dataset and told me to use model then would know what model to use and would get relatively good result since the data is made for specific algorithm but how do experienced people do data science on real world data that is not pre made for an algorithm in other word what is your process when doing feature engineering so you can have insightful data to plug into model thanks and have good day
normally speaking what do you do when your model fail after very long time of training,normally speaking what do you do when your model fail after very long time of training,1,normally speaking what do you do when your model fail after very long time of training,normally speaking what do you do when your model fail after very long time of training
on site or remote ds master degree,could have the opportunity to course master degree in two different universities but for the presencial one should move to other city leaving family home and that stuff to get bit of knowledge and big title from recognized university in the country but don if it worth leaving my home to get that title already work as data engineer for good company feel that now getting the knowledge from internet is more effective than college but you know the university recognition is supposed to worth should take the remote ms and that way not invest so much effort excuse my bad english,1,on site or remote d master degree,could have the opportunity to course master degree in two different university but for the presencial one should move to other city leaving family home and that stuff to get bit of knowledge and big title from recognized university in the country but don if it worth leaving my home to get that title already work a data engineer for good company feel that now getting the knowledge from internet is more effective than college but you know the university recognition is supposed to worth should take the remote m and that way not invest so much effort excuse my bad english
want to go to data science and visualization bootcamp is it worth it,an absolute novice revolving around data science but my school is hosting bootcamps and it starts around next month read that it flexible program but genuinely want to learn how to do this,1,want to go to data science and visualization bootcamp is it worth it,an absolute novice revolving around data science but my school is hosting bootcamps and it start around next month read that it flexible program but genuinely want to learn how to do this
just got entry level associate data scientist job need your help,hi am from mechanical engineering background who decided to make the switch into data science last year for the first months did all the self learning could do next months joined bootcamp to get certified have done few projects and am confident about my basic knowledge of python sql and tableau is there anything should keep in mind what should start learning next to make this shift easier any work related advice from you thanks,1,just got entry level associate data scientist job need your help,hi am from mechanical engineering background who decided to make the switch into data science last year for the first month did all the self learning could do next month joined bootcamp to get certified have done few project and am confident about my basic knowledge of python sql and tableau is there anything should keep in mind what should start learning next to make this shift easier any work related advice from you thanks
anyone familiar with purdue university ai ml post bacc program worth it,hey all just wondering if anyone can give advice on whether or not to pursue this online ai ml certification program offered by purdue it costs and will be year long my current company will likely cover the cost under their education benefits program bit of background former phd dropout in computational biophysics so thing have the math background to pick up the concepts through the program currently software engineer at my job and have been wanting to dig into ai ml eventually want to break into the game development scene and specialize in video game ai any thoughts suggestions would be appreciated thanks,1,anyone familiar with purdue university ai ml post bacc program worth it,hey all just wondering if anyone can give advice on whether or not to pursue this online ai ml certification program offered by purdue it cost and will be year long my current company will likely cover the cost under their education benefit program bit of background former phd dropout in computational biophysics so thing have the math background to pick up the concept through the program currently software engineer at my job and have been wanting to dig into ai ml eventually want to break into the game development scene and specialize in video game ai any thought suggestion would be appreciated thanks
so many opportunities in the job market right now,is anyone else experiencing this too recently applied to job at google and was asked if wanted to be considered for multiple positions and also got my first interview with apple seriously the best call back rate ve had like ever when applied in it was like crickets lol hearing similar things from friends and former colleagues in the industry too seems like now is great time to look for job opportunity,1,so many opportunity in the job market right now,is anyone else experiencing this too recently applied to job at google and wa asked if wanted to be considered for multiple position and also got my first interview with apple seriously the best call back rate ve had like ever when applied in it wa like cricket lol hearing similar thing from friend and former colleague in the industry too seems like now is great time to look for job opportunity
merge sort,merge sort,1,merge sort,merge sort
master degree vs experience,hi from chile and am doing an internship as data scientist at bank from my country can do master of data science but don know if it worth it think experience ex be senior data scientist is more valuable than master degree but not sure any recommendations thanks,1,master degree v experience,hi from chile and am doing an internship a data scientist at bank from my country can do master of data science but don know if it worth it think experience ex be senior data scientist is more valuable than master degree but not sure any recommendation thanks
was asked an interview as to how to extract only the medical terms in doctor note unsupervised approach when answered lda tf idf he wasn convinced but asked me how would deal with irrelevant keyword extracted using these techniques how would you answer these questions,was asked an interview as to how to extract only the medical terms in doctor note unsupervised approach when answered lda tf idf he wasn convinced but asked me how would deal with irrelevant keyword extracted using these techniques how would you answer these questions,1,wa asked an interview a to how to extract only the medical term in doctor note unsupervised approach when answered lda tf idf he wasn convinced but asked me how would deal with irrelevant keyword extracted using these technique how would you answer these question,wa asked an interview a to how to extract only the medical term in doctor note unsupervised approach when answered lda tf idf he wasn convinced but asked me how would deal with irrelevant keyword extracted using these technique how would you answer these question
how do convince my boss we should use python,hi using throwaway in the off chance my boss sees this early career yoe before my current position started this new position in relatively immature and small data team at large corporation few months ago the team has existed in some form for about years but other than my boss the team is fresh as of love the organization in the company overall is good and for all intents and purposes my boss is great manager and great coworker and the team works well together but we have strong disagreement about one thing when first started he told me that the team doesn really do coding any sort of programming language figured that was okay as long as the job gets done it doesn really matter what tools we use and at this point in my career didn want to do heavy software engineering mle however found that the no code tool we use has some major deficiencies we use ibm spss modeler and it has an abysmal community luckily it easy enough to learn and got the hang of it quickly but it really not capable of any advanced analytics or machine learning it has tools for many machine learning algorithms but they re very limited and they re black boxes trying to shape the data and manipulate the parameters for even simple regressions takes lot of struggling it requires dragging and dropping dozens of nodes to make the model function appropriately and even then it is sub optimal and cannot be tweaked easily spss modeler works great for simple manipulations etl eda and basic analysis it fast to get job running and easy to understand even if you ve never used it before because it visual but anything past the basics it slow convoluted and rigid it lacks functionality and it is difficult to impossible to document and replicate the jobs or at least that my experience my boss is extremely hesitant to introduce any code to our workflows he allowed me to write few scripts for use in production only when ve demonstrated that spss modeler is entirely incapable of it however things really heated up when we hired nd data scientist and they offered much more resistance to our current process essentially refusing to use spss modeler and write code in python anyway as team we re transitioning our use of some other tools at the moment and it seems like we re reaching point where we ll my boss be making more substantive decisions ve asked for advice from some others in the organization and they ve helped me make sure approaching this issue professionally was intending to make formal proposal for greater and official adoption of coding basically python but now it seems like ll have to speed that process up so to datascience more want advice on how to specifically justify argue the case for the technology amp x200b my boss concerns with python coding he and one other member on the team have no experience coding so we can totally abandon spss modeler anyway and wouldn argue for that related to above he doesn want to read bunch of code he doesn want to mix our process using some spss modeler and some python he doesn want to jump back and forth between tools to get job done he doesn believe that code is easily or effectively documented recently set up git repos and documentation for all of the code along with best and standard practices much of which is adopted from other organizations at our company he worried about continuity consistency if out he may have to run my code the documentation should solve this he worried about incongruent dev environments everything will be in jupyter notebooks this along with documentation should solve the issue there are different coding styles or the other ds may write our code differently and this introduces inconsistencies especially if someone new is hired documentation and sticking to our documented best practices should solve the bulk of this he concerned about turnover if or the other ds leaves someone will have to pick up that process code and maintain it he doesn believe this will be possible with code if someone new is hired they may have to learn python or they may introduce their own programming language and that ll complicate things further don think this is an issue it much easier to translate from python to or to java than from spss modeler to anything else chance someone being hired for the title of data scientist will know some programming language and chance they will not know spss modeler and they probably won be as patient as me as we have learned in the hiring process for the company at large there is coding test for data scientists if there is logical error in the code then something will get messed up this is especially worse if he or the other no code employee have to run our code but don know how to fix it don think this is an issue because we essentially just need to have high quality work and stick closely to our best practices the same exact thing can and does happen with spss modeler except it harder to diagnose and fix there are probably some other concerns he has but these represent his main issues essentially he doesn believe that code is maintainable documentable or replicable but spss modeler is and think the opposite is the truth oh forgot we also can version our spss modeler jobs other than saving multiple copies of it on the network drive and we can see the differences without opening them and every node in the job up amp x200b again he otherwise great person to work with and this hasn truly caused any problems yet but it probably will soon if we don settle it working on writing proposal but not sure how to highlight articulate the advantages of coding over this software most of his issues aren with the functionality of python just how it works operationally,1,how do convince my bos we should use python,hi using throwaway in the off chance my bos see this early career yoe before my current position started this new position in relatively immature and small data team at large corporation few month ago the team ha existed in some form for about year but other than my bos the team is fresh a of love the organization in the company overall is good and for all intent and purpose my bos is great manager and great coworker and the team work well together but we have strong disagreement about one thing when first started he told me that the team doesn really do coding any sort of programming language figured that wa okay a long a the job get done it doesn really matter what tool we use and at this point in my career didn want to do heavy software engineering mle however found that the no code tool we use ha some major deficiency we use ibm spss modeler and it ha an abysmal community luckily it easy enough to learn and got the hang of it quickly but it really not capable of any advanced analytics or machine learning it ha tool for many machine learning algorithm but they re very limited and they re black box trying to shape the data and manipulate the parameter for even simple regression take lot of struggling it requires dragging and dropping dozen of node to make the model function appropriately and even then it is sub optimal and cannot be tweaked easily spss modeler work great for simple manipulation etl eda and basic analysis it fast to get job running and easy to understand even if you ve never used it before because it visual but anything past the basic it slow convoluted and rigid it lack functionality and it is difficult to impossible to document and replicate the job or at least that my experience my bos is extremely hesitant to introduce any code to our workflow he allowed me to write few script for use in production only when ve demonstrated that spss modeler is entirely incapable of it however thing really heated up when we hired nd data scientist and they offered much more resistance to our current process essentially refusing to use spss modeler and write code in python anyway a team we re transitioning our use of some other tool at the moment and it seems like we re reaching point where we ll my bos be making more substantive decision ve asked for advice from some others in the organization and they ve helped me make sure approaching this issue professionally wa intending to make formal proposal for greater and official adoption of coding basically python but now it seems like ll have to speed that process up so to datascience more want advice on how to specifically justify argue the case for the technology amp x200b my bos concern with python coding he and one other member on the team have no experience coding so we can totally abandon spss modeler anyway and wouldn argue for that related to above he doesn want to read bunch of code he doesn want to mix our process using some spss modeler and some python he doesn want to jump back and forth between tool to get job done he doesn believe that code is easily or effectively documented recently set up git repos and documentation for all of the code along with best and standard practice much of which is adopted from other organization at our company he worried about continuity consistency if out he may have to run my code the documentation should solve this he worried about incongruent dev environment everything will be in jupyter notebook this along with documentation should solve the issue there are different coding style or the other d may write our code differently and this introduces inconsistency especially if someone new is hired documentation and sticking to our documented best practice should solve the bulk of this he concerned about turnover if or the other d leaf someone will have to pick up that process code and maintain it he doesn believe this will be possible with code if someone new is hired they may have to learn python or they may introduce their own programming language and that ll complicate thing further don think this is an issue it much easier to translate from python to or to java than from spss modeler to anything else chance someone being hired for the title of data scientist will know some programming language and chance they will not know spss modeler and they probably won be a patient a me a we have learned in the hiring process for the company at large there is coding test for data scientist if there is logical error in the code then something will get messed up this is especially worse if he or the other no code employee have to run our code but don know how to fix it don think this is an issue because we essentially just need to have high quality work and stick closely to our best practice the same exact thing can and doe happen with spss modeler except it harder to diagnose and fix there are probably some other concern he ha but these represent his main issue essentially he doesn believe that code is maintainable documentable or replicable but spss modeler is and think the opposite is the truth oh forgot we also can version our spss modeler job other than saving multiple copy of it on the network drive and we can see the difference without opening them and every node in the job up amp x200b again he otherwise great person to work with and this hasn truly caused any problem yet but it probably will soon if we don settle it working on writing proposal but not sure how to highlight articulate the advantage of coding over this software most of his issue aren with the functionality of python just how it work operationally
best coursera course for data science beginner,hello amp x200b am junior in college studying information science and my major generally prepares people better for data analysis and somewhat for data science so want to supplement my education with an online course get one free course from coursera through my university email so am curious which course should look into which course is beginner intermediate focused and would teach me the most skills and look most impressive for internships,1,best coursera course for data science beginner,hello amp x200b am junior in college studying information science and my major generally prepares people better for data analysis and somewhat for data science so want to supplement my education with an online course get one free course from coursera through my university email so am curious which course should look into which course is beginner intermediate focused and would teach me the most skill and look most impressive for internship
mid level data scientist and struggling with interview anxiety,sorry it another rant about ds interviews from mid level data scientist have master and almost years of experience in ds ml and analytics in mid sized company over the holidays wanted to start looking for new position so started cold applying for senior data scientist roles and turned on opentowork on linkedin when got back from the holidays had emails back from almost every application ve sent in in addition to recruiters messaging me on linkedin thought shit this is awesome and should be an easy switch especially in the current job market so just last week and this week went through recruiter phone calls with all of them translating into hiring manager interview or technical screen which are scheduled for this week and next week but this is where imposter syndrome kicks in because ds interviews are known to differ from company to company and even within teams of the same company figured it be best to ask the recruiters how can best prepare for each of hm interview or tech screen some tell me statistics others tell me sql others tell me python and others tell me ml concepts sometimes they tell me it all of the above yes data scientist should be fluent in most of these especially with my level of experience but if you ask me random textbook questions how am supposed to pull it out of my ass what if haven used an algorithm in years some expect that come prepared to answer leetcode questions for an interview that happening next week some don even feel inclined to reply to my emails don know how you all make the time to interview and actually get competitive job offers feel so underqualified and incompetent even with years of experience in this field to be transparent think so scared of embarrassing myself in an interview love to hear from mid level or senior level data scientists how do you deal with interview anxiety imposter syndrome or whatever this is called,1,mid level data scientist and struggling with interview anxiety,sorry it another rant about d interview from mid level data scientist have master and almost year of experience in d ml and analytics in mid sized company over the holiday wanted to start looking for new position so started cold applying for senior data scientist role and turned on opentowork on linkedin when got back from the holiday had email back from almost every application ve sent in in addition to recruiter messaging me on linkedin thought shit this is awesome and should be an easy switch especially in the current job market so just last week and this week went through recruiter phone call with all of them translating into hiring manager interview or technical screen which are scheduled for this week and next week but this is where imposter syndrome kick in because d interview are known to differ from company to company and even within team of the same company figured it be best to ask the recruiter how can best prepare for each of hm interview or tech screen some tell me statistic others tell me sql others tell me python and others tell me ml concept sometimes they tell me it all of the above yes data scientist should be fluent in most of these especially with my level of experience but if you ask me random textbook question how am supposed to pull it out of my as what if haven used an algorithm in year some expect that come prepared to answer leetcode question for an interview that happening next week some don even feel inclined to reply to my email don know how you all make the time to interview and actually get competitive job offer feel so underqualified and incompetent even with year of experience in this field to be transparent think so scared of embarrassing myself in an interview love to hear from mid level or senior level data scientist how do you deal with interview anxiety imposter syndrome or whatever this is called
have you had to roll back your ml models in production why,am curious to know how often this happens how your team handled it and the particular reasons it happened due to in my case client made some last minute updates to their processes which changed some of the underlying bandwidths of categories in some of the features breaking our ml models,1,have you had to roll back your ml model in production why,am curious to know how often this happens how your team handled it and the particular reason it happened due to in my case client made some last minute update to their process which changed some of the underlying bandwidth of category in some of the feature breaking our ml model
no clue what going on in internship,currently an undergrad doing data science internship this term at start up and for the most part feel like have no clue what going on have formal backgrounds in probability and stats with some programming experience but the stuff at work just seem much more complicated and involved than anything in school is this normal at all to feel like this just started my internship last week so understand it bit too early but will it get better,1,no clue what going on in internship,currently an undergrad doing data science internship this term at start up and for the most part feel like have no clue what going on have formal background in probability and stats with some programming experience but the stuff at work just seem much more complicated and involved than anything in school is this normal at all to feel like this just started my internship last week so understand it bit too early but will it get better
data governance,hi everyone the community brand manager at dataqg data governance resource platform for leaders and practitioners in the industry if you would like to join our community we welcome you with open arms dataqg com community other than that can wait to meet all of you and look forward to our conversations in the future,1,data governance,hi everyone the community brand manager at dataqg data governance resource platform for leader and practitioner in the industry if you would like to join our community we welcome you with open arm dataqg com community other than that can wait to meet all of you and look forward to our conversation in the future
taking month work break and then returning to analytics,planning on taking months off of work this year to hike currently wfh as data analyst doing sas sql report coding with gov contractor going to ask for an extended leave but if get denied ll give notice kind of doing this for mental health and to transition geographically if take months away from analytics work will this be looked at negatively when back in the job market looking for position would have better luck looking for contract work vs being fte somewhere after the months thoughts,1,taking month work break and then returning to analytics,planning on taking month off of work this year to hike currently wfh a data analyst doing sa sql report coding with gov contractor going to ask for an extended leave but if get denied ll give notice kind of doing this for mental health and to transition geographically if take month away from analytics work will this be looked at negatively when back in the job market looking for position would have better luck looking for contract work v being fte somewhere after the month thought
build system for machine learning,hey all setting up new machine learning project and know going to be collecting data over time and making model improvements is there best way to set up build system to generate models using my training script eval on test data all relatively automatically curious what other people have tried and liked or disliked thanks,1,build system for machine learning,hey all setting up new machine learning project and know going to be collecting data over time and making model improvement is there best way to set up build system to generate model using my training script eval on test data all relatively automatically curious what other people have tried and liked or disliked thanks
mlops engineer vs phd,hello everyone am very interested to know your opinion about this have an msc degree in ai computer vision have worked for about months now recently enrolled in an mlops and aws for ml nano degrees as want to start my career as an mlops engineer however sometimes just start thinking that it would be better if do phd any opinion is helpful to me thanks,1,mlops engineer v phd,hello everyone am very interested to know your opinion about this have an msc degree in ai computer vision have worked for about month now recently enrolled in an mlops and aws for ml nano degree a want to start my career a an mlops engineer however sometimes just start thinking that it would be better if do phd any opinion is helpful to me thanks
mlops engineer vs phd ai,mlops engineer vs phd ai,1,mlops engineer v phd ai,mlops engineer v phd ai
which tool to plot data with searchable dropdown menu other than plotly dash,hello do some data science engineering with measurement data and for the plots mostly use plotly since it is very responsive and interactive the biggest advantage is that can share the interactive figures as simple html files now want to take things further and want or multiple dropdown searchable menu so the user can filter the plot better since want the graph to be portable plotly dash is out of question because it requires server to be run on could you point me in the right direction what open source tool could use am open to every language,1,which tool to plot data with searchable dropdown menu other than plotly dash,hello do some data science engineering with measurement data and for the plot mostly use plotly since it is very responsive and interactive the biggest advantage is that can share the interactive figure a simple html file now want to take thing further and want or multiple dropdown searchable menu so the user can filter the plot better since want the graph to be portable plotly dash is out of question because it requires server to be run on could you point me in the right direction what open source tool could use am open to every language
getting into data science without an undergrad degree,hi folks year old guy doing my data science specialization from coursera and since do not have my undergrad degree yet will still be able to get any internship remote or on site anywhere just worried if made the right choice,1,getting into data science without an undergrad degree,hi folk year old guy doing my data science specialization from coursera and since do not have my undergrad degree yet will still be able to get any internship remote or on site anywhere just worried if made the right choice
job interview coding challenges,hi prepping for job interview coding challenges for data scientist position trying to find some example problems online for practice but majority of them seem to behind pay wall can anyone help me out with some free examples,1,job interview coding challenge,hi prepping for job interview coding challenge for data scientist position trying to find some example problem online for practice but majority of them seem to behind pay wall can anyone help me out with some free example
price comparisons databricks vs dataiku vs datarobot vs domino data vs h20 vs c3 vs sagemaker,can anyone share info about how these dss ml platforms are pricing their software curious about all tiers of pricing for varying scales how that breaks down person and mb of storage they don like to put on their websites so need customers to lmk,1,price comparison databricks v dataiku v datarobot v domino data v h20 v c3 v sagemaker,can anyone share info about how these ds ml platform are pricing their software curious about all tier of pricing for varying scale how that break down person and mb of storage they don like to put on their website so need customer to lmk
skills to learn to transition from data analysis to data engineering,hi all so recently started job in data analysis about months ago at small company after finishing my msc bioinformatics thinking about starting some freelancing projects if can but if not then just some courses and personal projects to try and make the transition to more of data engineering role in the future of my career it seems to be both more interesting in terms of handling data and more lucrative career path although in no rush to get there and have time to learn more skills to become more hireable am confident in python numpy and pandas and use lot of sas for work at the moment but was wondering what people would suggest for skillset to make this transition thank you,1,skill to learn to transition from data analysis to data engineering,hi all so recently started job in data analysis about month ago at small company after finishing my msc bioinformatics thinking about starting some freelancing project if can but if not then just some course and personal project to try and make the transition to more of data engineering role in the future of my career it seems to be both more interesting in term of handling data and more lucrative career path although in no rush to get there and have time to learn more skill to become more hireable am confident in python numpy and panda and use lot of sa for work at the moment but wa wondering what people would suggest for skillset to make this transition thank you
quit master in statistics or,started master in stats in and still not near getting degree actually can decide should just quit or should push it but one thing do know just for the love of god can find any motivation whatsoever to push myself and start writing the thesis and studying for my exams ve worked as data scientist for years now and during my bachelor days ve been freelancing ds ml that experience brought me an intermediate ds position very early on in my career the money been good ever since and just not seeing any source of motivation for very long time tried to put together list of pros and cons staying so here what came up with pros higher level of education potential access to some better payed research or academia positions later on not even sure if ll ever want those personal satisfaction but can decide if that truly personal thing or it just everybody and their mother have masters nowadays so why shouldn you kind of thing cons constant pressure on my mind don honestly believe that ll learn anything new in this masters we just repeat stuff we already learned during bachelor and therefore it not worth it scholarships working amp studying at the same time for title that can even decide if it means anything to me some additional context can also do data engineering which did in my former company and actually enjoyed lot more than ds stuff had to do what also don like about ds is that it almost always new thing in most companies research experimental thing so if it fails it doesn matter most of the times you ll just use pre trained model for task and that good enough might leave ds because of this at some point btw also man of many hobbies play in band dj occasionally like clubbing hanging out staying late etc so all of this tells me to drop out don misunderstand this for slacking at work even though the cons list is longer can drop out not just yet but don know why please do share similar dilemmas and experiences thanks lot,1,quit master in statistic or,started master in stats in and still not near getting degree actually can decide should just quit or should push it but one thing do know just for the love of god can find any motivation whatsoever to push myself and start writing the thesis and studying for my exam ve worked a data scientist for year now and during my bachelor day ve been freelancing d ml that experience brought me an intermediate d position very early on in my career the money been good ever since and just not seeing any source of motivation for very long time tried to put together list of pro and con staying so here what came up with pro higher level of education potential access to some better payed research or academia position later on not even sure if ll ever want those personal satisfaction but can decide if that truly personal thing or it just everybody and their mother have master nowadays so why shouldn you kind of thing con constant pressure on my mind don honestly believe that ll learn anything new in this master we just repeat stuff we already learned during bachelor and therefore it not worth it scholarship working amp studying at the same time for title that can even decide if it mean anything to me some additional context can also do data engineering which did in my former company and actually enjoyed lot more than d stuff had to do what also don like about d is that it almost always new thing in most company research experimental thing so if it fails it doesn matter most of the time you ll just use pre trained model for task and that good enough might leave d because of this at some point btw also man of many hobby play in band dj occasionally like clubbing hanging out staying late etc so all of this tell me to drop out don misunderstand this for slacking at work even though the con list is longer can drop out not just yet but don know why please do share similar dilemma and experience thanks lot
question about column based visuals for dataframes,does anyone know if there is package out there to pretty print univariate analysis of columns for pandas dataframes similar to how kaggle datasets have detail section example attached maybe something that can be used inside jupyter notebooks format png amp auto webp amp c680d8ed7d11bb9288762d3bbcb95ef3a46,1,question about column based visuals for dataframes,doe anyone know if there is package out there to pretty print univariate analysis of column for panda dataframes similar to how kaggle datasets have detail section example attached maybe something that can be used inside jupyter notebook format png amp auto webp amp c680d8ed7d11bb9288762d3bbcb95ef3a46
data scientists would make it in data science,know this probably pops up lot but guess ll try take my shot studied it all the way until finished college college in the uk is not university believe the american equivalent might be graduating high school anyways at age until age was doing it support didn really overly enjoy the role nor was major techie my dad pushed me towards it because of the money in this field while had no dreams of my own whilst it support did suck for me atleast had data department in my organization and was relatively fascinated by the work they do analysing data molding data for predictibility in growth of business and what not now work in warehouse doing nightshift yes know huge downgrade but wasn progressing or doing anything in my it support truth be told and recently have been reconsidering my future and want to get into data not just as an analyst however but as data scientist the issue is have no starting point ve been trying to self teach myself python to begin with which can be tedious at times considering can only really do in the mornings at like am to am before go to sleep after work also realising how many miles behind am in terms of technical knowledge and other skills such as mathematical knowledge barely passed maths at gcse got grade which believe would be huge hinderances to my advancement in the field of data especially when read that most data scientists have knowledge of atleast undergrad maths considering how competitve the field is my age my lack of experience in the skills required do you think overtly struggle in establishing my career in data ve applied for apprenticeships and even some courses but everyone just seems to reject ignore me don want to waste anymore of my time doing something that will eventually turn out useless so need your honest opinions,1,data scientist would make it in data science,know this probably pop up lot but guess ll try take my shot studied it all the way until finished college college in the uk is not university believe the american equivalent might be graduating high school anyways at age until age wa doing it support didn really overly enjoy the role nor wa major techie my dad pushed me towards it because of the money in this field while had no dream of my own whilst it support did suck for me atleast had data department in my organization and wa relatively fascinated by the work they do analysing data molding data for predictibility in growth of business and what not now work in warehouse doing nightshift yes know huge downgrade but wasn progressing or doing anything in my it support truth be told and recently have been reconsidering my future and want to get into data not just a an analyst however but a data scientist the issue is have no starting point ve been trying to self teach myself python to begin with which can be tedious at time considering can only really do in the morning at like am to am before go to sleep after work also realising how many mile behind am in term of technical knowledge and other skill such a mathematical knowledge barely passed math at gcse got grade which believe would be huge hinderance to my advancement in the field of data especially when read that most data scientist have knowledge of atleast undergrad math considering how competitve the field is my age my lack of experience in the skill required do you think overtly struggle in establishing my career in data ve applied for apprenticeship and even some course but everyone just seems to reject ignore me don want to waste anymore of my time doing something that will eventually turn out useless so need your honest opinion
career advice for student,hey has anyone tried the supply chain analytics specialization on coursera am currently in my third year at university and am major in international economics found out this specialization when almost finished the google data analytics just start learning the skills needed for analysis and would like to be supply chain analyst what do you guys think any recommendation or advice would be great for student like me thanks in advance pt english is not my main language so forgive me if say sth not appropriately,1,career advice for student,hey ha anyone tried the supply chain analytics specialization on coursera am currently in my third year at university and am major in international economics found out this specialization when almost finished the google data analytics just start learning the skill needed for analysis and would like to be supply chain analyst what do you guy think any recommendation or advice would be great for student like me thanks in advance pt english is not my main language so forgive me if say sth not appropriately
physical books for data science python and or,for all of my physical text people out there do you have any recommendations for books that you like to have on hand something that you reference for general data science methodology or python specifically ph student in psychology looking to start applying for people analytics jobs and trying to really hone in on my data science skills once comps are over,1,physical book for data science python and or,for all of my physical text people out there do you have any recommendation for book that you like to have on hand something that you reference for general data science methodology or python specifically ph student in psychology looking to start applying for people analytics job and trying to really hone in on my data science skill once comp are over
creating dashboards with python dash vs flask plotly vs others,hi joining team next week where they are developing bioinformatics pipelines on and using shiny apps for their dashboards user input going to be working on the development of new products strictly on python so looking for new toolset my work so far has been purely academic running on notebooks pandas and plotly don really know anything about web developent and don want it to become my focus since bioinformatician first but need to produce user friendly and scalable dashboards to create reports for this job and for my personal projects from what ve read dash is very easy to use but not sure if it can scale that well into the future or incorporate elements from other libraries that are not plotly flask plotly seems like safer bet if bit more complex to learn django seems overkill to learn but not sure what the limitations on flask are open to other options that havent listed assume being able to incorporate some of the work done on shiny would be nice if that matters at all any suggestions thanks,1,creating dashboard with python dash v flask plotly v others,hi joining team next week where they are developing bioinformatics pipeline on and using shiny apps for their dashboard user input going to be working on the development of new product strictly on python so looking for new toolset my work so far ha been purely academic running on notebook panda and plotly don really know anything about web developent and don want it to become my focus since bioinformatician first but need to produce user friendly and scalable dashboard to create report for this job and for my personal project from what ve read dash is very easy to use but not sure if it can scale that well into the future or incorporate element from other library that are not plotly flask plotly seems like safer bet if bit more complex to learn django seems overkill to learn but not sure what the limitation on flask are open to other option that havent listed assume being able to incorporate some of the work done on shiny would be nice if that matter at all any suggestion thanks
insightful inquiries,hello all am just now finding this place and hope everyone is having as good of year so far as can be expected am the owner of aucoin analytics private intelligence company that is also co host for the podcasts this week explained weekly geopolitical podcast and insightful inquiries monthly interview podcast the first installment of insightful inquiries was posted on sunday night and featured good friend of mine who is data scientist in the dc area we spoke about intelligence analysis within academia data exploration and th generation warfare hope you all find it interesting even though you all have ton more experience with data than do,1,insightful inquiry,hello all am just now finding this place and hope everyone is having a good of year so far a can be expected am the owner of aucoin analytics private intelligence company that is also co host for the podcasts this week explained weekly geopolitical podcast and insightful inquiry monthly interview podcast the first installment of insightful inquiry wa posted on sunday night and featured good friend of mine who is data scientist in the dc area we spoke about intelligence analysis within academia data exploration and th generation warfare hope you all find it interesting even though you all have ton more experience with data than do
is it worth it learning data science in,ok so this isn entirely my question the answer is yes it is worth it but someone just asked this question here and feel its valid question and that wrote decent reply posted it but then the user deleted their account this is my first time contributing here on reddit so was bit disappointed feel the question will contribute and don want my comment to go to waste also related lot to the question as basically in the same situation and was very interested to see what others had to say on the matter but also if anyone would have input on what had to comment which are main motivations for reposting so ll ask very similar question but rephrased let says one is web developer with an age of you re interested in switching careers into data science you ve invested the best part of the past years self learning data science but are worried you lack the skills especially in math stats and you re also worried about investing more time for things to not work out in the end and it all be waste of time you re concerned the whole data science industry is overhyped and oversaturated especially with all the students and graduates studying and entering the market finally you re concerned that you re family colleagues may think less of you for giving up on becoming long term web developer to focus on becoming data scientist so is it worth trying to become data scientist in and what advice can people give to help people in such situation so quick clarification do not think the data science industry is overhyped and am not worried about people thinking less off me want to keep the question similar to the original comment in case the op comes across this as it will be more useful for them also it saves having to re write my reply ha ll post my reply below rather than in the post,1,is it worth it learning data science in,ok so this isn entirely my question the answer is yes it is worth it but someone just asked this question here and feel it valid question and that wrote decent reply posted it but then the user deleted their account this is my first time contributing here on reddit so wa bit disappointed feel the question will contribute and don want my comment to go to waste also related lot to the question a basically in the same situation and wa very interested to see what others had to say on the matter but also if anyone would have input on what had to comment which are main motivation for reposting so ll ask very similar question but rephrased let say one is web developer with an age of you re interested in switching career into data science you ve invested the best part of the past year self learning data science but are worried you lack the skill especially in math stats and you re also worried about investing more time for thing to not work out in the end and it all be waste of time you re concerned the whole data science industry is overhyped and oversaturated especially with all the student and graduate studying and entering the market finally you re concerned that you re family colleague may think le of you for giving up on becoming long term web developer to focus on becoming data scientist so is it worth trying to become data scientist in and what advice can people give to help people in such situation so quick clarification do not think the data science industry is overhyped and am not worried about people thinking le off me want to keep the question similar to the original comment in case the op come across this a it will be more useful for them also it save having to re write my reply ha ll post my reply below rather than in the post
what the best platform to create portfolio of all your ds related projects,is github the best way to go about this it doesn seem that intuitive towards storing projects to me is there an alternative way,1,what the best platform to create portfolio of all your d related project,is github the best way to go about this it doesn seem that intuitive towards storing project to me is there an alternative way
how to present potential paper as valuable to my company,so collaborating with professor that had for the accounting data analytics grad certificate my co is paying for we re in the preliminary stages but think we ve settled on concept drift as the overarching topic however we need source for data was considering contacting the newly promoted data scientist about this also the position he was promoted from we call analysts specialists at this co but effectively the data analyst is vacant was wondering if you all had good strategy to spin this as value proposition for my co and if could go as far as convincing them to also change my position towards data whilst working on this project it would be incredible just don have any examples of anyone being successful in such venture any thoughts and suggestions are greatly appreciated thanks in advance,1,how to present potential paper a valuable to my company,so collaborating with professor that had for the accounting data analytics grad certificate my co is paying for we re in the preliminary stage but think we ve settled on concept drift a the overarching topic however we need source for data wa considering contacting the newly promoted data scientist about this also the position he wa promoted from we call analyst specialist at this co but effectively the data analyst is vacant wa wondering if you all had good strategy to spin this a value proposition for my co and if could go a far a convincing them to also change my position towards data whilst working on this project it would be incredible just don have any example of anyone being successful in such venture any thought and suggestion are greatly appreciated thanks in advance
shinyapps io or another solution,am an independent consultant in supply chain data science for some of my projects the end deliverables include shiny app that runs optimization or simulation code underneath the visualization layer some of these can be pretty memory and cpu intensive right now just have the freebie shinyapps io account like the ease of launching and debugging apps on the platform ll soon either need to start paying for higher tier shjnyapps account or start migrating to another option curious to hear this subs experience with different shiny apps solutions,1,shinyapps io or another solution,am an independent consultant in supply chain data science for some of my project the end deliverable include shiny app that run optimization or simulation code underneath the visualization layer some of these can be pretty memory and cpu intensive right now just have the freebie shinyapps io account like the ease of launching and debugging apps on the platform ll soon either need to start paying for higher tier shjnyapps account or start migrating to another option curious to hear this sub experience with different shiny apps solution
anyone else feel lot of anxiety,always feel awful when my model isn performing as expected when the predictive performance isn good enough or the inference isn reasonable this even affects my mental health start doubting myself feeling like an imposter and failure does anyone else feel this way,1,anyone else feel lot of anxiety,always feel awful when my model isn performing a expected when the predictive performance isn good enough or the inference isn reasonable this even affect my mental health start doubting myself feeling like an imposter and failure doe anyone else feel this way
sql mysql for data analytics and business intelligence,sql mysql for data analytics and business intelligence,1,sql mysql for data analytics and business intelligence,sql mysql for data analytics and business intelligence
hot under the collar latent measure of interstate hostility,tl dr bayesian ideal point model for modeling crises in international relations abstract the majority of studies on international conflict escalation use variety of measures of hostility including the use of force reciprocity and the number of fatalities the use of different measures however leads to different empirical results and creates difficulties when testing existing theories of interstate conflict furthermore hostility measures currently used in the conflict literature are ill suited to the task of identifying consistent predictors of international conflict escalation this article presents new dyadic latent measure of interstate hostility created using bayesian item response theory model and conflict data from the militarized interstate dispute mid and phoenix political event datasets this model provides more granular conceptually precise and validated measure of hostility which incorporates the uncertainty inherent in the latent variable and solves the problem of temporal variation in event data using varying intercept structure and human coded data as benchmark against which biases in machine coded data are corrected in addition this measurement model allows for the systematic evaluation of how existing measures relate to the construct of hostility the presented model will therefore enhance the ability of researchers to understand factors affecting conflict dynamics including escalation and de escalation processes paper non paywalled,1,hot under the collar latent measure of interstate hostility,tl dr bayesian ideal point model for modeling crisis in international relation abstract the majority of study on international conflict escalation use variety of measure of hostility including the use of force reciprocity and the number of fatality the use of different measure however lead to different empirical result and creates difficulty when testing existing theory of interstate conflict furthermore hostility measure currently used in the conflict literature are ill suited to the task of identifying consistent predictor of international conflict escalation this article present new dyadic latent measure of interstate hostility created using bayesian item response theory model and conflict data from the militarized interstate dispute mid and phoenix political event datasets this model provides more granular conceptually precise and validated measure of hostility which incorporates the uncertainty inherent in the latent variable and solves the problem of temporal variation in event data using varying intercept structure and human coded data a benchmark against which bias in machine coded data are corrected in addition this measurement model allows for the systematic evaluation of how existing measure relate to the construct of hostility the presented model will therefore enhance the ability of researcher to understand factor affecting conflict dynamic including escalation and de escalation process paper non paywalled
does this feedback hold up in the present day,does this feedback hold up in the present day,1,doe this feedback hold up in the present day,doe this feedback hold up in the present day
new clustering algorithms like dbscan and optics,hello am doing some research work am looking for clustering methods like dbscan and optics these were introduced in which are new and better can anyone tell me the names of newer algorithms if possible link to the research paper would be appreciable if they are already in implementation please mention,1,new clustering algorithm like dbscan and optic,hello am doing some research work am looking for clustering method like dbscan and optic these were introduced in which are new and better can anyone tell me the name of newer algorithm if possible link to the research paper would be appreciable if they are already in implementation please mention
singapore transport system dataset,hi may please know whether it is possible to find datasets about singapore transport system where many deep insights can be found,1,singapore transport system dataset,hi may please know whether it is possible to find datasets about singapore transport system where many deep insight can be found
how do get into data science,im currently freshman in chemical engineering and im thinking of getting master degree in data science are these fields quite far from each other how do get into data science without shifting from degree my uni has strict shifting policies if try to shift ill probably restart and dont think repeating year is worth it thanks in advance to whoever answers,1,how do get into data science,im currently freshman in chemical engineering and im thinking of getting master degree in data science are these field quite far from each other how do get into data science without shifting from degree my uni ha strict shifting policy if try to shift ill probably restart and dont think repeating year is worth it thanks in advance to whoever answer
are there any peer reviewed journals which take no to minimal charge for paper submission,are there any peer reviewed journals which take no to minimal charge for paper submission,1,are there any peer reviewed journal which take no to minimal charge for paper submission,are there any peer reviewed journal which take no to minimal charge for paper submission
how to make your meetings more effective in start in silence,how to make your meetings more effective in start in silence,1,how to make your meeting more effective in start in silence,how to make your meeting more effective in start in silence
which solution to aim data scientists types,am facing this issue at some project am working at love to open discussion which solution should aim for solution to problem with simple sql query or simple plot that will lead to an accuracy of lets say very complex solution complex models and thorough representation yealding to an accuracy of is it worth the efford know in some cases makes sense but also in another cases solution takes of the time of solution how do you sort out this kind of dilemmas in addition believe that there are kind of data scientist one aiming first to one kind of approach cheers,1,which solution to aim data scientist type,am facing this issue at some project am working at love to open discussion which solution should aim for solution to problem with simple sql query or simple plot that will lead to an accuracy of let say very complex solution complex model and thorough representation yealding to an accuracy of is it worth the efford know in some case make sense but also in another case solution take of the time of solution how do you sort out this kind of dilemma in addition believe that there are kind of data scientist one aiming first to one kind of approach cheer
what tools do you your team use to share and keep track of ad hoc analyses,my team handles lot of ad hoc analyses that aren all worth putting in dashboard we usually write up summary in google doc to share with any stakeholders after doing the work in jupyter notebook or rmd google doc write ups tend to get lost very quickly and are hard to surface once someone leaves is this problem for anyone else any solutions,1,what tool do you your team use to share and keep track of ad hoc analysis,my team handle lot of ad hoc analysis that aren all worth putting in dashboard we usually write up summary in google doc to share with any stakeholder after doing the work in jupyter notebook or rmd google doc write ups tend to get lost very quickly and are hard to surface once someone leaf is this problem for anyone else any solution
is data science worth learning in,ve been self learning data science but just the basic using sckit learn pandas and different models it would appear the field is bit overhyped and very oversaturated with millions of juniors graduating in data science trying to break into the industry currently work full time as web dev role worried ll spend the next couple of years self learning data science only to spend another years or more trying to get job should note have various interests do enjoy programming data but am weak in stats so find enjoyment in anything that relates to these although probably better programmer than statistician okay at maths but do have to relearn everything starting from algebra and always doubtful as to whether or not could ever apply machine learning algorithm to business problem just wondering if the right thing for me right now would be to switch to something like web development enjoyed learning basic html css and javascript whatever choose the idea of giving up on something ve dedicated of hours to only to start from scratch in another field is causing me all kinds of mental anguish especially at the age of it feels like giving up and feel like my family amp work colleagues will think less of me,1,is data science worth learning in,ve been self learning data science but just the basic using sckit learn panda and different model it would appear the field is bit overhyped and very oversaturated with million of junior graduating in data science trying to break into the industry currently work full time a web dev role worried ll spend the next couple of year self learning data science only to spend another year or more trying to get job should note have various interest do enjoy programming data but am weak in stats so find enjoyment in anything that relates to these although probably better programmer than statistician okay at math but do have to relearn everything starting from algebra and always doubtful a to whether or not could ever apply machine learning algorithm to business problem just wondering if the right thing for me right now would be to switch to something like web development enjoyed learning basic html cs and javascript whatever choose the idea of giving up on something ve dedicated of hour to only to start from scratch in another field is causing me all kind of mental anguish especially at the age of it feel like giving up and feel like my family amp work colleague will think le of me
starting my ds career and looking for advices,hello fellows student in the middle of my bachelor in informatics technologies nd course of around months ago started getting into ds but my interest didn pass further articles and ds roadmaps nowadays all in studying python and ds as science for hours week the other hours dedicate to music because that what love to do slightly more and planning to get student internship this autumn as an analyst or data scientist as wrote above have some roadmaps which say to learn python sql statistics and etc but want to receive more detailed advices on what is relevant in and what to look for in the start of career thanks for your help,1,starting my d career and looking for advice,hello fellow student in the middle of my bachelor in informatics technology nd course of around month ago started getting into d but my interest didn pas further article and d roadmaps nowadays all in studying python and d a science for hour week the other hour dedicate to music because that what love to do slightly more and planning to get student internship this autumn a an analyst or data scientist a wrote above have some roadmaps which say to learn python sql statistic and etc but want to receive more detailed advice on what is relevant in and what to look for in the start of career thanks for your help
complex heatmap language,complex heatmap language,1,complex heatmap language,complex heatmap language
no prediction can be accurate,anyone else have their blood pressure go up when someone says that their predictive model is accurate just can imagine that being realistic view poll,1,no prediction can be accurate,anyone else have their blood pressure go up when someone say that their predictive model is accurate just can imagine that being realistic view poll
how to get in meta facebook,hello there am trying to get job in meta facebook if any of you work in this company or any other big company like microsoft tesla google etc it will be interesting to share your experience how to get an interview what should you expect on interview and finally how hard is it to get job in the company also im trying to start working on remote have years of experience in ds finished bachelors of computer science and doing masters in applied mathematics right now and wonder if this experience is enough thank you,1,how to get in meta facebook,hello there am trying to get job in meta facebook if any of you work in this company or any other big company like microsoft tesla google etc it will be interesting to share your experience how to get an interview what should you expect on interview and finally how hard is it to get job in the company also im trying to start working on remote have year of experience in d finished bachelor of computer science and doing master in applied mathematics right now and wonder if this experience is enough thank you
does anyone repository of some sorts where there area lot of english sentences of different types imperative interrogative and assertive which are separated properly,don know if these types of questions are allowed and if they are no am very sorry just need lot of english sentences of the above types so that can use them any help would be greatly appreciated,1,doe anyone repository of some sort where there area lot of english sentence of different type imperative interrogative and assertive which are separated properly,don know if these type of question are allowed and if they are no am very sorry just need lot of english sentence of the above type so that can use them any help would be greatly appreciated
advice on how to help friend with bs in data analytics get his first job,hello data sciencers have this friend who got degree in data analytics back in march of he real smart dude and came out with gpa he wants to go into the data analytics field but he is intimidated by every job posting he sees and doesn think he can really do the work ve got degree in physics and felt the same way he does albeit less when was looking for my first position ve had couple of good engineering jobs and now know how to translate job postings to english ve taken to playing game like to call name redacted can do that job where we look at job postings together and explain what the responsibilities really mean so he less intimidated we do this every few months and feel like we make progress but then it seems as though he doesn actually apply when not there what else can do to help him get his first job in the field and make him feel less intimidated,1,advice on how to help friend with b in data analytics get his first job,hello data sciencers have this friend who got degree in data analytics back in march of he real smart dude and came out with gpa he want to go into the data analytics field but he is intimidated by every job posting he see and doesn think he can really do the work ve got degree in physic and felt the same way he doe albeit le when wa looking for my first position ve had couple of good engineering job and now know how to translate job posting to english ve taken to playing game like to call name redacted can do that job where we look at job posting together and explain what the responsibility really mean so he le intimidated we do this every few month and feel like we make progress but then it seems a though he doesn actually apply when not there what else can do to help him get his first job in the field and make him feel le intimidated
mac os or windows for data science programming,currently studying statistics and planning to buy new laptop bec my current laptop specs are very outdated anyway don know which operating system is better for data science programming it ll be one of our subjects this coming semester hope you can help me,1,mac o or window for data science programming,currently studying statistic and planning to buy new laptop bec my current laptop spec are very outdated anyway don know which operating system is better for data science programming it ll be one of our subject this coming semester hope you can help me
discover credit cards is hiring multiple entry level to senior data scientist roles in operations with focus on improving cx if you are passionate about using data to derive rich business insights and adept at python sql amp tableau this is an amazing opportunity dm me if interested,discover credit cards is hiring multiple entry level to senior data scientist roles in operations with focus on improving cx if you are passionate about using data to derive rich business insights and adept at python sql amp tableau this is an amazing opportunity dm me if interested,1,discover credit card is hiring multiple entry level to senior data scientist role in operation with focus on improving cx if you are passionate about using data to derive rich business insight and adept at python sql amp tableau this is an amazing opportunity dm me if interested,discover credit card is hiring multiple entry level to senior data scientist role in operation with focus on improving cx if you are passionate about using data to derive rich business insight and adept at python sql amp tableau this is an amazing opportunity dm me if interested
is there an online platform to showcase my microsoft excel skills work,know that people showcase their sql and python skills work in github am also aware that people display their tableau work on tableau public would like to know where can post my microsoft excel skills particularly in pivot tables and data visualization,1,is there an online platform to showcase my microsoft excel skill work,know that people showcase their sql and python skill work in github am also aware that people display their tableau work on tableau public would like to know where can post my microsoft excel skill particularly in pivot table and data visualization
what is nuclues sampling,can describe it in simple way,1,what is nuclues sampling,can describe it in simple way
searching for structure in point data,ve got bunch of point data basically how ever many points want want to look for structure in that data imagine that have hyper accurate cell phone locations for everyone in your town how would turn that location data into interesting spots in your town don need to know from the point data at least that is the town coffee shop just that that particular point is interesting what are some things should google,1,searching for structure in point data,ve got bunch of point data basically how ever many point want want to look for structure in that data imagine that have hyper accurate cell phone location for everyone in your town how would turn that location data into interesting spot in your town don need to know from the point data at least that is the town coffee shop just that that particular point is interesting what are some thing should google
future of design take pictures of any object and it will get converted to model,future of design take pictures of any object and it will get converted to model,1,future of design take picture of any object and it will get converted to model,future of design take picture of any object and it will get converted to model
how does one data science,am aware of machine learning algrothims and frameworks such as the basic ones like sci kit learn pytorch and tensorflow if you gave me perfect practice dataset and told me to use model then would know what model to use and would get relatively good results since the data is made for specific algorithm but how do experienced people do data science on real world data that is not pre made for an algorithm in other words what is your process when doing feature engineering so you can have insightful data to plug into model thanks and have good day,1,how doe one data science,am aware of machine learning algrothims and framework such a the basic one like sci kit learn pytorch and tensorflow if you gave me perfect practice dataset and told me to use model then would know what model to use and would get relatively good result since the data is made for specific algorithm but how do experienced people do data science on real world data that is not pre made for an algorithm in other word what is your process when doing feature engineering so you can have insightful data to plug into model thanks and have good day
complex matching exercise,hello all amp x200b not sure which community this fits into as it could fall into number of them please direct me better if you think that there is community which would rather chew on this problem amp x200b have received database enrichment data its rows and about columns need to find matches between this data and existing data we have in the database was able to match bunch with vlookup but because of simple formatting issues know that there are more similarities was able to match about rows with macro wrote that compares each cell in the enrichment data against all possible matching data in the database there was cells of data rows and columns to match know that this method is very innefficient but it is the lowest level of effort right now amp x200b thoughts,1,complex matching exercise,hello all amp x200b not sure which community this fit into a it could fall into number of them please direct me better if you think that there is community which would rather chew on this problem amp x200b have received database enrichment data it row and about column need to find match between this data and existing data we have in the database wa able to match bunch with vlookup but because of simple formatting issue know that there are more similarity wa able to match about row with macro wrote that compare each cell in the enrichment data against all possible matching data in the database there wa cell of data row and column to match know that this method is very innefficient but it is the lowest level of effort right now amp x200b thought
anyone here works for google,ll be honest really don know if this fits this sub but my big dream is to become data scientist for faang companies especially google if you work there how is it like how the pay the job itself do you ever get to create new things from scratch how often you build ml models so many questions is it worth trying to go for it,1,anyone here work for google,ll be honest really don know if this fit this sub but my big dream is to become data scientist for faang company especially google if you work there how is it like how the pay the job itself do you ever get to create new thing from scratch how often you build ml model so many question is it worth trying to go for it
data science poem inspired by if by rudyard kipling,know poetry probably isn this subreddit jam but wrote this poem about data science software development heavily inspired by the well known if by rudyard kipling would appreciate some love on my linkedin post basically never share anything there if you can run meeting when all about you are losing their heads and blaming it on you if you can trust your work when all reviewers doubt you but make allowance for their doubting too if you can wait for sign off and not be tired by waiting or hearing gossip don deal in lies or getting feedback don give way to hating and yet don look too good nor talk too wise if you can manage project and not make project management tools your hero if you can write code cleanly and not make clean code your aim if you can see true or and false or and treat those two booleans just the same if you can bear to hear the stats you ve spoken twisted by slides to make trap for fools or watch the calcs you gave your life to broken and stoop and build em up with deprecated tools if you can make binary heap of all your data after discussing at length with your boss but fail and find way to do it better and never breath word about your loss if you can document your code test and maintain to serve your team long after you are gone and so hold on when no one is there to train except the will which says to them hold on if you can talk with analysts and keep your virtue or walk with partners nor lose the common touch if neither clients not regulators can hurt you if all colleagues count with you but none too much if you can fill the unforgiving minute with sixty seconds worth of emails penned yours is the earth and everything that in it and which is more you ll be developer my friend,1,data science poem inspired by if by rudyard kipling,know poetry probably isn this subreddit jam but wrote this poem about data science software development heavily inspired by the well known if by rudyard kipling would appreciate some love on my linkedin post basically never share anything there if you can run meeting when all about you are losing their head and blaming it on you if you can trust your work when all reviewer doubt you but make allowance for their doubting too if you can wait for sign off and not be tired by waiting or hearing gossip don deal in lie or getting feedback don give way to hating and yet don look too good nor talk too wise if you can manage project and not make project management tool your hero if you can write code cleanly and not make clean code your aim if you can see true or and false or and treat those two booleans just the same if you can bear to hear the stats you ve spoken twisted by slide to make trap for fool or watch the calcs you gave your life to broken and stoop and build em up with deprecated tool if you can make binary heap of all your data after discussing at length with your bos but fail and find way to do it better and never breath word about your loss if you can document your code test and maintain to serve your team long after you are gone and so hold on when no one is there to train except the will which say to them hold on if you can talk with analyst and keep your virtue or walk with partner nor lose the common touch if neither client not regulator can hurt you if all colleague count with you but none too much if you can fill the unforgiving minute with sixty second worth of email penned yours is the earth and everything that in it and which is more you ll be developer my friend
transitioning from biotech to data science advice,recently graduated with degree in bioengineering and kind of regretting it because realized that the degree basically didn teach me any employable skills also took some introductory data computer science courses and have fair amount of experience with data analysis visualization through research internship any tips on building my data science skills online courses projects so can pivot away from in lab work and into more data focused role especially from anyone who has gone through similar situation,1,transitioning from biotech to data science advice,recently graduated with degree in bioengineering and kind of regretting it because realized that the degree basically didn teach me any employable skill also took some introductory data computer science course and have fair amount of experience with data analysis visualization through research internship any tip on building my data science skill online course project so can pivot away from in lab work and into more data focused role especially from anyone who ha gone through similar situation
ocr spelling correction nlp,hey guys was researching about how to auto correct spelling errors caused by ocr misreading things read this great article about using spell checker to detect the incorrectly spelled words first and then run bert model to get the predictions for the words one concern have is that common type of misread oftern involves reading letters as number and vice versa for example an is often misread as zero and is sometimes misread as tried to spell some words like these in microsoft word and see if the built in spell checker actually detects these type of words but the spell checker in word itself seems to just ignore something like l0rd lord or nake snake worried about if this approach that read was actually going to work since it relies on the spell checker has anyone done some related past works any possible ideas should look into thank you guys for sharing,1,ocr spelling correction nlp,hey guy wa researching about how to auto correct spelling error caused by ocr misreading thing read this great article about using spell checker to detect the incorrectly spelled word first and then run bert model to get the prediction for the word one concern have is that common type of misread oftern involves reading letter a number and vice versa for example an is often misread a zero and is sometimes misread a tried to spell some word like these in microsoft word and see if the built in spell checker actually detects these type of word but the spell checker in word itself seems to just ignore something like l0rd lord or nake snake worried about if this approach that read wa actually going to work since it relies on the spell checker ha anyone done some related past work any possible idea should look into thank you guy for sharing
does one need to become data analyst first,senior graduating with degrees in applied math stats and bioengineering bachelors in may from the postings ve been applying to seem to fit the qualifications in terms of software knowledge and experience but often read that becoming data analyst first is the more realistic option my fear is that none of the responsibilities finding on analyst positions seem to translate to the scientist positions posted if were to become an analyst would go for masters stats or ds after year or so but is this the proper route should be taking,1,doe one need to become data analyst first,senior graduating with degree in applied math stats and bioengineering bachelor in may from the posting ve been applying to seem to fit the qualification in term of software knowledge and experience but often read that becoming data analyst first is the more realistic option my fear is that none of the responsibility finding on analyst position seem to translate to the scientist position posted if were to become an analyst would go for master stats or d after year or so but is this the proper route should be taking
work of data scientist in an insurance company,what type of work is involved in an insurance company as data scientist got position as data scientist in an insurance company and don know what type of work they do this is my first job after college and am bit nervous because don have any domain knowledge my background is in pure mathematics and did some coding courses here and there learned some machine learning in the last year of my degree and here am going for data science job what type of machine learning algorithms do they use do they use anything fancy have six months before join the company any advice will be highly appreciated like what should learn before join the company and what should expect from them,1,work of data scientist in an insurance company,what type of work is involved in an insurance company a data scientist got position a data scientist in an insurance company and don know what type of work they do this is my first job after college and am bit nervous because don have any domain knowledge my background is in pure mathematics and did some coding course here and there learned some machine learning in the last year of my degree and here am going for data science job what type of machine learning algorithm do they use do they use anything fancy have six month before join the company any advice will be highly appreciated like what should learn before join the company and what should expect from them
looks like they just put in all the words they could find btw although it says experience on linkedin it under entry level job,looks like they just put in all the words they could find btw although it says experience on linkedin it under entry level job,1,look like they just put in all the word they could find btw although it say experience on linkedin it under entry level job,look like they just put in all the word they could find btw although it say experience on linkedin it under entry level job
looks like they just put in all the words they could find btw although it says experience on linkedin it under entry level job,looks like they just put in all the words they could find btw although it says experience on linkedin it under entry level job,1,look like they just put in all the word they could find btw although it say experience on linkedin it under entry level job,look like they just put in all the word they could find btw although it say experience on linkedin it under entry level job
datasource for sentiment,does anyone know good datasource for nba fan sentiment in relation for sports teams for example have seen some projects that will pull tweets related to teams performance in game and quantifies the fan base sentiment with regards to the team performance currently looking for nba but like to see it for mlb and nfl,1,datasource for sentiment,doe anyone know good datasource for nba fan sentiment in relation for sport team for example have seen some project that will pull tweet related to team performance in game and quantifies the fan base sentiment with regard to the team performance currently looking for nba but like to see it for mlb and nfl
fellow wfh ers how many hours week do you think you actually work,for those of you that work from home how many hours week do you think you spend actually working ve found my productivity has been going down the last few months and my anxiety up as result spending about hours actually doing work in hour work week although spend at my desk is anyone else finding it hard to concentrate at home is this level of production satisfactory still get everything done that asked of but could definitely go beyond feel awful because used to be so motivated and hardworking,1,fellow wfh er how many hour week do you think you actually work,for those of you that work from home how many hour week do you think you spend actually working ve found my productivity ha been going down the last few month and my anxiety up a result spending about hour actually doing work in hour work week although spend at my desk is anyone else finding it hard to concentrate at home is this level of production satisfactory still get everything done that asked of but could definitely go beyond feel awful because used to be so motivated and hardworking
why python for data science is easy to learn for beginners,why python for data science is easy to learn for beginners,1,why python for data science is easy to learn for beginner,why python for data science is easy to learn for beginner
starting out data science the first time,hey guys have applied to intern and they asked me to extract textual data articles from the given url and perform text analysis to compute variables with an excel sheet around url links for data extraction now is there any program can write to use extract link from the spreadsheet and do the analysis process please help me out guys just now starting to learn,1,starting out data science the first time,hey guy have applied to intern and they asked me to extract textual data article from the given url and perform text analysis to compute variable with an excel sheet around url link for data extraction now is there any program can write to use extract link from the spreadsheet and do the analysis process please help me out guy just now starting to learn
blindfolded ml,received an unusual request solve supervised learning problem on tabular data without data access apparently these data are top secret and they just cannot be accessed am given yaml file containing metadata and supposed to develop code based on that once ready code will be sent to the other side and run any error messages will be reported back to me so that can fix bugs plus will get notified on key metrics to assess model performance and iterate they told me data are ml ready there is single table with features and target variable will take this for granted and pray for it to be true this is far from efficient but looks like have to run with this the yaml file looks like this n_rows n_columns column_names id target columna columnb columnc id_column_name id target_column_name target column_types columna scale columnb scale columnc categorical target categorical column_null_values columna columnb columnc column_categorical_values columnc red green blue target has anyone ever faced something similar do you think this process will yield lower model accuracy with respect to having direct data access how would you tackle this problem the first thing thought about is to generate some synthetic data from the yaml file in order to detect bugs sooner using these data will not tell me anything about data relationships but at least it will lower the chance of receiving an error message due to stupid bug feature engineering puzzles me first thing that comes to mind is brute force on second thought maybe can guess some relationships from column names tough what else,1,blindfolded ml,received an unusual request solve supervised learning problem on tabular data without data access apparently these data are top secret and they just cannot be accessed am given yaml file containing metadata and supposed to develop code based on that once ready code will be sent to the other side and run any error message will be reported back to me so that can fix bug plus will get notified on key metric to ass model performance and iterate they told me data are ml ready there is single table with feature and target variable will take this for granted and pray for it to be true this is far from efficient but look like have to run with this the yaml file look like this n_rows n_columns column_names id target columna columnb columnc id_column_name id target_column_name target column_types columna scale columnb scale columnc categorical target categorical column_null_values columna columnb columnc column_categorical_values columnc red green blue target ha anyone ever faced something similar do you think this process will yield lower model accuracy with respect to having direct data access how would you tackle this problem the first thing thought about is to generate some synthetic data from the yaml file in order to detect bug sooner using these data will not tell me anything about data relationship but at least it will lower the chance of receiving an error message due to stupid bug feature engineering puzzle me first thing that come to mind is brute force on second thought maybe can guess some relationship from column name tough what else
mac air m1 for data science,hey doing my masters in data science and was planning on getting mac m1 for uni is it good idea hear the m1 suffers because of poor app support has that been resolved,1,mac air m1 for data science,hey doing my master in data science and wa planning on getting mac m1 for uni is it good idea hear the m1 suffers because of poor app support ha that been resolved
advice on mit applied data science program,hey guys new here so apologies if this post doesnt belong was recently accepted into the program but trying to figure out if mit applied data science program is actually worth taking for context complete newbie to the field recent grad who got my bachelors in exercise physiology thinking was gonna be sports med doctor lol more or less aware that alot of these certificate programs bootcamps are subpar in teaching the skills needed to flourish in the feild but as this is mit decided to look further into it help would be appreciated thanks in advance,1,advice on mit applied data science program,hey guy new here so apology if this post doesnt belong wa recently accepted into the program but trying to figure out if mit applied data science program is actually worth taking for context complete newbie to the field recent grad who got my bachelor in exercise physiology thinking wa gonna be sport med doctor lol more or le aware that alot of these certificate program bootcamps are subpar in teaching the skill needed to flourish in the feild but a this is mit decided to look further into it help would be appreciated thanks in advance
what is the career path from solutions engineer to data scientist,accepted an internship as solutions engineer at vmware for the summer don know if they will offer me full time position after my internship or not but wanted to know if you think that finding job in the united states as junior data scientist want to do more technical can be envisaged after an internship as solutions engineer my internship is in paris and french,1,what is the career path from solution engineer to data scientist,accepted an internship a solution engineer at vmware for the summer don know if they will offer me full time position after my internship or not but wanted to know if you think that finding job in the united state a junior data scientist want to do more technical can be envisaged after an internship a solution engineer my internship is in paris and french
dissecting and implementing research papers in python,hi wrote part article on creating interaction networks between characters in novels and other bodies of text the first part is detailed literature review outlining and dissecting research papers relevant to the topic and the second part is the implementation of the thoughts and ideas presented in the first part in python check it out if you re interested part part,1,dissecting and implementing research paper in python,hi wrote part article on creating interaction network between character in novel and other body of text the first part is detailed literature review outlining and dissecting research paper relevant to the topic and the second part is the implementation of the thought and idea presented in the first part in python check it out if you re interested part part
the story of the women in data science wids datathon kdnuggets,the story of the women in data science wids datathon kdnuggets,1,the story of the woman in data science wids datathon kdnuggets,the story of the woman in data science wids datathon kdnuggets
could should javascript have been the defacto ds language,specifically possibly naively wish python didn exist and javascript was used in it place but this might be misguided so am here to ask for counter arguments and by javascript mean javascript with runtime like node or deno but why javascript visualisation is large part of data science and most libraries seem to use web technologies under the hood for good reason since web technologies are powerful and mature and make sharing visualiations over the web easy while most libraries provide python wrapper which will be sufficient in many cases often find it easier to work directly in javascript especially when the end product is dashboard or entire website rather than single chart in my limited understanding javascript is similar enough to python that it could be used for data science in place of python the main points similarities being it has short simple easy to learn syntax which doesn require annotating types the language itself is not that fast but can easily call libraries for the computationally intensive tasks required in ds flexible multi paradigm language which allows libraries to provide an easy to use api without much cermony and boilerplate repl the difference is that where to me it seems javascript could be used in place of python for ds python could not have been used in place of javascript as the web browser language since it is blocking doesn run in an event loop like python javascript was purpose built for web browsers python was not purpose built for ds to me the only reason python is used for ds is because of it first mover advantage and the huge third party library ecosystem many languages are popular more because mostly because of their ecosystem and that fine don expect this to change but it is just annonying that it seems just javascript could be used where instead we need to use python and javascript know people will argue that multiple programming languages are inevitable and you should pick the right tool for the job etc and agree but having to use two languages simultaneously is much more inefficient learning the languages themselves is not so bad but having to learn two different ecosystems and constantly switch between the languages and remeber the different apis has large impact on productivity for me at least this is compounded by the fact that data scientists are often not the most experienced programmers where much of the training and experience is dedicated to analysis rather than simply programming like developer could make similar but admitedly less convincing arguments for rust replacing the entire python javascript data science stack but that should be separate post,1,could should javascript have been the defacto d language,specifically possibly naively wish python didn exist and javascript wa used in it place but this might be misguided so am here to ask for counter argument and by javascript mean javascript with runtime like node or deno but why javascript visualisation is large part of data science and most library seem to use web technology under the hood for good reason since web technology are powerful and mature and make sharing visualiations over the web easy while most library provide python wrapper which will be sufficient in many case often find it easier to work directly in javascript especially when the end product is dashboard or entire website rather than single chart in my limited understanding javascript is similar enough to python that it could be used for data science in place of python the main point similarity being it ha short simple easy to learn syntax which doesn require annotating type the language itself is not that fast but can easily call library for the computationally intensive task required in d flexible multi paradigm language which allows library to provide an easy to use api without much cermony and boilerplate repl the difference is that where to me it seems javascript could be used in place of python for d python could not have been used in place of javascript a the web browser language since it is blocking doesn run in an event loop like python javascript wa purpose built for web browser python wa not purpose built for d to me the only reason python is used for d is because of it first mover advantage and the huge third party library ecosystem many language are popular more because mostly because of their ecosystem and that fine don expect this to change but it is just annonying that it seems just javascript could be used where instead we need to use python and javascript know people will argue that multiple programming language are inevitable and you should pick the right tool for the job etc and agree but having to use two language simultaneously is much more inefficient learning the language themselves is not so bad but having to learn two different ecosystem and constantly switch between the language and remeber the different apis ha large impact on productivity for me at least this is compounded by the fact that data scientist are often not the most experienced programmer where much of the training and experience is dedicated to analysis rather than simply programming like developer could make similar but admitedly le convincing argument for rust replacing the entire python javascript data science stack but that should be separate post
is it worth it to take data entry job while applying to data analyst jobs,currently working my way towards getting data analyst job currently have certifications in python sql and data visualization used to work math based job but quit due to covid mental health reasons have portfolio with couple projects in it they re all sports based know need to branch out to other topics right now working retail job that allows me to work on courses to gain more certs work hour shifts and can easily dedicate of those hours to learning which is why ve been able to cover ton of ground in months have degree but it not computer science the issue is the retail job is low pay and most of the financial burden is on my partner and it starting to eat away at the relationship applying to as many data analyst jobs as can haven had much luck yet but ve only been trying for couple weeks would it be worth it to take data entry job would more than likely have to be remote and continue applying for data analyst be giving up majority of my learning time but need to make more than am now would having data entry on my resume make it easier to switch,1,is it worth it to take data entry job while applying to data analyst job,currently working my way towards getting data analyst job currently have certification in python sql and data visualization used to work math based job but quit due to covid mental health reason have portfolio with couple project in it they re all sport based know need to branch out to other topic right now working retail job that allows me to work on course to gain more cert work hour shift and can easily dedicate of those hour to learning which is why ve been able to cover ton of ground in month have degree but it not computer science the issue is the retail job is low pay and most of the financial burden is on my partner and it starting to eat away at the relationship applying to a many data analyst job a can haven had much luck yet but ve only been trying for couple week would it be worth it to take data entry job would more than likely have to be remote and continue applying for data analyst be giving up majority of my learning time but need to make more than am now would having data entry on my resume make it easier to switch
personal development plans for data science,an experienced ish ds working for finance marketing consulting agency and looking to create personal development plan for ideally like to use the output of this to justify training amp and make deliberate overtures to the kind of projects like to work on and career progression for others in similar situations eg data science consultancy what resources have you used for personal development planning are pdps widely used within your place of work thanks,1,personal development plan for data science,an experienced ish d working for finance marketing consulting agency and looking to create personal development plan for ideally like to use the output of this to justify training amp and make deliberate overture to the kind of project like to work on and career progression for others in similar situation eg data science consultancy what resource have you used for personal development planning are pdps widely used within your place of work thanks
how to present small size of inference data to client,am data scientist time to time represent reports to the clients my company has working model but for specific client we may have data points for inference of course the clients are asking is this even useful analysis you know it done based on so small number of data am telling them you can read the findings of the report cautiously it still has value it can be supplemental argument for another analysis if you have no idea about what is happening this analysis still gives snapshot we just need to be aware of the weakness how would you sell report created by small number of data such as data points what would be your potential selling points,1,how to present small size of inference data to client,am data scientist time to time represent report to the client my company ha working model but for specific client we may have data point for inference of course the client are asking is this even useful analysis you know it done based on so small number of data am telling them you can read the finding of the report cautiously it still ha value it can be supplemental argument for another analysis if you have no idea about what is happening this analysis still give snapshot we just need to be aware of the weakness how would you sell report created by small number of data such a data point what would be your potential selling point
don look up pierced my soul,don look up pierced my soul,1,don look up pierced my soul,don look up pierced my soul
algorithms as discrimination detectors,not sure if we still use this subreddit to discuss improvement findings in the field but figured throw this one out there for those interested algorithms as discrimination detectors really interesting points on current laws are unable to handle algorithmic discrimination algorithms can use other algorithms to detect bias in algorithms say that five times fast if people like research papers am happy to post the ones find that are interesting,1,algorithm a discrimination detector,not sure if we still use this subreddit to discus improvement finding in the field but figured throw this one out there for those interested algorithm a discrimination detector really interesting point on current law are unable to handle algorithmic discrimination algorithm can use other algorithm to detect bias in algorithm say that five time fast if people like research paper am happy to post the one find that are interesting
what is the role of gfs master during read and write operations,am really confused by this question as there are multiple possible answers of this question so am asking this there is very minimum role of gfs master in writing and reading operations you can either go through these or just tell me what is the correct answer if you are bored to go through all of these my try read and write operations on gfs stefa distributed_systems_18 schedule_files google file system pdf matei courses s897 slides gfs pdf slide,1,what is the role of gfs master during read and write operation,am really confused by this question a there are multiple possible answer of this question so am asking this there is very minimum role of gfs master in writing and reading operation you can either go through these or just tell me what is the correct answer if you are bored to go through all of these my try read and write operation on gfs stefa distributed_systems_18 schedule_files google file system pdf matei course s897 slide gfs pdf slide
interested in dataquest you be helping me,hello an aspiring data analyst currently using dataquest to study it an awesome platform to learn data analysis however have reached my freebies limit and can afford the monthly or yearly subscription if you guys on this subreddit were considering subscribing to dataquest here my referral link app dataquest io referral signup x9u1utng it would save you know not whole lot of savings but be able to get free annual access once ve reached annual subs,1,interested in dataquest you be helping me,hello an aspiring data analyst currently using dataquest to study it an awesome platform to learn data analysis however have reached my freebie limit and can afford the monthly or yearly subscription if you guy on this subreddit were considering subscribing to dataquest here my referral link app dataquest io referral signup x9u1utng it would save you know not whole lot of saving but be able to get free annual access once ve reached annual sub
can we train classifier with overlapping marginal distributions,have very large high dimensional dataset with two classes but the marginal distributions in all individual dimensions overlap nearly perfectly what does this usually mean is it possible to train classifier on this,1,can we train classifier with overlapping marginal distribution,have very large high dimensional dataset with two class but the marginal distribution in all individual dimension overlap nearly perfectly what doe this usually mean is it possible to train classifier on this
quantumblack data scientist interview,currently in the recruitment process for data scientist position at mckinsey quantumblack and my case study interview they call it the blended problem solving scenario is this week since most resources online are for regular mckinsey case study interviews does anyone have any experience with this that they can shed some light on like the structure of the interview what types of questions might be asked examples etc would really appreciate any advice,1,quantumblack data scientist interview,currently in the recruitment process for data scientist position at mckinsey quantumblack and my case study interview they call it the blended problem solving scenario is this week since most resource online are for regular mckinsey case study interview doe anyone have any experience with this that they can shed some light on like the structure of the interview what type of question might be asked example etc would really appreciate any advice
where to get started,so am really intrigued with the use of data from the way data is collected to formatting and interpreting it into information however have done almost all of my work on spreadsheets so far and believe have pretty good foundation of it all however want to go further wondering if anyone is able to tell me where to get started with going deeper with data where do get started do you have any resources you recommend to being to learn code that is relevant to data amp x200b any replies are appreciated and sorry if any of the things said are stupid,1,where to get started,so am really intrigued with the use of data from the way data is collected to formatting and interpreting it into information however have done almost all of my work on spreadsheet so far and believe have pretty good foundation of it all however want to go further wondering if anyone is able to tell me where to get started with going deeper with data where do get started do you have any resource you recommend to being to learn code that is relevant to data amp x200b any reply are appreciated and sorry if any of the thing said are stupid
data science full course,data science full course,1,data science full course,data science full course
please advise me for data science interview exercise,am in the selection process for data science position have already done few interviews and now they have given me take home assignment they gave me dataset and some questions to answer the dataset is about some sales and includes variables such as location day of week month time weather revenue and few others they asked me the following describe the business what influences the sales performance confidence in predicting future revenue what important variables are missing in the dataset how could they influence sales performance could you please guide me give some advice on how to answer these questions also the exercise is quite open so can make any extra analysis that consider appropriate what could do to stand out thank you,1,please advise me for data science interview exercise,am in the selection process for data science position have already done few interview and now they have given me take home assignment they gave me dataset and some question to answer the dataset is about some sale and includes variable such a location day of week month time weather revenue and few others they asked me the following describe the business what influence the sale performance confidence in predicting future revenue what important variable are missing in the dataset how could they influence sale performance could you please guide me give some advice on how to answer these question also the exercise is quite open so can make any extra analysis that consider appropriate what could do to stand out thank you
increase your data science skills in no time,increase your data science skills in no time,1,increase your data science skill in no time,increase your data science skill in no time
evolution of programmers communities on stack overflow,some research did on the stack oveflow website abstract question and answer amp websites are medium where people can communicate and help each other stack overflow is one of the most popular amp websites about programming where millions of developers seek help or provide valuable assistance activity on the stack overflow website is moderated by the user community utilizing voting system to promote high quality content the website was created on and has accumulated large amount of crowd wisdom about the software development industry here we analyse this data to examine trends in the grouping of technologies and their users into different sub communities in our work we analysed all questions answers votes and tags from stack overflow between and we generated series of user technology interaction graphs and applied community detection algorithms to identify the biggest user communities for each year to examine which technologies those communities incorporate how they are interconnected and how they evolve through time the biggest and most persistent communities were related to web development in general there is little movement between communities users tend to either stay within the same community or not acquire any score at all community evolution reveals the popularity of different programming languages and frameworks on stack overflow over time these findings give insight into the user community on stack overflow and reveal long term trends on the software development industry,1,evolution of programmer community on stack overflow,some research did on the stack oveflow website abstract question and answer amp website are medium where people can communicate and help each other stack overflow is one of the most popular amp website about programming where million of developer seek help or provide valuable assistance activity on the stack overflow website is moderated by the user community utilizing voting system to promote high quality content the website wa created on and ha accumulated large amount of crowd wisdom about the software development industry here we analyse this data to examine trend in the grouping of technology and their user into different sub community in our work we analysed all question answer vote and tag from stack overflow between and we generated series of user technology interaction graph and applied community detection algorithm to identify the biggest user community for each year to examine which technology those community incorporate how they are interconnected and how they evolve through time the biggest and most persistent community were related to web development in general there is little movement between community user tend to either stay within the same community or not acquire any score at all community evolution reveals the popularity of different programming language and framework on stack overflow over time these finding give insight into the user community on stack overflow and reveal long term trend on the software development industry
best way to upskill ds team,hi all looking into different options for upskilling relatively small team of data scientists the idea is to provide general set of flexible options allowing them to forge their own path to some extent and to facilitate their transition from jnr ds gt ds gt snr ds really like the coursera platform and have personally used it in the past but also aware that there are multitude of other options cloud academy does anyone have experience of using any of these platforms in an enterprise context do people actually engage with them if given personal training allowance cheers,1,best way to upskill d team,hi all looking into different option for upskilling relatively small team of data scientist the idea is to provide general set of flexible option allowing them to forge their own path to some extent and to facilitate their transition from jnr d gt d gt snr d really like the coursera platform and have personally used it in the past but also aware that there are multitude of other option cloud academy doe anyone have experience of using any of these platform in an enterprise context do people actually engage with them if given personal training allowance cheer
consulting vs internalized,found job as data scientist in consulting company the more think about it the more questions have wonder how different it is to be consultant vs internalized on ds what is expected from the consultant client pov how different are the relation between consultants and employees in client company compared to being internalized what should be the behavior of ds consultant any additionnal information you have experienced is welcome,1,consulting v internalized,found job a data scientist in consulting company the more think about it the more question have wonder how different it is to be consultant v internalized on d what is expected from the consultant client pov how different are the relation between consultant and employee in client company compared to being internalized what should be the behavior of d consultant any additionnal information you have experienced is welcome
recommended courses for python for someone with existing database sql knowledge,have few small projects that like to get done using python and sql consider my knowledge in sql advanced but in python it pretty basic in pandas and numpy etc it almost non existent am looking for some recommendations on some python fundamentals for working with data that don assume that don understand data types models optimisation etc am asking too much or can you guys help,1,recommended course for python for someone with existing database sql knowledge,have few small project that like to get done using python and sql consider my knowledge in sql advanced but in python it pretty basic in panda and numpy etc it almost non existent am looking for some recommendation on some python fundamental for working with data that don assume that don understand data type model optimisation etc am asking too much or can you guy help
how do you encourage knowledge sharing in your data science team,can anyone share their team practice in term of technical knowledge sharing how often do your team share how do you manage the sharing session,1,how do you encourage knowledge sharing in your data science team,can anyone share their team practice in term of technical knowledge sharing how often do your team share how do you manage the sharing session
need suggestions on getting into data science,hello all an experienced dev and want to branch little big data was always interesting to me and always found probability and combinatorics fascinating so want to get into that bit data science and python now can someone suggest me good startin point be it book online course something the choice is overwhelming thank you all,1,need suggestion on getting into data science,hello all an experienced dev and want to branch little big data wa always interesting to me and always found probability and combinatorics fascinating so want to get into that bit data science and python now can someone suggest me good startin point be it book online course something the choice is overwhelming thank you all
advice on getting ds internship for summer,advice on getting ds internship for summer,1,advice on getting d internship for summer,advice on getting d internship for summer
master in data science amp ai vs master in cs specialising in data science,hello ve seen few times here that data science degrees are discouraged have bachelor degree in computer science and want to do my master degree which will be completed in either netherlands or germany would prefer to do it in computer science specialising in data science however certain institutes offer simply data science degree for example the eindhoven university of technology it really well known and highly regarded technical university who just converted their master in cs with data science specialization into an independent master in data science degree are there any data scientists based in the eu who can tell me if degree in data science limits me somewhat as compared to cs degree with specialization in data science which involves lot more choice in electives outside of the field of data science at least with respect to the job market in the eu are master graduates in cs at an advantage compared to data science and ai graduates,1,master in data science amp ai v master in c specialising in data science,hello ve seen few time here that data science degree are discouraged have bachelor degree in computer science and want to do my master degree which will be completed in either netherlands or germany would prefer to do it in computer science specialising in data science however certain institute offer simply data science degree for example the eindhoven university of technology it really well known and highly regarded technical university who just converted their master in c with data science specialization into an independent master in data science degree are there any data scientist based in the eu who can tell me if degree in data science limit me somewhat a compared to c degree with specialization in data science which involves lot more choice in elective outside of the field of data science at least with respect to the job market in the eu are master graduate in c at an advantage compared to data science and ai graduate
mood,mood,1,mood,mood
starting my first job out of college as healthcare data analyst revenue cycling,hi just got my first out of college gig at healthcare startup based in la was wondering what are the possible exits from healthcare data analyst position and are there opportunities to eventually become data scientist is there such thing as healthcare data scientist thank you everyone,1,starting my first job out of college a healthcare data analyst revenue cycling,hi just got my first out of college gig at healthcare startup based in la wa wondering what are the possible exit from healthcare data analyst position and are there opportunity to eventually become data scientist is there such thing a healthcare data scientist thank you everyone
help me pick between two programs,received acceptance letters for the following two programs feel like is more broader than the can someone help me understand how each will benefit me over the other,1,help me pick between two program,received acceptance letter for the following two program feel like is more broader than the can someone help me understand how each will benefit me over the other
free reliable global easily update able data,want to create predictive tool that could be useful to the most number of people weather prediction for example are there free reliable global data that can start exploring don want to narrow my scope by industry to begin with,1,free reliable global easily update able data,want to create predictive tool that could be useful to the most number of people weather prediction for example are there free reliable global data that can start exploring don want to narrow my scope by industry to begin with
datascience with python online training at hachion,datascience with python online training at hachion,1,datascience with python online training at hachion,datascience with python online training at hachion
matrix multiplication explained with python examples complete guide,matrix multiplication explained with python examples complete guide,1,matrix multiplication explained with python example complete guide,matrix multiplication explained with python example complete guide
should put certs and badges on my resume,have few azure certifications and snowflake badges that related to data science but just wondering should put them on my resume to boost getting interviews do you guys think it helpful to have them on the resume,1,should put cert and badge on my resume,have few azure certification and snowflake badge that related to data science but just wondering should put them on my resume to boost getting interview do you guy think it helpful to have them on the resume
literary works sorted by closest matching story template,literary works sorted by closest matching story template,1,literary work sorted by closest matching story template,literary work sorted by closest matching story template
should study data science now or later,so about to start my economics degree also trying to learn programming even though my crippling anxiety stops me most of the times my uni just announced they re opening bachelor degree in data science same length as the econ degree and ve been considering switching but my friend believes it better to do data science master says in two years you can learn everything important and not to quit economics because do like it is he right and could become data scientist through master or is it more recommended to get your bachelors in it,1,should study data science now or later,so about to start my economics degree also trying to learn programming even though my crippling anxiety stop me most of the time my uni just announced they re opening bachelor degree in data science same length a the econ degree and ve been considering switching but my friend belief it better to do data science master say in two year you can learn everything important and not to quit economics because do like it is he right and could become data scientist through master or is it more recommended to get your bachelor in it
ml ai freelancer,anyone here working full time part time as ml ai freelancer have some questions if you would like to share networking is important for freelancing how did you build your network comment if otherwise how matured is the freelancing business with respect to ml how difficult easy it is to maintain stable income do you suggest team up with some developer to increase chances for projects is freelancing worth leaving stable job any suggestions for new freelancer for my background have years of experience working at faang in ml domain and would like to venture into freelancing can start with small projects hrs week then scale my problem is lack of networking and visibility into freelancing any help would be appreciated,1,ml ai freelancer,anyone here working full time part time a ml ai freelancer have some question if you would like to share networking is important for freelancing how did you build your network comment if otherwise how matured is the freelancing business with respect to ml how difficult easy it is to maintain stable income do you suggest team up with some developer to increase chance for project is freelancing worth leaving stable job any suggestion for new freelancer for my background have year of experience working at faang in ml domain and would like to venture into freelancing can start with small project hr week then scale my problem is lack of networking and visibility into freelancing any help would be appreciated
prospects for job with my experience,wondering if anyone entered data science without formal education in math data science or another technical degree ve been with my company for years with over years experience in data analytics and financial reporting currently manage few data analysts but not happy in my position the only experience have is with sql and excel vba learned sql on the job and say really good at it proficient writing queries procedures and functions using dynamic code cursors and the like most of my big projects have involved data wrangling where gathering and combining data from sources and producing reports showing the financial impact of certain decisions well respected but lot of the job is mis reporting and it just not challenging like digging into data and processes and investigating discrepancies or finding trends and insights that people didn see otherwise since don have formal education lack confidence in taking my skills outside of my current organization as feel people will look at my resume and wonder why even applying for technical job not necessarily looking for data science job but am looking for job that involves some sort of coding willing to learn any language and data reading posts about people with education in math stats or data science and seeing how they re struggling to find jobs make me think won be good candidate did anyone get job in data science without formal education,1,prospect for job with my experience,wondering if anyone entered data science without formal education in math data science or another technical degree ve been with my company for year with over year experience in data analytics and financial reporting currently manage few data analyst but not happy in my position the only experience have is with sql and excel vba learned sql on the job and say really good at it proficient writing query procedure and function using dynamic code cursor and the like most of my big project have involved data wrangling where gathering and combining data from source and producing report showing the financial impact of certain decision well respected but lot of the job is mi reporting and it just not challenging like digging into data and process and investigating discrepancy or finding trend and insight that people didn see otherwise since don have formal education lack confidence in taking my skill outside of my current organization a feel people will look at my resume and wonder why even applying for technical job not necessarily looking for data science job but am looking for job that involves some sort of coding willing to learn any language and data reading post about people with education in math stats or data science and seeing how they re struggling to find job make me think won be good candidate did anyone get job in data science without formal education
first time updating my resume since took on first ever analyst role,it my first time applying to new job after getting some experience in the data business analytics field this is my first time writing down what do on paper so was wondering if some with more experience can help me with my resume wording and whatnot particularly my summary and how detail my analyst responsibilities accomplishments see below summary strategic data analyst with dependable ability in exploring data identifying trends and generating visualizations for key stakeholders experience with edw and number of business intelligence tools flexible and teamwork oriented familiar with agile project management education reddit university bachelor of science in business and economics awarded may double major finance business information systems skills programming sql sas python tools sas eg excel ssms aginity pro power bi tableau language native russian intermediate spanish professional experience business analyst large insurance company january present modified and maintained complex automated sas projects that collected and organized data related to sales service and underwriting performance of the company regional operations and incorporated it into various trend analysis reports utilized by corporate and country wide regional management generated sql queries to extract and provide data necessary to present recommendations on various financial operating decisions including an external vendor data analysis that helped save hundreds of thousands of dollars in potential user fees contributed to the development of bots intended to increase the efficiency of the company underwriting operations by effectively assisting and communicating with process analysts and bot developers saving approximately in annual labor costs per bot implemented two questions does the last part of my second bullet point sound unprofessional where saying save hundreds of thousands of dollars in potential user fees the vendor data in question that my analysis helped them decide not to go with charged per query so it hard to gauge how much money it saved them assuming if it couple thousand per query per some googling then it could have saved them upwards of six figures should further categorize my skills list at all by type of tool since power bi is used for different purposes than ssms or experience level with language as more experienced with sql than amp x200b any feedback at all would be appreciated,1,first time updating my resume since took on first ever analyst role,it my first time applying to new job after getting some experience in the data business analytics field this is my first time writing down what do on paper so wa wondering if some with more experience can help me with my resume wording and whatnot particularly my summary and how detail my analyst responsibility accomplishment see below summary strategic data analyst with dependable ability in exploring data identifying trend and generating visualization for key stakeholder experience with edw and number of business intelligence tool flexible and teamwork oriented familiar with agile project management education reddit university bachelor of science in business and economics awarded may double major finance business information system skill programming sql sa python tool sa eg excel ssms aginity pro power bi tableau language native russian intermediate spanish professional experience business analyst large insurance company january present modified and maintained complex automated sa project that collected and organized data related to sale service and underwriting performance of the company regional operation and incorporated it into various trend analysis report utilized by corporate and country wide regional management generated sql query to extract and provide data necessary to present recommendation on various financial operating decision including an external vendor data analysis that helped save hundred of thousand of dollar in potential user fee contributed to the development of bot intended to increase the efficiency of the company underwriting operation by effectively assisting and communicating with process analyst and bot developer saving approximately in annual labor cost per bot implemented two question doe the last part of my second bullet point sound unprofessional where saying save hundred of thousand of dollar in potential user fee the vendor data in question that my analysis helped them decide not to go with charged per query so it hard to gauge how much money it saved them assuming if it couple thousand per query per some googling then it could have saved them upwards of six figure should further categorize my skill list at all by type of tool since power bi is used for different purpose than ssms or experience level with language a more experienced with sql than amp x200b any feedback at all would be appreciated
do you feel like there plenty of opportunity different roles and availability of them and growth moving up the ladder in sense within ds wondering if the saturation is region specific and how happy people are in their roles,do you feel like there plenty of opportunity different roles and availability of them and growth moving up the ladder in sense within ds wondering if the saturation is region specific and how happy people are in their roles,1,do you feel like there plenty of opportunity different role and availability of them and growth moving up the ladder in sense within d wondering if the saturation is region specific and how happy people are in their role,do you feel like there plenty of opportunity different role and availability of them and growth moving up the ladder in sense within d wondering if the saturation is region specific and how happy people are in their role
time management in ml projects,hi fellow data scientists someone with to years ds experience and have been working in commercial organizations banking and retail straight to the point what the best strategy for setting time expectations on projects have to set up jira stories for ml model improvements and so far it has been disaster the project was handed over to me by another employee who left the firm and find that every time set up an expected time duration for task really cannot get it done in time ve been very good at time management for engineering tasks but when it comes to ml models there is so much unexpected stuff going on crappy data really crappy results causing me to redo the modeling by adjusting data sources or model assumptions rinse and repeat due to data size and other issues it takes around days to repeat an experiment but there is no telling what the outcome is or even if can improve the model in any significant manner my anxiety levels are over the roof my boss is nice guy but he is over optimistic about this model can blame him he has invested lot on this and it too big to fail now so in summary how to set up time expectations for ml projects and hot do strategically deal with non performant models given deadlines,1,time management in ml project,hi fellow data scientist someone with to year d experience and have been working in commercial organization banking and retail straight to the point what the best strategy for setting time expectation on project have to set up jira story for ml model improvement and so far it ha been disaster the project wa handed over to me by another employee who left the firm and find that every time set up an expected time duration for task really cannot get it done in time ve been very good at time management for engineering task but when it come to ml model there is so much unexpected stuff going on crappy data really crappy result causing me to redo the modeling by adjusting data source or model assumption rinse and repeat due to data size and other issue it take around day to repeat an experiment but there is no telling what the outcome is or even if can improve the model in any significant manner my anxiety level are over the roof my bos is nice guy but he is over optimistic about this model can blame him he ha invested lot on this and it too big to fail now so in summary how to set up time expectation for ml project and hot do strategically deal with non performant model given deadline
help did anyone try fitting models to experimental results using the symfit python package,have been hitting my head on this one for while am trying to fit an ode model to some experimental results with no success after reading much symfit seems to be the best python package for the job but still not getting the results am looking for here is also the so question comment124852464_70615240 posted couple of days ago if anyone feels like giving it try,1,help did anyone try fitting model to experimental result using the symfit python package,have been hitting my head on this one for while am trying to fit an ode model to some experimental result with no success after reading much symfit seems to be the best python package for the job but still not getting the result am looking for here is also the so question comment124852464_70615240 posted couple of day ago if anyone feel like giving it try
data science statistics or middle ground eu msc,hi everyone finishing my bachelor in statistics in italy and would like to continue my studies with master degree in good university in europe in these years ve appreciate both the theoretical and the applied stuff ve met in university therefore torn between applying for the msc in statistics and the one in data science either at eth zurich moreover there are some programs in statistics and data science such as the one at ku leuven nevertheless think can build an hybrid path as well doing either the statistical and the data science one considering the large amount of elective exams in conclusion would like to ask you what should do to really understand which is the the branch of studies really want to continue in for istance under or postgraduate courses that better represent each field studies and work prospectives or any other discriminant that may help my choice thank you in advance,1,data science statistic or middle ground eu msc,hi everyone finishing my bachelor in statistic in italy and would like to continue my study with master degree in good university in europe in these year ve appreciate both the theoretical and the applied stuff ve met in university therefore torn between applying for the msc in statistic and the one in data science either at eth zurich moreover there are some program in statistic and data science such a the one at ku leuven nevertheless think can build an hybrid path a well doing either the statistical and the data science one considering the large amount of elective exam in conclusion would like to ask you what should do to really understand which is the the branch of study really want to continue in for istance under or postgraduate course that better represent each field study and work prospectives or any other discriminant that may help my choice thank you in advance
automatic wordle solving,people are looking into solving wordle algorithmically yotamyachmoorgafni automatic wordle solving a305954b746e yotamyachmoorgafni automatic wordle solving a305954b746e does anyone know of solution that solves the hard mode any interesting implementations,1,automatic wordle solving,people are looking into solving wordle algorithmically yotamyachmoorgafni automatic wordle solving a305954b746e yotamyachmoorgafni automatic wordle solving a305954b746e doe anyone know of solution that solves the hard mode any interesting implementation
trying to get an entry level data analyst scientist job and not getting anywhere,so on applications and months of applying for an entry level data analyst scientist job and ve only gotten interview for part time business analyst job people on this subreddit have told me have the qualifications bsc math phys double major experience as teaching assistant and research assistant knowledge of python sql power bi excel my resume is very well written ve been trying to make connections through my school alumni and linkedin but just not getting anywhere it took me awhile to write solid resume about weeks and know lot of people wouldn be hiring around the new year and mostly looking for remote job canada which makes it harder but this still seems incredibly slow not even looking for that much be happy with cad year just need enough to live in toronto should just keep at it mostly apply on indeed without cover letter if they ask me why want the job what makes me stand out have few little sentence write ups ready should change this approach could someone direct me to connection that could help me get job,1,trying to get an entry level data analyst scientist job and not getting anywhere,so on application and month of applying for an entry level data analyst scientist job and ve only gotten interview for part time business analyst job people on this subreddit have told me have the qualification bsc math phys double major experience a teaching assistant and research assistant knowledge of python sql power bi excel my resume is very well written ve been trying to make connection through my school alumnus and linkedin but just not getting anywhere it took me awhile to write solid resume about week and know lot of people wouldn be hiring around the new year and mostly looking for remote job canada which make it harder but this still seems incredibly slow not even looking for that much be happy with cad year just need enough to live in toronto should just keep at it mostly apply on indeed without cover letter if they ask me why want the job what make me stand out have few little sentence write ups ready should change this approach could someone direct me to connection that could help me get job
how to convert internship into job opportunity,tldr need do and donts in order to convert an internship into full time job hi so for context an undergrad final semester doing mathematics have done internships previously where mainly worked on web scrapping and then experimentation automation reporting landed new internship at young unicorn where the employer mentioned the scope of getting job offer at the end of the internship which would be based on performance so wanted to know what sort of things should shouldn do during this internship to be able to maximize the chances of converting it into full time job thanks lot in advance,1,how to convert internship into job opportunity,tldr need do and donts in order to convert an internship into full time job hi so for context an undergrad final semester doing mathematics have done internship previously where mainly worked on web scrapping and then experimentation automation reporting landed new internship at young unicorn where the employer mentioned the scope of getting job offer at the end of the internship which would be based on performance so wanted to know what sort of thing should shouldn do during this internship to be able to maximize the chance of converting it into full time job thanks lot in advance
how to convert an internship to job opportunity,tldr need do and donts in order to convert an internship into full time job hi so for context an undergrad final semester doing mathematics have done internships previously where mainly worked on web scrapping and then experimentation automation reporting landed new internship at young unicorn where the employer mentioned the scope of getting job offer at the end of the internship which would be based on performance so wanted to know what sort of things should shouldn do during this internship to be able to maximize the chances of converting it into full time job thanks lot in advance,1,how to convert an internship to job opportunity,tldr need do and donts in order to convert an internship into full time job hi so for context an undergrad final semester doing mathematics have done internship previously where mainly worked on web scrapping and then experimentation automation reporting landed new internship at young unicorn where the employer mentioned the scope of getting job offer at the end of the internship which would be based on performance so wanted to know what sort of thing should shouldn do during this internship to be able to maximize the chance of converting it into full time job thanks lot in advance
topological da for network neuroscience and clinical psychology material,please suggest to me some good places to learn tda from scratch amp x200b am interested in using topological da to study neurological conditions such as classifying autistic subjects from typically developing ones clustering students having anxiety assessing psychosis in community or detecting depression from audio clips using this method we can gain insights into the working of various brain regions for example how we encode our position in space how the motor cortex prepares and executes movement how to understand the representation of abstraction and generalization brain tda can apply this in researching how inhibitory and excitatory signals that originated in the brain are related it has already been used to signify how direction is perceived in our brain as we would imagine the data to be one dimensional dependent on one variable the research confirmed this since the topology is about how things are connected and where the gaps are tda is well suited for neuroscience especially analyses involving connectivity networks tda can capture global and higher dimensional features where other methods such as graph theory fail some pioneering work has shown that topological features extracted from eeg signals reveal relevant information for various neurological disorders,1,topological da for network neuroscience and clinical psychology material,please suggest to me some good place to learn tda from scratch amp x200b am interested in using topological da to study neurological condition such a classifying autistic subject from typically developing one clustering student having anxiety assessing psychosis in community or detecting depression from audio clip using this method we can gain insight into the working of various brain region for example how we encode our position in space how the motor cortex prepares and executes movement how to understand the representation of abstraction and generalization brain tda can apply this in researching how inhibitory and excitatory signal that originated in the brain are related it ha already been used to signify how direction is perceived in our brain a we would imagine the data to be one dimensional dependent on one variable the research confirmed this since the topology is about how thing are connected and where the gap are tda is well suited for neuroscience especially analysis involving connectivity network tda can capture global and higher dimensional feature where other method such a graph theory fail some pioneering work ha shown that topological feature extracted from eeg signal reveal relevant information for various neurological disorder
data experts are becoming football best signings,data experts are becoming football best signings,1,data expert are becoming football best signing,data expert are becoming football best signing
interview hour take home assignment for job lol,received an offer but had to do take home assignment and interviews with total of different team members to get ds job in scale up is this unusual,1,interview hour take home assignment for job lol,received an offer but had to do take home assignment and interview with total of different team member to get d job in scale up is this unusual
help me advice needed,can get good university in canada with cgpa of of doing masters in ds,1,help me advice needed,can get good university in canada with cgpa of of doing master in d
what is the default value for chunk size for gensim lda,how many documents does it process at once if you don pass the chunksize parameter sorry if this sounds like basic question but have searched and can find the answer,1,what is the default value for chunk size for gensim lda,how many document doe it process at once if you don pas the chunksize parameter sorry if this sound like basic question but have searched and can find the answer
interview advice please,hi everyone ve been working as data analyst for more than years now primarily working in sql and sometimes python for small data engineering tasks recently ve been exploring jobs in the data science realm have an interview coming up it hr technical interview and have no clue what to expect all they shared with me was that their ds role works in sql and python and after the technical round will be tested on product metrics any advice on typical ds interview question it the python part that getting me anxious but considering that the role sounds closer to product analyst job don expect anything more than the standard data libraries like numpy and pandas and perhaps some statistics and math questions as well any guesses or advice is welcome and please let me know of any good resources to study from thank you,1,interview advice please,hi everyone ve been working a data analyst for more than year now primarily working in sql and sometimes python for small data engineering task recently ve been exploring job in the data science realm have an interview coming up it hr technical interview and have no clue what to expect all they shared with me wa that their d role work in sql and python and after the technical round will be tested on product metric any advice on typical d interview question it the python part that getting me anxious but considering that the role sound closer to product analyst job don expect anything more than the standard data library like numpy and panda and perhaps some statistic and math question a well any guess or advice is welcome and please let me know of any good resource to study from thank you
aggregate presto data into list structure that can be exploded later in python,hi want to write presto query to summarize sales by city but need output limited to lt rows in order to analyze this dataset later want to write query in way that can include multiple values into one cell so can explode this specific cell in eg python can utilize some kind of array agg function or whatever to give you an idea of what trying to do have multiple columns that want to aggregate into one column name value product hairdryer is delivered true date would like to have these columns combined into one cell eg product hairdryer is delivered true date also what if some value is null want to rebuild original dataset in excel python out of these columns so can split this by specified delimiter thanks,1,aggregate presto data into list structure that can be exploded later in python,hi want to write presto query to summarize sale by city but need output limited to lt row in order to analyze this dataset later want to write query in way that can include multiple value into one cell so can explode this specific cell in eg python can utilize some kind of array agg function or whatever to give you an idea of what trying to do have multiple column that want to aggregate into one column name value product hairdryer is delivered true date would like to have these column combined into one cell eg product hairdryer is delivered true date also what if some value is null want to rebuild original dataset in excel python out of these column so can split this by specified delimiter thanks
any chemical engineers that found their way into data science or engineering,any chemical engineers that found their way into data science or engineering,1,any chemical engineer that found their way into data science or engineering,any chemical engineer that found their way into data science or engineering
how do you decide when model just isn going to work,feel like this is asking an artist how they know when they are finished with an art piece you just know but genuinely curious about this for example working on model that severely imbalanced at first was excited because the accuracy was great well duh because it could technically predict no and still be accurate but the confusion matrix precision and recall and roc score are all pretty terrible messed with the threshold and in order to get tp even close to correct have to lower it to which means get crazy amount of fp tried logistic regression decision tree and random forest as well as xgboost also adjusted parameters such as max depth etc on all of these and selected the parameters by creating list of the different auc results for example tried upsampling and downsampling and am fixing to try smote getting more data is out of the question when output the results on my test set either get yes or yes which is what brought me here feel like what trying to predict just can be predicted with the data we have problem is team of so have nobody to bounce this off of so how do you decide that project just probably isn going to work out,1,how do you decide when model just isn going to work,feel like this is asking an artist how they know when they are finished with an art piece you just know but genuinely curious about this for example working on model that severely imbalanced at first wa excited because the accuracy wa great well duh because it could technically predict no and still be accurate but the confusion matrix precision and recall and roc score are all pretty terrible messed with the threshold and in order to get tp even close to correct have to lower it to which mean get crazy amount of fp tried logistic regression decision tree and random forest a well a xgboost also adjusted parameter such a max depth etc on all of these and selected the parameter by creating list of the different auc result for example tried upsampling and downsampling and am fixing to try smote getting more data is out of the question when output the result on my test set either get yes or yes which is what brought me here feel like what trying to predict just can be predicted with the data we have problem is team of so have nobody to bounce this off of so how do you decide that project just probably isn going to work out
neural search library for medium sized corpora,cherche search in french allows you to create neural search pipeline using retrievers and pre trained language models as rankers cherche is meant to be used with small to medium sized corpora cherche main strength is its ability to build diverse and end to end pipelines format png amp auto webp amp ec2aec185017a2b34b02b45919838cdb7b6938,1,neural search library for medium sized corpus,cherche search in french allows you to create neural search pipeline using retriever and pre trained language model a ranker cherche is meant to be used with small to medium sized corpus cherche main strength is it ability to build diverse and end to end pipeline format png amp auto webp amp ec2aec185017a2b34b02b45919838cdb7b6938
advice on anxiety issues as coder and data analyst,am data analyst with less than year of experience ever since started working realized that my anxiety is very easily triggered and it is causing me issues professionally and in my own learning journey for example even while solving minor issues tend to get tunnel vision preventing me from analyzing all available info which leads to me asking for help from teammates unnecessarily this happens much more when working on new environments or tools when self study find myself filled with nervous energy with my brain jumping around causing me whole lot of panic and not lot of learning it also pops up when am trying to quickly process information or when am put under the spotlight once panic gets triggered lose focus and make ditzy errors have had these problems since forever but never really thought much about them just thought was dumb or something but feel am not dumb these traits are limiting me especially now when am trying to give my throughout the day following things have helped bit but still have long way to go taking mental break when start to panic and tunnel vision thinking about some random thread for while and coming back helps lot even if only momentary writing find describing the error and the coding have up to that point in writing usually helps center myself bit mindfulness taking moment to myself when start to feel like am losing it wanted to ask whether any of you have or do feel the same and what you all do about it,1,advice on anxiety issue a coder and data analyst,am data analyst with le than year of experience ever since started working realized that my anxiety is very easily triggered and it is causing me issue professionally and in my own learning journey for example even while solving minor issue tend to get tunnel vision preventing me from analyzing all available info which lead to me asking for help from teammate unnecessarily this happens much more when working on new environment or tool when self study find myself filled with nervous energy with my brain jumping around causing me whole lot of panic and not lot of learning it also pop up when am trying to quickly process information or when am put under the spotlight once panic get triggered lose focus and make ditzy error have had these problem since forever but never really thought much about them just thought wa dumb or something but feel am not dumb these trait are limiting me especially now when am trying to give my throughout the day following thing have helped bit but still have long way to go taking mental break when start to panic and tunnel vision thinking about some random thread for while and coming back help lot even if only momentary writing find describing the error and the coding have up to that point in writing usually help center myself bit mindfulness taking moment to myself when start to feel like am losing it wanted to ask whether any of you have or do feel the same and what you all do about it
what are the criteria needed to apply for ds job have no cs statistics background,my education background is in engineering and petroleum geology currently working in drilling service company planning to venture into ds but not sure if have the qualifications ve completed few of the ds courses in coursera and udacity with capstone projects will this be sufficient for me to apply for role in ds currently working on my webscraping skills with python hopefully can get my hands on some freelance project just to get some experience on the data collecting part of ds any advice are welcome,1,what are the criterion needed to apply for d job have no c statistic background,my education background is in engineering and petroleum geology currently working in drilling service company planning to venture into d but not sure if have the qualification ve completed few of the d course in coursera and udacity with capstone project will this be sufficient for me to apply for role in d currently working on my webscraping skill with python hopefully can get my hand on some freelance project just to get some experience on the data collecting part of d any advice are welcome
collatz conjecture visualized with python and turtle,amp x200b format png amp auto webp amp a00e8ebe2843fd83faee760059ebf129a433b1d,1,collatz conjecture visualized with python and turtle,amp x200b format png amp auto webp amp a00e8ebe2843fd83faee760059ebf129a433b1d
any suggestions for resume writing service specifically geared for data science,or will any generic one do,1,any suggestion for resume writing service specifically geared for data science,or will any generic one do
stylegan2 artificial wildlife interpolation video,stylegan2 artificial wildlife interpolation video,1,stylegan2 artificial wildlife interpolation video,stylegan2 artificial wildlife interpolation video
weekly entering amp transitioning thread jan jan,welcome to this week entering amp transitioning thread this thread is for any questions about getting started studying or transitioning into the data science field topics include learning resources books tutorials videos traditional education schools degrees electives alternative education online courses bootcamps job search questions resumes applying career prospects elementary questions where to start what next while you wait for answers from the community check out the faq and resources resources pages on our wiki you can also search for answers in past weekly threads restrict_sr amp sort new,1,weekly entering amp transitioning thread jan jan,welcome to this week entering amp transitioning thread this thread is for any question about getting started studying or transitioning into the data science field topic include learning resource book tutorial video traditional education school degree elective alternative education online course bootcamps job search question resume applying career prospect elementary question where to start what next while you wait for answer from the community check out the faq and resource resource page on our wiki you can also search for answer in past weekly thread restrict_sr amp sort new
how much senior executive earn in india in data science,how much senior executives like director vp earn in india in data science profile,1,how much senior executive earn in india in data science,how much senior executive like director vp earn in india in data science profile
in google file system hotspots haven been major issue because our applications mostly read large multi chunk files sequentially what it mean,context amp x200b see chunk size amp x200b hotspot region of computer program where high proportion of executed instructions occur lazy space allocation amp x200b with lazy space allocation the physical allocation of space is delayed as long as possible until data at the size of the chunk size in gfs case mb according the paper is accumulated large chunk size in gfs gt large chunk size even with lazy space allocation has its disadvantages gt small file consists of small number of chunks perhaps just one gt the chunkservers storing those chunks may become hot spots if many clients are accessing the same file gt in practice hotspots haven been major issue because our applications mostly read large multi chunk files sequentially don understand how hotspots are no issue when we read large multi chunk files sequentially they say hotspots are issue if clients are accessing same small file file of just chunk amp x200b lebfky7 lebfky7 it makes sense why chunkservers will be hotspot in this case as they will be active if they are being accessed by multiple clients but it absolutely doesn make sense when the research paper say in practice hotspots haven been major issue because our applications mostly read large multi chunk files sequentially what the difference if imagine scenario like above here file is made up of multiple chunks and rest is same what difference is made here,1,in google file system hotspot haven been major issue because our application mostly read large multi chunk file sequentially what it mean,context amp x200b see chunk size amp x200b hotspot region of computer program where high proportion of executed instruction occur lazy space allocation amp x200b with lazy space allocation the physical allocation of space is delayed a long a possible until data at the size of the chunk size in gfs case mb according the paper is accumulated large chunk size in gfs gt large chunk size even with lazy space allocation ha it disadvantage gt small file consists of small number of chunk perhaps just one gt the chunkservers storing those chunk may become hot spot if many client are accessing the same file gt in practice hotspot haven been major issue because our application mostly read large multi chunk file sequentially don understand how hotspot are no issue when we read large multi chunk file sequentially they say hotspot are issue if client are accessing same small file file of just chunk amp x200b lebfky7 lebfky7 it make sense why chunkservers will be hotspot in this case a they will be active if they are being accessed by multiple client but it absolutely doesn make sense when the research paper say in practice hotspot haven been major issue because our application mostly read large multi chunk file sequentially what the difference if imagine scenario like above here file is made up of multiple chunk and rest is same what difference is made here
suggestion statistics topics required and not required for data science,hi newbie data science enthusiast and my academic background is in computer science engineering however after reading few books like swt from_search true amp qid tvgernkaim amp rank ba from_search true amp qid vs1lxqnv1n amp rank and homl from_search true amp qid bx57cpoawi amp rank really interested to learn data science in detail thus over the next few months planning to learn statistical part of the data science thus looking for some suggestions about what are the good statistics books can add to my library isl esl from_search true amp qid whrqu2yk6u amp rank are already in my library multiple colleagues have told me mfml from_search true amp qid wikjbnuutk amp rank is great one stop book for majority of math required for ml and ds should this book suffice the need toc of mfml from_search true amp qid wikjbnuutk amp rank is available here statistics is huuuge all of statistics larry wasserman so any tips on topics from statistics not required for data science also if missing out on some key points to keep in mind before taking journey of learning statistics data science specific only please let me know,1,suggestion statistic topic required and not required for data science,hi newbie data science enthusiast and my academic background is in computer science engineering however after reading few book like swt from_search true amp qid tvgernkaim amp rank ba from_search true amp qid vs1lxqnv1n amp rank and homl from_search true amp qid bx57cpoawi amp rank really interested to learn data science in detail thus over the next few month planning to learn statistical part of the data science thus looking for some suggestion about what are the good statistic book can add to my library isl esl from_search true amp qid whrqu2yk6u amp rank are already in my library multiple colleague have told me mfml from_search true amp qid wikjbnuutk amp rank is great one stop book for majority of math required for ml and d should this book suffice the need toc of mfml from_search true amp qid wikjbnuutk amp rank is available here statistic is huuuge all of statistic larry wasserman so any tip on topic from statistic not required for data science also if missing out on some key point to keep in mind before taking journey of learning statistic data science specific only please let me know
why learn sql for ml tasks,should learn sql mainly do ml tasks and often the data is in the form of things like numpy arrays panda dataframes or pytorch dataloaders so why learn sql most internship job apps say you should have some sql experience,1,why learn sql for ml task,should learn sql mainly do ml task and often the data is in the form of thing like numpy array panda dataframes or pytorch dataloaders so why learn sql most internship job apps say you should have some sql experience
what are some things potential mistakes pitfalls for this synthetic data idea,task have to write regression model the inputs to the model are element vectors that are themselves derived from different sized cohorts the cohorts generally have individual users problem don have enough data have maybe of these cohorts potential solution once the cohorts come into our pipeline the individuals data are available including what cohort they came from each cohort is defined by parameters country age range gender income range so my idea is to take my entire data set of individual users slice the data along each of those parameter sets and then repeatedly sample that subset to generate my own synthetic cohorts potential problems see repeated sampling leads to overlapping cohorts which is not how the original data is ve crunched the numbers on this hypergeometric expected values and determined that can control the expected overlap ve arbitrarily chosen overlap to be acceptable the synthetic cohorts values might not accurately represent real cohorts for any number of unknown unknowns this could give my model high accuracy on the synthetic cohorts but poor results on future real cohorts due to the paucity of real data not entirely sure how to counteract this concern balancing the data set some cohorts are much bigger so those parameter restrictions can be sampled more leading to overrepresentation in the synthetic cohorts this is tedious but not impossible to overcome with computations what other pitfalls might run into here sure there will be questions or clarifications needed and happy to address those for anyone who wants to think about this,1,what are some thing potential mistake pitfall for this synthetic data idea,task have to write regression model the input to the model are element vector that are themselves derived from different sized cohort the cohort generally have individual user problem don have enough data have maybe of these cohort potential solution once the cohort come into our pipeline the individual data are available including what cohort they came from each cohort is defined by parameter country age range gender income range so my idea is to take my entire data set of individual user slice the data along each of those parameter set and then repeatedly sample that subset to generate my own synthetic cohort potential problem see repeated sampling lead to overlapping cohort which is not how the original data is ve crunched the number on this hypergeometric expected value and determined that can control the expected overlap ve arbitrarily chosen overlap to be acceptable the synthetic cohort value might not accurately represent real cohort for any number of unknown unknown this could give my model high accuracy on the synthetic cohort but poor result on future real cohort due to the paucity of real data not entirely sure how to counteract this concern balancing the data set some cohort are much bigger so those parameter restriction can be sampled more leading to overrepresentation in the synthetic cohort this is tedious but not impossible to overcome with computation what other pitfall might run into here sure there will be question or clarification needed and happy to address those for anyone who want to think about this
strange blue light ice discovered in arctic,strange blue light ice discovered in arctic,1,strange blue light ice discovered in arctic,strange blue light ice discovered in arctic
advise for sr digital advertising exec,dear all been lurker for more than year here to try to learn about this field have spent close to years in the digital advertising industry and now my role is leadership management to spectrum of services ranging from commerce platform strategy and data measurement analytics to fortune brands academically had ba in media studies and later mba in marketing problem statement while my experience has held my forte in the evolving nature of digital marketing advertising suite feel handicapped in the areas of software development side since did not have computer science background for ex what new product service do need to build from scratch say in adtech for my edtech brand can formulate the business objective and strategy but cannot be the product architect as am not from tech background while my tech teams do help in solutioning feel my lack of formal education in ds or cs is somewhat not helping me progress in my career or take control of the product as am responsible for pnl feel in few years time might get redundant given these days all projects in digital advertising are highly skewed towards to ml or ds when it comes to product wish to learn ds or cs from scratch and don mind an online degree to start off with be it bachelors or masters what would you recommend my current role looks like this keywords mindshare keywords mindshare,1,advise for sr digital advertising exec,dear all been lurker for more than year here to try to learn about this field have spent close to year in the digital advertising industry and now my role is leadership management to spectrum of service ranging from commerce platform strategy and data measurement analytics to fortune brand academically had ba in medium study and later mba in marketing problem statement while my experience ha held my forte in the evolving nature of digital marketing advertising suite feel handicapped in the area of software development side since did not have computer science background for ex what new product service do need to build from scratch say in adtech for my edtech brand can formulate the business objective and strategy but cannot be the product architect a am not from tech background while my tech team do help in solutioning feel my lack of formal education in d or c is somewhat not helping me progress in my career or take control of the product a am responsible for pnl feel in few year time might get redundant given these day all project in digital advertising are highly skewed towards to ml or d when it come to product wish to learn d or c from scratch and don mind an online degree to start off with be it bachelor or master what would you recommend my current role look like this keywords mindshare keywords mindshare
data analytics giant alteryx buys rival trifacta in deal,data analytics giant alteryx buys rival trifacta in deal,1,data analytics giant alteryx buy rival trifacta in deal,data analytics giant alteryx buy rival trifacta in deal
actuaries who transitioned into data science why,why did you decide to pursue data science instead of continuing your career as an actuary also did you find the transition easy how was your past experience exam progress viewed,1,actuary who transitioned into data science why,why did you decide to pursue data science instead of continuing your career a an actuary also did you find the transition easy how wa your past experience exam progress viewed
how important are previous year university grades to getting data science job,part vent part actual question so just recieved message after being told that was advancing to the rd round of interviews with company that was being disqualified because of my grades on the transcript for my bachelor of science in biology also have data science diploma but was told that submitting university transcripts was required for the application my diploma was bootcamp without grades but series of projects that have on my resume where was one of the top students in my cohort now ll admit my undergraduate grades are not great did not have the same work ethic back then and averaged about throughout my degree with some course retakes graduated years ago and have been working constantly since then in dead end field and hope to transition to data science as career change as realized should have pursued comp sci in university and ended up creating some rudimentary quality management databases at previous jobs at part of my role as quality assurance technician would hope my work experience and high scores on the assessments ve done so far in the interview process would have had greater impact on my value to the company but it seems like they don care because of frankly irrelevant previous grades also am very annoyed that this wasn screened out earlier and wasted my time doing tests for company that was never going to hire me in the first place yes interview experience is experience but other than basic math test and coding test it was nothing haven seen before,1,how important are previous year university grade to getting data science job,part vent part actual question so just recieved message after being told that wa advancing to the rd round of interview with company that wa being disqualified because of my grade on the transcript for my bachelor of science in biology also have data science diploma but wa told that submitting university transcript wa required for the application my diploma wa bootcamp without grade but series of project that have on my resume where wa one of the top student in my cohort now ll admit my undergraduate grade are not great did not have the same work ethic back then and averaged about throughout my degree with some course retake graduated year ago and have been working constantly since then in dead end field and hope to transition to data science a career change a realized should have pursued comp sci in university and ended up creating some rudimentary quality management database at previous job at part of my role a quality assurance technician would hope my work experience and high score on the assessment ve done so far in the interview process would have had greater impact on my value to the company but it seems like they don care because of frankly irrelevant previous grade also am very annoyed that this wasn screened out earlier and wasted my time doing test for company that wa never going to hire me in the first place yes interview experience is experience but other than basic math test and coding test it wa nothing haven seen before
data science books,am looking for recommendations on data science books that can read am learning python presently and will appreciate books that will make this easy for me,1,data science book,am looking for recommendation on data science book that can read am learning python presently and will appreciate book that will make this easy for me
need to run gensim lda for roughly days,have large binary file several gbs that need to run through gensim and my personal laptop crashes roughly hours after start am guessing its memory issue because my ram is mere gb considered google collab but even their pro plan disconnects after hours atleast that what read on couple of blogs what are my options,1,need to run gensim lda for roughly day,have large binary file several gb that need to run through gensim and my personal laptop crash roughly hour after start am guessing it memory issue because my ram is mere gb considered google collab but even their pro plan disconnect after hour atleast that what read on couple of blog what are my option
research experience,am currently in the process of hopefully getting an undergrad research assistant position was wondering how much employers would care if you had research assistant position,1,research experience,am currently in the process of hopefully getting an undergrad research assistant position wa wondering how much employer would care if you had research assistant position
what should do in college for career in data science,ll be starting college this year should go for the traditional cs major perhaps with minor in statistics math or even data science should go for double major major in statistics confused any suggestions would be great,1,what should do in college for career in data science,ll be starting college this year should go for the traditional c major perhaps with minor in statistic math or even data science should go for double major major in statistic confused any suggestion would be great
how do you keep up to date with emerging tech and methodologies,have data science degree but working as data engineer at the moment while kind of enjoy the work better than just data science don want to fall behind in keeping up to date with the latest advancements techniques and methods in the data science field what resources do you guys use to keep your knowledge up to date links or specific resources is highly appreciated,1,how do you keep up to date with emerging tech and methodology,have data science degree but working a data engineer at the moment while kind of enjoy the work better than just data science don want to fall behind in keeping up to date with the latest advancement technique and method in the data science field what resource do you guy use to keep your knowledge up to date link or specific resource is highly appreciated
mathematics of data science,currently on the learning journey of data science and wondered does every data scientist understand the math behind most learning algorithms do they all know up to calc you need to know linear algebra statistics and basic calculus but do have to learn calc and to be considered job ready,1,mathematics of data science,currently on the learning journey of data science and wondered doe every data scientist understand the math behind most learning algorithm do they all know up to calc you need to know linear algebra statistic and basic calculus but do have to learn calc and to be considered job ready
are there any online journals for data science that don have publishing fees,and aren predatory,1,are there any online journal for data science that don have publishing fee,and aren predatory
game changer for metaverse imagine being able to actually walk your avatar in the virtual world reconstructed from the physical world in this case university campus reconstructed using lidar,game changer for metaverse imagine being able to actually walk your avatar in the virtual world reconstructed from the physical world in this case university campus reconstructed using lidar,1,game changer for metaverse imagine being able to actually walk your avatar in the virtual world reconstructed from the physical world in this case university campus reconstructed using lidar,game changer for metaverse imagine being able to actually walk your avatar in the virtual world reconstructed from the physical world in this case university campus reconstructed using lidar
looking for individual real estate sale records,looking for datasets containing information on real estate sales for project working on does anyone know where to find individual sale data,1,looking for individual real estate sale record,looking for datasets containing information on real estate sale for project working on doe anyone know where to find individual sale data
forecasting sales,hello everybody need your advice please we research the sales of product eg coffee with all its subcategories pods instant etc in two quite similar markets eg australi and new zealand this year we want to model the nz coffee sales based on aus trends and findings to save time instead of researching the countries separately from an econometrics viewpoint what is the best solution know we can do simple univariate time series forecasting using previous years sales of nz do multivariate regression where is nz sales and one of the xs is aus coffee sales thanks,1,forecasting sale,hello everybody need your advice please we research the sale of product eg coffee with all it subcategories pod instant etc in two quite similar market eg australi and new zealand this year we want to model the nz coffee sale based on au trend and finding to save time instead of researching the country separately from an econometrics viewpoint what is the best solution know we can do simple univariate time series forecasting using previous year sale of nz do multivariate regression where is nz sale and one of the x is au coffee sale thanks
deep dive into health care companies transactions with doctors,over the past few months worked on web application that uses government datasets to display transactions between companies and doctors listing the drugs or devices that were involved my goal is to make this data more widespread and known to allow for more research and analysis into this important topic wrote more detailed article on medium if you have any interest in learning more camabeel where does your doctor take money from cc25996e38 the website is located at healthtrace io let me know what all think also please don scrape this is all public data and don create revenue from this website,1,deep dive into health care company transaction with doctor,over the past few month worked on web application that us government datasets to display transaction between company and doctor listing the drug or device that were involved my goal is to make this data more widespread and known to allow for more research and analysis into this important topic wrote more detailed article on medium if you have any interest in learning more camabeel where doe your doctor take money from cc25996e38 the website is located at healthtrace io let me know what all think also please don scrape this is all public data and don create revenue from this website
learning curve after cross validation am forced to use only the test set data for producing learning curve after having used cross validation randomsearch on the training data,hello everyone working on the diabetes test dataset for practice used the learning curve function from sklearn model selection to better evaluate my learning model after first evaluation with learning curve used cross validation to better tune the hyperparameters used in svr but only using my training set data at this point wanted to once again make use of the learning data on the whole dataset but was made to notice that was using cross validation based evaluation on data already cross validated what should do at this point should use only the test set for such evaluation as limited as it is or should somehow modify the code to somehow have only data from the training set used for fitting the model and only data from the test set used for testing it thanks in advance here the code used to graph the learning curve feel free to use it import numpy as np import matplotlib pyplot as plt from sklearn model_selection import learning_curve from sklearn model_selection import shufflesplit def plot_learning_curve model train_sizes np linspace ylim cv shufflesplit n_splits test_size train_sizes train_scores cv_scores learning_curve model cv cv n_jobs train_sizes train_sizes train_scores_mean np mean train_scores axis train_scores_std np std train_scores axis cv_scores_mean np mean cv_scores axis cv_scores_std np std cv_scores axis plt fill_between train_sizes train_scores_mean train_scores_std train_scores_mean train_scores_std alpha color plt fill_between train_sizes cv_scores_mean cv_scores_std cv_scores_mean cv_scores_std alpha color plt plot train_sizes train_scores_mean color label training score plt plot train_sizes cv_scores_mean color label cross validation score plt ylim ylim plt legend training set test set prop size return plt amp x200b one of the learning curve graphs calculated only with test set data so as you can see very limited in size format png amp auto webp amp d1ef369613db0fd890c00c33d63f866a5e2f625,1,learning curve after cross validation am forced to use only the test set data for producing learning curve after having used cross validation randomsearch on the training data,hello everyone working on the diabetes test dataset for practice used the learning curve function from sklearn model selection to better evaluate my learning model after first evaluation with learning curve used cross validation to better tune the hyperparameters used in svr but only using my training set data at this point wanted to once again make use of the learning data on the whole dataset but wa made to notice that wa using cross validation based evaluation on data already cross validated what should do at this point should use only the test set for such evaluation a limited a it is or should somehow modify the code to somehow have only data from the training set used for fitting the model and only data from the test set used for testing it thanks in advance here the code used to graph the learning curve feel free to use it import numpy a np import matplotlib pyplot a plt from sklearn model_selection import learning_curve from sklearn model_selection import shufflesplit def plot_learning_curve model train_sizes np linspace ylim cv shufflesplit n_splits test_size train_sizes train_scores cv_scores learning_curve model cv cv n_jobs train_sizes train_sizes train_scores_mean np mean train_scores axis train_scores_std np std train_scores axis cv_scores_mean np mean cv_scores axis cv_scores_std np std cv_scores axis plt fill_between train_sizes train_scores_mean train_scores_std train_scores_mean train_scores_std alpha color plt fill_between train_sizes cv_scores_mean cv_scores_std cv_scores_mean cv_scores_std alpha color plt plot train_sizes train_scores_mean color label training score plt plot train_sizes cv_scores_mean color label cross validation score plt ylim ylim plt legend training set test set prop size return plt amp x200b one of the learning curve graph calculated only with test set data so a you can see very limited in size format png amp auto webp amp d1ef369613db0fd890c00c33d63f866a5e2f625
can synthetic data really be used in machine learning,can synthetic data really be used in machine learning,1,can synthetic data really be used in machine learning,can synthetic data really be used in machine learning
help with getting started,hi guys so am trying to get into data analysis have little background in python visual basic and computational mathematics tools such as matlab all of those were just uni courses which were quite bad and easy so would call myself newbie in programming domain how do you recommend starting to learn data science is sql good start should take udemy or coursera course or are there good yt tutorials any recommendation would help thx guys my goal is to go into bi amp dwh stuff,1,help with getting started,hi guy so am trying to get into data analysis have little background in python visual basic and computational mathematics tool such a matlab all of those were just uni course which were quite bad and easy so would call myself newbie in programming domain how do you recommend starting to learn data science is sql good start should take udemy or coursera course or are there good yt tutorial any recommendation would help thx guy my goal is to go into bi amp dwh stuff
best os for data science analytics,hi all about to accept an analytics developer position at salesforce consulting company mainly using sql hadoop suite chrome and tie ins tableau and tableau crm with touch of python here and there to contrast in my current role went with windows mostly for excel macros and integration with explorer at the new company have the option to choose between windows and macos intel mac though fully versed in both os any thoughts on which would be most appropriate some of the companies we consult to might not use macs and likely have crm data in excel,1,best o for data science analytics,hi all about to accept an analytics developer position at salesforce consulting company mainly using sql hadoop suite chrome and tie in tableau and tableau crm with touch of python here and there to contrast in my current role went with window mostly for excel macro and integration with explorer at the new company have the option to choose between window and macos intel mac though fully versed in both o any thought on which would be most appropriate some of the company we consult to might not use mac and likely have crm data in excel
anyone data scientist at digital marketing agency,anyone have experience at this type of business if so what are some of the things you might do,1,anyone data scientist at digital marketing agency,anyone have experience at this type of business if so what are some of the thing you might do
student at uw madison majoring in economics and personal finance along with getting minor in data science is this good enough combination to apply for business and data analyst jobs post graduation already self studying more python and and considering doing data science masters,student at uw madison majoring in economics and personal finance along with getting minor in data science is this good enough combination to apply for business and data analyst jobs post graduation already self studying more python and and considering doing data science masters,1,student at uw madison majoring in economics and personal finance along with getting minor in data science is this good enough combination to apply for business and data analyst job post graduation already self studying more python and and considering doing data science master,student at uw madison majoring in economics and personal finance along with getting minor in data science is this good enough combination to apply for business and data analyst job post graduation already self studying more python and and considering doing data science master
excel pivot table for rows,trying to analyze some data with rows and only columns but inserting pivot table is taking minutes just to process and take me to the pivot table sheet is this normal or is my computer messed up have gb ram,1,excel pivot table for row,trying to analyze some data with row and only column but inserting pivot table is taking minute just to process and take me to the pivot table sheet is this normal or is my computer messed up have gb ram
what are people thoughts on having portfolio project where the end result isn necessarily impressive but the project shows that you have learned lot through the process of working on it,for context am in the final stages of wrapping up time series forecasting problem that would have had some pretty useful business utility didn know much about ts before starting it admittedly was pretty guilty of watching youtube videos of guys solving trivial ts problems and just assuming that an arima model can handle anything boy was wrong about half way through working on the problem came to the conclusion discovered that the predictions were not going to be that good and that the data was far too noisy to yield realistic results that being said pursued through and learned ton about the foundations of ts am by no means an expert definitely still learning but would say that now know what to look out for and can comfortably handle my own if was faced with similar problem again my plan is to put the project up on my github but word it in this is what have learned kind of way as well as pointing out the pitfalls and issues with trying to forecast noisy data am kind of worried that this might not look good in portfolio though hoping to get some thoughts from people that look at candidates or have been in similar situation,1,what are people thought on having portfolio project where the end result isn necessarily impressive but the project show that you have learned lot through the process of working on it,for context am in the final stage of wrapping up time series forecasting problem that would have had some pretty useful business utility didn know much about t before starting it admittedly wa pretty guilty of watching youtube video of guy solving trivial t problem and just assuming that an arima model can handle anything boy wa wrong about half way through working on the problem came to the conclusion discovered that the prediction were not going to be that good and that the data wa far too noisy to yield realistic result that being said pursued through and learned ton about the foundation of t am by no mean an expert definitely still learning but would say that now know what to look out for and can comfortably handle my own if wa faced with similar problem again my plan is to put the project up on my github but word it in this is what have learned kind of way a well a pointing out the pitfall and issue with trying to forecast noisy data am kind of worried that this might not look good in portfolio though hoping to get some thought from people that look at candidate or have been in similar situation
what are your favorite preferred job search platforms,what are your favorite preferred job search platforms,1,what are your favorite preferred job search platform,what are your favorite preferred job search platform
what the best major to pursue in order to get job as data scientist,was doing some browsing on indeed and noticed that lot of the data scientist jobs require either degree in statistics mathematics or computer science don know which one is the easiest but willing to start school online for whichever is better to get degree in or would it be better to have major in one and minor in the another know that just having degree isn going to guarantee me job when finished with school and that lot of jobs require years of experience but willing to take whatever steps there are to take and any advice is appreciated,1,what the best major to pursue in order to get job a data scientist,wa doing some browsing on indeed and noticed that lot of the data scientist job require either degree in statistic mathematics or computer science don know which one is the easiest but willing to start school online for whichever is better to get degree in or would it be better to have major in one and minor in the another know that just having degree isn going to guarantee me job when finished with school and that lot of job require year of experience but willing to take whatever step there are to take and any advice is appreciated
yolox emotion detection with facial recognition enroll now,yolox emotion detection with facial recognition enroll now,1,yolox emotion detection with facial recognition enroll now,yolox emotion detection with facial recognition enroll now
first data science job with focus around sap hana,landed datascience position but it centred around sap hana self datascience submitted an hour ago by tonosnaj freshly graduated from phd in mathematics eu based my research has nothing to do with ds stats just mathphys and quantum mechanics all theoretical ve always wanted to switch job into doing more exciting stuff like data science and ml ve applied everywhere and usually got rejections perhaps due to my non programming experience in my research and that the my topic do not match all except this local company who offered my junior position as far as understood it their service is centered around sap hana cursory search on sap gives the impression that once you re in sap you re stuck with sap with little room for expansion into other careers would this be good first career move the salary will be lesser than my current post doc salary but am sick of my research and academia,1,first data science job with focus around sap hana,landed datascience position but it centred around sap hana self datascience submitted an hour ago by tonosnaj freshly graduated from phd in mathematics eu based my research ha nothing to do with d stats just mathphys and quantum mechanic all theoretical ve always wanted to switch job into doing more exciting stuff like data science and ml ve applied everywhere and usually got rejection perhaps due to my non programming experience in my research and that the my topic do not match all except this local company who offered my junior position a far a understood it their service is centered around sap hana cursory search on sap give the impression that once you re in sap you re stuck with sap with little room for expansion into other career would this be good first career move the salary will be lesser than my current post doc salary but am sick of my research and academia
data science vs data analysis where is the difference,hi am cs student and recently started job as working student in data analysis obviously you often stumble across the terms data science and data analysis my question now is what is the difference between data science and data analysis and what is done in these two fields,1,data science v data analysis where is the difference,hi am c student and recently started job a working student in data analysis obviously you often stumble across the term data science and data analysis my question now is what is the difference between data science and data analysis and what is done in these two field
practice problems for probability and combinatorics,does anyone have any consistent resources they use to practice prob and combinatorics with solutions preferably thanks,1,practice problem for probability and combinatorics,doe anyone have any consistent resource they use to practice prob and combinatorics with solution preferably thanks
landed datascience position but it centred around sap hana,freshly graduated from phd in mathematics eu based my research has nothing to do with ds stats just mathphys and quantum mechanics all theoretical ve always wanted to switch job into doing more exciting stuff like data science and ml ve applied everywhere and usually got rejections perhaps due to my non programming experience in my research and that the my topic do not match all except this local company who offered my junior position as far as understood it their service is centered around sap hana cursory search on sap gives the impression that once you re in sap you re stuck with sap with little room for expansion into other careers would this be good first career move the salary will be lesser than my current post doc salary but am sick of my research and academia,1,landed datascience position but it centred around sap hana,freshly graduated from phd in mathematics eu based my research ha nothing to do with d stats just mathphys and quantum mechanic all theoretical ve always wanted to switch job into doing more exciting stuff like data science and ml ve applied everywhere and usually got rejection perhaps due to my non programming experience in my research and that the my topic do not match all except this local company who offered my junior position a far a understood it their service is centered around sap hana cursory search on sap give the impression that once you re in sap you re stuck with sap with little room for expansion into other career would this be good first career move the salary will be lesser than my current post doc salary but am sick of my research and academia
did anyone shift from data analyst to investment banking how did you do it how was your experience,did anyone shift from data analyst to investment banking how did you do it how was your experience,1,did anyone shift from data analyst to investment banking how did you do it how wa your experience,did anyone shift from data analyst to investment banking how did you do it how wa your experience
mozilla interview,hi all have an interview with mozilla next week for data scientist role wondering if any of you work have worked there and could give some insight on company culture what is the data science department like etc,1,mozilla interview,hi all have an interview with mozilla next week for data scientist role wondering if any of you work have worked there and could give some insight on company culture what is the data science department like etc
how should constant ad hoc requests be handled by data science team,in one of my old jobs found that it was hard to make progress on various projects when would receive ton of emails from various teams asking one off questions about data we were receiving it was one of the big reasons why switched into more engineering focused job do you think these requests should be handled by specialized team or should responsibility be shared across data science organization,1,how should constant ad hoc request be handled by data science team,in one of my old job found that it wa hard to make progress on various project when would receive ton of email from various team asking one off question about data we were receiving it wa one of the big reason why switched into more engineering focused job do you think these request should be handled by specialized team or should responsibility be shared across data science organization
the pymc devs have made their book available free online,the pymc devs have made their book available free online,1,the pymc devs have made their book available free online,the pymc devs have made their book available free online
cloud vendors certifications,anybody have cloud vendors certifications from like microsoft amazon google and snowflake not from udemy coursera etc just wondering the usefulness in job searching and getting pay increase this question might be too broad but asking in general,1,cloud vendor certification,anybody have cloud vendor certification from like microsoft amazon google and snowflake not from udemy coursera etc just wondering the usefulness in job searching and getting pay increase this question might be too broad but asking in general
does anyone else get imposter syndrome about their role vs the of data science,been thinking about this after couple of chats in this subreddit this week senior lead ds and would say of my job is pretty much analytics with spin wrangling data that pure sql analysts can get maybe performing hypothesis testing if it sampled once quarter there big ml project but even that is usually used for insight internal monitoring amp reporting think got into ds to build ml led software that changes experiences for millions of people after doing several interviews recently ve kinda realised barely anyone is doing that hence why calling it the obviously selection bias here with who interviewing with but tried to select across the spectrum of startup to big tech to fortune corporate on paper probably in the truly big data big tech cloud infra read academic papers on nns to keep up to date do get to play with ml however don feel like in whatever people decided was the sexiest job of the st century love it don get me wrong but feel like most people are doing analytics with flavour what do you all reckon am interviewing in the wrong places am talking rubbish,1,doe anyone else get imposter syndrome about their role v the of data science,been thinking about this after couple of chat in this subreddit this week senior lead d and would say of my job is pretty much analytics with spin wrangling data that pure sql analyst can get maybe performing hypothesis testing if it sampled once quarter there big ml project but even that is usually used for insight internal monitoring amp reporting think got into d to build ml led software that change experience for million of people after doing several interview recently ve kinda realised barely anyone is doing that hence why calling it the obviously selection bias here with who interviewing with but tried to select across the spectrum of startup to big tech to fortune corporate on paper probably in the truly big data big tech cloud infra read academic paper on nns to keep up to date do get to play with ml however don feel like in whatever people decided wa the sexiest job of the st century love it don get me wrong but feel like most people are doing analytics with flavour what do you all reckon am interviewing in the wrong place am talking rubbish
kindly help diagnose my resume have gone through an incredible number of internship applications with no success and cannot seem to decipher why do think that the credentials are there entirely unable to translate it effectively,kindly help diagnose my resume have gone through an incredible number of internship applications with no success and cannot seem to decipher why do think that the credentials are there entirely unable to translate it effectively,1,kindly help diagnose my resume have gone through an incredible number of internship application with no success and cannot seem to decipher why do think that the credential are there entirely unable to translate it effectively,kindly help diagnose my resume have gone through an incredible number of internship application with no success and cannot seem to decipher why do think that the credential are there entirely unable to translate it effectively
the iea wants to make their data available to the public now it is on governments of the world rich countries to make this happen,the iea wants to make their data available to the public now it is on governments of the world rich countries to make this happen,1,the iea want to make their data available to the public now it is on government of the world rich country to make this happen,the iea want to make their data available to the public now it is on government of the world rich country to make this happen
need advice for my data science path,hi all currently in my rd year of college in the netherlands doing data science internship at an accounting firm my internship exists out of machine learning work and have to admit that not really into that don like building models anymore such as last year think it is overrated in the last months got messages from recruiters from uber and amazon for data science internship for uber received no invite but only an assignment and never heard from them again and from amazon got an invite and had my first round this week the role for amazon was for business analysis role but due to my current work and skillset for my internship wasn able to answer some of the questions they asked currently don know which path would like to take in the data science field would like to do some technical stuff like data engineering because like programming but on the other hand think would like to be product data analyst and need to work on my current skillset because received messages for that role and like analysing data any advice on which skillset should focus on and which roles should focus on that are entry level roles thank you in advance,1,need advice for my data science path,hi all currently in my rd year of college in the netherlands doing data science internship at an accounting firm my internship exists out of machine learning work and have to admit that not really into that don like building model anymore such a last year think it is overrated in the last month got message from recruiter from uber and amazon for data science internship for uber received no invite but only an assignment and never heard from them again and from amazon got an invite and had my first round this week the role for amazon wa for business analysis role but due to my current work and skillset for my internship wasn able to answer some of the question they asked currently don know which path would like to take in the data science field would like to do some technical stuff like data engineering because like programming but on the other hand think would like to be product data analyst and need to work on my current skillset because received message for that role and like analysing data any advice on which skillset should focus on and which role should focus on that are entry level role thank you in advance
creating knowledge graphs using deep learning and rule based methods,creating knowledge graphs using deep learning and rule based methods,1,creating knowledge graph using deep learning and rule based method,creating knowledge graph using deep learning and rule based method
practical data science resources,so am looking for few resources which bridge the gap between academic data science and industrial data science found almost every course to be more theoretical with implementation in programming language but am looking for source that highlights the mistakes that we do which go unnoticed due to our lack of knowledge,1,practical data science resource,so am looking for few resource which bridge the gap between academic data science and industrial data science found almost every course to be more theoretical with implementation in programming language but am looking for source that highlight the mistake that we do which go unnoticed due to our lack of knowledge
open sourced all my meals,open sourced all my meals,1,open sourced all my meal,open sourced all my meal
best tool to collect data that can auto update within dashboard,hi all wondering what useful software platform options there are out there that would do the following tasks for efficiently currently collect data periodically from numerous stakeholders using survey ms forms or qualtrix survey the data gets collated and then put into dashboard for quarterly reporting in excel the method ve been using is clunky and involves sending out surveys for people to complete which are collated at set deadline for an excel dashboard like to use something web based where people can log into portal and submit data that goes into dashboard in live fashion so can report whenever want based on most recent data they can also see their own history of data submissions ideally wondered what platforms would serve this purpose,1,best tool to collect data that can auto update within dashboard,hi all wondering what useful software platform option there are out there that would do the following task for efficiently currently collect data periodically from numerous stakeholder using survey m form or qualtrix survey the data get collated and then put into dashboard for quarterly reporting in excel the method ve been using is clunky and involves sending out survey for people to complete which are collated at set deadline for an excel dashboard like to use something web based where people can log into portal and submit data that go into dashboard in live fashion so can report whenever want based on most recent data they can also see their own history of data submission ideally wondered what platform would serve this purpose
object oriented programming,hi everyone im in my first year of my bachelor data science and ve had few courses about programming it was mostly basic stuff and some numpy and panda but also some object oriented programming for me oop was the most difficult to get used to so kinda understand the basics but if it gets more complex get lost also most of the object we built were easily repeatable in normal python code so my question is how much do you use oop in data science,1,object oriented programming,hi everyone im in my first year of my bachelor data science and ve had few course about programming it wa mostly basic stuff and some numpy and panda but also some object oriented programming for me oop wa the most difficult to get used to so kinda understand the basic but if it get more complex get lost also most of the object we built were easily repeatable in normal python code so my question is how much do you use oop in data science
how to create your own dataset in roboflow,how to create your own dataset in roboflow,1,how to create your own dataset in roboflow,how to create your own dataset in roboflow
advice as an applied physics senior,almost done with my ba in applied physics along the way ve touched bit of python used once and used lot of excel in lab work what would anyone think my next steps are to get closer to landing at minimum an entry level data science job ve heard lot about bootcamps and certificates from google ibm but ve always thought experience is better would maybe pursuing msda be worth it another thing ve posted on other subreddits about advice for pursuing possible careers know how marketable degree in physics can be and not quite ready to put all my eggs in one basket quite yet ve nailed it down to between data or medical physics leaning towards data because starting to get sick and tired of deriving equations always enjoyed the analysis that comes after work in labs amp x200b any advice would be appreciated,1,advice a an applied physic senior,almost done with my ba in applied physic along the way ve touched bit of python used once and used lot of excel in lab work what would anyone think my next step are to get closer to landing at minimum an entry level data science job ve heard lot about bootcamps and certificate from google ibm but ve always thought experience is better would maybe pursuing msda be worth it another thing ve posted on other subreddits about advice for pursuing possible career know how marketable degree in physic can be and not quite ready to put all my egg in one basket quite yet ve nailed it down to between data or medical physic leaning towards data because starting to get sick and tired of deriving equation always enjoyed the analysis that come after work in lab amp x200b any advice would be appreciated
need to become good at data analysis good online resources,im manufacturing engineer working in automotive and due to position changes need to become really good at data analysis and data visualization am already six sigma black belt but to be honest needed lot of help to achieve that back in the day and now am kind of dusty when it comes to statistics what online resources would you recommend for me to get the hang of it and apply that knowledge to my job any help is very much appreciated,1,need to become good at data analysis good online resource,im manufacturing engineer working in automotive and due to position change need to become really good at data analysis and data visualization am already six sigma black belt but to be honest needed lot of help to achieve that back in the day and now am kind of dusty when it come to statistic what online resource would you recommend for me to get the hang of it and apply that knowledge to my job any help is very much appreciated
what is the best format that you used to land your current job,what is the best format that you used to land your current job,1,what is the best format that you used to land your current job,what is the best format that you used to land your current job
what are some fields or subjects where operations research amp ml overlap,notice lot of people get their phds in or and go work in ds at big company like google still don really understand the explicit intersection of these two skill sets though it makes sense to me that ml ds skills can allow you to draw conclusions from data that can then be fed into decision making models but are there any fields where these two fields directly intersect why do companies seem to value this combination,1,what are some field or subject where operation research amp ml overlap,notice lot of people get their phd in or and go work in d at big company like google still don really understand the explicit intersection of these two skill set though it make sense to me that ml d skill can allow you to draw conclusion from data that can then be fed into decision making model but are there any field where these two field directly intersect why do company seem to value this combination
beginner can come up with ideas for data science portfolio,graduated college several months ago with relevant degree and haven been able to land data science job or anything related suspect one of the problems is my portfolio have few things in it but not proud of them and they re just about simple data exploration visualization ever since graduation ve been trying to come up with cool data science projects to build my portfolio search for datasets on kaggle and elsewhere but can think of interesting things to do with them or come up with an idea that been beaten to death already for example music recommender based on mood spotify already did it movie recommender based on mood movie you want to see similar one of overdone hobby recommender no decent dataset for it no relevant website to scrape to make my own can talk to an interviewer about most of these confidently since everyone and their mom has already done them maybe if there was way to spice them up add unique feature etc but guess lack creativity can think of anything worth doing any advice apologies if seem dense wish had some sort of mentor guidance because afraid that it my way of thinking that the problem and need to be shown the way,1,beginner can come up with idea for data science portfolio,graduated college several month ago with relevant degree and haven been able to land data science job or anything related suspect one of the problem is my portfolio have few thing in it but not proud of them and they re just about simple data exploration visualization ever since graduation ve been trying to come up with cool data science project to build my portfolio search for datasets on kaggle and elsewhere but can think of interesting thing to do with them or come up with an idea that been beaten to death already for example music recommender based on mood spotify already did it movie recommender based on mood movie you want to see similar one of overdone hobby recommender no decent dataset for it no relevant website to scrape to make my own can talk to an interviewer about most of these confidently since everyone and their mom ha already done them maybe if there wa way to spice them up add unique feature etc but guess lack creativity can think of anything worth doing any advice apology if seem dense wish had some sort of mentor guidance because afraid that it my way of thinking that the problem and need to be shown the way
it finally happened was asked brain teaser for my interview,kept hearing about these questions getting asked for data science interviews but never experienced myself didn want to lol but now finally have was quite happy with live coding statistics ml business case studies what is the point of the brain teaser honestly how can you assess candidate this way or are you more interested in how they behave when you ask this question am more surprised was asked this brain teaser and barely any questions about my statistics ml knowledge maybe was interviewed for shits and giggles maybe this belongs in recruitinghell,1,it finally happened wa asked brain teaser for my interview,kept hearing about these question getting asked for data science interview but never experienced myself didn want to lol but now finally have wa quite happy with live coding statistic ml business case study what is the point of the brain teaser honestly how can you ass candidate this way or are you more interested in how they behave when you ask this question am more surprised wa asked this brain teaser and barely any question about my statistic ml knowledge maybe wa interviewed for shit and giggle maybe this belongs in recruitinghell
data pipeline alternatives to database table reinsertion for large post processed dataset,have dataset of millions of transactions that am pulling from multiple data tables handle the data query merge cleaning amp processing in python when run ml script now also need way to have the processed data accessible for bi dashboard the direct way can think of is inserting the data back into data table but wondering if there are better ways to do this not sure whether dashboards like tableau can be set up to read in pickle file from remote server directory but if it possible this would be easy since my current process exports the data to pickle file not data engineer by trade but my hunch was there should also be way to run the python data processing code in data pipeline that would then store the data in data table that is accessible to my ml code or the dashboard connection the benefit being wouldn have to run my ml script to get the processed data,1,data pipeline alternative to database table reinsertion for large post processed dataset,have dataset of million of transaction that am pulling from multiple data table handle the data query merge cleaning amp processing in python when run ml script now also need way to have the processed data accessible for bi dashboard the direct way can think of is inserting the data back into data table but wondering if there are better way to do this not sure whether dashboard like tableau can be set up to read in pickle file from remote server directory but if it possible this would be easy since my current process export the data to pickle file not data engineer by trade but my hunch wa there should also be way to run the python data processing code in data pipeline that would then store the data in data table that is accessible to my ml code or the dashboard connection the benefit being wouldn have to run my ml script to get the processed data
apd crs cure rate survival analysis in python,traditional survival analysis which traces its origins to medical studies estimates the time to event after diagnosis while assuming all individuals undergo the event however in practice some individuals are cured and therefore never experience the event allowing the possibility that some individuals may be cured while estimating the time to event is called cure rate survival analysis fortunately cure rate survival analysis also finds many uses in business not every user clicks on an ad nor does every machine fail nor does every customer churn at least in the short term so your model shouldn assume this as value add incorporating probability of cure with time to event model we better understand consumer machine lifetimes which leads to reduced costs and higher retention while the python ecosystem has many excellent packages for traditional survival analysis apd crs is at least to our knowledge the first open source python package for cure rate survival analysis it uses techniques from learning from positive and unlabeled data classification pu learning to estimate cure labels apd crs is easy to download comes with complete hands on tutorials and docs as member of the data science practice at aimpoint digital am happy to field any questions,1,apd cr cure rate survival analysis in python,traditional survival analysis which trace it origin to medical study estimate the time to event after diagnosis while assuming all individual undergo the event however in practice some individual are cured and therefore never experience the event allowing the possibility that some individual may be cured while estimating the time to event is called cure rate survival analysis fortunately cure rate survival analysis also find many us in business not every user click on an ad nor doe every machine fail nor doe every customer churn at least in the short term so your model shouldn assume this a value add incorporating probability of cure with time to event model we better understand consumer machine lifetime which lead to reduced cost and higher retention while the python ecosystem ha many excellent package for traditional survival analysis apd cr is at least to our knowledge the first open source python package for cure rate survival analysis it us technique from learning from positive and unlabeled data classification pu learning to estimate cure label apd cr is easy to download come with complete hand on tutorial and doc a member of the data science practice at aimpoint digital am happy to field any question
finding airbnb hosts that are companies by analyzing the host about page,hi all have dataset with airbnb listings of listings want to find out which of these listings are managed by companies of people for now know for fact that are run by companies and have their about pages what want to do is to analyze the companies host about pages find what is similar and then compare other host about pages to theirs to find if they are more similar or not kinda like fuzzy matching but more powerful how can do that,1,finding airbnb host that are company by analyzing the host about page,hi all have dataset with airbnb listing of listing want to find out which of these listing are managed by company of people for now know for fact that are run by company and have their about page what want to do is to analyze the company host about page find what is similar and then compare other host about page to theirs to find if they are more similar or not kinda like fuzzy matching but more powerful how can do that
so difficult to break into this industry,graduated with my masters in data analytics engineering last spring my masters was almost entirely online due to covid so did not get the chance to network and talk to people granted not very social so even if it was in person may still have not had any connections don know anybody in the field and have not had any mentors to help with what to do for next step work at credit union that recently started developing their data team but when tried reaching out about assisting or even just shadowing them got nothing back from the head of the department had interviewed for position for them back at the start of my masters and so didn have experience with azure or power bi which is what they were asking for at the time for reference what do now is not related at all to data science in customer service role which good lord want to get out of it ends up being that get so mentally exhausted from socializing with customers all day pretty heavy introvert intj for any mbti nerds out there that end up too feeling too exhausted to practice hours of leetcode and work on big projects if there is glaring issue have please let me know believe take criticism fairly well ideally would love to have very strong foundation with an extra skill that would not normally be taught in schools for data science but ends up being used lot,1,so difficult to break into this industry,graduated with my master in data analytics engineering last spring my master wa almost entirely online due to covid so did not get the chance to network and talk to people granted not very social so even if it wa in person may still have not had any connection don know anybody in the field and have not had any mentor to help with what to do for next step work at credit union that recently started developing their data team but when tried reaching out about assisting or even just shadowing them got nothing back from the head of the department had interviewed for position for them back at the start of my master and so didn have experience with azure or power bi which is what they were asking for at the time for reference what do now is not related at all to data science in customer service role which good lord want to get out of it end up being that get so mentally exhausted from socializing with customer all day pretty heavy introvert intj for any mbti nerd out there that end up too feeling too exhausted to practice hour of leetcode and work on big project if there is glaring issue have please let me know believe take criticism fairly well ideally would love to have very strong foundation with an extra skill that would not normally be taught in school for data science but end up being used lot
hello all,am year old that didn get the grades in high school to get into computer science degree but am currently taking finance program like to learn data science on the side amp was wondering if anyone could give me insight into what steps they would take if they were to learn all over again,1,hello all,am year old that didn get the grade in high school to get into computer science degree but am currently taking finance program like to learn data science on the side amp wa wondering if anyone could give me insight into what step they would take if they were to learn all over again
afraid of freelancing data analysis jobs any advice to get started,hello everyone as the title says am afraid of pursuing freelancing jobs because don really know what clients want saw some data analysts cleaning data and visualizing it after extracting insights and patterns from it so guess they are using python power bi tableau to do that what else do data analysts do and how to know that am ready for project freelancing job without butchering it in the end or not being able to complete it,1,afraid of freelancing data analysis job any advice to get started,hello everyone a the title say am afraid of pursuing freelancing job because don really know what client want saw some data analyst cleaning data and visualizing it after extracting insight and pattern from it so guess they are using python power bi tableau to do that what else do data analyst do and how to know that am ready for project freelancing job without butchering it in the end or not being able to complete it
is my finance phd degree plus for landing job in ds,phd candidate in finance and am looking to finish my degree this year for multiple reasons have decided to quit academy and want to look for job in data science know there are couple things need to pick up so planning on learning python and machine learning and work on my own project but concerned about how competitive will be for the job market after doing these studies have experience working with data as the analyses in my dissertation involves data cleaning and working with large datasets millions of observations however didn use software like python sql which know are very commonly used in the industry and the statistical techniques are little different too for example haven used any machine learning models in my dissertation academic research in the social science area tend to focus on iv regression natural experiments etc so guess my question is given that many data science employers are looking for people with experience does my research experience qualify as such once ve learned the technical skills and worked on few of my own projects is it realistic to aim for data science job right off the bat or am more likely have to start with data analyst job thanks in advance for any help and comments,1,is my finance phd degree plus for landing job in d,phd candidate in finance and am looking to finish my degree this year for multiple reason have decided to quit academy and want to look for job in data science know there are couple thing need to pick up so planning on learning python and machine learning and work on my own project but concerned about how competitive will be for the job market after doing these study have experience working with data a the analysis in my dissertation involves data cleaning and working with large datasets million of observation however didn use software like python sql which know are very commonly used in the industry and the statistical technique are little different too for example haven used any machine learning model in my dissertation academic research in the social science area tend to focus on iv regression natural experiment etc so guess my question is given that many data science employer are looking for people with experience doe my research experience qualify a such once ve learned the technical skill and worked on few of my own project is it realistic to aim for data science job right off the bat or am more likely have to start with data analyst job thanks in advance for any help and comment
tips on cleaning data using python and excel,was wondering what professional data analysts do when preparing data and cleaning it here is what currently do remove duplicates handle null values melt using pandas to make the table cleaner spot outliers and handle them what more can do also watched someone remove some fields columns because they weren important enough to include how do people determine what important to keep and what not,1,tip on cleaning data using python and excel,wa wondering what professional data analyst do when preparing data and cleaning it here is what currently do remove duplicate handle null value melt using panda to make the table cleaner spot outlier and handle them what more can do also watched someone remove some field column because they weren important enough to include how do people determine what important to keep and what not
can someone help me understand these courses and what is the difference,considering few data engineering courses and only get to pick trying to understand what the biggest difference between the courses are are they just different skillsets data engineer would do given their industry bad example but like supervised learning vs unsupervised learning or is it more of different type of data engineering specialities person that focuses on ml vs analytics so as pick which classes to take and where currently work big tech how do know which one is most applicable amp x200b analytics application engineering this course covers programming components essential to the development of analytics applications the focus is analytics software engineering students learn to develop desktop and client server solutions they learn about web based solutions employing variety of front end and back end system components the course introduces machine learning operations and engineering students use cloud systems to package and distribute containerized computer software they develop software working on open source programming database and systems integration projects they employ best practices in software development analytics systems engineering this course introduces design principles and best practices for implementing large scale systems for data ingestion processing storage and analytics students learn about cloud based computer architecture and scalable systems for data science they evaluate performance and resource utilization in batch interactive and streaming environments students review protocols for application programming interfaces they compare data models resource requirements and performance of applications implemented with relational versus graph database systems amp x200b real time interactive processing and analytics this course introduces application engineering and analytics within an integrated environment and full stack development process students implement client side web based applications using model view controller framework they use server side systems for responding to website requests and database queries they prepare indices for efficient relevant search across large document collections they find information in databases and document collections make service and product recommendations and detect anomalies or security violations real time stream processing this course introduces application engineering and analytics within stream and event processing environments students learn how to work with various data feeds and sources including electronic sensors monitoring continuous processes observing communication traffic and social interaction and tracking goods through production and distribution students implement stream processing solutions providing high throughput and low latency they use relational and graph databases they analyze event logs and business processes this is case study and project based course with strong programming component,1,can someone help me understand these course and what is the difference,considering few data engineering course and only get to pick trying to understand what the biggest difference between the course are are they just different skillsets data engineer would do given their industry bad example but like supervised learning v unsupervised learning or is it more of different type of data engineering speciality person that focus on ml v analytics so a pick which class to take and where currently work big tech how do know which one is most applicable amp x200b analytics application engineering this course cover programming component essential to the development of analytics application the focus is analytics software engineering student learn to develop desktop and client server solution they learn about web based solution employing variety of front end and back end system component the course introduces machine learning operation and engineering student use cloud system to package and distribute containerized computer software they develop software working on open source programming database and system integration project they employ best practice in software development analytics system engineering this course introduces design principle and best practice for implementing large scale system for data ingestion processing storage and analytics student learn about cloud based computer architecture and scalable system for data science they evaluate performance and resource utilization in batch interactive and streaming environment student review protocol for application programming interface they compare data model resource requirement and performance of application implemented with relational versus graph database system amp x200b real time interactive processing and analytics this course introduces application engineering and analytics within an integrated environment and full stack development process student implement client side web based application using model view controller framework they use server side system for responding to website request and database query they prepare index for efficient relevant search across large document collection they find information in database and document collection make service and product recommendation and detect anomaly or security violation real time stream processing this course introduces application engineering and analytics within stream and event processing environment student learn how to work with various data feed and source including electronic sensor monitoring continuous process observing communication traffic and social interaction and tracking good through production and distribution student implement stream processing solution providing high throughput and low latency they use relational and graph database they analyze event log and business process this is case study and project based course with strong programming component
global carbon emissions from to the richest were responsible for of emissions,global carbon emissions from to the richest were responsible for of emissions,1,global carbon emission from to the richest were responsible for of emission,global carbon emission from to the richest were responsible for of emission
used python to make dot density map with dot per person for the us decennial censuses from to this is over billion points the result is an amazing way to visualize population and demographic changes over the last years wanted to share the code process used,hey all wanted to share dot density project worked on recently hoping the code can be helpful for others and the maps fun to explore ve been huge fan of dot density maps since saw many years ago now the new york times and university of virginia ones for the census xkcd has great one for the election know it not always the right visualization choice but for certain types of data find it unmatched in how intuitive it is knew the census data was coming out and thought it could be really cool to make dot density data set for multiple census years as way to visualize city and neighborhood changes over time here the final dashboard used python pandas geopandas and shapely to take the census blockgroup polygons and population counts and generate the points the notebooks can be found here n4y xfp usp sharing scrollto zcxbx907hqjj scrollto zcxbx907hqjj xnvs8cymdmyvsbeb64oor6u authuser scrollto b8hthvkh8ljs scrollto b8hthvkh8ljs the core functions for the points creation comes from andrew guidus post visualizing population distributions with dot density maps seed randomstate seed if seed else randomstate seed def gen_random_points_poly poly num_points returns list of randomly generated points within polygon min_x min_y max_x max_y poly bounds points while len points lt num_points random_point point uniform min_x max_x uniform min_y max_y if random_point within poly points append random_point return points def gen_points_in_gdf_polys geometry values points_per_value none take geoseries of polygons along with series of values and returns randomly generated points within these polygons optionally takes points_per_value integer which indicates the number of points that should be generated for each value if points_per_value new_values values points_per_value astype int else new_values values new_values new_values new_values gt if new_values size gt gpd geodataframe data vals new_values geometry geometry apply lambda row tuple gen_random_points_poly row geometry row vals gpd geoseries apply pd series stack crs geometry crs name geometry return wrote about the process in this blog post not trying to make this promotional only post for my employer hoping this code can help others to create similar maps do have to mention that omnisci server side rendering use of gpus makes it possible to have fast dashboard with over billion points don know of other solutions that can do this but you could certainly use the code here to generate smaller dataset either by using smaller area or using more than point per person in many cases it cartographically better to use more than one point per person check out the dashboard and code and let me know if you have any comments or feedback,1,used python to make dot density map with dot per person for the u decennial census from to this is over billion point the result is an amazing way to visualize population and demographic change over the last year wanted to share the code process used,hey all wanted to share dot density project worked on recently hoping the code can be helpful for others and the map fun to explore ve been huge fan of dot density map since saw many year ago now the new york time and university of virginia one for the census xkcd ha great one for the election know it not always the right visualization choice but for certain type of data find it unmatched in how intuitive it is knew the census data wa coming out and thought it could be really cool to make dot density data set for multiple census year a way to visualize city and neighborhood change over time here the final dashboard used python panda geopandas and shapely to take the census blockgroup polygon and population count and generate the point the notebook can be found here n4y xfp usp sharing scrollto zcxbx907hqjj scrollto zcxbx907hqjj xnvs8cymdmyvsbeb64oor6u authuser scrollto b8hthvkh8ljs scrollto b8hthvkh8ljs the core function for the point creation come from andrew guidus post visualizing population distribution with dot density map seed randomstate seed if seed else randomstate seed def gen_random_points_poly poly num_points return list of randomly generated point within polygon min_x min_y max_x max_y poly bound point while len point lt num_points random_point point uniform min_x max_x uniform min_y max_y if random_point within poly point append random_point return point def gen_points_in_gdf_polys geometry value points_per_value none take geoseries of polygon along with series of value and return randomly generated point within these polygon optionally take points_per_value integer which indicates the number of point that should be generated for each value if points_per_value new_values value points_per_value astype int else new_values value new_values new_values new_values gt if new_values size gt gpd geodataframe data vals new_values geometry geometry apply lambda row tuple gen_random_points_poly row geometry row vals gpd geoseries apply pd series stack cr geometry cr name geometry return wrote about the process in this blog post not trying to make this promotional only post for my employer hoping this code can help others to create similar map do have to mention that omnisci server side rendering use of gpus make it possible to have fast dashboard with over billion point don know of other solution that can do this but you could certainly use the code here to generate smaller dataset either by using smaller area or using more than point per person in many case it cartographically better to use more than one point per person check out the dashboard and code and let me know if you have any comment or feedback
time series problem,assess whether price trend from seasonal decompose measure has accuracy in predicting binary target variable for future price direction over days or months time window am not sure what this means and how to do it what should be my approach,1,time series problem,ass whether price trend from seasonal decompose measure ha accuracy in predicting binary target variable for future price direction over day or month time window am not sure what this mean and how to do it what should be my approach
job hunting observation area of expertise shifts depending on the size of the company,to be specific in the course of my interviews with different sized firms smaller companies expect generalists know bit of everything such as bit of data engineering data science and software development for example if the data scientist reports to engineering then they re more likely to do data engineering analytics engineering and less data science role if the data scientist reports to business product then they re more likely to do data analytics dashboarding role however in both cases because either the data science department doesn exist or because they have an immature data team there little room to explore more data science activities such as experimentation applied machine learning or amp being specialist puts one in disadvantage because it takes while to acquire all those additional skills larger companies expect specialists must be domain expert for the role one is applying to for example technical questions revolve around depth of understanding of the mathematics algorithms for testing or machine learning models clearer path for amp and specialization but being pigeonholed is definitely concern makes it harder to look for other jobs as you are too specialized in field and focused on specific skillset tough for generalists to enter because they need time to learn certain concepts more in depth and lack of deep experience in the field may hinder one application caveat of course this is all based on answers by the interviewer when asked how their data science team is structured so take my observation with grain of salt given your work environment do these observations reflect the state of your current organization,1,job hunting observation area of expertise shift depending on the size of the company,to be specific in the course of my interview with different sized firm smaller company expect generalist know bit of everything such a bit of data engineering data science and software development for example if the data scientist report to engineering then they re more likely to do data engineering analytics engineering and le data science role if the data scientist report to business product then they re more likely to do data analytics dashboarding role however in both case because either the data science department doesn exist or because they have an immature data team there little room to explore more data science activity such a experimentation applied machine learning or amp being specialist put one in disadvantage because it take while to acquire all those additional skill larger company expect specialist must be domain expert for the role one is applying to for example technical question revolve around depth of understanding of the mathematics algorithm for testing or machine learning model clearer path for amp and specialization but being pigeonholed is definitely concern make it harder to look for other job a you are too specialized in field and focused on specific skillset tough for generalist to enter because they need time to learn certain concept more in depth and lack of deep experience in the field may hinder one application caveat of course this is all based on answer by the interviewer when asked how their data science team is structured so take my observation with grain of salt given your work environment do these observation reflect the state of your current organization
time series problem,assess whether price trend from seasonal decompose measure has accuracy in predicting binary target variable for future price direction over month window am not sure what this means and how to do it what should be my approach,1,time series problem,ass whether price trend from seasonal decompose measure ha accuracy in predicting binary target variable for future price direction over month window am not sure what this mean and how to do it what should be my approach
what is your job like working as data scientist,hello am curious to field insights into what data science job actually looks like on an ongoing basis for folks here wonder things such as what kind of work do you do what industry do you work in what sort of timelines deadlines do you have if any what sort of meetings collaboration do you have if any ll give some background on myself worked for some time in bank developing credit risk models from subject matter point of view enjoyed the work but from practical point of view it became challenge found the corporate structure diminished the experience found that non expert managers underwriters dictated what we ought to do by what time and as model developers we had to conform to their expectations as opposed to letting the data dictate our expectations limitations this induced lot of stress as the data do not always support proposed initiative like data science and predictive modeling but despise forcing such discipline into corporate assembly line way of thinking and so am curious to get sense of other people experiences thank you,1,what is your job like working a data scientist,hello am curious to field insight into what data science job actually look like on an ongoing basis for folk here wonder thing such a what kind of work do you do what industry do you work in what sort of timeline deadline do you have if any what sort of meeting collaboration do you have if any ll give some background on myself worked for some time in bank developing credit risk model from subject matter point of view enjoyed the work but from practical point of view it became challenge found the corporate structure diminished the experience found that non expert manager underwriter dictated what we ought to do by what time and a model developer we had to conform to their expectation a opposed to letting the data dictate our expectation limitation this induced lot of stress a the data do not always support proposed initiative like data science and predictive modeling but despise forcing such discipline into corporate assembly line way of thinking and so am curious to get sense of other people experience thank you
beginner wanting to build full stack ds application from scratch do need to build my own api,hello am trying to create plan to produce full stack ds application right now am researching apis how they work and how to use them at this point am just looking for any and all information you think is important have not even decided what want build neccessarily have pretty decent experience base with coding modeling statistics as well as some experience using restful apis to build web applications in ruby lol my impression is that need to have an idea of what want to build before consider what to do regarding the api would need different api to collect data from different source so for my first project should stick with something from youtube google or amazon so that can use their apis to collect data if choose to do some web scraping from random websites then will need to build my own api from scratch again any and all advice is appreciated,1,beginner wanting to build full stack d application from scratch do need to build my own api,hello am trying to create plan to produce full stack d application right now am researching apis how they work and how to use them at this point am just looking for any and all information you think is important have not even decided what want build neccessarily have pretty decent experience base with coding modeling statistic a well a some experience using restful apis to build web application in ruby lol my impression is that need to have an idea of what want to build before consider what to do regarding the api would need different api to collect data from different source so for my first project should stick with something from youtube google or amazon so that can use their apis to collect data if choose to do some web scraping from random website then will need to build my own api from scratch again any and all advice is appreciated
best resources for applying ds in business,was just hired as senior data scientist at smaller company what are your favorite resources books websites etc for the questions you have about ds when applying it to business,1,best resource for applying d in business,wa just hired a senior data scientist at smaller company what are your favorite resource book website etc for the question you have about d when applying it to business
how do people with tech background view non tech background data scientists,recently promoted to data scientist years of experience as data analyst but ve been with the same company since the beginning and would like an outside opinion bit about myself have ba in french language and literature and an ma that was mostly focused on french linguistics after that had decided to pursue an nlp specialization and did four semester worth of courses to get this msc graduated in time with honors had to work so hard but learned lot found job as data analyst and worked my way up even getting promotion to senior data scientist with my own team however even after all this validation still tend to feel insecure because of my background in some ways this has been beneficial because come overprepared and always look to fill in any potential gaps especially math and spent lot of time coding so that am comfortable with it but feel like unicorn and have no baseline for my skills compared to other data scientists how do you tend to view professionals with similar profile,1,how do people with tech background view non tech background data scientist,recently promoted to data scientist year of experience a data analyst but ve been with the same company since the beginning and would like an outside opinion bit about myself have ba in french language and literature and an ma that wa mostly focused on french linguistics after that had decided to pursue an nlp specialization and did four semester worth of course to get this msc graduated in time with honor had to work so hard but learned lot found job a data analyst and worked my way up even getting promotion to senior data scientist with my own team however even after all this validation still tend to feel insecure because of my background in some way this ha been beneficial because come overprepared and always look to fill in any potential gap especially math and spent lot of time coding so that am comfortable with it but feel like unicorn and have no baseline for my skill compared to other data scientist how do you tend to view professional with similar profile
intuit summer data science internship,has anyone interviewed for intuit summer data science internship please do you have any information regarding timelines and if offers have been made did final round interview long while ago and no feedback want to know if being ghosted by my recruiter thanks,1,intuit summer data science internship,ha anyone interviewed for intuit summer data science internship please do you have any information regarding timeline and if offer have been made did final round interview long while ago and no feedback want to know if being ghosted by my recruiter thanks
non obvious best practices,context former biochemist that starting doing computational bio and is now data scientist in computer vision ve learned lot of things along this unconventional path but have likely missed lot as well for example coworker today asked for me to change all of my variables to lowercamelcase which had never heard of this took me down rabbit hole of finding and reading through pep8 and trying but failing to get flake8 linter working on the jupyterlab version am stuck with on gcp amp x200b question what are the non ml related best practices should be aware of at least know what don know about certain types of models and know how to learn more about them but before today would never have thought to look into industry standards on where to put spaces any resources would be greatly appreciated,1,non obvious best practice,context former biochemist that starting doing computational bio and is now data scientist in computer vision ve learned lot of thing along this unconventional path but have likely missed lot a well for example coworker today asked for me to change all of my variable to lowercamelcase which had never heard of this took me down rabbit hole of finding and reading through pep8 and trying but failing to get flake8 linter working on the jupyterlab version am stuck with on gcp amp x200b question what are the non ml related best practice should be aware of at least know what don know about certain type of model and know how to learn more about them but before today would never have thought to look into industry standard on where to put space any resource would be greatly appreciated
need career guide for de ds and da,hi am currently in a2 cambridge advanced level and have decided to pursue career in data science so am confused about which bachelors degree to pursue to be data scientist engineer analyst therefore need little bit of help from you guys anyone who can guide me on the best possible path and is completing bachelors degree in computer engineering science is good option,1,need career guide for de d and da,hi am currently in a2 cambridge advanced level and have decided to pursue career in data science so am confused about which bachelor degree to pursue to be data scientist engineer analyst therefore need little bit of help from you guy anyone who can guide me on the best possible path and is completing bachelor degree in computer engineering science is good option
good resources on switching from ds to mle,background bs in math stats masters in ops research working as ds for months now some experience in software development wasn very in depth did it for research project in my undergrad where built program for archeologists to use have done some small projects here and there like analyzing data from spotify api have come to realize dislike the business focus of ds and would rather focus my efforts on the engineering side of the discipline what tools should be in an mle toolkit what are the differences in responsibilities between ds and mle is there straightforward path to switch from ds to mle thanks,1,good resource on switching from d to mle,background b in math stats master in ops research working a d for month now some experience in software development wasn very in depth did it for research project in my undergrad where built program for archeologist to use have done some small project here and there like analyzing data from spotify api have come to realize dislike the business focus of d and would rather focus my effort on the engineering side of the discipline what tool should be in an mle toolkit what are the difference in responsibility between d and mle is there straightforward path to switch from d to mle thanks
found it interesting to mine different kinds of session duration reports for web apps from db table once you have specific metric captured properly in well defined table schema time on site analytics,found it interesting to mine different kinds of session duration reports for web apps from db table once you have specific metric captured properly in well defined table schema time on site analytics,1,found it interesting to mine different kind of session duration report for web apps from db table once you have specific metric captured properly in well defined table schema time on site analytics,found it interesting to mine different kind of session duration report for web apps from db table once you have specific metric captured properly in well defined table schema time on site analytics
the data science olympics coding challenge already has some strong competitors try it over the weekend the top contestants will be matched with amazing companies engineering teams in north america and europe,the data science olympics coding challenge already has some strong competitors try it over the weekend the top contestants will be matched with amazing companies engineering teams in north america and europe,1,the data science olympics coding challenge already ha some strong competitor try it over the weekend the top contestant will be matched with amazing company engineering team in north america and europe,the data science olympics coding challenge already ha some strong competitor try it over the weekend the top contestant will be matched with amazing company engineering team in north america and europe
what the best book for self learner to learn advanced statiscs and machine learning algorithms,just finished reading an introduction to statistics probability distributions hepothesis testing two sample inference regression and wondring what the next book need to read in order to get the basics concepts and algorithms used in data science real world projects do you think esl is good book for self learner,1,what the best book for self learner to learn advanced statiscs and machine learning algorithm,just finished reading an introduction to statistic probability distribution hepothesis testing two sample inference regression and wondring what the next book need to read in order to get the basic concept and algorithm used in data science real world project do you think esl is good book for self learner
what would be some real world scenarios where we can use plotly and dash over excel or advanced visualization tools like tableau ibm cognos,worked with different charts visualization and dashboards that were created in excel and ibm cognos which weren just visually appealing but also quite flexible and easy to use however creating the same dashboards in dash with python would take hundreds of lines of codes one example would be creating the waffle chart in plotly without using the recently created library pywaffle would take lots of efforts amp x200b don mean any discredit to plotly or dash just curious if someone uses it differently or in particular scenario,1,what would be some real world scenario where we can use plotly and dash over excel or advanced visualization tool like tableau ibm cognos,worked with different chart visualization and dashboard that were created in excel and ibm cognos which weren just visually appealing but also quite flexible and easy to use however creating the same dashboard in dash with python would take hundred of line of code one example would be creating the waffle chart in plotly without using the recently created library pywaffle would take lot of effort amp x200b don mean any discredit to plotly or dash just curious if someone us it differently or in particular scenario
off topic who among you are into couponing,kinda of weird questions being an aspiring da and getting into couponing is nightmare ordering food literally takes hour of getting back and forth caught myself googling difference in ounces of meat in leg piece and breast piece of friend chicken while thinking maybe should look into creating pipeline and using the review of restaurants as basis of food ordering model but how else can you make an optimum decision when there is multiple different discounts thought graphing them would be nice am hungry stingy and miserable,1,off topic who among you are into couponing,kinda of weird question being an aspiring da and getting into couponing is nightmare ordering food literally take hour of getting back and forth caught myself googling difference in ounce of meat in leg piece and breast piece of friend chicken while thinking maybe should look into creating pipeline and using the review of restaurant a basis of food ordering model but how else can you make an optimum decision when there is multiple different discount thought graphing them would be nice am hungry stingy and miserable
advice for anyone trying to get more immersed in datascience in general,medium towards data science so many good blogs and posts made by professionals on the field for some time looked for content that could read in bed when wanted to learn but not enough to get up and work lol and medium is gold mine for this so many good short stories tutorials and advices enjoy it lot and wanted to share with you guys do you have any other sugestion for me how do you keep up with the field,1,advice for anyone trying to get more immersed in datascience in general,medium towards data science so many good blog and post made by professional on the field for some time looked for content that could read in bed when wanted to learn but not enough to get up and work lol and medium is gold mine for this so many good short story tutorial and advice enjoy it lot and wanted to share with you guy do you have any other sugestion for me how do you keep up with the field
questions for freelancers,considering shifting from large established team to project based freelance style of working trying to wrap my head around what this would actually look like if you ve done freelance ds work curious what was the ask goal of your project was it one time question or something that needed to be answered on an ongoing basis as new data came in what was your project deliverable thank you for your help,1,question for freelancer,considering shifting from large established team to project based freelance style of working trying to wrap my head around what this would actually look like if you ve done freelance d work curious what wa the ask goal of your project wa it one time question or something that needed to be answered on an ongoing basis a new data came in what wa your project deliverable thank you for your help
on prem model monitoring,is anyone aware of model monitoring tool which can be hosted on premise want to track the following concept drift population drift psi csi metrics ks stats etc,1,on prem model monitoring,is anyone aware of model monitoring tool which can be hosted on premise want to track the following concept drift population drift psi csi metric k stats etc
got it for my brother who works at the computer museum in boston,got it for my brother who works at the computer museum in boston,1,got it for my brother who work at the computer museum in boston,got it for my brother who work at the computer museum in boston
coding language for front end interface and user interaction,my understanding is that python is best for the data processing and ml engineering aspect of ds but what about the regular swe side of bringing ds application to users stakeholders should use language like java for this thanks,1,coding language for front end interface and user interaction,my understanding is that python is best for the data processing and ml engineering aspect of d but what about the regular swe side of bringing d application to user stakeholder should use language like java for this thanks
fundamentals,going through resumes and see lot of data scientists so none know sql none know any basic oop concepts no one has clue what mandelbrot set is and only know what bubble sort is but they all know python which is useless in reality so what do you do have to offer that will put you above current employees that know how to code work with data fluent in many programming languages oo or functional,1,fundamental,going through resume and see lot of data scientist so none know sql none know any basic oop concept no one ha clue what mandelbrot set is and only know what bubble sort is but they all know python which is useless in reality so what do you do have to offer that will put you above current employee that know how to code work with data fluent in many programming language oo or functional
joined company with an inflated title now what,joined company after finishing my masters with sr ai engineer title in the bay area after about year was promoted to staff engineer now thinking of changing jobs but have to come to realize that my company has inflated my title having to answer awkward questions in interviews about why my title is that of very experienced person also concerned about job change looking like step down in my resume since ll most likely get something like an engineer position do you see this situation as problem suspect it because ai practitioners command higher salary in the bay area than the company salary levels allowed for entry level jobs so to provide us with competitive salary our titles are inflated it healthcare company,1,joined company with an inflated title now what,joined company after finishing my master with sr ai engineer title in the bay area after about year wa promoted to staff engineer now thinking of changing job but have to come to realize that my company ha inflated my title having to answer awkward question in interview about why my title is that of very experienced person also concerned about job change looking like step down in my resume since ll most likely get something like an engineer position do you see this situation a problem suspect it because ai practitioner command higher salary in the bay area than the company salary level allowed for entry level job so to provide u with competitive salary our title are inflated it healthcare company
is master in data science viable option for me,is master in data science viable option for somebody with adhd and dyscalculia who is getting back to studying after long break who got no stem background if it is how do you suppose one should approach it read somewhere that one is expected to be proficient in topics like algebra calculus statistics and probability are these topics hard to pick up on for somebody who self learner and to what extent is knowledge on these topics required when applying for master course would foundational knowledge suffice was thinking of taking up help from private tutor on these topics in order to prepare for the course before do it haven studied maths since high school and that was years ago should also enrol into any preliminary courses before apply for it any suggestions would be much appreciated,1,is master in data science viable option for me,is master in data science viable option for somebody with adhd and dyscalculia who is getting back to studying after long break who got no stem background if it is how do you suppose one should approach it read somewhere that one is expected to be proficient in topic like algebra calculus statistic and probability are these topic hard to pick up on for somebody who self learner and to what extent is knowledge on these topic required when applying for master course would foundational knowledge suffice wa thinking of taking up help from private tutor on these topic in order to prepare for the course before do it haven studied math since high school and that wa year ago should also enrol into any preliminary course before apply for it any suggestion would be much appreciated
dataset augmentation for deep learning,dataset augmentation for deep learning,1,dataset augmentation for deep learning,dataset augmentation for deep learning
how to establish relation between inputs and output,dear community facing kind of serious issue in project concerning finding an equation explaining the relation between the input features and the output of dataset the equation should be like ax1 bx2 cx3 kxn where is my output and are coefficients and x1 x2 x3 xn are the input features of my dataset the aim goal is to find way to forecast the output based on the equation above using python so if we want to find an exact value of the output we have an idea how to change the input variables thank you,1,how to establish relation between input and output,dear community facing kind of serious issue in project concerning finding an equation explaining the relation between the input feature and the output of dataset the equation should be like ax1 bx2 cx3 kxn where is my output and are coefficient and x1 x2 x3 xn are the input feature of my dataset the aim goal is to find way to forecast the output based on the equation above using python so if we want to find an exact value of the output we have an idea how to change the input variable thank you
how to elaborate an equation between inputs and outputs in dataset,dear community facing kind of serious issue in project concerning finding an equation explaining the relation between the input features and the output of dataset the equation should be like ax1 bx2 cx3 kxn where is my output and are coefficients and x1 x2 x3 xn are the input features of my dataset the aim goal is to find way to forecast the output based on the equation above using python so if we want to find an exact value of the output we have an idea how to change the input variables thank you,1,how to elaborate an equation between input and output in dataset,dear community facing kind of serious issue in project concerning finding an equation explaining the relation between the input feature and the output of dataset the equation should be like ax1 bx2 cx3 kxn where is my output and are coefficient and x1 x2 x3 xn are the input feature of my dataset the aim goal is to find way to forecast the output based on the equation above using python so if we want to find an exact value of the output we have an idea how to change the input variable thank you
which type of data science projects gives better understanding of machine learning and hypertuning,which type of data science projects gives better understanding of machine learning and hypertuning,1,which type of data science project give better understanding of machine learning and hypertuning,which type of data science project give better understanding of machine learning and hypertuning
top data science trends and predictions for,top data science trends and predictions for,1,top data science trend and prediction for,top data science trend and prediction for
numbers of training dataset vs number of independent variables,understand you should have training data for every independent variable is dummy variables from categorical variable consider as independent variable or does each of the dummy variables consider as new independent variable,1,number of training dataset v number of independent variable,understand you should have training data for every independent variable is dummy variable from categorical variable consider a independent variable or doe each of the dummy variable consider a new independent variable
datacamp amp data leakage practices,must be going crazy anyone take datacamp courses and do projects so quite lot of projects and courses actually apply data preprocessing techniques before splitting data that encourage data leakage can believe paid for their services anyone catch other bad habits that datacamp encourages,1,datacamp amp data leakage practice,must be going crazy anyone take datacamp course and do project so quite lot of project and course actually apply data preprocessing technique before splitting data that encourage data leakage can believe paid for their service anyone catch other bad habit that datacamp encourages
the right amount of domain knowledge for ds,hi guys one of the pros of being data scientist is that you are somewhat versatile and may work on different domains am realizing however how important is domain knowledge in extracting something valuable from your data especially in small teams where you may not be backed up by data analysts and other colleagues on one hand it seems obvious that some domain knowledge is required on the other hand the high demand of domain knowledge required may actually be due to other reasons mismatch in your role or data readiness any story to share about this,1,the right amount of domain knowledge for d,hi guy one of the pro of being data scientist is that you are somewhat versatile and may work on different domain am realizing however how important is domain knowledge in extracting something valuable from your data especially in small team where you may not be backed up by data analyst and other colleague on one hand it seems obvious that some domain knowledge is required on the other hand the high demand of domain knowledge required may actually be due to other reason mismatch in your role or data readiness any story to share about this
fetching real time ecommerce data api key from any analytics platform,hi am doing data analytics project on ecommerce and was about to fetch an api key on google analytics for any ecommerce data but was not able to do not have any ecommerce website on my own just want fetech real data for ecommerce api key data using python for mini project of my own it will be great help if there any leads or how one can get any random ecommerce real data online through api do not want to use any kaggle already present data set thank you,1,fetching real time ecommerce data api key from any analytics platform,hi am doing data analytics project on ecommerce and wa about to fetch an api key on google analytics for any ecommerce data but wa not able to do not have any ecommerce website on my own just want fetech real data for ecommerce api key data using python for mini project of my own it will be great help if there any lead or how one can get any random ecommerce real data online through api do not want to use any kaggle already present data set thank you
non profit jobs for data scientists,just graduated with ba in sociology and music and really love research and data and want to make career out of it eventually looking to get masters in data analytics and then hopefully doctorate in sociology but when read about data science and analytics there seems to be large focus on business and big tech and know those fields would make me miserable much more into non profit work and social services are there any careers for data scientists in those fields or should be looking into different education path,1,non profit job for data scientist,just graduated with ba in sociology and music and really love research and data and want to make career out of it eventually looking to get master in data analytics and then hopefully doctorate in sociology but when read about data science and analytics there seems to be large focus on business and big tech and know those field would make me miserable much more into non profit work and social service are there any career for data scientist in those field or should be looking into different education path
data science in energy renewable and non renewable,hi folks considering becoming data scientist with specialization in the energy industry hold master degree in energy and environmental engineering love to hear from anyone applying data science to energy thanks,1,data science in energy renewable and non renewable,hi folk considering becoming data scientist with specialization in the energy industry hold master degree in energy and environmental engineering love to hear from anyone applying data science to energy thanks
what do data analyst really need to know for entry level roles,know this is data science subreddit but this feels like the best place to ask if you look online there are countless threads about how to get started in becoming data analyst but they all list many different programs languages and tools you can help but feel the need to learn sql python excel tableau and power bi all at once to land something what do people actually need to know to have shot at the majority of entry level roles or conduct basic data analysis in their role,1,what do data analyst really need to know for entry level role,know this is data science subreddit but this feel like the best place to ask if you look online there are countless thread about how to get started in becoming data analyst but they all list many different program language and tool you can help but feel the need to learn sql python excel tableau and power bi all at once to land something what do people actually need to know to have shot at the majority of entry level role or conduct basic data analysis in their role
on prem model monitoring,is anyone aware of model monitoring tool which can be hosted on premise want to track the following concept drift population drift psi csi metrics ks stats etc,1,on prem model monitoring,is anyone aware of model monitoring tool which can be hosted on premise want to track the following concept drift population drift psi csi metric k stats etc
offered job titled graph knowledge scientist don understand what it is,hi have been working as data scientist for about year and hold knowledge in statistics holds master took class in algorithms and data structures but am not an algorithmic researcher by no means representative of small startup in the healthcare domaine that deals with medical texts approached me and said that they are looking for knowledge graph scientist ontology specialist they are looking for someone to build their knowledge graph test it using automated test and build the tests and apis and support the data scientists the team is now being established and the data scientists they are recruiting are much more experienced than am years my main goal is to learn as much as can in the field and try to lean towards nlp don really care about the domain even though they offered me really good price for my time am having doubts know the saying that in startups everybody does everything but couldn get an understanding from the conversation had with the cto of the company what ml tasks are expected of me and couldn find clear description on how much ml the position involves,1,offered job titled graph knowledge scientist don understand what it is,hi have been working a data scientist for about year and hold knowledge in statistic hold master took class in algorithm and data structure but am not an algorithmic researcher by no mean representative of small startup in the healthcare domaine that deal with medical text approached me and said that they are looking for knowledge graph scientist ontology specialist they are looking for someone to build their knowledge graph test it using automated test and build the test and apis and support the data scientist the team is now being established and the data scientist they are recruiting are much more experienced than am year my main goal is to learn a much a can in the field and try to lean towards nlp don really care about the domain even though they offered me really good price for my time am having doubt know the saying that in startup everybody doe everything but couldn get an understanding from the conversation had with the cto of the company what ml task are expected of me and couldn find clear description on how much ml the position involves
using toy simulations to understand the statistical learning theory framework for supervised classification,using toy simulations to understand the statistical learning theory framework for supervised classification,1,using toy simulation to understand the statistical learning theory framework for supervised classification,using toy simulation to understand the statistical learning theory framework for supervised classification
eeg headband microvolts to frequencies,hi people so testing this muse headband it an eeg device that has four sensors where you can get the raw microvolts data each sensor produces using javascript library would like to know if you people can give me bit of guidance as fairly new with data analysis and neuroscience and working on cool app that needs to process the eeg data basically want to get the alpha beta and gamma frequencies bands but only have the microvolts read by the sensors of my eeg headband according to what have read need to use fast fourier transformation to obtain set of frequency bands but how do feed the data and if you have javascript library you would advise using that going to be brilliant let me know,1,eeg headband microvolt to frequency,hi people so testing this muse headband it an eeg device that ha four sensor where you can get the raw microvolt data each sensor produce using javascript library would like to know if you people can give me bit of guidance a fairly new with data analysis and neuroscience and working on cool app that need to process the eeg data basically want to get the alpha beta and gamma frequency band but only have the microvolt read by the sensor of my eeg headband according to what have read need to use fast fourier transformation to obtain set of frequency band but how do feed the data and if you have javascript library you would advise using that going to be brilliant let me know
trying to get into data science degree paths and certifications seem confusing,help haha idk what degree or certifications need to pursue in order to land position using data to drive more sales or leveraging data to do any kind of marketing it the most fun for me currently full time in sales have gi bill can use need to know what path would get me there without taking away too many hours of the day,1,trying to get into data science degree path and certification seem confusing,help haha idk what degree or certification need to pursue in order to land position using data to drive more sale or leveraging data to do any kind of marketing it the most fun for me currently full time in sale have gi bill can use need to know what path would get me there without taking away too many hour of the day
school cs curriculum,am wondering if the following classes are important for data science am debating replacing these with other ones comp introduction to software engineering credits prerequisite comp encs software development process models linear vs iterative project management roles activities and deliverables for each software life cycle phase requirements management analysis elicitation and scope architecture design and the mapping of requirements to design and design to implementation traceability software quality assurance verification validation and the role of testing maintenance and evolution project lectures three hours per week tutorial one hour per week laboratory two hours per week amp x200b comp operating systems credits prerequisite comp or soen comp fundamentals of operating system functionalities design and implementation multiprogramming processes and threads context switching queuing models and scheduling interprocess communication and synchronization principles of concurrency synchronization primitives deadlock detection and recovery prevention and avoidance schemes memory management device management file systems protection models and schemes lectures three hours per week tutorial one hour per week laboratory two hours per week amp x200b comp introduction to theoretical computer science credits prerequisite comp or coen comp or coen finite state automata and regular languages push down automata and context free languages pumping lemmas applications to parsing turing machines unde cidability and decidability lectures three hours per week tutorial one hour per week,1,school c curriculum,am wondering if the following class are important for data science am debating replacing these with other one comp introduction to software engineering credit prerequisite comp encs software development process model linear v iterative project management role activity and deliverable for each software life cycle phase requirement management analysis elicitation and scope architecture design and the mapping of requirement to design and design to implementation traceability software quality assurance verification validation and the role of testing maintenance and evolution project lecture three hour per week tutorial one hour per week laboratory two hour per week amp x200b comp operating system credit prerequisite comp or soen comp fundamental of operating system functionality design and implementation multiprogramming process and thread context switching queuing model and scheduling interprocess communication and synchronization principle of concurrency synchronization primitive deadlock detection and recovery prevention and avoidance scheme memory management device management file system protection model and scheme lecture three hour per week tutorial one hour per week laboratory two hour per week amp x200b comp introduction to theoretical computer science credit prerequisite comp or coen comp or coen finite state automaton and regular language push down automaton and context free language pumping lemma application to parsing turing machine unde cidability and decidability lecture three hour per week tutorial one hour per week
looking for hpc help,am trying to learn to use my organizations hpcs and the documentation when written is poorly written some script examples in our hpc documentation are just plain wrong even after years of python scripting on windows os this hpc business is an entirely different game know that each and every hpc is its own special thing but am running out of places to turn to my background is in the natural sciences and haven had the traditional education that many ds ml experts have that is might not know of some obvious reference material thought should ask for help the hpcs in question are run by linux command line no problem there git bash it is for what it worth the documentation has me downloading and installing miniconda that not big deal either want to use the mpi4py python library as wrapper for mpi to run parallelized code across the clusters running pip install for mpi4py isn problem and can easily enough parallelize my scripts the problem am having regards figuring out which hpc modules that is not the python modules libraries but modules for the hpc itself need to install and load between only having couple hundred hours on anything linux and never having logged into an hpc before couple months ago have no idea which modules need to run ml python scripts ve tried asking the hpc staff for help but they don respond to my emails does generic linux hpc documentation exist can someone point me toward some reference material or is every hpc just too different to have generic documentation,1,looking for hpc help,am trying to learn to use my organization hpcs and the documentation when written is poorly written some script example in our hpc documentation are just plain wrong even after year of python scripting on window o this hpc business is an entirely different game know that each and every hpc is it own special thing but am running out of place to turn to my background is in the natural science and haven had the traditional education that many d ml expert have that is might not know of some obvious reference material thought should ask for help the hpcs in question are run by linux command line no problem there git bash it is for what it worth the documentation ha me downloading and installing miniconda that not big deal either want to use the mpi4py python library a wrapper for mpi to run parallelized code across the cluster running pip install for mpi4py isn problem and can easily enough parallelize my script the problem am having regard figuring out which hpc module that is not the python module library but module for the hpc itself need to install and load between only having couple hundred hour on anything linux and never having logged into an hpc before couple month ago have no idea which module need to run ml python script ve tried asking the hpc staff for help but they don respond to my email doe generic linux hpc documentation exist can someone point me toward some reference material or is every hpc just too different to have generic documentation
portfolio projects,hi everyone am mscs student and looking for some data science project ideas for my resume wanna get into data science so am planning to apply to summer internships as dont have any prior experience ll need some projects for my resume any suggestions,1,portfolio project,hi everyone am msc student and looking for some data science project idea for my resume wanna get into data science so am planning to apply to summer internship a dont have any prior experience ll need some project for my resume any suggestion
master in statistics,as someone who graduated with finance bachelor degree could not apply for more technical computer sciency program due to my lack of relevant credits that why looking at master program in stat specifically the one in ku leuven which heard is quite theoretical and doesn allow you to code much do you think can compensate for the lack of coding by self learning outside the course here is the program syllabus fiy htm,1,master in statistic,a someone who graduated with finance bachelor degree could not apply for more technical computer sciency program due to my lack of relevant credit that why looking at master program in stat specifically the one in ku leuven which heard is quite theoretical and doesn allow you to code much do you think can compensate for the lack of coding by self learning outside the course here is the program syllabus fiy htm
masters in data science or in artificial intelligence,am masters student we have to decide whether to go for specialization in data science or in ai in our tech computer science program location india subjects data science data mining and warehousing data visualizations with big data analytics cryptography and network security capstone project artificial intelligence artificial intelligence natural language processing image and vision processing ai for iot reinforcement learning these are the specialization specific subjects rest subjects are common to both and haven mentioned them here doubts ve heard and seen from job boards that data science is hot there are lot of jobs what kind of similar entry level jobs are there that are ai specific my interest is in computer vision but getting job right now is also priority how easy difficult is it in india location not an issue to get started as computer vision engineer or machine learning engineer for masters graduate with no prior full time work experience thanks ps about me pursuing tech computer science with specialization in lt undecided gt good hands on knowledge above average cgpa about to get aws saa certification my first cloud certification completed one research internship in machine learning good grasp over english ielts academic quick learner tldr which is better between data science and ai for specialization during masters from purely an employment perspective,1,master in data science or in artificial intelligence,am master student we have to decide whether to go for specialization in data science or in ai in our tech computer science program location india subject data science data mining and warehousing data visualization with big data analytics cryptography and network security capstone project artificial intelligence artificial intelligence natural language processing image and vision processing ai for iot reinforcement learning these are the specialization specific subject rest subject are common to both and haven mentioned them here doubt ve heard and seen from job board that data science is hot there are lot of job what kind of similar entry level job are there that are ai specific my interest is in computer vision but getting job right now is also priority how easy difficult is it in india location not an issue to get started a computer vision engineer or machine learning engineer for master graduate with no prior full time work experience thanks p about me pursuing tech computer science with specialization in lt undecided gt good hand on knowledge above average cgpa about to get aws saa certification my first cloud certification completed one research internship in machine learning good grasp over english ielts academic quick learner tldr which is better between data science and ai for specialization during master from purely an employment perspective
ds to de folks and de to ds folks why are you switching,for anyone that is switching is it aspiring to be one over the other curious to know why you are switching am considering the switch from ds to de do just want to hear thoughts what work were you trying to do less vs what work were you trying to do more,1,d to de folk and de to d folk why are you switching,for anyone that is switching is it aspiring to be one over the other curious to know why you are switching am considering the switch from d to de do just want to hear thought what work were you trying to do le v what work were you trying to do more
help needed on where can get on going market sizing data for hockey equipment market,does anyone have any recommendations for how can get hockey equipment data in north america the data would be used to calculate market share for sticks skates helmets protective equipment and goalie equipment by brand and by geography country state provice and city if you have good sources data providers that would be able to provide information on an ongoing basis that would be great one example aware of is nielson but have to look into if they offer this information,1,help needed on where can get on going market sizing data for hockey equipment market,doe anyone have any recommendation for how can get hockey equipment data in north america the data would be used to calculate market share for stick skate helmet protective equipment and goalie equipment by brand and by geography country state provice and city if you have good source data provider that would be able to provide information on an ongoing basis that would be great one example aware of is nielson but have to look into if they offer this information
bringing azure ml to and organization,hello everyone work for relatively large organization with thousands of employees but in terms of technology we are firmly in the early am working with large climate datasets where am processing very large netcdf files from my desktop am making case to my boss for us to purchase azure ml so that can actually do my job amp x200b looking for advice or just anecdotes from people that have introduced this technology to their companies also looking for any resources people have used to get quickly up to speed with azure ml currently am the sole data scientist at my company so don really have anyone to lean on to learn this amp x200b thanks all,1,bringing azure ml to and organization,hello everyone work for relatively large organization with thousand of employee but in term of technology we are firmly in the early am working with large climate datasets where am processing very large netcdf file from my desktop am making case to my bos for u to purchase azure ml so that can actually do my job amp x200b looking for advice or just anecdote from people that have introduced this technology to their company also looking for any resource people have used to get quickly up to speed with azure ml currently am the sole data scientist at my company so don really have anyone to lean on to learn this amp x200b thanks all
power bi can do dual axis chart with two lines overlaying bar chart,so far can get but not all am basically trying to plot the bar chart for covid cases by week have this query merged with tables for vaccinations by week and stock closing prices by week would like this bar chart total covid cases that week cases to date first line vaccinations done that week nd line stock closing price at the end of that week is this possible also now realize shouldn do total cases to date but rather total new cases that week is this possible,1,power bi can do dual axis chart with two line overlaying bar chart,so far can get but not all am basically trying to plot the bar chart for covid case by week have this query merged with table for vaccination by week and stock closing price by week would like this bar chart total covid case that week case to date first line vaccination done that week nd line stock closing price at the end of that week is this possible also now realize shouldn do total case to date but rather total new case that week is this possible
someone is know label studio,so accidentally made an account and realised added wrong email address how to delete it how delete the account please help litteraly shaking,1,someone is know label studio,so accidentally made an account and realised added wrong email address how to delete it how delete the account please help litteraly shaking
optimization means finding the minimum or maximum point of defined function in certain limit ranges and the variables that provide this point,optimization means finding the minimum or maximum point of defined function in certain limit ranges and the variables that provide this point,1,optimization mean finding the minimum or maximum point of defined function in certain limit range and the variable that provide this point,optimization mean finding the minimum or maximum point of defined function in certain limit range and the variable that provide this point
questions about setting up mamba conda forge pyenv on windows,howdy trying to setup python workflow that involves primarily mamba instead of conda and conda forge as package channel instead of anaconda on windows machine also like to use pyenv in case need to use venv for whatever reason on windows machine would you recommend install everything on the ubuntu virtual machine that is used with wsl or would it be better to install everything on windows directly in addition do you have any good resources for setting up mamba micromamba and conda forge instead of the more standard workflow with conda and anaconda miniconda have some experience using nvm windows to deal with node on windows but finding the mamba docs little hard to follow thanks,1,question about setting up mamba conda forge pyenv on window,howdy trying to setup python workflow that involves primarily mamba instead of conda and conda forge a package channel instead of anaconda on window machine also like to use pyenv in case need to use venv for whatever reason on window machine would you recommend install everything on the ubuntu virtual machine that is used with wsl or would it be better to install everything on window directly in addition do you have any good resource for setting up mamba micromamba and conda forge instead of the more standard workflow with conda and anaconda miniconda have some experience using nvm window to deal with node on window but finding the mamba doc little hard to follow thanks
considering my career goals should add minor,am majoring in public health and ultimately want to work with data don care if its necessarily public health data or not although my background may help me do so have full semesters left and small third semester consisting of credits before graduate wondering if should add minor in information systems as that covers some python and sql it would make my third semester another full semester and tack on fourth full semester before graduate ve learned some sql on healthcare data udemy course which have enjoyed and felt like learned from am willing to keep learning database analyzing visualizing tools through similar resources essentially wondering if should continue learning sql and start learning tools like python and tableau through cheaper online resources or if it worth it learn these through minor which will take more time and money im also curious as to how my choice in this could affect my career and job prospects thank you in advance,1,considering my career goal should add minor,am majoring in public health and ultimately want to work with data don care if it necessarily public health data or not although my background may help me do so have full semester left and small third semester consisting of credit before graduate wondering if should add minor in information system a that cover some python and sql it would make my third semester another full semester and tack on fourth full semester before graduate ve learned some sql on healthcare data udemy course which have enjoyed and felt like learned from am willing to keep learning database analyzing visualizing tool through similar resource essentially wondering if should continue learning sql and start learning tool like python and tableau through cheaper online resource or if it worth it learn these through minor which will take more time and money im also curious a to how my choice in this could affect my career and job prospect thank you in advance
necessary qualifications,hello datascience reaching out to ask what sort of qualifications certifications am going to need to break into the data science field my undergraduate degree is in finance and have an mba as well am currently working as senior finance analyst on cross functional team at car manufacturing company on this team am beginning to dip my toes into python and as learn more my interest in the data science field is growing at my current business my vertical movement is currently capped until become manager however am seeing data science as particular good alternative path being in finance as basically just meant that work with lots of data in excel so moving into manipulating data with programming seems like logical next step so with all that background out of the way back to my original question what sort of qualifications am going to need my plan is to learn as much as can over the next few years in my current position department is offering to pay for whatever learning material courses want and then potentially move into data science next either at my current company or another thanks in advance for any assistance with this,1,necessary qualification,hello datascience reaching out to ask what sort of qualification certification am going to need to break into the data science field my undergraduate degree is in finance and have an mba a well am currently working a senior finance analyst on cross functional team at car manufacturing company on this team am beginning to dip my toe into python and a learn more my interest in the data science field is growing at my current business my vertical movement is currently capped until become manager however am seeing data science a particular good alternative path being in finance a basically just meant that work with lot of data in excel so moving into manipulating data with programming seems like logical next step so with all that background out of the way back to my original question what sort of qualification am going to need my plan is to learn a much a can over the next few year in my current position department is offering to pay for whatever learning material course want and then potentially move into data science next either at my current company or another thanks in advance for any assistance with this
has fa reached overkill,use fa to log into the vpn then into the sso then for any given aws role need for that project the aws and most of the sso logins are frequently expiring and asking me for another authenticator code really just in the last year spend weird amount of my day authenticating logins on my phone is it just me or is this also you is it too much or just the new normal we all need to accept should start revolt,1,ha fa reached overkill,use fa to log into the vpn then into the sso then for any given aws role need for that project the aws and most of the sso logins are frequently expiring and asking me for another authenticator code really just in the last year spend weird amount of my day authenticating logins on my phone is it just me or is this also you is it too much or just the new normal we all need to accept should start revolt
amazon sde intern vs jp morgan data science ai intern,hi currently junior in college and trying to decide between two internship offers for summer have an offer from amazon for software development engineer internship and an offer from jp morgan for data science ai internship ultimately want to become data scientist so leaning towards jp morgan but have some concerns and would appreciate some advice already accepted amazon offer back in september applied to both amazon and jp morgan in august didn hear back from jp morgan so accepted amazon then got an interview with jp morgan in october and got the offer in december so if take jp morgan would have to renege on amazon amazon has better location and higher salary those aren huge concern for the internship but the location would be plus if get full time offer there however ve heard bad things about amazon culture so think it would be better to work at jp morgan full time for jp morgan can choose between two teams hr data and analytics solutions for amazon don know anything about my specific team yet but know that the office got assigned to has teams that work with devices and ground stations both of these sound more interesting to me than the jp morgan teams but would be doing software engineering work instead of data science so there tradeoff also worried that software engineering internship would make it harder to get full time job as data scientist overall just confused about which would be the better option and it would be super helpful to get any advice from people in the industry thank you,1,amazon sde intern v jp morgan data science ai intern,hi currently junior in college and trying to decide between two internship offer for summer have an offer from amazon for software development engineer internship and an offer from jp morgan for data science ai internship ultimately want to become data scientist so leaning towards jp morgan but have some concern and would appreciate some advice already accepted amazon offer back in september applied to both amazon and jp morgan in august didn hear back from jp morgan so accepted amazon then got an interview with jp morgan in october and got the offer in december so if take jp morgan would have to renege on amazon amazon ha better location and higher salary those aren huge concern for the internship but the location would be plus if get full time offer there however ve heard bad thing about amazon culture so think it would be better to work at jp morgan full time for jp morgan can choose between two team hr data and analytics solution for amazon don know anything about my specific team yet but know that the office got assigned to ha team that work with device and ground station both of these sound more interesting to me than the jp morgan team but would be doing software engineering work instead of data science so there tradeoff also worried that software engineering internship would make it harder to get full time job a data scientist overall just confused about which would be the better option and it would be super helpful to get any advice from people in the industry thank you
advice for re learning statistics,took stats few years back and didn do that well in it transitioning to point where probably going to have to use stats more and was wondering what the best way to go about re learning stats was short of reading textbook front to back have all my old stats notes so can find how to do test or do things of that nature but part of why didn do well was because we were just given bunch of equations and no explanation of the underlying context purpose not as interested in the formulas would just implement these in python scipy anyways lol so much as the how why when of when to apply the various formulas and when to model what data as what distribution and so on any advice liked jake vanderplaas statistics for hackers but as far as know that only exists as minute lecture and not textbook,1,advice for re learning statistic,took stats few year back and didn do that well in it transitioning to point where probably going to have to use stats more and wa wondering what the best way to go about re learning stats wa short of reading textbook front to back have all my old stats note so can find how to do test or do thing of that nature but part of why didn do well wa because we were just given bunch of equation and no explanation of the underlying context purpose not a interested in the formula would just implement these in python scipy anyways lol so much a the how why when of when to apply the various formula and when to model what data a what distribution and so on any advice liked jake vanderplaas statistic for hacker but a far a know that only exists a minute lecture and not textbook
how to spice up your data science journey when you lack motivation,hey guys happy new year to everyone following on from the title feel as though my data science journey has plateaued bit of late seem to have less motivation for projects and extra curricular bits these days in had lot of energy for side projects and courses leading to me learning lot and managing to get new job in early which was new and exciting but feel as though the learning has tapered off bit and am looking for way to step it up to the next level again like to know if anyone has any ideas on how to shake it up bit and get out of rut what projects did you find really motivating addicting what re ignited your love for ds when you felt like you were losing steam things can think of are either beginning freelance side hustle as well as contributing to and engaging with the community more through reddit but would appreciate your thoughts thanks,1,how to spice up your data science journey when you lack motivation,hey guy happy new year to everyone following on from the title feel a though my data science journey ha plateaued bit of late seem to have le motivation for project and extra curricular bit these day in had lot of energy for side project and course leading to me learning lot and managing to get new job in early which wa new and exciting but feel a though the learning ha tapered off bit and am looking for way to step it up to the next level again like to know if anyone ha any idea on how to shake it up bit and get out of rut what project did you find really motivating addicting what re ignited your love for d when you felt like you were losing steam thing can think of are either beginning freelance side hustle a well a contributing to and engaging with the community more through reddit but would appreciate your thought thanks
only remaining data scientist at company,at small startup lt and due to attrition and downsizing am the only data scientist left at the company while it presents lots of opportunities for growth in skills be lying if said wasn anxious just wondering if anyone has similar experiences or advice to share,1,only remaining data scientist at company,at small startup lt and due to attrition and downsizing am the only data scientist left at the company while it present lot of opportunity for growth in skill be lying if said wasn anxious just wondering if anyone ha similar experience or advice to share
mitx micromasters in statistics and data science includes nothing on large scale data processing tools like spark and hadoop still worth it,hi all bi professional with years experience working heavily with sql and various analytics tools see data science as logical extension of this and have foundational knowledge of python and libraries like pandas matplotlib scikitlearn etc at this point feel like need some more rigorous training and am enticed by the mitx micromasters programs specifically the one in the title however notice as do with most similar programs lack of any modules on large scale data processing tools nosql databases scaling etc is this as big of gap as making it out to be if want to actually land job as data scientist what about as data engineer architect thanks,1,mitx micromasters in statistic and data science includes nothing on large scale data processing tool like spark and hadoop still worth it,hi all bi professional with year experience working heavily with sql and various analytics tool see data science a logical extension of this and have foundational knowledge of python and library like panda matplotlib scikitlearn etc at this point feel like need some more rigorous training and am enticed by the mitx micromasters program specifically the one in the title however notice a do with most similar program lack of any module on large scale data processing tool nosql database scaling etc is this a big of gap a making it out to be if want to actually land job a data scientist what about a data engineer architect thanks
did you choose your working sector or did you join whichever industry was available at that moment,as fellow student was wondering whether there is such thing as getting job in specific industry and wether into would be even worth it building portfolio towards that purpose as would like to get into the energy sector would like to understand if there such thing as targeting specific industry or do people in general just roll with whatever best offer was given at the beginning of the career what is your experience like,1,did you choose your working sector or did you join whichever industry wa available at that moment,a fellow student wa wondering whether there is such thing a getting job in specific industry and wether into would be even worth it building portfolio towards that purpose a would like to get into the energy sector would like to understand if there such thing a targeting specific industry or do people in general just roll with whatever best offer wa given at the beginning of the career what is your experience like
where to find some take home ds challenges for practice,context am starting to interview for ds positions in bay area not faang and have an ms in stats with ds da internships am not asking about the projects done on toy data sets but something with similar level of complexity like those you get as the part of interviewing process,1,where to find some take home d challenge for practice,context am starting to interview for d position in bay area not faang and have an m in stats with d da internship am not asking about the project done on toy data set but something with similar level of complexity like those you get a the part of interviewing process
recommended software cloud options for fitting algorithms on tb of data,hello have collected hundreds of csvs of monthly loan and economic data which continues to grow each month the bulk of the data is the loans and tracks individuals loan performance over time such as the payment amount made whether the customer paid off more than they needed to whether they went delinquent refinanced etc it also has borrower characteristics like fico scores and dti ratios what would like to do is build model to predict prepayments delinquencies refinances etc with consideration for macro conditions and borrower characteristics if successful this model could be implemented at my company to replace our vendor model conceptually have ideas about how this might work have built many ml models with datasets that were small enough to work on my local machine but the computing requirements of this are beyond that am wondering what the lowest cost method would be to store manipulate and fit models on this set first for the proof of concept and then potentially longer term for running loans through this model on monthly basis right now am thinking of simply storing the data on some low cost cloud service like amazon s3 and using apache spark via databricks to manipulate analyze and fit models on it is this is good idea or is it more or less than would need at least for the proof of concept work for small company that has relatively weak and outdated data support so am leading this alone but could get little bit of money towards it apologies if this is outside of the scope of this subreddit thanks,1,recommended software cloud option for fitting algorithm on tb of data,hello have collected hundred of csvs of monthly loan and economic data which continues to grow each month the bulk of the data is the loan and track individual loan performance over time such a the payment amount made whether the customer paid off more than they needed to whether they went delinquent refinanced etc it also ha borrower characteristic like fico score and dti ratio what would like to do is build model to predict prepayment delinquency refinances etc with consideration for macro condition and borrower characteristic if successful this model could be implemented at my company to replace our vendor model conceptually have idea about how this might work have built many ml model with datasets that were small enough to work on my local machine but the computing requirement of this are beyond that am wondering what the lowest cost method would be to store manipulate and fit model on this set first for the proof of concept and then potentially longer term for running loan through this model on monthly basis right now am thinking of simply storing the data on some low cost cloud service like amazon s3 and using apache spark via databricks to manipulate analyze and fit model on it is this is good idea or is it more or le than would need at least for the proof of concept work for small company that ha relatively weak and outdated data support so am leading this alone but could get little bit of money towards it apology if this is outside of the scope of this subreddit thanks
tableau course,tableau course,1,tableau course,tableau course
is there problem with my causal estimates if they are very similar to nave estimates difference in outcome means,apologies if the question is unclear not too familiar with causal inference ve been using few different methods to estimate causal effects for an outcome variable through microsoft dowhy library for python despite using different methods propensity backdoor matching linear regression etc the causal estimates are always very similar to nave estimate where just take the difference in outcome means between the treated and untreated groups ve used the dowhy library to test my assumptions through few methods of refuting the estimates adding random confounders removing random data subset etc and they all seem to work fine and verify my assumptions but still worried the estimates are wrong due to their similarity to the nave estimates that don take into account any possible confounding variables selection biases does this mean there problem with my causal estimates or could the estimates still be fine if there problem is there any way to check whether it has something to do with my data too high dimensionality the dag causal model ve created or something else,1,is there problem with my causal estimate if they are very similar to nave estimate difference in outcome mean,apology if the question is unclear not too familiar with causal inference ve been using few different method to estimate causal effect for an outcome variable through microsoft dowhy library for python despite using different method propensity backdoor matching linear regression etc the causal estimate are always very similar to nave estimate where just take the difference in outcome mean between the treated and untreated group ve used the dowhy library to test my assumption through few method of refuting the estimate adding random confounders removing random data subset etc and they all seem to work fine and verify my assumption but still worried the estimate are wrong due to their similarity to the nave estimate that don take into account any possible confounding variable selection bias doe this mean there problem with my causal estimate or could the estimate still be fine if there problem is there any way to check whether it ha something to do with my data too high dimensionality the dag causal model ve created or something else
is it just me or is sql critically and chronically underappreciated in the ds community,totally get that ml ai is the sexiest hype iest part of ds but acting like sql is easy coming to realize is just utter nonsense people tend to think sql oh yeah select from easy day just like statistics oh yeah values know everything about stats starting to realize that people who know how to wrangle data across tables warehouses servers etc at scale efficiently and know that their approaches are actually addressing the business ask are incredibly valuable and they re compensated as such at the faangs for some reason sql like stats became this taboo word in the ds community like sql oh no mean only if can get some junior schmuck to do it for me,1,is it just me or is sql critically and chronically underappreciated in the d community,totally get that ml ai is the sexiest hype iest part of d but acting like sql is easy coming to realize is just utter nonsense people tend to think sql oh yeah select from easy day just like statistic oh yeah value know everything about stats starting to realize that people who know how to wrangle data across table warehouse server etc at scale efficiently and know that their approach are actually addressing the business ask are incredibly valuable and they re compensated a such at the faangs for some reason sql like stats became this taboo word in the d community like sql oh no mean only if can get some junior schmuck to do it for me
data science update,what are some use full data science newsletters and blogs to followed as beginner,1,data science update,what are some use full data science newsletter and blog to followed a beginner
want to start career in data science have knowledge of basic mathematics have knowledge of python pandas numpy have done few certificate courses but haven managed to secure an internship any help suggestions will be valuable,want to start career in data science have knowledge of basic mathematics have knowledge of python pandas numpy have done few certificate courses but haven managed to secure an internship any help suggestions will be valuable,1,want to start career in data science have knowledge of basic mathematics have knowledge of python panda numpy have done few certificate course but haven managed to secure an internship any help suggestion will be valuable,want to start career in data science have knowledge of basic mathematics have knowledge of python panda numpy have done few certificate course but haven managed to secure an internship any help suggestion will be valuable
methods question classic zoo scenario what the name of the method described and or is there better model to use in this instance,this was the best way for me to describe the challenge we are working with and was hoping for some insight as to best way to approach amp x200b let say work at zoo for animal intake we have database of all of our animals past and present and their attributes name age height weight length skin type presenting color diet is deadly temp threshold species country of origin previous zoo and few others when we get the information about the animals arriving we have all the attributes listed above but not the animal name is there method that would take the attributes for each animal and you could compare against the incoming list and get probability score of what the animal is so take the entries we have for lions and then run comparison of that against all the incoming animals then take the entries we have for zebras and compare that set against everything coming entries for snakes etc in the end be able to sort each incoming animal by the highest probability score to determine what animal most likely is singular analysis that is batched is this thing amp x200b new to ds business analyst and just trying to learn where common sense amp proper methods combine and if this is something can investigate further using proper names thanks,1,method question classic zoo scenario what the name of the method described and or is there better model to use in this instance,this wa the best way for me to describe the challenge we are working with and wa hoping for some insight a to best way to approach amp x200b let say work at zoo for animal intake we have database of all of our animal past and present and their attribute name age height weight length skin type presenting color diet is deadly temp threshold specie country of origin previous zoo and few others when we get the information about the animal arriving we have all the attribute listed above but not the animal name is there method that would take the attribute for each animal and you could compare against the incoming list and get probability score of what the animal is so take the entry we have for lion and then run comparison of that against all the incoming animal then take the entry we have for zebra and compare that set against everything coming entry for snake etc in the end be able to sort each incoming animal by the highest probability score to determine what animal most likely is singular analysis that is batched is this thing amp x200b new to d business analyst and just trying to learn where common sense amp proper method combine and if this is something can investigate further using proper name thanks
interactive circle venn diagram in js,does anyone know of library for interactive hoverable circle venn diagrams in js,1,interactive circle venn diagram in j,doe anyone know of library for interactive hoverable circle venn diagram in j
moving from public accounting to data analytics,hi everyone has anyone here ever made the jump from public accounting auditing to sql based analyst role looking to do the same right now and looking for some advice specifically oh how you marketed your experience in accounting in relevant way during the recruiting phase info from your resumes would be especially helpful and for context applying for an analyst position with yr worth of public experience,1,moving from public accounting to data analytics,hi everyone ha anyone here ever made the jump from public accounting auditing to sql based analyst role looking to do the same right now and looking for some advice specifically oh how you marketed your experience in accounting in relevant way during the recruiting phase info from your resume would be especially helpful and for context applying for an analyst position with yr worth of public experience
how can add little bit of data science practices to my business intelligence related work the two are closely related but appreciate some practical tips use cases that can implement thank you,hi all how can add little bit of data science practices to my business intelligence related work the two are closely related but appreciate some practical tips use cases that can implement thank you,1,how can add little bit of data science practice to my business intelligence related work the two are closely related but appreciate some practical tip use case that can implement thank you,hi all how can add little bit of data science practice to my business intelligence related work the two are closely related but appreciate some practical tip use case that can implement thank you
duckdb duckdb time zones supporting calendar extensions,duckdb duckdb time zones supporting calendar extensions,1,duckdb duckdb time zone supporting calendar extension,duckdb duckdb time zone supporting calendar extension
mods please remove the post if it is not relevant moved into data science from sales background enjoying my current role but now want to know where can further move into feel the thirst to learn more any suggestions which direction should move towards,mods please remove the post if it is not relevant moved into data science from sales background enjoying my current role but now want to know where can further move into feel the thirst to learn more any suggestions which direction should move towards,1,mod please remove the post if it is not relevant moved into data science from sale background enjoying my current role but now want to know where can further move into feel the thirst to learn more any suggestion which direction should move towards,mod please remove the post if it is not relevant moved into data science from sale background enjoying my current role but now want to know where can further move into feel the thirst to learn more any suggestion which direction should move towards
how do you feel about libraries that build models for you in python,hey all data analyst here and dipping into the machine learning ds world because works wants linear logistic models have learned good bit about python and now which has been challenging and fun in use library called rattle to build linear logistic models it been great user friendly and really helped me understand the concept of modeling however could not even begin to manually code or make model like this in python feel bit like fraud and am wondering how others feel about using libraries for these things is it seen as crutch or used by people with less understanding or is it smart not to be reinventing the wheel thanks,1,how do you feel about library that build model for you in python,hey all data analyst here and dipping into the machine learning d world because work want linear logistic model have learned good bit about python and now which ha been challenging and fun in use library called rattle to build linear logistic model it been great user friendly and really helped me understand the concept of modeling however could not even begin to manually code or make model like this in python feel bit like fraud and am wondering how others feel about using library for these thing is it seen a crutch or used by people with le understanding or is it smart not to be reinventing the wheel thanks
video highlights the issues data professionals are having now which are the same what software developers used to have during their early days of their profession,video highlights the issues data professionals are having now which are the same what software developers used to have during their early days of their profession,1,video highlight the issue data professional are having now which are the same what software developer used to have during their early day of their profession,video highlight the issue data professional are having now which are the same what software developer used to have during their early day of their profession
ai got your back segmented pytorch,ai got your back segmented pytorch,1,ai got your back segmented pytorch,ai got your back segmented pytorch
what should non ds degrees learn that is underrated,saw post about how bootcamps grads are annoying to work with because they don fit well with the business implementation side of data science work ve got masters in engineering but trying to transition to do data science at the intersection of my field taking online courses and have good coding experience what are the things should learn apart from statistics modeling to get into the field that might not see in online courses,1,what should non d degree learn that is underrated,saw post about how bootcamps grad are annoying to work with because they don fit well with the business implementation side of data science work ve got master in engineering but trying to transition to do data science at the intersection of my field taking online course and have good coding experience what are the thing should learn apart from statistic modeling to get into the field that might not see in online course
how to enter in the gaming industry as data science what projects should do to be noticed,happy new year everyone studying and working as ds for almost years now today work in pr agency inside the data team data analyts so python streamlit machine learning and cloud is some of the things that usually worked this year my focus will be more in big data and artificial intelligence always loved games always wanted to be part of game and create something bigger and unique understand that are lot of ds inside some studios but wonder what type of project should do to get the attention when apply for some job or be found in linkedin for instance do you guys have any tips,1,how to enter in the gaming industry a data science what project should do to be noticed,happy new year everyone studying and working a d for almost year now today work in pr agency inside the data team data analyts so python streamlit machine learning and cloud is some of the thing that usually worked this year my focus will be more in big data and artificial intelligence always loved game always wanted to be part of game and create something bigger and unique understand that are lot of d inside some studio but wonder what type of project should do to get the attention when apply for some job or be found in linkedin for instance do you guy have any tip
can catch up on university courses on data science without foundation on calculus,hi highschool student looking into data science but didn take calculus in hs but genuinely interested in the field and have growing interest in maths in general the main concern have right now is afraid might be too far behind on math in data science so it ll either be terribly stressful or ll just get an every semester should apply for the major or look elsewhere,1,can catch up on university course on data science without foundation on calculus,hi highschool student looking into data science but didn take calculus in h but genuinely interested in the field and have growing interest in math in general the main concern have right now is afraid might be too far behind on math in data science so it ll either be terribly stressful or ll just get an every semester should apply for the major or look elsewhere
loess smoothing for sound signal,hello guys have to do research about digital hearing aids for academical purposes and decided to focus on the denoising aspect in their opitimization firstly thought of using the kalman filter but it was just too complicated for my actual level then wondered if we could use the loess smoothing to reduce the noise in an acoustic signal thanks for your answers,1,loess smoothing for sound signal,hello guy have to do research about digital hearing aid for academical purpose and decided to focus on the denoising aspect in their opitimization firstly thought of using the kalman filter but it wa just too complicated for my actual level then wondered if we could use the loess smoothing to reduce the noise in an acoustic signal thanks for your answer
data science coding amp career development,summary how can faster learn how to translate my thoughts and solutions in code my background did my bachelor in economics and business did master in marketing management where specialised in neuromarketing science and did bunch of marketing analytics courses when graduated interned as marketeer for an ai machine learning start up and was introduced to this whole new world of data science and programming so enrolled in another master called data science and marketing analytics it was very heavily focused on concept development problem solving and less on the programming part they taught because we did do lot of statistics and data manipulation for marketing analytics then interned in data science for pretty big company but their data science department is very new they offered me full time role graduate data scientist which in for few months now taught myself python throughout consider myself beginner given think only have at most year of programming experience in and python notice really struggle to translate my data science ideas solutions into code can code when told what to do but not when need to make something up from scratch this became very clear to me when they hired other data scientists who come from computer science and econometrics and have years of coding experience literally spitting out models and code in day which would take me week to do coding mentorship is not possible at the moment because no one has time really like what am doing and want to get better can help but compare myself with the others which is not fair do have very high expectations of myself has anyone been in similar position and if so even not do you have advice on how to improve coding skills try to practise as much as can through applications in the work projects and some exercises challenges not work related also suffer from burnout and anxiety so while feel like need to quickly get this together for my career find it counter productive to force myself to do more on top of what am doing now thanks for reading,1,data science coding amp career development,summary how can faster learn how to translate my thought and solution in code my background did my bachelor in economics and business did master in marketing management where specialised in neuromarketing science and did bunch of marketing analytics course when graduated interned a marketeer for an ai machine learning start up and wa introduced to this whole new world of data science and programming so enrolled in another master called data science and marketing analytics it wa very heavily focused on concept development problem solving and le on the programming part they taught because we did do lot of statistic and data manipulation for marketing analytics then interned in data science for pretty big company but their data science department is very new they offered me full time role graduate data scientist which in for few month now taught myself python throughout consider myself beginner given think only have at most year of programming experience in and python notice really struggle to translate my data science idea solution into code can code when told what to do but not when need to make something up from scratch this became very clear to me when they hired other data scientist who come from computer science and econometrics and have year of coding experience literally spitting out model and code in day which would take me week to do coding mentorship is not possible at the moment because no one ha time really like what am doing and want to get better can help but compare myself with the others which is not fair do have very high expectation of myself ha anyone been in similar position and if so even not do you have advice on how to improve coding skill try to practise a much a can through application in the work project and some exercise challenge not work related also suffer from burnout and anxiety so while feel like need to quickly get this together for my career find it counter productive to force myself to do more on top of what am doing now thanks for reading
how can you put the knowledge of data science to solve problems related to neuroscience,hi am recent graduate computer engineer currently working in consulting firm have always wanted to solve problems related to the human brain cause there are lot of unanswered question in that aspect however now that am working and am seeing that data science is mostly used for solving business problems and not really cognitive problems wanted to ask if you guys knew any solution that would help bridge the gap on how can use the data science knowledgeable into helping neuroscience without letting my computer engineering degree and the data science work experience go into use mean how can better use the work experience to solve problems related to brain will doing masters in neuroscience help or can work my way around doing masters in business analytics and then solve problems related to brain,1,how can you put the knowledge of data science to solve problem related to neuroscience,hi am recent graduate computer engineer currently working in consulting firm have always wanted to solve problem related to the human brain cause there are lot of unanswered question in that aspect however now that am working and am seeing that data science is mostly used for solving business problem and not really cognitive problem wanted to ask if you guy knew any solution that would help bridge the gap on how can use the data science knowledgeable into helping neuroscience without letting my computer engineering degree and the data science work experience go into use mean how can better use the work experience to solve problem related to brain will doing master in neuroscience help or can work my way around doing master in business analytics and then solve problem related to brain
how can you put the data science knowledge into solving problema for neuroscience,hi am recent graduate computer engineer currently working in consulting firm have always wanted to solve problems related to the human brain cause there are lot of unanswered question in that aspect however now that am working and am seeing that data science is mostly used for solving business problems and not really cognitive problems wanted to ask if you guys knew any solution that would help bridge the gap on how can use the data science knowledgeable into helping neuroscience without letting my computer engineering degree and the data science work experience go into use mean how can better use the work experience to solve problems related to brain will doing masters in neuroscience help or can work my way around doing masters in business analytics and then solve problems related to brain,1,how can you put the data science knowledge into solving problema for neuroscience,hi am recent graduate computer engineer currently working in consulting firm have always wanted to solve problem related to the human brain cause there are lot of unanswered question in that aspect however now that am working and am seeing that data science is mostly used for solving business problem and not really cognitive problem wanted to ask if you guy knew any solution that would help bridge the gap on how can use the data science knowledgeable into helping neuroscience without letting my computer engineering degree and the data science work experience go into use mean how can better use the work experience to solve problem related to brain will doing master in neuroscience help or can work my way around doing master in business analytics and then solve problem related to brain
what sorts of questions should be asking to improve my skills,ve been working in this field and it dawned on me that ve almost never asked any questions or sought help with my work just sorta figure out what think they re asking for and do that somehow this has worked so far but starting to get little suspicious of the fact that never have anything to answer to the do you have any questions anything you need from us ve gotten at the end of every checkup meeting ever,1,what sort of question should be asking to improve my skill,ve been working in this field and it dawned on me that ve almost never asked any question or sought help with my work just sorta figure out what think they re asking for and do that somehow this ha worked so far but starting to get little suspicious of the fact that never have anything to answer to the do you have any question anything you need from u ve gotten at the end of every checkup meeting ever
can we copy bak file to sql database table in azure data factory,want to know if can copy bak backup dump file to sql database table in azure data factory without using sql server management studio if yes then how if no what are the possible reasons,1,can we copy bak file to sql database table in azure data factory,want to know if can copy bak backup dump file to sql database table in azure data factory without using sql server management studio if yes then how if no what are the possible reason
strip net semantic similarity of scientific papers network,strip net semantic similarity of scientific papers network,1,strip net semantic similarity of scientific paper network,strip net semantic similarity of scientific paper network
data engineer machine learning engineer vs software engineer,hello am junior in college with major in statistics selected that major after reading book called how to get job in big data because met the author while eating dinner at hotel am wondering if maybe should not have selected my career based on such an event development honestly as cringe as it may be am mostly just focused on making as much money as possible feel like will enjoy anything do as long as it is challenging and have room to grow improve work hard and solve problems so have been thinking that maybe should just go into swe instead of ds space it seems like will likely be paid more for my efforts and will have more opportunity for entrepreneurship and career development in the future still would like to justify staying in the ds space if can have research mentor at my university who runs the predictive analytics for our university and have some understanding and knowledge regarding ds my goal for the next mo is to create full stack ds project and turn it into research publication so feel like have good head start on ds and trying to switch to swe will just set me back whole year however if am really thinking long term about my goals it sounds like objectively swe will make more sense over the course of decade ore more the only reason can justify staying with ds is if mle and de roles can provide similar compensation to swe consistently any thoughts,1,data engineer machine learning engineer v software engineer,hello am junior in college with major in statistic selected that major after reading book called how to get job in big data because met the author while eating dinner at hotel am wondering if maybe should not have selected my career based on such an event development honestly a cringe a it may be am mostly just focused on making a much money a possible feel like will enjoy anything do a long a it is challenging and have room to grow improve work hard and solve problem so have been thinking that maybe should just go into swe instead of d space it seems like will likely be paid more for my effort and will have more opportunity for entrepreneurship and career development in the future still would like to justify staying in the d space if can have research mentor at my university who run the predictive analytics for our university and have some understanding and knowledge regarding d my goal for the next mo is to create full stack d project and turn it into research publication so feel like have good head start on d and trying to switch to swe will just set me back whole year however if am really thinking long term about my goal it sound like objectively swe will make more sense over the course of decade ore more the only reason can justify staying with d is if mle and de role can provide similar compensation to swe consistently any thought
declarative libraries for analysis where are they,hi as the title suggest find it suprisingly difficult to find libraries where analysis modelling is done in declarative way say for example want to train and evaluate logistic regression model then need to split my data in test train train it on the train set predict on test and evaluate all these steps are usually easy enough to do and have high level functions but still manually putting the steps together describing how_ to do what want instead of just describing what_ want to me this gives huge uncertainty in my own analysis yes have split my data into test train but there no certainty that actually using them in the correct way in this simple case it might be easy enough but add pre processing calibration and so on and suddenly the complexity explodes this came up for me because used to do this kind of work in and now have project in python in there tidymodels where especially the package workflows is very close to what want workflows is still surprisingly new for something that think is neccesary for being confident in your model evaluations and such using scikit learn in python but can find anything similar to that think the closest thing is big machine learning experiment management like neptune but that also seems bit overkill so where are these libraries do they exist and just haven searched enough or am wrong and dumb and missing something for wanting this or am really just interrested in standarized and validated setups workflows pipelines and the declarative part is actually not that important looking forward to your comments,1,declarative library for analysis where are they,hi a the title suggest find it suprisingly difficult to find library where analysis modelling is done in declarative way say for example want to train and evaluate logistic regression model then need to split my data in test train train it on the train set predict on test and evaluate all these step are usually easy enough to do and have high level function but still manually putting the step together describing how_ to do what want instead of just describing what_ want to me this give huge uncertainty in my own analysis yes have split my data into test train but there no certainty that actually using them in the correct way in this simple case it might be easy enough but add pre processing calibration and so on and suddenly the complexity explodes this came up for me because used to do this kind of work in and now have project in python in there tidymodels where especially the package workflow is very close to what want workflow is still surprisingly new for something that think is neccesary for being confident in your model evaluation and such using scikit learn in python but can find anything similar to that think the closest thing is big machine learning experiment management like neptune but that also seems bit overkill so where are these library do they exist and just haven searched enough or am wrong and dumb and missing something for wanting this or am really just interrested in standarized and validated setup workflow pipeline and the declarative part is actually not that important looking forward to your comment
electrical engineering to data science machine learning,electrical engineering to data science machine learning,1,electrical engineering to data science machine learning,electrical engineering to data science machine learning
how to use openimages to create datasets for yolo,how to use openimages to create datasets for yolo,1,how to use openimages to create datasets for yolo,how to use openimages to create datasets for yolo
data scientists of all levels what was your first internship experience like,freshman in college that was fortunate enough to land an internship at startup but of the time have absolutely no idea what doing dw asking lot of questions and generally learning bunch but it can be demoralizing at times when business lingo is flying above my head and don really know how to approach the vague ass insights requested ngl ve probably already done said bunch of dumb shit my first week also got onboarded during sort of hectic time period so got bunch of stuff thrown at me all at once wondering if any of you have had similar experiences and or funny stories that can help me get over my imposter syndrome and realize that this should be normal even though it doesn feel that way and that not complete dumbass thanks,1,data scientist of all level what wa your first internship experience like,freshman in college that wa fortunate enough to land an internship at startup but of the time have absolutely no idea what doing dw asking lot of question and generally learning bunch but it can be demoralizing at time when business lingo is flying above my head and don really know how to approach the vague as insight requested ngl ve probably already done said bunch of dumb shit my first week also got onboarded during sort of hectic time period so got bunch of stuff thrown at me all at once wondering if any of you have had similar experience and or funny story that can help me get over my imposter syndrome and realize that this should be normal even though it doesn feel that way and that not complete dumbass thanks
phd in applied math statistics or operations research,wanna do phd and then one of the careers that aiming for is in data science however not sure as to which subject should pursue phd in statistics appears to be the most common one on linkedin at companies however my concern is that then my skillset would be limited to statistical modeling operations research sounds like the one thats most directly applicable but there limited programs for it plus it sounds too specific in its focus and oddly enough too broad in the topics it covers applied math sounds like the hardest but could learn all three optimization and probabilistic statistical and numerical mathematical modeling but perhaps its bit excessive appreciate any input,1,phd in applied math statistic or operation research,wanna do phd and then one of the career that aiming for is in data science however not sure a to which subject should pursue phd in statistic appears to be the most common one on linkedin at company however my concern is that then my skillset would be limited to statistical modeling operation research sound like the one thats most directly applicable but there limited program for it plus it sound too specific in it focus and oddly enough too broad in the topic it cover applied math sound like the hardest but could learn all three optimization and probabilistic statistical and numerical mathematical modeling but perhaps it bit excessive appreciate any input
phd in applied math statistics or operations research,wanna do phd and then one of the careers that aiming for is in data science however not sure as to which subject should pursue phd in statistics appears to be the most common one on linkedin at companies however my concern is that then my skillset would be limited to statistical modeling operations research sounds like the one thats most directly applicable but there limited programs for it plus it sounds too specific in its focus and oddly enough too broad in the topics it covers applied math sounds like the hardest but could learn all three optimization and probabilistic statistical and numerical mathematical modeling but perhaps its bit excessive appreciate any input,1,phd in applied math statistic or operation research,wanna do phd and then one of the career that aiming for is in data science however not sure a to which subject should pursue phd in statistic appears to be the most common one on linkedin at company however my concern is that then my skillset would be limited to statistical modeling operation research sound like the one thats most directly applicable but there limited program for it plus it sound too specific in it focus and oddly enough too broad in the topic it cover applied math sound like the hardest but could learn all three optimization and probabilistic statistical and numerical mathematical modeling but perhaps it bit excessive appreciate any input
what are the reasons machine learning qsar model might fail,ve been working on constructing random forest regression qsar model using high throughput screening bioassay data from pubchem using ochem and the results got found absolutely no correlation and no sense whatsoever with the molecular descriptors and the measured results from my understanding qsar models find correlation between the molecular descriptors calculations and the results you re trying to get for example logp very new to this and very confused so here are some details about what going on with my model and it would be greatly appreciated if anyone has any suggestions to help troubleshoot the goal of my model is to predict the rfu of novel compound as if it were tested in the bioassay all of the compounds in the bioassay for tested for the same purpose there are compounds in my data set only using randomly selected compounds from active compounds and from inactive because of resource limitations all of the compounds are from the same bioassay the descriptors checked on ochem were oestate and alogps and the method used was random forest regression the measured value that trying to find is the relative fluorescence units rfu inputted the pubchem cid name of the molecule on pubchem and the rfu units to train the model some of the rfu values are extremely high for some reason the active compounds range from around to values in the thousands and not sure if that would be an error the training set consists of of the data and test set contains of the data can qsar models fail just because there simply no correlation is my model basically like trying to predict the number of milk cartons hoarded by how many freckles someone has as random example of useless correlation in total how can improve my model is there anything can do to clean up my data or anything can do to make it more accurate super confused so any help would be desperately needed if there any information that would be needed to find the problem please let me know thank you here picture of the confusing model format png amp auto webp amp ee4a118ffa6cafb2fc446505ba91e59ba6f90ff0,1,what are the reason machine learning qsar model might fail,ve been working on constructing random forest regression qsar model using high throughput screening bioassay data from pubchem using ochem and the result got found absolutely no correlation and no sense whatsoever with the molecular descriptor and the measured result from my understanding qsar model find correlation between the molecular descriptor calculation and the result you re trying to get for example logp very new to this and very confused so here are some detail about what going on with my model and it would be greatly appreciated if anyone ha any suggestion to help troubleshoot the goal of my model is to predict the rfu of novel compound a if it were tested in the bioassay all of the compound in the bioassay for tested for the same purpose there are compound in my data set only using randomly selected compound from active compound and from inactive because of resource limitation all of the compound are from the same bioassay the descriptor checked on ochem were oestate and alogps and the method used wa random forest regression the measured value that trying to find is the relative fluorescence unit rfu inputted the pubchem cid name of the molecule on pubchem and the rfu unit to train the model some of the rfu value are extremely high for some reason the active compound range from around to value in the thousand and not sure if that would be an error the training set consists of of the data and test set contains of the data can qsar model fail just because there simply no correlation is my model basically like trying to predict the number of milk carton hoarded by how many freckle someone ha a random example of useless correlation in total how can improve my model is there anything can do to clean up my data or anything can do to make it more accurate super confused so any help would be desperately needed if there any information that would be needed to find the problem please let me know thank you here picture of the confusing model format png amp auto webp amp ee4a118ffa6cafb2fc446505ba91e59ba6f90ff0
million source code file dataset,million source code file dataset,1,million source code file dataset,million source code file dataset
making predictions on streaming data that often have missing features what is good algorithm to use,suppose trained an algorithm on dataset with features which is the maximum possible number of features that may be present in the test set for example suppose have an existing dataset of people on single day that has features for each person such as caffeine intake hours of exercise resting heart rate bmi etc during that day with the outcome variable being number of hours of sleep obtained that night in the test set people vary in how many features they choose or are able to submit what is sensible algorithm or way to deal with the variable number of features in the test set,1,making prediction on streaming data that often have missing feature what is good algorithm to use,suppose trained an algorithm on dataset with feature which is the maximum possible number of feature that may be present in the test set for example suppose have an existing dataset of people on single day that ha feature for each person such a caffeine intake hour of exercise resting heart rate bmi etc during that day with the outcome variable being number of hour of sleep obtained that night in the test set people vary in how many feature they choose or are able to submit what is sensible algorithm or way to deal with the variable number of feature in the test set
everybody working in data science what are your opinions on non ml stats phd being your bosses particularly if they got into the field after you,everybody working in data science what are your opinions on non ml stats phd being your bosses particularly if they got into the field after you,1,everybody working in data science what are your opinion on non ml stats phd being your boss particularly if they got into the field after you,everybody working in data science what are your opinion on non ml stats phd being your boss particularly if they got into the field after you
dealing with highly imbalanced data,when to not use smote,1,dealing with highly imbalanced data,when to not use smote
project monitoring amp control,hello everyone happy new year have been working as program manager at an insuretech company and have been handling data migration and transformation programs across business till mid last year have been assigned the responsibility to lead an data analytics program the team constitutes of data engineers scientists ba head of analytics the team has been working without clear roadmap till date and has just managed barely to keep up with the analytics requirements till date of generating meaningful insights have received backlog of ml ai models from the team and our ceo has signed off on having couple of them prioritised for the next two quarters my ceo expects roadmap and continuous reporting of the progress of the models and an expected date of getting it in an operational phase am getting resistance from my team in terms of giving dates which is fine but how should approach this issue how can set up expectations in terms of when can business start getting some value out of these models have created milestones for each of these models exploration amp cleaning training tuning evaluation and am trying to track the progress based on that,1,project monitoring amp control,hello everyone happy new year have been working a program manager at an insuretech company and have been handling data migration and transformation program across business till mid last year have been assigned the responsibility to lead an data analytics program the team constitutes of data engineer scientist ba head of analytics the team ha been working without clear roadmap till date and ha just managed barely to keep up with the analytics requirement till date of generating meaningful insight have received backlog of ml ai model from the team and our ceo ha signed off on having couple of them prioritised for the next two quarter my ceo expects roadmap and continuous reporting of the progress of the model and an expected date of getting it in an operational phase am getting resistance from my team in term of giving date which is fine but how should approach this issue how can set up expectation in term of when can business start getting some value out of these model have created milestone for each of these model exploration amp cleaning training tuning evaluation and am trying to track the progress based on that
does anyone here make over what your title what do you do,mods remove if this type of post isn allowed was just blown away by how many north cal software engineers make from another thread and wondering if ds has the same type of opportunities,1,doe anyone here make over what your title what do you do,mod remove if this type of post isn allowed wa just blown away by how many north cal software engineer make from another thread and wondering if d ha the same type of opportunity
not good at math,come from non mathematical background pharmacist and interested in data science how much math do need to know to be able to understand ml it did take me while to understand linear regression and classification mathematical concepts tried watching calculus probability and statistics but the more progress through the course the less understand its new concepts my question is how much math is needed and if anyone has some good resources to watch study from thanks,1,not good at math,come from non mathematical background pharmacist and interested in data science how much math do need to know to be able to understand ml it did take me while to understand linear regression and classification mathematical concept tried watching calculus probability and statistic but the more progress through the course the le understand it new concept my question is how much math is needed and if anyone ha some good resource to watch study from thanks
psych major pursuing stats ms the goal of going into ds,hi hope this is the appropriate sub for this having bit of crisis deciding what career like to be in and honestly love stats do different analyses in spss for my psych degree and will be graduating in year with the final semester being research seminar wanted to be physician assistant and still considering it but not so sure being able to work on what seems to be achievement based system rather than appointment system in many health care jobs seems nice also not fan of pure coding though be willing to learn python sql whatever need just like data and stats so too far in to ditch my psych major knew end up doing some post grad stuff so not too worried about that but considering an ms in statistics in order to get into the data science field because many people say degree in ds isn really required and others say ds is another term for statistician nowadays regarding job posts also kinda curious about base pay if anyone has any insights know is average to begin with but has anyone started off higher any advice or input is welcomed thank you,1,psych major pursuing stats m the goal of going into d,hi hope this is the appropriate sub for this having bit of crisis deciding what career like to be in and honestly love stats do different analysis in spss for my psych degree and will be graduating in year with the final semester being research seminar wanted to be physician assistant and still considering it but not so sure being able to work on what seems to be achievement based system rather than appointment system in many health care job seems nice also not fan of pure coding though be willing to learn python sql whatever need just like data and stats so too far in to ditch my psych major knew end up doing some post grad stuff so not too worried about that but considering an m in statistic in order to get into the data science field because many people say degree in d isn really required and others say d is another term for statistician nowadays regarding job post also kinda curious about base pay if anyone ha any insight know is average to begin with but ha anyone started off higher any advice or input is welcomed thank you
thought experiment if you had to compare of millions of records to million or so others about x10 record cross join with over columns how would you go about doing it is there even way to accomplish it,thought experiment if you had to compare of millions of records to million or so others about x10 record cross join with over columns how would you go about doing it is there even way to accomplish it,1,thought experiment if you had to compare of million of record to million or so others about x10 record cross join with over column how would you go about doing it is there even way to accomplish it,thought experiment if you had to compare of million of record to million or so others about x10 record cross join with over column how would you go about doing it is there even way to accomplish it
how to set up consulting gig,previous employer of mine had reached out to ask for my help as an outside consultant interested but have no experience working as consultant the work will be supporting operational report run building and automating etl processes and likely analysis work in other words there will be weekly monthly deliverables and mid to long term projects don know what an appropriate rate is my current hourly salary reasonable also don know if need to create company is that better for tax purposes liability or can keep it simple and collect monthly check like back in the days when do tutoring and just claim it as ordinary income what else do need to know at the don even know what don know stage and just looking for any information,1,how to set up consulting gig,previous employer of mine had reached out to ask for my help a an outside consultant interested but have no experience working a consultant the work will be supporting operational report run building and automating etl process and likely analysis work in other word there will be weekly monthly deliverable and mid to long term project don know what an appropriate rate is my current hourly salary reasonable also don know if need to create company is that better for tax purpose liability or can keep it simple and collect monthly check like back in the day when do tutoring and just claim it a ordinary income what else do need to know at the don even know what don know stage and just looking for any information
what is the highest job title you can attain in data science,does it differ by company or is there general name for it for non manager position,1,what is the highest job title you can attain in data science,doe it differ by company or is there general name for it for non manager position
in your experience for dataviz which are the programs that could be better mean don want to invest time in something couldn use thanks,in your experience for dataviz which are the programs that could be better mean don want to invest time in something couldn use thanks,1,in your experience for dataviz which are the program that could be better mean don want to invest time in something couldn use thanks,in your experience for dataviz which are the program that could be better mean don want to invest time in something couldn use thanks
how do make the most money in this field,hi bachelor student who recently got his first job in data analytics it not data science which had hard on for before but it still fun and like my job very much so thought damn really do not care at all what do do as long as have to think bit talk with people bit and my colleagues don suck applying for masters right now eu so it free and wondering what should focus on where should my path lead if wanted to base my decision purely on money is it the senior data scientists is it the big data architects is it the cloud architects or do have to wait few years to become tech lead or project manager and only then become the best paid any experience is welcome thank you all and wish you happy new year,1,how do make the most money in this field,hi bachelor student who recently got his first job in data analytics it not data science which had hard on for before but it still fun and like my job very much so thought damn really do not care at all what do do a long a have to think bit talk with people bit and my colleague don suck applying for master right now eu so it free and wondering what should focus on where should my path lead if wanted to base my decision purely on money is it the senior data scientist is it the big data architect is it the cloud architect or do have to wait few year to become tech lead or project manager and only then become the best paid any experience is welcome thank you all and wish you happy new year
search engine for time series,hi last year developed passenger flow forecasting model passenger flows are heavily influenced by lockdown measures and the weather so wanted to incorporate features relating to that into my model doing so encountered various frustrations amp x200b data providers generally focus on specific domain covid weather ecommerce forecasts however can be influenced by data from many domains finding these providers signing up and reading documentation is very time consuming it takes lot of code just to get the data you want for example to get covid data for my province first had to call list endpoint then retrieve the province identifier and then loop over the data endpoint because the maximum range was months think the core problem is that api as the acronym indicates are meant to be interfaced with programmatically but that data scientists end up calling api manually before the model runs in production my proposed solution is to build platform that partners with many data providers indexes their time series and make them searchable with semantic search each result would be single time series this search would be usable via web ui and endpoint screenshot of web ui prototype amp x200b format png amp auto webp amp d34025f812891757ea5f6050f85f641a2ef19c75 library amp x200b format png amp auto webp amp ee059e35ffe2e458cec11f0772bb927b366aaafc currently have working prototype with pretty decent search but haven really validated my assumptions it would be great if you could provide some feedback are you also interested in adding external data into your models what type of models are these how do you currently get external data into your model what are your major pain points do you semantic search could speed up the process of acquiring external datasets any other thoughts thanks for your feedback,1,search engine for time series,hi last year developed passenger flow forecasting model passenger flow are heavily influenced by lockdown measure and the weather so wanted to incorporate feature relating to that into my model doing so encountered various frustration amp x200b data provider generally focus on specific domain covid weather ecommerce forecast however can be influenced by data from many domain finding these provider signing up and reading documentation is very time consuming it take lot of code just to get the data you want for example to get covid data for my province first had to call list endpoint then retrieve the province identifier and then loop over the data endpoint because the maximum range wa month think the core problem is that api a the acronym indicates are meant to be interfaced with programmatically but that data scientist end up calling api manually before the model run in production my proposed solution is to build platform that partner with many data provider index their time series and make them searchable with semantic search each result would be single time series this search would be usable via web ui and endpoint screenshot of web ui prototype amp x200b format png amp auto webp amp d34025f812891757ea5f6050f85f641a2ef19c75 library amp x200b format png amp auto webp amp ee059e35ffe2e458cec11f0772bb927b366aaafc currently have working prototype with pretty decent search but haven really validated my assumption it would be great if you could provide some feedback are you also interested in adding external data into your model what type of model are these how do you currently get external data into your model what are your major pain point do you semantic search could speed up the process of acquiring external datasets any other thought thanks for your feedback
which uni to choose for msc business analytics in uk,which uni to choose for msc business analytics in uk,1,which uni to choose for msc business analytics in uk,which uni to choose for msc business analytics in uk
is moda or management science important for data science analysis,am studying my master in data science amp decision support and there are particularly two courses that find interesting management science and multi objective decision analysis moda would like to become data business analyst in consulting company know these courses are offered for reason but am wondering whether there is direct or indirect implication in the real world are there any specific common situation in data science field where the knowledge of the previously mentioned courses will come in handy,1,is moda or management science important for data science analysis,am studying my master in data science amp decision support and there are particularly two course that find interesting management science and multi objective decision analysis moda would like to become data business analyst in consulting company know these course are offered for reason but am wondering whether there is direct or indirect implication in the real world are there any specific common situation in data science field where the knowledge of the previously mentioned course will come in handy
resources for data quality in banking financial services,hello part of team tasked with assuring data quality of large database of financial data credit cards collaterals etc we re essentially looking for anomalies such as single outliers or abnormal grouped behavior by analyzing trends through time we re doing very basic stuff average std med range through time with powerbi but would like for us to go beyond and increase the complexity of our analysis by using python or the stuff find online seems to be mostly related to it such as network data monitoring but maybe not searching using the right terms would greatly appreciate if you had any titles of books or research paper pertaining to techniques or methods that would be applicable to the banking sector or maybe just concepts that could search for thanks,1,resource for data quality in banking financial service,hello part of team tasked with assuring data quality of large database of financial data credit card collateral etc we re essentially looking for anomaly such a single outlier or abnormal grouped behavior by analyzing trend through time we re doing very basic stuff average std med range through time with powerbi but would like for u to go beyond and increase the complexity of our analysis by using python or the stuff find online seems to be mostly related to it such a network data monitoring but maybe not searching using the right term would greatly appreciate if you had any title of book or research paper pertaining to technique or method that would be applicable to the banking sector or maybe just concept that could search for thanks
quantifying impact of variables in glm,say have function in the form of yi exp b0 b1 x1 b2 x2 b1 and b2 how would go about quantifying the impact of both x1 and x2 on yi,1,quantifying impact of variable in glm,say have function in the form of yi exp b0 b1 x1 b2 x2 b1 and b2 how would go about quantifying the impact of both x1 and x2 on yi
quantifying impact of variables in glm,say have function in the form of yi exp b0 b1 x1 b2 x2 b1 and b2 how would go about quantifying the impact of both x1 and x2 on yi,1,quantifying impact of variable in glm,say have function in the form of yi exp b0 b1 x1 b2 x2 b1 and b2 how would go about quantifying the impact of both x1 and x2 on yi
any good data sources for first time project that uses csv web data postgres and ssms in power bi,am working on project and basically just want to get handful of data sources from several different ways to show it can work together for demo at an interview it would be ideal to have ssis get data in from somewhere clean it load into ssms then ssms connects to power bi in power bi it should input from ssms as well as other sources like an api web data excel csv postgres etc dynamic data would be great as that would really show what the model is capable of from there will connect al the data sources and visualize in some way in power bi and publish it to online my problem is am struggling to find data if can find some reliable basic datasets for architecture healthcare environmental studies stock data that can somehow be used that would be awesome thanks any advice is helpful,1,any good data source for first time project that us csv web data postgres and ssms in power bi,am working on project and basically just want to get handful of data source from several different way to show it can work together for demo at an interview it would be ideal to have ssis get data in from somewhere clean it load into ssms then ssms connects to power bi in power bi it should input from ssms a well a other source like an api web data excel csv postgres etc dynamic data would be great a that would really show what the model is capable of from there will connect al the data source and visualize in some way in power bi and publish it to online my problem is am struggling to find data if can find some reliable basic datasets for architecture healthcare environmental study stock data that can somehow be used that would be awesome thanks any advice is helpful
is it worthwhile to make the switch from scratch to python for machine learning,scratch is what am most proficient in and have already completed various ai projects with but my colleagues tell me it will be worth it to learn how to program in python even though will be set back in the short term is this true or is scratch just as sophisticated of language for ai my goal is to get into faang company and am in some talks so does anyone know if they have preference,1,is it worthwhile to make the switch from scratch to python for machine learning,scratch is what am most proficient in and have already completed various ai project with but my colleague tell me it will be worth it to learn how to program in python even though will be set back in the short term is this true or is scratch just a sophisticated of language for ai my goal is to get into faang company and am in some talk so doe anyone know if they have preference
is data structures and algorithms worth spending my time on before start my job,recently signed an offer to start working as data scientist after the summer extremely excited as the role is known for being ml stats data engineering heavy dashboarding type projects will not be part of the role as they fall under the other team my background is masters level non cs so haven formally learnt data structures and algorithms know the basics of the most important data structures lists sets hashmaps queues contigous data structures etc and intuitions about their corresponding time space complexity they were needed for some advanced courses combinatorial optimisation together with some sorting search algorithms binary search dfs bfs but here typically only what was needed was covered for now until graduate looking at areas can improve before start working aside from ds amp ve also identified extra cloud skills have entry level azure certs already but can go for more ci cd testing frameworks and web api skills for model deployment do kind of reason ds amp while writing code but maybe actually formally learning about them would make me far better programmer should potentially just try and wing it with my current knowledge what do you think are they really that important aside from leetcode you get on interviews which no longer have to do in the past ve always avoided self learning these as believed my time was better spent on extra math stat than on ds amp,1,is data structure and algorithm worth spending my time on before start my job,recently signed an offer to start working a data scientist after the summer extremely excited a the role is known for being ml stats data engineering heavy dashboarding type project will not be part of the role a they fall under the other team my background is master level non c so haven formally learnt data structure and algorithm know the basic of the most important data structure list set hashmaps queue contigous data structure etc and intuition about their corresponding time space complexity they were needed for some advanced course combinatorial optimisation together with some sorting search algorithm binary search dfs bfs but here typically only what wa needed wa covered for now until graduate looking at area can improve before start working aside from d amp ve also identified extra cloud skill have entry level azure cert already but can go for more ci cd testing framework and web api skill for model deployment do kind of reason d amp while writing code but maybe actually formally learning about them would make me far better programmer should potentially just try and wing it with my current knowledge what do you think are they really that important aside from leetcode you get on interview which no longer have to do in the past ve always avoided self learning these a believed my time wa better spent on extra math stat than on d amp
ray skorch distributed pytorch on ray with sklearn api,tl dr train pytorch models on large tabular datasets with scikit learn skorch api hi datascience the principal author of ray skorch library that lets you run distributed pytorch training on large scale datasets while providing familiar scikit learn compatible skorch api integrating well with the rest of the scikit learn ecosystem under the hood ray skorch uses ray train for distributed pytorch training and ray data for handling and shuffling large datasets ray skorch works only with tabular data currently it can use numpy arrays pandas dataframes and ray data datasets pip install ray skorch you can switch your skorch code to ray skorch just by changing few lines import numpy as np from sklearn datasets import make_classification from torch import nn pip install pytorch_tabnet from pytorch_tabnet tab_network import tabnet from ray_skorch import raytrainneuralnet make_classification n_informative random_state astype np float32 astype np int64 net raytrainneuralnet tabnet num_workers the only new mandatory argument criterion nn crossentropyloss max_epochs lr tabnet specific arguments module__input_dim module__output_dim required for classification loss funcs iterator_train__unsqueeze_label_tensor false iterator_valid__unsqueeze_label_tensor false net fit predict_proba returns ray data dataset y_proba net predict_proba to_pandas more examples including ones on bigger datasets can be found here the package is experimental and love to hear your feedback both on the package itself and on the concept of distributed training on tabular data with simple familiar apis any comments suggestions or bug reports are hugely appreciated,1,ray skorch distributed pytorch on ray with sklearn api,tl dr train pytorch model on large tabular datasets with scikit learn skorch api hi datascience the principal author of ray skorch library that let you run distributed pytorch training on large scale datasets while providing familiar scikit learn compatible skorch api integrating well with the rest of the scikit learn ecosystem under the hood ray skorch us ray train for distributed pytorch training and ray data for handling and shuffling large datasets ray skorch work only with tabular data currently it can use numpy array panda dataframes and ray data datasets pip install ray skorch you can switch your skorch code to ray skorch just by changing few line import numpy a np from sklearn datasets import make_classification from torch import nn pip install pytorch_tabnet from pytorch_tabnet tab_network import tabnet from ray_skorch import raytrainneuralnet make_classification n_informative random_state astype np float32 astype np int64 net raytrainneuralnet tabnet num_workers the only new mandatory argument criterion nn crossentropyloss max_epochs lr tabnet specific argument module__input_dim module__output_dim required for classification loss funcs iterator_train__unsqueeze_label_tensor false iterator_valid__unsqueeze_label_tensor false net fit predict_proba return ray data dataset y_proba net predict_proba to_pandas more example including one on bigger datasets can be found here the package is experimental and love to hear your feedback both on the package itself and on the concept of distributed training on tabular data with simple familiar apis any comment suggestion or bug report are hugely appreciated
is data science better than software engineering for degree,hey guys wish to study software engineering in my country for my degree but really like to study data science want to know which one is best for future and which one is demand in future interested about both of them,1,is data science better than software engineering for degree,hey guy wish to study software engineering in my country for my degree but really like to study data science want to know which one is best for future and which one is demand in future interested about both of them
anyone have data science readiness template or report,working with client to asses their data science capabilities in current environment,1,anyone have data science readiness template or report,working with client to ass their data science capability in current environment
map in object detection,map in object detection,1,map in object detection,map in object detection
deploy dash app for free,hello sure it dummy question but give it try just in case would like to create dash app for my team it is possible to upload it online in free amp fully protected way my entreprise is fully on microsoft sharepoint powerbi amp co so really can ask for help to my it department for that almost sure it not possible do need host server that can be not free since can use personal computer for that but prefer to ask love to learn new skills outside microsoft tools and it would be great opportunity for me to do so also have quite lot of free time for now thanks for any input,1,deploy dash app for free,hello sure it dummy question but give it try just in case would like to create dash app for my team it is possible to upload it online in free amp fully protected way my entreprise is fully on microsoft sharepoint powerbi amp co so really can ask for help to my it department for that almost sure it not possible do need host server that can be not free since can use personal computer for that but prefer to ask love to learn new skill outside microsoft tool and it would be great opportunity for me to do so also have quite lot of free time for now thanks for any input
learning data science,what is the best place to learn data science view poll,1,learning data science,what is the best place to learn data science view poll
ds bootcamp grads wtf is your deal,this is now the third time ve had ds bootcamp grad join my team not by choice and having disastrous results do they not teach you how to operate computers or use the command line how to deploy to anything but one click paas with no iam why is the answer to every business question cloned repo you found on tds that uses apis we can license how can you not know sql or how to set up local instance why is docker such fucking mystery to all of you ever heard of venv yes know it works on your personal machine because you re using web based ide our company blocks for reason like get it everyone else in the company is ready to fire the money cannon straight at your face every time you say neural network but ffs learn how to run something anything on an actual enterprise grade cloud provider az aws or gcp if you must learn to use git so can actually work with your stupid ipynb notebooks and for the love of god do not keep asking me to call you data scientist in meetings you were barista weeks ago no matter what they told you at general assembly sorry for the rant just boggled by what these programs are teaching people and telling them they re qualified to do after they ve taken your money if you re not walking away from your program without knowing how to at least do tiny bit of etl or understanding that not all the data in industry is going to be as clean as the yelp twitter kaggle data you ve been using your little pip install imadatascientist toys on you re going to run into people like me who know enough to know you re sham but who won tell you so but instead rip you anonymously on reddit there now feel better love to all the actual data scientists out there also do of these things as well but just dumb product manager so allowed to,1,d bootcamp grad wtf is your deal,this is now the third time ve had d bootcamp grad join my team not by choice and having disastrous result do they not teach you how to operate computer or use the command line how to deploy to anything but one click paas with no iam why is the answer to every business question cloned repo you found on tds that us apis we can license how can you not know sql or how to set up local instance why is docker such fucking mystery to all of you ever heard of venv yes know it work on your personal machine because you re using web based ide our company block for reason like get it everyone else in the company is ready to fire the money cannon straight at your face every time you say neural network but ffs learn how to run something anything on an actual enterprise grade cloud provider az aws or gcp if you must learn to use git so can actually work with your stupid ipynb notebook and for the love of god do not keep asking me to call you data scientist in meeting you were barista week ago no matter what they told you at general assembly sorry for the rant just boggled by what these program are teaching people and telling them they re qualified to do after they ve taken your money if you re not walking away from your program without knowing how to at least do tiny bit of etl or understanding that not all the data in industry is going to be a clean a the yelp twitter kaggle data you ve been using your little pip install imadatascientist toy on you re going to run into people like me who know enough to know you re sham but who won tell you so but instead rip you anonymously on reddit there now feel better love to all the actual data scientist out there also do of these thing a well but just dumb product manager so allowed to
where to start learning data science and analytics,hello everyone thought all could help guide me to good courses youtube channels books podcasts or any learning materials that could help me start my journey as data analyst my academic background is mechanical engineering but now considering career shift anything informative but also fun interactive and challenging would be awesome,1,where to start learning data science and analytics,hello everyone thought all could help guide me to good course youtube channel book podcasts or any learning material that could help me start my journey a data analyst my academic background is mechanical engineering but now considering career shift anything informative but also fun interactive and challenging would be awesome
from owning financial assets to data assets new norm for cfos,from owning financial assets to data assets new norm for cfos,1,from owning financial asset to data asset new norm for cfo,from owning financial asset to data asset new norm for cfo
what is data science course really allabout,what is data science course really allabout,1,what is data science course really allabout,what is data science course really allabout
data science made simple even your kids can do it,data science made simple even your kids can do it,1,data science made simple even your kid can do it,data science made simple even your kid can do it
should circle back to data structures and algorithms or is time best spent elsewhere,hey reddit recently signed an offer to start working as data scientist after the summer extremely excited as the role is known for being ml stats data engineering heavy heavy emphasis on dashboarding will not be part of the role for now until graduate looking at areas can improve before start working aside from ds amp ve also identified cloud skills ci cd testing frameworks and basic web skills for model deployment my background is masters level non cs so haven formally learnt data structures and algorithms so this seems like good place to begin know the basics of the most important data structures lists sets hashmaps queues and intuitions about their corresponding time space complexity they were needed for some advanced courses discrete optimisation together with some sorting search algorithms but here typically only what was needed was covered do kind of reason about them while writing code but maybe actually formally learning about them would make me far better programmer should potentially just try and wing it with my current knowledge what do you think in the past ve always avoided self learning these as believed my time was better spent on extra math stat than on ds amp if you believe it is smart idea can you recommend some resources know could more or less find an open sourced university course and watch the videos and do the exercises but mostly looking for something maybe tad less rigorous and more hands on it can be in any of python java or scala don have preference here,1,should circle back to data structure and algorithm or is time best spent elsewhere,hey reddit recently signed an offer to start working a data scientist after the summer extremely excited a the role is known for being ml stats data engineering heavy heavy emphasis on dashboarding will not be part of the role for now until graduate looking at area can improve before start working aside from d amp ve also identified cloud skill ci cd testing framework and basic web skill for model deployment my background is master level non c so haven formally learnt data structure and algorithm so this seems like good place to begin know the basic of the most important data structure list set hashmaps queue and intuition about their corresponding time space complexity they were needed for some advanced course discrete optimisation together with some sorting search algorithm but here typically only what wa needed wa covered do kind of reason about them while writing code but maybe actually formally learning about them would make me far better programmer should potentially just try and wing it with my current knowledge what do you think in the past ve always avoided self learning these a believed my time wa better spent on extra math stat than on d amp if you believe it is smart idea can you recommend some resource know could more or le find an open sourced university course and watch the video and do the exercise but mostly looking for something maybe tad le rigorous and more hand on it can be in any of python java or scala don have preference here
what is the difference between statistical data science and normal data science,what is the difference between statistical data science offered by some university and normal data science also offered by another university from what know usually not the same university am aware that statistical data science is more on applied mathematics but what is the difference plus the advantages and disadvantages between both of them,1,what is the difference between statistical data science and normal data science,what is the difference between statistical data science offered by some university and normal data science also offered by another university from what know usually not the same university am aware that statistical data science is more on applied mathematics but what is the difference plus the advantage and disadvantage between both of them
is the insight fellowship program done,title says it all just graduated and had plans to apply for the program if didn have any luck in my job search but their website and the social media accounts seems to all by dead can seem to find something that mentions break or timeline for returning just looking for any info on this,1,is the insight fellowship program done,title say it all just graduated and had plan to apply for the program if didn have any luck in my job search but their website and the social medium account seems to all by dead can seem to find something that mention break or timeline for returning just looking for any info on this
is it possible to do free lancing in data science,hi newbie with free lancing here worked in company as software testing engineer for months after that due to personal reasons couldn continue my software career recently learnt data science course and as not fresher and took career gaps of years it hard for me to get opportunities with good package so planning to start work on my own as freelancer in data science is anyone there here who is doing freelancing in data science how do you begin your career things to do for freelancing and experiences over it any suggestion would be helpful thanks in advance,1,is it possible to do free lancing in data science,hi newbie with free lancing here worked in company a software testing engineer for month after that due to personal reason couldn continue my software career recently learnt data science course and a not fresher and took career gap of year it hard for me to get opportunity with good package so planning to start work on my own a freelancer in data science is anyone there here who is doing freelancing in data science how do you begin your career thing to do for freelancing and experience over it any suggestion would be helpful thanks in advance
what programming languages are usually used for data science,wanna get head start on some of the languages before college if any of you know playlist could watch that could help it would be really nice just incase saying this would also help live in the philippines,1,what programming language are usually used for data science,wanna get head start on some of the language before college if any of you know playlist could watch that could help it would be really nice just incase saying this would also help live in the philippine
should bother starting career in data science honest opinion needed,long story short didn go university and did help desk support job didn like the role at all but found about data science and as started learning more and more about it it began to fascinate me so much so wanted to start career in it however the issue is firstly have no programming experience in microsoft sql in pythonand don even know how to use spreadsheets properly but the biggest hindrance for me would probably be the lack of mathematical ability in school was not great at maths did pass it but barely now only maths know is addition subtraction multiplication really not division so much know some will say the maths isn imperative for career in data but think deep down it is all the data scientists in my old workplace were all mathematical and logically minded do you think should be real and accept that my maths ability would probably hinder me becoming data scientist,1,should bother starting career in data science honest opinion needed,long story short didn go university and did help desk support job didn like the role at all but found about data science and a started learning more and more about it it began to fascinate me so much so wanted to start career in it however the issue is firstly have no programming experience in microsoft sql in pythonand don even know how to use spreadsheet properly but the biggest hindrance for me would probably be the lack of mathematical ability in school wa not great at math did pas it but barely now only math know is addition subtraction multiplication really not division so much know some will say the math isn imperative for career in data but think deep down it is all the data scientist in my old workplace were all mathematical and logically minded do you think should be real and accept that my math ability would probably hinder me becoming data scientist
moving to data science at,dears hope you are all doing great today so have been working in supply chain for the past years and am planning to study programing to become data science analyst focused in supply chain procurement data price trends export and imports dutties lead times etc however would like to hear from you anyone here works on this field am too old to start learning with no tech background where should start thanks,1,moving to data science at,dear hope you are all doing great today so have been working in supply chain for the past year and am planning to study programing to become data science analyst focused in supply chain procurement data price trend export and import dutties lead time etc however would like to hear from you anyone here work on this field am too old to start learning with no tech background where should start thanks
how to simply characterize curve shapes,hello so have time series dataframe where one column is chronological timestamps and then next column are unique values in values column with each value associated with its corresponding timestamp that it just those two columns in the dataframe can then plot this data and produce plotted curve with timestamps on the axis and values on the axis can observe how the curve goes up and down down and up and how many times the slope changes from positive to negative and vice versa now have many of these same type of dataframes let say have of these dataframes each with defining curve shape what want to do is eventually cluster these curve shapes into essentially taking all of the curve shapes and essentially saying ok see we have curve type curve type and curve type etc and these are the categories all dataframes fall into would then assign each dataframe its identified curve type now the big challenge am struggling with is how do you characterize curve what would the characteristics be so that they can be grouped together know there are probably some really complex curve shape clustering packages out there but in my experience these are very in depth and complex and can be bit of black box am looking for method that is super intuitive and straightforward for example one characteristic can think of is the number of count of the number of peaks and valleys in curve for instance lets say we have two types of curves where we see lots of ups and downs and where there are just few ups and downs just counting the number of peaks and valleys we could then cluster the curve populations into two distinct curve shape types but what other characteristics specifically simple and direct characteristics that could define curves that could count calculate and then use for clustering am open to suggestions here and would really appreciate any guidance on this also my apologies for this probably not being clear as could be here am really struggling to articulate this objective into the most clear terms and explanation thanks,1,how to simply characterize curve shape,hello so have time series dataframe where one column is chronological timestamps and then next column are unique value in value column with each value associated with it corresponding timestamp that it just those two column in the dataframe can then plot this data and produce plotted curve with timestamps on the axis and value on the axis can observe how the curve go up and down down and up and how many time the slope change from positive to negative and vice versa now have many of these same type of dataframes let say have of these dataframes each with defining curve shape what want to do is eventually cluster these curve shape into essentially taking all of the curve shape and essentially saying ok see we have curve type curve type and curve type etc and these are the category all dataframes fall into would then assign each dataframe it identified curve type now the big challenge am struggling with is how do you characterize curve what would the characteristic be so that they can be grouped together know there are probably some really complex curve shape clustering package out there but in my experience these are very in depth and complex and can be bit of black box am looking for method that is super intuitive and straightforward for example one characteristic can think of is the number of count of the number of peak and valley in curve for instance let say we have two type of curve where we see lot of ups and down and where there are just few ups and down just counting the number of peak and valley we could then cluster the curve population into two distinct curve shape type but what other characteristic specifically simple and direct characteristic that could define curve that could count calculate and then use for clustering am open to suggestion here and would really appreciate any guidance on this also my apology for this probably not being clear a could be here am really struggling to articulate this objective into the most clear term and explanation thanks
new to data science and don know how to start how did you start,new to data science and don know how to start how did you start,1,new to data science and don know how to start how did you start,new to data science and don know how to start how did you start
misleading claims of model accuracy,in group competition in machine learning class trying to solve classification problem one team claimed to have solved the challenge which was total piece of cake quote and to have accuracy quote too very smug attitude the professor was very skeptical he designed the challenge so he went over there and checked out their model he asked to see the confusion matrix it turns out that what they meant by accuracy was actually recall all the actual positives labels were indeed predicted to be positive by their model the catch their model predicted anything to be positive lol regardless of the input so yea recall facepalm guess it was at least useful so that the professor could re iterate how you must balance precision amp recall thru the importance of understanding the performance claims about machine learning model etc makes me wonder how many data scientists out there actually fool knowingly or not not sure which one is worse their business partners under claims of very high accuracy and flashy plots showing of some ill defined metric and which one is actually worse ignorance or manipulation,1,misleading claim of model accuracy,in group competition in machine learning class trying to solve classification problem one team claimed to have solved the challenge which wa total piece of cake quote and to have accuracy quote too very smug attitude the professor wa very skeptical he designed the challenge so he went over there and checked out their model he asked to see the confusion matrix it turn out that what they meant by accuracy wa actually recall all the actual positive label were indeed predicted to be positive by their model the catch their model predicted anything to be positive lol regardless of the input so yea recall facepalm guess it wa at least useful so that the professor could re iterate how you must balance precision amp recall thru the importance of understanding the performance claim about machine learning model etc make me wonder how many data scientist out there actually fool knowingly or not not sure which one is worse their business partner under claim of very high accuracy and flashy plot showing of some ill defined metric and which one is actually worse ignorance or manipulation
in an enterprise setting what is mostly used for data visualization,starting my data science journey and my question is in an enterprise setting what is mostly used for visualizations know there is power bi and tableau but can also create visualizations in or jupyter are the visualizations create in or jupyter just for me personally and the other visualization tools are used to present the data to the executives or are they interchangeable,1,in an enterprise setting what is mostly used for data visualization,starting my data science journey and my question is in an enterprise setting what is mostly used for visualization know there is power bi and tableau but can also create visualization in or jupyter are the visualization create in or jupyter just for me personally and the other visualization tool are used to present the data to the executive or are they interchangeable
master in data science at uw madison expectations,hey everyone finishing undergrad this year hopefully with ba in econ in the process of applying to the program and just was wondering what should expect in the admissions process during the program and job prospects afterward have the following questions sorry if they re too much just figured this is the only place where can get lot of these questions answered have ug gpa but no research experience and weak major how does this make me stack up in the admissions process am weak applicant how nervous should be during the program any advice on maximizing return getting into research or job market prep prefer to try and pursue an mle based job following the program how confident should be in getting job after this program given the university know the job market is an absolute mess and oversaturated so will admit to being nervous any tips on what to study or project pursuance in order to make sure can get position somewhere at the very least again thank you guys for any responses ve found reading through this sub so helpful in my own growth and am grateful for the wisdom lot of you guys share,1,master in data science at uw madison expectation,hey everyone finishing undergrad this year hopefully with ba in econ in the process of applying to the program and just wa wondering what should expect in the admission process during the program and job prospect afterward have the following question sorry if they re too much just figured this is the only place where can get lot of these question answered have ug gpa but no research experience and weak major how doe this make me stack up in the admission process am weak applicant how nervous should be during the program any advice on maximizing return getting into research or job market prep prefer to try and pursue an mle based job following the program how confident should be in getting job after this program given the university know the job market is an absolute mess and oversaturated so will admit to being nervous any tip on what to study or project pursuance in order to make sure can get position somewhere at the very least again thank you guy for any response ve found reading through this sub so helpful in my own growth and am grateful for the wisdom lot of you guy share
for all metaverse and vr lovers who want to transfer themselves into the metaverse state of the art in real time motion capture,for all metaverse and vr lovers who want to transfer themselves into the metaverse state of the art in real time motion capture,1,for all metaverse and vr lover who want to transfer themselves into the metaverse state of the art in real time motion capture,for all metaverse and vr lover who want to transfer themselves into the metaverse state of the art in real time motion capture
job opening,job opening,1,job opening,job opening
do you use your numpad on your keyboard for data science related projects,honestly am just watching for new keyboard and like the slick compact design of shortened keyboard and never got used of using the numpad is it worth trying or nah view poll,1,do you use your numpad on your keyboard for data science related project,honestly am just watching for new keyboard and like the slick compact design of shortened keyboard and never got used of using the numpad is it worth trying or nah view poll
build with diffbot ai hackathon remote and in person jan rd build recommendation systems chatbots data reporting tools computer vision tools with ai web scrapers nlp and the world largest knowledge graph teams or individuals welcome your hack your ip,build with diffbot ai hackathon remote and in person jan rd build recommendation systems chatbots data reporting tools computer vision tools with ai web scrapers nlp and the world largest knowledge graph teams or individuals welcome your hack your ip,1,build with diffbot ai hackathon remote and in person jan rd build recommendation system chatbots data reporting tool computer vision tool with ai web scraper nlp and the world largest knowledge graph team or individual welcome your hack your ip,build with diffbot ai hackathon remote and in person jan rd build recommendation system chatbots data reporting tool computer vision tool with ai web scraper nlp and the world largest knowledge graph team or individual welcome your hack your ip
request need screen shot of data science at work,one of my clients wants to create an advertisement for their computer hardware which they aim to sell to data scientists and reasonably enough they like the screen in the photo to show something related to data science rather than generic swirly lines picture my suggestion how about cat photo was not received well for some reason even though had willing cat model so need an image to include that doesn raise any issues of intellectual property the last thing want to do is talk with lawyers figure that the best option is an image of someone using an open source data science tool working on some sort of open data nasa data set could someone indulge me it be groovy if the real data science at work image looked sexy with cool visualization or whatever but it fine too if it screen shot of an open source tool chomping on data it going to be in photo in not very big pdf after all send me to imjur or whatnot and private message telling me what the image is and that it free for anyone to use besides can imagine that show and tell might be fun for the denizens here tl dr could anyone give me screen shot of real data science project,1,request need screen shot of data science at work,one of my client want to create an advertisement for their computer hardware which they aim to sell to data scientist and reasonably enough they like the screen in the photo to show something related to data science rather than generic swirly line picture my suggestion how about cat photo wa not received well for some reason even though had willing cat model so need an image to include that doesn raise any issue of intellectual property the last thing want to do is talk with lawyer figure that the best option is an image of someone using an open source data science tool working on some sort of open data nasa data set could someone indulge me it be groovy if the real data science at work image looked sexy with cool visualization or whatever but it fine too if it screen shot of an open source tool chomping on data it going to be in photo in not very big pdf after all send me to imjur or whatnot and private message telling me what the image is and that it free for anyone to use besides can imagine that show and tell might be fun for the denizen here tl dr could anyone give me screen shot of real data science project
what could be an short roadmap to get enough knowledge and apply to jr data science jobs,hi reddit comunity currently in the university informatic enginner and was wondering if you could help me with some roadmap suggest to get an jr data science job have seen little of programming and learning also was thinking of what aspects do you think recruiters take most into consideration for jr positions,1,what could be an short roadmap to get enough knowledge and apply to jr data science job,hi reddit comunity currently in the university informatic enginner and wa wondering if you could help me with some roadmap suggest to get an jr data science job have seen little of programming and learning also wa thinking of what aspect do you think recruiter take most into consideration for jr position
taking months work break and then returning to analytics workforce,planning on taking months off of work this year to hike currently wfh as data analyst doing sas sql report coding with gov contractor going to ask for an extended leave but if get denied ll give notice kind of doing this for mental health and to transition geographically if take months away from analytics work will this be looked at negatively when back in the job market looking for position would have better luck looking for contract work vs being fte somewhere after the months thoughts,1,taking month work break and then returning to analytics workforce,planning on taking month off of work this year to hike currently wfh a data analyst doing sa sql report coding with gov contractor going to ask for an extended leave but if get denied ll give notice kind of doing this for mental health and to transition geographically if take month away from analytics work will this be looked at negatively when back in the job market looking for position would have better luck looking for contract work v being fte somewhere after the month thought
are data science bootcamps worth it in,is data science bootcamp worth it if anyone has been through data science bootcamp how was the experience like does it prepare you for entry level jobs,1,are data science bootcamps worth it in,is data science bootcamp worth it if anyone ha been through data science bootcamp how wa the experience like doe it prepare you for entry level job
most relevant and applicable statistic courses to take,current life sciences phd student looking to transfer into data science ve been taking elective courses in statistics linear regression multivariate analysis programming what statistics courses did you focus on and which ones prepared you most for your career thanks,1,most relevant and applicable statistic course to take,current life science phd student looking to transfer into data science ve been taking elective course in statistic linear regression multivariate analysis programming what statistic course did you focus on and which one prepared you most for your career thanks
how do you encode multi class target variable,hello everyone am working on multi class classification problem my question is since our target variable is also categorical variable we have to encode it before training it but haven been able to decide how to encode it since it the target variable it not ordinal so we can use simple label encoder for this would love any insights about this,1,how do you encode multi class target variable,hello everyone am working on multi class classification problem my question is since our target variable is also categorical variable we have to encode it before training it but haven been able to decide how to encode it since it the target variable it not ordinal so we can use simple label encoder for this would love any insight about this
anyone have experience as business intelligence analyst was told to apply and am feeling under qualified for the interview,positioned opened up at my current company and was encouraged to apply for it by my superior however feel extremely under qualified and am not feeling very confident my current role do almost the same type of work data analyst does although my title isn data analyst and have heavy experience with powerbi excel etc use those daily at work build dashboards manipulate datasets etc and am comfortable with python although don use it at work use it at home almost daily don know much about sql or but am currently in the middle of data analyst certificate from ibm which covers sql also sure could at least get the basics down on the fly with some quick googling with sql looks like will definitely need some time with other than that don really know what else to expect in the interview not sure if will be tested or not or what type of questions they will ask don know doing some research on bi analysts they seem far more advanced than what used too so not really sure why was encouraged to go for it don get me wrong the pay increase would be nice just don want to embarrass myself in an interview and not know single answer to any of the questions anyone have experience with this position and can offer up advice thank you,1,anyone have experience a business intelligence analyst wa told to apply and am feeling under qualified for the interview,positioned opened up at my current company and wa encouraged to apply for it by my superior however feel extremely under qualified and am not feeling very confident my current role do almost the same type of work data analyst doe although my title isn data analyst and have heavy experience with powerbi excel etc use those daily at work build dashboard manipulate datasets etc and am comfortable with python although don use it at work use it at home almost daily don know much about sql or but am currently in the middle of data analyst certificate from ibm which cover sql also sure could at least get the basic down on the fly with some quick googling with sql look like will definitely need some time with other than that don really know what else to expect in the interview not sure if will be tested or not or what type of question they will ask don know doing some research on bi analyst they seem far more advanced than what used too so not really sure why wa encouraged to go for it don get me wrong the pay increase would be nice just don want to embarrass myself in an interview and not know single answer to any of the question anyone have experience with this position and can offer up advice thank you
which of these ds courses would be better for beginner that wants to basically learn bit of all the core ds principles coursera ibm vs google analytics,utm_medium institutions amp utm_campaign gwgsite howitworks,1,which of these d course would be better for beginner that want to basically learn bit of all the core d principle coursera ibm v google analytics,utm_medium institution amp utm_campaign gwgsite howitworks
good beginner online data science projects for someone interested in visual data and predictive analytics,so work in data warehousing and have taken handful of courses after finished my ms in mis my school added ms in data science option that probably would have done at work every day write sql or work in ssis sometimes tableau power bi but feel lack good understanding of data science overall are there any good course options for me to start with something could treat as personal project while also going through video text tutorials would be great ideally it is free too any good ones out there heard kaggle titanic data competition might be good but seems little overwhelming also andrew ng machine learning course however this also feels the same thanks any advice is appreciated,1,good beginner online data science project for someone interested in visual data and predictive analytics,so work in data warehousing and have taken handful of course after finished my m in mi my school added m in data science option that probably would have done at work every day write sql or work in ssis sometimes tableau power bi but feel lack good understanding of data science overall are there any good course option for me to start with something could treat a personal project while also going through video text tutorial would be great ideally it is free too any good one out there heard kaggle titanic data competition might be good but seems little overwhelming also andrew ng machine learning course however this also feel the same thanks any advice is appreciated
light read for our beloved data amp stat dorks,light read for our beloved data amp stat dorks,1,light read for our beloved data amp stat dork,light read for our beloved data amp stat dork
are neural nets and artificial intelligence considered data science,in my last year of data science degree at my university ve had classes that talk about machine learning in some form or another have this professor head of the data science department that says he knows people in the data science field that shake their heads at nn and ai and consider them to be apart of something else entirely he believes data science to be more statistically driven is it true is there divide in the community where ai and nn fall because of their black box nature,1,are neural net and artificial intelligence considered data science,in my last year of data science degree at my university ve had class that talk about machine learning in some form or another have this professor head of the data science department that say he know people in the data science field that shake their head at nn and ai and consider them to be apart of something else entirely he belief data science to be more statistically driven is it true is there divide in the community where ai and nn fall because of their black box nature
newsletter data science,tweet per day and newsletter per week with data science articles,1,newsletter data science,tweet per day and newsletter per week with data science article
those of you who are taking data science courses at university what the most memorable moment you ve witnessed in data science class,lots of questions in this subreddit have to do with career advice or discussing the job market but want to do something more fun senior majoring in computer science but have taken plethora of data science courses offered at the undergrad level at my university some classes intertwine with master level courses and wanted to share and see if other students have similar memorable or fun experiences in their classes amp x200b for me last semester took data mining course and in one of our assignments we were tasked with using means clustering on dataset of our choice one group decided to do clustering of based off of two explanatory variables one that can really recall let just say it was income and the other one was userid not only did visibly cringe but had to stop myself from facepalming couldn believe group decided to use userid and another variable and try to draw meaningful analysis from it there was part of me that wanted to raise my hand and ask questions about their cluster but the professor got to them first and went in about how you re not supposed to use userid in any type of analysis they had other issues with their presentation and she went into those as well making it clear that the group wasn prepared for what hit them and kind of felt bad she gave the group an opportunity to make up the assignment and come up with legitimate cluster with legitimate analysis so that saving grace amp x200b if you have similar stories or different yet still memorable or fun stories from your data science classes to share please share,1,those of you who are taking data science course at university what the most memorable moment you ve witnessed in data science class,lot of question in this subreddit have to do with career advice or discussing the job market but want to do something more fun senior majoring in computer science but have taken plethora of data science course offered at the undergrad level at my university some class intertwine with master level course and wanted to share and see if other student have similar memorable or fun experience in their class amp x200b for me last semester took data mining course and in one of our assignment we were tasked with using mean clustering on dataset of our choice one group decided to do clustering of based off of two explanatory variable one that can really recall let just say it wa income and the other one wa userid not only did visibly cringe but had to stop myself from facepalming couldn believe group decided to use userid and another variable and try to draw meaningful analysis from it there wa part of me that wanted to raise my hand and ask question about their cluster but the professor got to them first and went in about how you re not supposed to use userid in any type of analysis they had other issue with their presentation and she went into those a well making it clear that the group wasn prepared for what hit them and kind of felt bad she gave the group an opportunity to make up the assignment and come up with legitimate cluster with legitimate analysis so that saving grace amp x200b if you have similar story or different yet still memorable or fun story from your data science class to share please share
what do you find interesting in data science isn it boring,data engineering student in the past years was more interested in things like web dev and automations building stuff is enjoyable to me because you get the chance to make an idea come to life last year had to choose between software or data engineering in my school chose data because it seems bit interesting and things like ml sounds awesome but it is not good experience so far for one simple reason this is new to our school and professors here are just maths professors that are trying to teach you something they don really understand themselves so school isn doing much for me now tried to do same thing that did with web dev which is learn by myself at home now following the data science by imb course at coursera and it awfully boring to me my question is is it boring just because it full of theory now and didn get to practice yet or is finding it boring sign that this field isn really something like please help me make my choice because really in bad situation where don know what to do should keep doing effort to learn data science things or should just return to improve my skills in web dev graduating next year and starting to panic because didn make this choice yet,1,what do you find interesting in data science isn it boring,data engineering student in the past year wa more interested in thing like web dev and automation building stuff is enjoyable to me because you get the chance to make an idea come to life last year had to choose between software or data engineering in my school chose data because it seems bit interesting and thing like ml sound awesome but it is not good experience so far for one simple reason this is new to our school and professor here are just math professor that are trying to teach you something they don really understand themselves so school isn doing much for me now tried to do same thing that did with web dev which is learn by myself at home now following the data science by imb course at coursera and it awfully boring to me my question is is it boring just because it full of theory now and didn get to practice yet or is finding it boring sign that this field isn really something like please help me make my choice because really in bad situation where don know what to do should keep doing effort to learn data science thing or should just return to improve my skill in web dev graduating next year and starting to panic because didn make this choice yet
different stages of data scientist,maybe it will ring bell for you one algorithm person the beginner will have one answer to all your problems which is the one algorithm they like for some silly reason it usually neural network so from images classification to revenue prediction to fraud detection to recommendation engines the solution is simple just give to the net baby know them all person this is more seasoned data scientist you will hear them talking about how decision trees are better for anomalies detection because the gaussian process isn sensitive enough and so on they will know seven different types of softmax and they will want to try them all just in case why don we maybe er don do model this time person this is the ultimate stage the data scientist finally realizes how expensive both in time and money unstable unpredictive and unexplainable the models are if possible they will go for something smaller quicker and most importantly reliable and interpretable like few well chosen if else conditions or simple linear regression with hyperbolic tangent to polish the answer something simple elegant and deadly effective,1,different stage of data scientist,maybe it will ring bell for you one algorithm person the beginner will have one answer to all your problem which is the one algorithm they like for some silly reason it usually neural network so from image classification to revenue prediction to fraud detection to recommendation engine the solution is simple just give to the net baby know them all person this is more seasoned data scientist you will hear them talking about how decision tree are better for anomaly detection because the gaussian process isn sensitive enough and so on they will know seven different type of softmax and they will want to try them all just in case why don we maybe er don do model this time person this is the ultimate stage the data scientist finally realizes how expensive both in time and money unstable unpredictive and unexplainable the model are if possible they will go for something smaller quicker and most importantly reliable and interpretable like few well chosen if else condition or simple linear regression with hyperbolic tangent to polish the answer something simple elegant and deadly effective
great article in the nyt about specificity in genetic testing,an important reminder to be ethical in your metric choice tests for rare genetic disorders in fetuses marketed as extremely accurate have specificities below article podcast,1,great article in the nyt about specificity in genetic testing,an important reminder to be ethical in your metric choice test for rare genetic disorder in fetus marketed a extremely accurate have specificity below article podcast
auc sklearn,auc sklearn,1,auc sklearn,auc sklearn
is it possible to combine data science with hacking it security,since was kid years ago was interested in it security even though there was not much information out there still did simple stuff like scripting playing with trojans viruses wireless wiretap hacking computer games etc but had no support help person to ask so it got more and more difficult for my year old brain and stopped now many years later study data science which is lot of statistics also with many ml topics like un supervised learning reinforcement learning etc with bit of compsci topics like data structures algorithm programming data bases sw engineering my question is there any way to combine this data science knowledge with hacking it security so that can specialize in topic im really interested in so far really miss the technical stuff in data science which inspires me since was kid finding out gaps errors and weak points in system is something that really intrinsically motivates me thanks in advance,1,is it possible to combine data science with hacking it security,since wa kid year ago wa interested in it security even though there wa not much information out there still did simple stuff like scripting playing with trojan virus wireless wiretap hacking computer game etc but had no support help person to ask so it got more and more difficult for my year old brain and stopped now many year later study data science which is lot of statistic also with many ml topic like un supervised learning reinforcement learning etc with bit of compsci topic like data structure algorithm programming data base sw engineering my question is there any way to combine this data science knowledge with hacking it security so that can specialize in topic im really interested in so far really miss the technical stuff in data science which inspires me since wa kid finding out gap error and weak point in system is something that really intrinsically motivates me thanks in advance
how many concurrent projects is normal,ve been in the field for years at an expanding startup used to work on one or two projects at time and their deadlines wouldn overlap too much it was very manageable and was able to produce thorough good work but recently have been put on projects all on different teams some of them take more time than others and there is prioritization for some of them over others however they are big clients and important for the company am feeling extremely burnt out and exhausted my quality of work is definitely falling apart and can afford to spend every waking hour of my life to meet the demands of all am okay working from or if need be but more than that is way too much for me is normal or the new normal for how many projects data scientist should be on don know how can keep up with this job if thats normal workload,1,how many concurrent project is normal,ve been in the field for year at an expanding startup used to work on one or two project at time and their deadline wouldn overlap too much it wa very manageable and wa able to produce thorough good work but recently have been put on project all on different team some of them take more time than others and there is prioritization for some of them over others however they are big client and important for the company am feeling extremely burnt out and exhausted my quality of work is definitely falling apart and can afford to spend every waking hour of my life to meet the demand of all am okay working from or if need be but more than that is way too much for me is normal or the new normal for how many project data scientist should be on don know how can keep up with this job if thats normal workload
thought it could be added here as well,thought it could be added here as well,1,thought it could be added here a well,thought it could be added here a well
looking for an open source tool like knime amp rapidminer but allows downloading of code of the workflow,am looking for an open source tool that allows user to create workflows by dragging and dropping the operations making the connections and then creating visualizations in the end allowing users to download the code of workflow,1,looking for an open source tool like knime amp rapidminer but allows downloading of code of the workflow,am looking for an open source tool that allows user to create workflow by dragging and dropping the operation making the connection and then creating visualization in the end allowing user to download the code of workflow
mathematical analysis of two changing variables affecting an output through an unknown formula,don know if this is the place to pose such question but ll try my best to describe the problem interested to discover if there is mathematical analytical approach to solving it let say you have device in front of you that has digital number display and two dials and both dials go from the number to built into the logic of this device there is formula that takes the output of both dials and generates number on the display that number can only be positive thinks pseudo random numbers now an objective of using this device would be to turn both dials and find the best combination of both turned dials to output the maximum highest number possible what makes this more difficult is that the relationship between any one dial value in isolation to the output value on the display is not linear meaning that setting only dial or alone to wont give the highest number on the display meaning that if you set dial at and only turn dial to see that the display shows that number on the display seems to go up and down seemingly randomly but consistent whilst turning directly from to on dial the same thing on dial is apparent but with it turned in isolation shows different values although these values shown are repeatable according to the location of the dials between to now if this information was simply fed into computer it could simply brute force the best combination of both dials by comparing every possible combination and checking the output display but here is where my question spawns from without testing both dials in combination until you find the highest display value is there mathematical approach to find the highest value possible from setting both dials in combination but only using the information gathered from each dial in isolation to tell you what those two dails numbers should be setting dial to zero and sweeping dial from to and recording all displayed outputs then setting dial to and sweeping dial from to and record all displayed outputs then somehow using this data to predict which combination of dial set to number between to and dial set to number between to would result in the highest displayed number without brute forcing it by setting dial to then recording all output values of dial sweeping to then setting dial to etc let me know if this is dumb question but don know how else to describe this problem,1,mathematical analysis of two changing variable affecting an output through an unknown formula,don know if this is the place to pose such question but ll try my best to describe the problem interested to discover if there is mathematical analytical approach to solving it let say you have device in front of you that ha digital number display and two dial and both dial go from the number to built into the logic of this device there is formula that take the output of both dial and generates number on the display that number can only be positive think pseudo random number now an objective of using this device would be to turn both dial and find the best combination of both turned dial to output the maximum highest number possible what make this more difficult is that the relationship between any one dial value in isolation to the output value on the display is not linear meaning that setting only dial or alone to wont give the highest number on the display meaning that if you set dial at and only turn dial to see that the display show that number on the display seems to go up and down seemingly randomly but consistent whilst turning directly from to on dial the same thing on dial is apparent but with it turned in isolation show different value although these value shown are repeatable according to the location of the dial between to now if this information wa simply fed into computer it could simply brute force the best combination of both dial by comparing every possible combination and checking the output display but here is where my question spawn from without testing both dial in combination until you find the highest display value is there mathematical approach to find the highest value possible from setting both dial in combination but only using the information gathered from each dial in isolation to tell you what those two dail number should be setting dial to zero and sweeping dial from to and recording all displayed output then setting dial to and sweeping dial from to and record all displayed output then somehow using this data to predict which combination of dial set to number between to and dial set to number between to would result in the highest displayed number without brute forcing it by setting dial to then recording all output value of dial sweeping to then setting dial to etc let me know if this is dumb question but don know how else to describe this problem
things you wish you knew before learning data science,looking for inspirations and tips as about to begin my journey,1,thing you wish you knew before learning data science,looking for inspiration and tip a about to begin my journey
md data science background what kind of jobs are out there that would allow me to work from home,ll be done with anesthesia residency soon and did year of data science research several years ago mainly used and became pretty well versed in the basics of data science by going through datacamp com and applying what learned to my own research are there any jobs that would value having someone with medical knowledge basic data science knowledge that are non research positions honestly like to find something that allows me to work from home with data science and scale back the hours that in the hospital,1,md data science background what kind of job are out there that would allow me to work from home,ll be done with anesthesia residency soon and did year of data science research several year ago mainly used and became pretty well versed in the basic of data science by going through datacamp com and applying what learned to my own research are there any job that would value having someone with medical knowledge basic data science knowledge that are non research position honestly like to find something that allows me to work from home with data science and scale back the hour that in the hospital
python compiler,advanced interactive python ide for android compile run save python programs python compiler is an advanced ide for compiling python programs on your mobile phone compile run save python programs without computer for free features amp x200b compile compile python programs run run python files save save python programs practice questions practice questions to improve your python knowledge python program examples syntax of common python programs amp x200b download from the links below android ios amp x200b compiler android python programming amp x200b format pjpg amp auto webp amp c152de8e02fc77b48aad48a4d3ebd7077de55e7e,1,python compiler,advanced interactive python ide for android compile run save python program python compiler is an advanced ide for compiling python program on your mobile phone compile run save python program without computer for free feature amp x200b compile compile python program run run python file save save python program practice question practice question to improve your python knowledge python program example syntax of common python program amp x200b download from the link below android io amp x200b compiler android python programming amp x200b format pjpg amp auto webp amp c152de8e02fc77b48aad48a4d3ebd7077de55e7e
which statistics curriculum should choose for my career,great day everyone data science and statistics are the fields want to get into because of the wide application it offers with that am currently enrolled in four year data science program but due to financial issues am considering changing course and shifting to statistics so have schools to choose from where could transfer to and am having dilemma on which of them is the best the first one data science is my current curriculum those other two are my current choices prefer the one that is somehow the middle ground for data science and statistics amp x200b,1,which statistic curriculum should choose for my career,great day everyone data science and statistic are the field want to get into because of the wide application it offer with that am currently enrolled in four year data science program but due to financial issue am considering changing course and shifting to statistic so have school to choose from where could transfer to and am having dilemma on which of them is the best the first one data science is my current curriculum those other two are my current choice prefer the one that is somehow the middle ground for data science and statistic amp x200b
data to engineers ratio deep dive into top european tech companies,used linkedin data to analyse the data to engineers ratio for of the top european tech companies few interesting insights the median data to engineers ratio is but companies who are most data focused have more than data roles per engineer compared to those with the lowest ratio the ratio is surprisingly good at predicting business models read the full piece here format png amp auto webp amp d30a733c7a98e8070c9b4510fe26a021f52666,1,data to engineer ratio deep dive into top european tech company,used linkedin data to analyse the data to engineer ratio for of the top european tech company few interesting insight the median data to engineer ratio is but company who are most data focused have more than data role per engineer compared to those with the lowest ratio the ratio is surprisingly good at predicting business model read the full piece here format png amp auto webp amp d30a733c7a98e8070c9b4510fe26a021f52666
conspiracy no vax controversial subreddits,hi to everyone an italian computer science student and searching conspiracy no vax subreddits for university resarch do you know anyone thanks lot,1,conspiracy no vax controversial subreddits,hi to everyone an italian computer science student and searching conspiracy no vax subreddits for university resarch do you know anyone thanks lot
individuals who work on implementing optimising or maintaining search any lessons learned,individuals who work on implementing optimising or maintaining search any lessons learned,1,individual who work on implementing optimising or maintaining search any lesson learned,individual who work on implementing optimising or maintaining search any lesson learned
haven had job title for over year and confused would you consider me in some ways data scientist,finished my biology degree and then after two months of job searching was hired in slightly degree related company as warehouse worker digital marketer weird right well progressively added on the responsibilities of what roughly describe as website manager it support technician customer service representative receptionist business process automation developer database developer and administrator and graphic designer was all of these at once including the original warehouse worker digital marketer my job title never changed my pay had to fight for whatever don know what to call myself now that working remotely working with our ecommerce apis to generate sales reports and analytics because the platform generated reports are lacking but also creating gui programs to manage the business more efficiently and in school for software engineering overall think really drawn to programming with service apis to create highly functional but simple tools that meets the needs of businesses don care if it to make their life easier or to access hidden information or to analyze sales because once it done who cares get to start on the next project have adhd so help me please is any of this related to data science do you think it something be interested and what the hell do put on resume,1,haven had job title for over year and confused would you consider me in some way data scientist,finished my biology degree and then after two month of job searching wa hired in slightly degree related company a warehouse worker digital marketer weird right well progressively added on the responsibility of what roughly describe a website manager it support technician customer service representative receptionist business process automation developer database developer and administrator and graphic designer wa all of these at once including the original warehouse worker digital marketer my job title never changed my pay had to fight for whatever don know what to call myself now that working remotely working with our ecommerce apis to generate sale report and analytics because the platform generated report are lacking but also creating gui program to manage the business more efficiently and in school for software engineering overall think really drawn to programming with service apis to create highly functional but simple tool that meet the need of business don care if it to make their life easier or to access hidden information or to analyze sale because once it done who care get to start on the next project have adhd so help me please is any of this related to data science do you think it something be interested and what the hell do put on resume
how do reject job on good terms,my placement cell hooked me up with data analyst bi based company initially was very excited that will get to work hands on related to database to kick start my career in ds but then got to know about their pay and it was very freakingly low mean live in india but in indian terms too its very low lakhs per annum and they stated that we re freshers so we should not expect more than this and shit now ll be happy if they except me as an intern how do convince them on this for limited tenure but full time job will not work because pay is low my ds course is still remaining so any help on how to reject this offer have my interview scheduled tomorrow and any indian guys what should expect as fresher for ds da roles any idea my background is in btech automobile,1,how do reject job on good term,my placement cell hooked me up with data analyst bi based company initially wa very excited that will get to work hand on related to database to kick start my career in d but then got to know about their pay and it wa very freakingly low mean live in india but in indian term too it very low lakh per annum and they stated that we re fresher so we should not expect more than this and shit now ll be happy if they except me a an intern how do convince them on this for limited tenure but full time job will not work because pay is low my d course is still remaining so any help on how to reject this offer have my interview scheduled tomorrow and any indian guy what should expect a fresher for d da role any idea my background is in btech automobile
how do reject job on good terms,my placement cell hooked me up with data analyst bi based company initially was very excited that will get to work hands on related to database to kick start my career in ds but then got to know about their pay and it was very freakingly low mean live in india but in indian terms too its very low lakhs per annum and they stated that we re freshers so we should not expect more than this and shit now ll be happy if they except me as an intern how do convince them on this for limited tenure but full time job will not work because pay is low my ds course is still remaining so any help on how to reject this offer have my interview scheduled tomorrow and any indian guys what should expect as fresher for ds da roles any idea my background is in btech automobile,1,how do reject job on good term,my placement cell hooked me up with data analyst bi based company initially wa very excited that will get to work hand on related to database to kick start my career in d but then got to know about their pay and it wa very freakingly low mean live in india but in indian term too it very low lakh per annum and they stated that we re fresher so we should not expect more than this and shit now ll be happy if they except me a an intern how do convince them on this for limited tenure but full time job will not work because pay is low my d course is still remaining so any help on how to reject this offer have my interview scheduled tomorrow and any indian guy what should expect a fresher for d da role any idea my background is in btech automobile
sieve we processed hours of security footage in lt mins now semantically searchable per frame,hey everyone one of the creators of sieve and excited to be sharing it sieve is an api that helps you store process and automatically search your video data instantly and efficiently just think cameras recording footage at fps that would be million frames generated in single day the videos might be searchable by timestamp but finding moments of interest is like searching for needle in haystack we built this visual demo link here little while back which we love to get feedback on it hours of security footage that our api processed in lt mins and has simple querying and export functionality enabled we see applications in better understanding what data you have figuring out which data to send to labeling sampling datasets for training and building multiple test sets for models by scenario to try it on your videos visual dashboard walkthrough uyjp hgzl4 format png amp auto webp amp e29064212c56daa865d96e0877d7f0c75e3f120d format png amp auto webp amp eeee147e6d99d0b9bc233e0195775dda15963c1a format png amp auto webp amp d46d3ee335250bfc4f4cb04b92d8410b3c format png amp auto webp amp f3d7d022fc648a4bc98bb3cd75250 format png amp auto webp amp fdf1d7937f2df230a2bab6e136fbaa8f3eb92f,1,sieve we processed hour of security footage in lt min now semantically searchable per frame,hey everyone one of the creator of sieve and excited to be sharing it sieve is an api that help you store process and automatically search your video data instantly and efficiently just think camera recording footage at fps that would be million frame generated in single day the video might be searchable by timestamp but finding moment of interest is like searching for needle in haystack we built this visual demo link here little while back which we love to get feedback on it hour of security footage that our api processed in lt min and ha simple querying and export functionality enabled we see application in better understanding what data you have figuring out which data to send to labeling sampling datasets for training and building multiple test set for model by scenario to try it on your video visual dashboard walkthrough uyjp hgzl4 format png amp auto webp amp e29064212c56daa865d96e0877d7f0c75e3f120d format png amp auto webp amp eeee147e6d99d0b9bc233e0195775dda15963c1a format png amp auto webp amp d46d3ee335250bfc4f4cb04b92d8410b3c format png amp auto webp amp f3d7d022fc648a4bc98bb3cd75250 format png amp auto webp amp fdf1d7937f2df230a2bab6e136fbaa8f3eb92f
lets learn how to implement bubble sort,lets learn how to implement bubble sort,1,let learn how to implement bubble sort,let learn how to implement bubble sort
curious on the education background of this sub,view poll,1,curious on the education background of this sub,view poll
just started making youtube videos about data science my latest video is about python data visualization packages my goal was to make something my year old son would sit through he said it was good but needed more memes what do you think,just started making youtube videos about data science my latest video is about python data visualization packages my goal was to make something my year old son would sit through he said it was good but needed more memes what do you think,1,just started making youtube video about data science my latest video is about python data visualization package my goal wa to make something my year old son would sit through he said it wa good but needed more meme what do you think,just started making youtube video about data science my latest video is about python data visualization package my goal wa to make something my year old son would sit through he said it wa good but needed more meme what do you think
just started making youtube videos about data science my latest video is about python data visualization packages my goal was to make something my year old son would sit through he said it was good but needed more memes what do you think,just started making youtube videos about data science my latest video is about python data visualization packages my goal was to make something my year old son would sit through he said it was good but needed more memes what do you think,1,just started making youtube video about data science my latest video is about python data visualization package my goal wa to make something my year old son would sit through he said it wa good but needed more meme what do you think,just started making youtube video about data science my latest video is about python data visualization package my goal wa to make something my year old son would sit through he said it wa good but needed more meme what do you think
linear search algorithm implemented with python,linear search algorithm implemented with python,1,linear search algorithm implemented with python,linear search algorithm implemented with python
lets learn how to implement bubble sort,lets learn how to implement bubble sort,1,let learn how to implement bubble sort,let learn how to implement bubble sort
business analysis positions business intelligence vs visual analytics amp big data,hello would land future position on the business analysis field which master degree would be more suitable for me business intelligence or visual analytics amp big data amp x200b context come from social science background currently doing postgraduate diploma in business but have always loved technology and programming for that reason would like to mix them both currently learning python and sql at the moment thanks,1,business analysis position business intelligence v visual analytics amp big data,hello would land future position on the business analysis field which master degree would be more suitable for me business intelligence or visual analytics amp big data amp x200b context come from social science background currently doing postgraduate diploma in business but have always loved technology and programming for that reason would like to mix them both currently learning python and sql at the moment thanks
realtime cro agricultural monitoring with api building framework,want to build website that would be dedicated to crop monitoring there are few great companies working on it want to start mine the parameters would be soil health water stress and vegetation indices the ui and the interactivity would be like eos crop monitoring can mark the boundary at any time and choose the date have to compute some mathematical calculations on satellite images by deploying those apis the image database collection will be linked to the cloud platform then will retrieve the images alongside graphs charts to get real time results how would construct the framework,1,realtime cro agricultural monitoring with api building framework,want to build website that would be dedicated to crop monitoring there are few great company working on it want to start mine the parameter would be soil health water stress and vegetation index the ui and the interactivity would be like eos crop monitoring can mark the boundary at any time and choose the date have to compute some mathematical calculation on satellite image by deploying those apis the image database collection will be linked to the cloud platform then will retrieve the image alongside graph chart to get real time result how would construct the framework
how would you define data science in,seriously what would you consider the hallmarks of data science role what should data scientist ideally be spending their time on how does that differ from reality are those differences good bad or neutral thing we ve had the field in various forms for several years just curious what others opinions are in,1,how would you define data science in,seriously what would you consider the hallmark of data science role what should data scientist ideally be spending their time on how doe that differ from reality are those difference good bad or neutral thing we ve had the field in various form for several year just curious what others opinion are in
just learned command line and bash now what,ve always heard that cli and bash is useful in the data science world decided to spend day on datacamp and went through all their cli and bash courses so while not an expert have much better underestanding of the syntax how to move around the terminal and how to look up if not write simple bash scripts with that said how do put it in actual practice is cli and bash mostly used to connect to vm and model deployment where should go to practice what learned and see it in action,1,just learned command line and bash now what,ve always heard that cli and bash is useful in the data science world decided to spend day on datacamp and went through all their cli and bash course so while not an expert have much better underestanding of the syntax how to move around the terminal and how to look up if not write simple bash script with that said how do put it in actual practice is cli and bash mostly used to connect to vm and model deployment where should go to practice what learned and see it in action
help finding data science internship,hi am masters student studying computational mathematics at top ranked canadian university and will be apply to co op internship jobs for the summer my bachelors was in environmental engineering and worked about months as software engineer in data analytics firm in pakistan before starting my graduate studies got my previous jobs through my contacts and have never really interviewed for job how should prepare for the interview do data science roles have technical interviews if so are they similar to the ones for software engineering jobs also what skills and experiences should focus on in my cv thanks,1,help finding data science internship,hi am master student studying computational mathematics at top ranked canadian university and will be apply to co op internship job for the summer my bachelor wa in environmental engineering and worked about month a software engineer in data analytics firm in pakistan before starting my graduate study got my previous job through my contact and have never really interviewed for job how should prepare for the interview do data science role have technical interview if so are they similar to the one for software engineering job also what skill and experience should focus on in my cv thanks
what are the best tools that make data scientists life easier when it comes to data governance amp organizing metadata,what are the best tools that make data scientists life easier when it comes to data governance amp organizing metadata,1,what are the best tool that make data scientist life easier when it come to data governance amp organizing metadata,what are the best tool that make data scientist life easier when it come to data governance amp organizing metadata
please help me understand this data from vaers,so please first let me state that absolutely pro vaccine and not trying to demonstrate anything here just thought that the vaers vaccine adverse event report system data was an interesting place to learn some data analysis and data investigation skills on my free time during holidays here is the claim trying to investigate among the deaths linked to the pfizer vaccine in the us only handful have been causally attributed to the vaccine and the rest is supposed to be temporal coincidences people will sometimes die wether they have taken vaccine or not to investigate this claim propose to look at the distribution of the number of days elapsed between taking the vaccine and dying for each case recorded in vaers in my hypothesis is if the death and the vaccine aren causally linked the distribution of the observed time delay between vaccination and death should be approximately uniform people have the same probability of dying the day when they are vaccinated the morrow the day after that the th day after that here is my procedure using pandas on the data downloadable from vaers found death events with non nan entries remained after removing badly formatted datedied and vax date entries remained after removing entries with negative number of days elapsed between death and vaccine amp x200b amp x200b plotted two histograms one with the axis ranging from to days the other from to days see attached contrary to my hypothesis the distribution is not at all uniform but instead seems to be exponentially decreasing with maximum at day also there is small bump centered around days my tentavive explanations are not everyone has been vaccinated at the same time and this will tend to skew the distribution to the left simply because people vaccinated during last month cannot contribute to values greater than days for example dont know how to estimate this effect overall but since vaccination rates have plateaued for at least few months dont expect it to be significant in the range days am right is there any other probabilistic effect that missing the adverse event are under reported and the more the vaccine is ancient the less probable it is that death is reported in vaers this is think almost surely happening but wouldnt expect such big drop in reports between day and especially for an adverse event as tragic as death the bump at days may be due to the pediod around april where vaccination rates were steeply increasing the causal link between vaccines and death are stronger than expected causing the left skew amp x200b thank you in advance for your help,1,please help me understand this data from vaers,so please first let me state that absolutely pro vaccine and not trying to demonstrate anything here just thought that the vaers vaccine adverse event report system data wa an interesting place to learn some data analysis and data investigation skill on my free time during holiday here is the claim trying to investigate among the death linked to the pfizer vaccine in the u only handful have been causally attributed to the vaccine and the rest is supposed to be temporal coincidence people will sometimes die wether they have taken vaccine or not to investigate this claim propose to look at the distribution of the number of day elapsed between taking the vaccine and dying for each case recorded in vaers in my hypothesis is if the death and the vaccine aren causally linked the distribution of the observed time delay between vaccination and death should be approximately uniform people have the same probability of dying the day when they are vaccinated the morrow the day after that the th day after that here is my procedure using panda on the data downloadable from vaers found death event with non nan entry remained after removing badly formatted datedied and vax date entry remained after removing entry with negative number of day elapsed between death and vaccine amp x200b amp x200b plotted two histogram one with the axis ranging from to day the other from to day see attached contrary to my hypothesis the distribution is not at all uniform but instead seems to be exponentially decreasing with maximum at day also there is small bump centered around day my tentavive explanation are not everyone ha been vaccinated at the same time and this will tend to skew the distribution to the left simply because people vaccinated during last month cannot contribute to value greater than day for example dont know how to estimate this effect overall but since vaccination rate have plateaued for at least few month dont expect it to be significant in the range day am right is there any other probabilistic effect that missing the adverse event are under reported and the more the vaccine is ancient the le probable it is that death is reported in vaers this is think almost surely happening but wouldnt expect such big drop in report between day and especially for an adverse event a tragic a death the bump at day may be due to the pediod around april where vaccination rate were steeply increasing the causal link between vaccine and death are stronger than expected causing the left skew amp x200b thank you in advance for your help
data science and personal finance,have some of you tried to apply ai to your personal finance and investment decisions whether it be stocks real estate cryptos,1,data science and personal finance,have some of you tried to apply ai to your personal finance and investment decision whether it be stock real estate cryptos
please advice me,hey an recent bachelors grad majored in mechanical engineering to layout plainly ve never typed single line of code due to pandemic had lot of time in my hands and decided to learn something after some research ended up enrolling in month data science certification program offered by prestigious it institute where you are taught from the ground up from how to code to creating ml models was taught sql python excel however due to short duration of the course it was too fast paced for me tried my best to learn as much as could but nothing to my course is going to end soon in months and plan to find job in data science field need advice on what skills should perfect before go for job since am half knowledged in most what type of projects will help me secure job since don have any experience nor am from computer science background what questions can expect in the interviews any experienced professional or hiring manager reading this please help sincere thanks,1,please advice me,hey an recent bachelor grad majored in mechanical engineering to layout plainly ve never typed single line of code due to pandemic had lot of time in my hand and decided to learn something after some research ended up enrolling in month data science certification program offered by prestigious it institute where you are taught from the ground up from how to code to creating ml model wa taught sql python excel however due to short duration of the course it wa too fast paced for me tried my best to learn a much a could but nothing to my course is going to end soon in month and plan to find job in data science field need advice on what skill should perfect before go for job since am half knowledged in most what type of project will help me secure job since don have any experience nor am from computer science background what question can expect in the interview any experienced professional or hiring manager reading this please help sincere thanks
automated relevant business performance insights,hello work in mid size tech company and decent amount of resources is spent on producing weekly insights about business performance revenues engagement etc and its drivers the process is fairly manual as what is important insightful changes all the time still have been wondering whether there is better way and imagine this must be very common problem how is this managed in your company are there ds ways to produce automated relevant insights something like anomaly detection plus recommendation thanks,1,automated relevant business performance insight,hello work in mid size tech company and decent amount of resource is spent on producing weekly insight about business performance revenue engagement etc and it driver the process is fairly manual a what is important insightful change all the time still have been wondering whether there is better way and imagine this must be very common problem how is this managed in your company are there d way to produce automated relevant insight something like anomaly detection plus recommendation thanks
better method to predict frequency timing of payment,am working on project to better predict when invoices to clients will be paid by region as this is multinational corporation have years of historical data that show invoice number date invoice was sent date invoice was paid amount paid of the invoice can anyone suggest good method that might help me better predict future payments for new invoices being sent out,1,better method to predict frequency timing of payment,am working on project to better predict when invoice to client will be paid by region a this is multinational corporation have year of historical data that show invoice number date invoice wa sent date invoice wa paid amount paid of the invoice can anyone suggest good method that might help me better predict future payment for new invoice being sent out
what should expect for product case study in the interview,hello this is for the data analyst position in the interview what do need to know about the service offered by the tech company what questions would need to consider any suggestions to work on sql statistics and python thanks,1,what should expect for product case study in the interview,hello this is for the data analyst position in the interview what do need to know about the service offered by the tech company what question would need to consider any suggestion to work on sql statistic and python thanks
graduate in want to work as business analyst but haven learned any ds languages yet,hi everyone graduate uni in have an internship for business analysis this summer and my goal is to work my way up and eventually become project manager scrum master product owner or something along those lines lot of these roles require experience with sql tableau python sap etc the firm interning at gives most of their interns full time offers but deans list student with high gpa so not too worried about that however haven learned any data science languages yet bootcamps want like even online from what ve seen is it worth trying to learn some data science languages thru bootcamp or should start working and then learn as go having my employer also pay for certifications etc is it hard to learn languages after starting job as business analyst thanks,1,graduate in want to work a business analyst but haven learned any d language yet,hi everyone graduate uni in have an internship for business analysis this summer and my goal is to work my way up and eventually become project manager scrum master product owner or something along those line lot of these role require experience with sql tableau python sap etc the firm interning at give most of their intern full time offer but dean list student with high gpa so not too worried about that however haven learned any data science language yet bootcamps want like even online from what ve seen is it worth trying to learn some data science language thru bootcamp or should start working and then learn a go having my employer also pay for certification etc is it hard to learn language after starting job a business analyst thanks
the benefits of nlp natural language processing in business analytics,the benefits of nlp natural language processing in business analytics,1,the benefit of nlp natural language processing in business analytics,the benefit of nlp natural language processing in business analytics
data science as second career,hi all am currently physical therapist in rural area in nc have my doctorate and have been out of school and working for almost years am interested in returning to school training to bridge into healthcare data analytics hope to leave healthcare and bridge into field where can equally engage my brain without direct patient care also feel like was hoodwinked while pursuing my doctorate nobody had the guts to have conversation with us trainees about the realities of the job prior to us actually entering the workforce in physical therapy there is little to no upward mobility very little opportunity for salary growth and any additional certs training expertise is not reflected in reimbursement thus not reflected in our salary so here is where my questions come in did you anyone you know transition into data science as second career or from different position what route did you they take the local university near me has an ms program that could do part time while continuing to work over years is an ms degree the demand at this time truthfully my knowledge is minimal and am open to all input and advice thanks all,1,data science a second career,hi all am currently physical therapist in rural area in nc have my doctorate and have been out of school and working for almost year am interested in returning to school training to bridge into healthcare data analytics hope to leave healthcare and bridge into field where can equally engage my brain without direct patient care also feel like wa hoodwinked while pursuing my doctorate nobody had the gut to have conversation with u trainee about the reality of the job prior to u actually entering the workforce in physical therapy there is little to no upward mobility very little opportunity for salary growth and any additional cert training expertise is not reflected in reimbursement thus not reflected in our salary so here is where my question come in did you anyone you know transition into data science a second career or from different position what route did you they take the local university near me ha an m program that could do part time while continuing to work over year is an m degree the demand at this time truthfully my knowledge is minimal and am open to all input and advice thanks all
looking for resources on practical time series analysis forecasting anomaly and change point detection,hello fellow data scientists amp x200b as the title states looking for resources on time series analysis especially pertaining to anomaly and change point detection which might involve techniques like forecasting and clustering blog posts textbooks are fine but looking for something little more hands on like tutorial or course so can get some direct practice in bonus points if real world or reasonably realistic datasets are involved amp x200b thanks,1,looking for resource on practical time series analysis forecasting anomaly and change point detection,hello fellow data scientist amp x200b a the title state looking for resource on time series analysis especially pertaining to anomaly and change point detection which might involve technique like forecasting and clustering blog post textbook are fine but looking for something little more hand on like tutorial or course so can get some direct practice in bonus point if real world or reasonably realistic datasets are involved amp x200b thanks
helping business analysts understand data science and machine learning,have been charged with creating presentation that will help the business analyst ba at my company better understand what data science and machine learning is the ba at my company are primarily responsible for gathering business requirements and helping software engineers execute on projects the data science team is separate team that works to integrate machine learning into our current software algorithms right now the relationship between all of these teams is fairly weak and part of the goal of this presentation is to help us all work better together and understand what our different roles entail fairly new to the industry and still getting my feet under me but like to know what you as data science professionals would like bas to know about what you do and how it relates to what they do what would you consider basic data science knowledge that bas should know what questions do you wish that they already knew the answer to what the most important thing you wish they understood about your job how can ba best support your efforts as data scientist or machine learning engineer thank you in advance for your feedback,1,helping business analyst understand data science and machine learning,have been charged with creating presentation that will help the business analyst ba at my company better understand what data science and machine learning is the ba at my company are primarily responsible for gathering business requirement and helping software engineer execute on project the data science team is separate team that work to integrate machine learning into our current software algorithm right now the relationship between all of these team is fairly weak and part of the goal of this presentation is to help u all work better together and understand what our different role entail fairly new to the industry and still getting my foot under me but like to know what you a data science professional would like ba to know about what you do and how it relates to what they do what would you consider basic data science knowledge that ba should know what question do you wish that they already knew the answer to what the most important thing you wish they understood about your job how can ba best support your effort a data scientist or machine learning engineer thank you in advance for your feedback
the best machine learning courses on udemy,the best machine learning courses on udemy,1,the best machine learning course on udemy,the best machine learning course on udemy
can you connect pycharm community to microsoft sql server,as the title really trying to connect pycharm community to microsoft sql server but so far over only seen documentation on how to connect pycharm pro to sql server the jetbrain documentation tells you to go to view gt gt tool windows gt gt database but don see database under tool windows,1,can you connect pycharm community to microsoft sql server,a the title really trying to connect pycharm community to microsoft sql server but so far over only seen documentation on how to connect pycharm pro to sql server the jetbrain documentation tell you to go to view gt gt tool window gt gt database but don see database under tool window
experienced data scientists on what do you spend your most time on at work,am curious about the general bau day to day activities of data scientist what do all spend your most time on at work data viz wrangling ml my apologies if it dumb question am noob who trying to break into this field,1,experienced data scientist on what do you spend your most time on at work,am curious about the general bau day to day activity of data scientist what do all spend your most time on at work data viz wrangling ml my apology if it dumb question am noob who trying to break into this field
bootcamp graduation project generating new cooking recipes from list of ingredients which models could fit the task,hi we re team of four working on this datascience project and we finished our second day pretty depressed about which models could fit the task at hand we have week and half left to complete our project python rather than using kaggle dataset in english we re trying to make it work with data scraped on french website we re aiming for recipes this is the website for those interested gratin dauphinois express au lait aspx we ve been thinking about separating the whole thing into two tables here are the features we ve decided to scrap so far for each table ingredients table recipe name recipe id ingredient one row per ingredient for each recipe ingredient quantity no units of measure for now of salt spoon of oil recipes table recipe id recipe name recipe steps we re having trouble deciding if this should be single string or list of strings amp x200b we ve had two weeks of machine deep learning in this course and have seen fair amount of library and tools like scikit learn keras pandas numpy we ve done some research but we don have clear understanding of the following models we ve been thinking about using for this task and would appreciate any sort of advice or source about which way to go next gpt lstm rnn bert or maybe albert textgenrnn tablegan gru word2vec would it work in another language amp x200b amp x200b amp x200b we don even know if we should use an nlp model or not anymore we want to give it our all and are not afraid of pretty challenging method we re still quite new to the field any help is welcomed,1,bootcamp graduation project generating new cooking recipe from list of ingredient which model could fit the task,hi we re team of four working on this datascience project and we finished our second day pretty depressed about which model could fit the task at hand we have week and half left to complete our project python rather than using kaggle dataset in english we re trying to make it work with data scraped on french website we re aiming for recipe this is the website for those interested gratin dauphinois express au lait aspx we ve been thinking about separating the whole thing into two table here are the feature we ve decided to scrap so far for each table ingredient table recipe name recipe id ingredient one row per ingredient for each recipe ingredient quantity no unit of measure for now of salt spoon of oil recipe table recipe id recipe name recipe step we re having trouble deciding if this should be single string or list of string amp x200b we ve had two week of machine deep learning in this course and have seen fair amount of library and tool like scikit learn kera panda numpy we ve done some research but we don have clear understanding of the following model we ve been thinking about using for this task and would appreciate any sort of advice or source about which way to go next gpt lstm rnn bert or maybe albert textgenrnn tablegan gru word2vec would it work in another language amp x200b amp x200b amp x200b we don even know if we should use an nlp model or not anymore we want to give it our all and are not afraid of pretty challenging method we re still quite new to the field any help is welcomed
report framework trying to move away from excel for report creation and management,originally posted on analytics but submitting here as do believe there is data science solution to this problem am facing first and foremost am new to data science and have no coding experience however am willing to learn anything that will help me solve or simplify my work processes current process work as data analyst within the fp amp team of large organization build and manage the budget amp forecast reports and other ad hoc analysis reports for the team use excel to build everything using power query to etl the data received from sap data dump files combine it with dimensional data files have created in other excel files and finally pull the data into table which use to populate an excel report template using sumifs and xlookups using this method works fine in the first few months of the year as the raw data files are still small however when we reach month or these files have too many rows and columns leading to these excel files slowing down dramatically making the files pretty much unusable have begun using alteryx to perform the etl instead and saved the files in csv format to ease the burden on excel the problem with this is that am the only user with an alteryx license so when refreshing this etl workflow no one other than myself can do that making this an ineffective solution for the team need your feedback would like to streamline and possibly automate in some capacity this process and move away from exclusively using excel for everything with the start of the new year would like to start building framework to be able to do the following create workflow that etls the erp raw data but can be run by anyone on the team way to store all my dimension and fact tables but also be able to update the information easily would snowflake be good platform for this connect the dimension and fact tables to the workflow populate the excel template with this information know am basically describing excel power query and power pivot here but this is the only solution know this does not solve my excel slowdown issues it also is not simple solution for the rest of the team as they need to fully understand the data contained within all the files and how to update it for the report to refresh correctly you all are the experts so am looking for your guidance to solve this problem this solution would be immensely helpful in many areas of my work and may even assist others using similar approach to build their reports,1,report framework trying to move away from excel for report creation and management,originally posted on analytics but submitting here a do believe there is data science solution to this problem am facing first and foremost am new to data science and have no coding experience however am willing to learn anything that will help me solve or simplify my work process current process work a data analyst within the fp amp team of large organization build and manage the budget amp forecast report and other ad hoc analysis report for the team use excel to build everything using power query to etl the data received from sap data dump file combine it with dimensional data file have created in other excel file and finally pull the data into table which use to populate an excel report template using sumifs and xlookups using this method work fine in the first few month of the year a the raw data file are still small however when we reach month or these file have too many row and column leading to these excel file slowing down dramatically making the file pretty much unusable have begun using alteryx to perform the etl instead and saved the file in csv format to ease the burden on excel the problem with this is that am the only user with an alteryx license so when refreshing this etl workflow no one other than myself can do that making this an ineffective solution for the team need your feedback would like to streamline and possibly automate in some capacity this process and move away from exclusively using excel for everything with the start of the new year would like to start building framework to be able to do the following create workflow that etls the erp raw data but can be run by anyone on the team way to store all my dimension and fact table but also be able to update the information easily would snowflake be good platform for this connect the dimension and fact table to the workflow populate the excel template with this information know am basically describing excel power query and power pivot here but this is the only solution know this doe not solve my excel slowdown issue it also is not simple solution for the rest of the team a they need to fully understand the data contained within all the file and how to update it for the report to refresh correctly you all are the expert so am looking for your guidance to solve this problem this solution would be immensely helpful in many area of my work and may even assist others using similar approach to build their report
the benefits of nlp natural language processing in business analytics,the benefits of nlp natural language processing in business analytics,1,the benefit of nlp natural language processing in business analytics,the benefit of nlp natural language processing in business analytics
can anyone think of fast way to collect customer service emails belonging to various companies want to gather list of customer service emails is there database somewhere,ve thought about writing script that would find the emails but would need to gather company ulrs for that thoughts,1,can anyone think of fast way to collect customer service email belonging to various company want to gather list of customer service email is there database somewhere,ve thought about writing script that would find the email but would need to gather company ulrs for that thought
is it practical to continue part time masters or solely focus on full time career,on my nd year out of years if done as part time of masters in computer science majoring in machine learning from top research university in third world country in southeast asia currently have years of industry experience in data science and working for series startup decided to take part time ms after realized wanna know what under the hood of all these ml tools used in the industry not just plug and play couldn afford to stop working as support my family financially can do both but can do both well think ll do better if focus on doing one not planning in getting into managerial roles love building and coding stuff that rather be in technical path hopefully in an amp ai company moving forward in my career some notes pay for my masters tuition thinking of using the ms qualification as gateway to work and hopefully migrate to canada or other better countries tl dr if have years industry ds exp is part time masters taking years to finish and paid by myself worth continuing if aim to get into machine learning research and development jobs thanks,1,is it practical to continue part time master or solely focus on full time career,on my nd year out of year if done a part time of master in computer science majoring in machine learning from top research university in third world country in southeast asia currently have year of industry experience in data science and working for series startup decided to take part time m after realized wanna know what under the hood of all these ml tool used in the industry not just plug and play couldn afford to stop working a support my family financially can do both but can do both well think ll do better if focus on doing one not planning in getting into managerial role love building and coding stuff that rather be in technical path hopefully in an amp ai company moving forward in my career some note pay for my master tuition thinking of using the m qualification a gateway to work and hopefully migrate to canada or other better country tl dr if have year industry d exp is part time master taking year to finish and paid by myself worth continuing if aim to get into machine learning research and development job thanks
looking for advice on certs bootcamps ms for tapping into ds da roles currently an industrial engineer,currently years into my career as an ie working in semiconductor mfg spend majority of my time analyzing manufacturing data for process improvement utilizing software tools like knime and sql developer then summarizing and visualizing the data in tableau and excel so some of the preliminary skills are there my question really is to tap more into da ds role would bootcamp be worth it in my case especially if my company will pay for half of it realize questions like this get asked daily but wanted to ask the question relating to my current situation thank you,1,looking for advice on cert bootcamps m for tapping into d da role currently an industrial engineer,currently year into my career a an ie working in semiconductor mfg spend majority of my time analyzing manufacturing data for process improvement utilizing software tool like knime and sql developer then summarizing and visualizing the data in tableau and excel so some of the preliminary skill are there my question really is to tap more into da d role would bootcamp be worth it in my case especially if my company will pay for half of it realize question like this get asked daily but wanted to ask the question relating to my current situation thank you
data scientists working in healthcare how do you keep abreast of the latest developments,ve been doing ds in healthcare startup for about year and was curious to know what my peers do to keep informed about the latest developments in the field already keeping an eye on stat health tech and ai columns following bunch of startups on linkedin but would welcome any other resources on what happening out there,1,data scientist working in healthcare how do you keep abreast of the latest development,ve been doing d in healthcare startup for about year and wa curious to know what my peer do to keep informed about the latest development in the field already keeping an eye on stat health tech and ai column following bunch of startup on linkedin but would welcome any other resource on what happening out there
ideas for relevant data related tasks can do in my internship,am majoring in public health and found that would like to work in the data science field and have added minor in information systems and am learning some tools on the side currently have public health related internship working with my schools food pantry we have few programs that help students get more access to fresh food my internship supervisor is awesome and helps me do things with the internship that will help me with my career wanting to take on some more responsibility and work on things that will be useful for data science career and wondering if you guys have any idea as to what could do currently have some basic sql skills and decent excel skills and am going to continue working on other tools such as python tableau and power bi any help with this is very appreciated thank you,1,idea for relevant data related task can do in my internship,am majoring in public health and found that would like to work in the data science field and have added minor in information system and am learning some tool on the side currently have public health related internship working with my school food pantry we have few program that help student get more access to fresh food my internship supervisor is awesome and help me do thing with the internship that will help me with my career wanting to take on some more responsibility and work on thing that will be useful for data science career and wondering if you guy have any idea a to what could do currently have some basic sql skill and decent excel skill and am going to continue working on other tool such a python tableau and power bi any help with this is very appreciated thank you
querying wrangling and visualizing automation in aws,trying to automate some of my data wrangling analysis and visualization into aws originally would have to query some data off of redshift then wrangle it with few csvs stored on my hard drive in jupyter notebook before making some visualizations with matplotlib my organization has been asking me to constantly update the visualizations with new data so trying to find way to automate the querying wrangling and visualizing in aws ve also looked into my organization third party bi tool but it seems to have some trouble handling python does anyone have any suggestions on where to start with this,1,querying wrangling and visualizing automation in aws,trying to automate some of my data wrangling analysis and visualization into aws originally would have to query some data off of redshift then wrangle it with few csvs stored on my hard drive in jupyter notebook before making some visualization with matplotlib my organization ha been asking me to constantly update the visualization with new data so trying to find way to automate the querying wrangling and visualizing in aws ve also looked into my organization third party bi tool but it seems to have some trouble handling python doe anyone have any suggestion on where to start with this
need some guidance should do cert with my ms or something else,hi there currently working on ms in data analytics through oregon state university all stats and cs courses the way things lined up for me going to end up having some straggling single classes during quarter but need to take at least to keep my part time status for financial aid purposes so thinking about adding cert and wanted to get your opinions on the options available for cert at the masters level the university offers that relevant to me gis or engineering management for context have bs in geology and up until the pandemic have been working in the oilfield as an engineer and as civil engineer before that when you take typical geology career route gis is almost required with your degree however never actually landed geology job and have been in engineering now that ve started switching over to ds lost haha there are so many routes can take it overwhelming think like to stick to oil and gas but not opposed to any certain industry either with that in mind not sure what better get the cert in one or the other or stick to things like aws google oracle certifications instead should keep it general until land job and build from there or start trying to specialize now even though don know what kind of data science route really want to take from lack of exposure to what what thank you for any guidance,1,need some guidance should do cert with my m or something else,hi there currently working on m in data analytics through oregon state university all stats and c course the way thing lined up for me going to end up having some straggling single class during quarter but need to take at least to keep my part time status for financial aid purpose so thinking about adding cert and wanted to get your opinion on the option available for cert at the master level the university offer that relevant to me gi or engineering management for context have b in geology and up until the pandemic have been working in the oilfield a an engineer and a civil engineer before that when you take typical geology career route gi is almost required with your degree however never actually landed geology job and have been in engineering now that ve started switching over to d lost haha there are so many route can take it overwhelming think like to stick to oil and gas but not opposed to any certain industry either with that in mind not sure what better get the cert in one or the other or stick to thing like aws google oracle certification instead should keep it general until land job and build from there or start trying to specialize now even though don know what kind of data science route really want to take from lack of exposure to what what thank you for any guidance
project tried models all give bad accuracy,hi in time stress with project for last months have tried models on my data in no way able to get high accuracy for my model any tips could it be that my variables just have no correlation with at all what should do,1,project tried model all give bad accuracy,hi in time stress with project for last month have tried model on my data in no way able to get high accuracy for my model any tip could it be that my variable just have no correlation with at all what should do
nlp hybridization of statistical approach and expert system,hi everyone have question for you for context we aggregate on platform the various ai apis on the market gcp azure etc and including nlp apis keyword extraction sentiment analysis ner etc the idea is that developer doesn have to create accounts with different providers and can have them all on one api to test compare and change whenever he wants however many customers ask us how to mix the statistical approach behind these apis with expert systems and how to achieve hybridization do you have any idea how to do this thanks,1,nlp hybridization of statistical approach and expert system,hi everyone have question for you for context we aggregate on platform the various ai apis on the market gcp azure etc and including nlp apis keyword extraction sentiment analysis ner etc the idea is that developer doesn have to create account with different provider and can have them all on one api to test compare and change whenever he want however many customer ask u how to mix the statistical approach behind these apis with expert system and how to achieve hybridization do you have any idea how to do this thanks
can get away with using studio to run python code only or will have to eventually switch to different platform,aware that tools like pycharm and spyder exist but just find studio so much cleaner and intuitive to use plus love using markdown to create documents will run into any roadblocks in my work if just rely on studio for python or will have to switch eventually most of my background is in so that could be why prefer it but not sure,1,can get away with using studio to run python code only or will have to eventually switch to different platform,aware that tool like pycharm and spyder exist but just find studio so much cleaner and intuitive to use plus love using markdown to create document will run into any roadblock in my work if just rely on studio for python or will have to switch eventually most of my background is in so that could be why prefer it but not sure
data science is just numbers in sausage machine,currently working with director who said the above and then asked me an analyst bi consultant if could learn python in to days and do the above so they wouldn need to hire data scientists im pretty sure the ask is ridiculous immediately in my head know enough about data science that need to learn python plus high level of statistics my data pulling and cleansing are decent and then data science theory itself to interpret the results imagine questions was hoping you could help with am correct in my assumption how would you eloquently explain why it not just sausage machine he very arrogant with data and analytics as he solid with excel and pivot table and has that vibe where he thinks all other departments and people are idiots so will doubt any answer give,1,data science is just number in sausage machine,currently working with director who said the above and then asked me an analyst bi consultant if could learn python in to day and do the above so they wouldn need to hire data scientist im pretty sure the ask is ridiculous immediately in my head know enough about data science that need to learn python plus high level of statistic my data pulling and cleansing are decent and then data science theory itself to interpret the result imagine question wa hoping you could help with am correct in my assumption how would you eloquently explain why it not just sausage machine he very arrogant with data and analytics a he solid with excel and pivot table and ha that vibe where he think all other department and people are idiot so will doubt any answer give
python tutorial,learn python the easy way with our detailed programming tutorial amp x200b whether you want to learn python as hobby for school college or want to build career in the field this tutorial is for you this tutorial covers everything from the basics of python to advanced concepts like data structures we at onepercent strive to make the concepts of programming simple easy interesting and understandable to everyone download and start learning today android ios amp x200b format pjpg amp auto webp amp e2afdfa99eb612182758ed7a2fba0668c986b9d,1,python tutorial,learn python the easy way with our detailed programming tutorial amp x200b whether you want to learn python a hobby for school college or want to build career in the field this tutorial is for you this tutorial cover everything from the basic of python to advanced concept like data structure we at onepercent strive to make the concept of programming simple easy interesting and understandable to everyone download and start learning today android io amp x200b format pjpg amp auto webp amp e2afdfa99eb612182758ed7a2fba0668c986b9d
advice on freelancing in ds,has anyone had any luck doing some side work freelancing as data scientist ve been thinking about starting llc and begin trying to reach out to smaller companies any advice would be greatly appreciated,1,advice on freelancing in d,ha anyone had any luck doing some side work freelancing a data scientist ve been thinking about starting llc and begin trying to reach out to smaller company any advice would be greatly appreciated
cracking the ds coding interviews,currently work as data analyst for large financial institution my work uses some sql but most of the work is done using our proprietary data systems so non transferable skills over the past year or so ve been learning python mathematics like probability linear alg and vector calc as well as reviewing wide range of machine learning algos from linear logistic regressions to sota stuff like transformers because find the modeling side really interesting ve had two coding challenges presented so far to me as screening exercises and don feel like doing as well as need to be to be considered yesterday did hackerrank challenge for verisk company and was able to answer both sql questions but only of the python coding ones it frustrating because know how to make things work using python but maybe not always the fastest way ve never worked in an environment with code review is the answer here to just chug away at leetcode hackerrank mediums and hards power through an introduction to comp sci open courseware,1,cracking the d coding interview,currently work a data analyst for large financial institution my work us some sql but most of the work is done using our proprietary data system so non transferable skill over the past year or so ve been learning python mathematics like probability linear alg and vector calc a well a reviewing wide range of machine learning algos from linear logistic regression to sota stuff like transformer because find the modeling side really interesting ve had two coding challenge presented so far to me a screening exercise and don feel like doing a well a need to be to be considered yesterday did hackerrank challenge for verisk company and wa able to answer both sql question but only of the python coding one it frustrating because know how to make thing work using python but maybe not always the fastest way ve never worked in an environment with code review is the answer here to just chug away at leetcode hackerrank medium and hards power through an introduction to comp sci open courseware
revisiting the the sexiest job of the st century article decade later what still remains true and what is no longer true about the industry from the article and have their speculations bore out,so it which means it almost decade since the original sexiest job of the st century article was published by the harvard business review hbr link to the original article is here data scientist the sexiest job of the st century the hbr makes some interesting speculations about the field and think the article is good snapshot of what data science was like back in or at least the aspirations for data science interested in revisiting this article and see what still holds true and what doesn any more years on since its publication in addition what changed about data science since then in other words what speculations did they get right and what did they get wrong,1,revisiting the the sexiest job of the st century article decade later what still remains true and what is no longer true about the industry from the article and have their speculation bore out,so it which mean it almost decade since the original sexiest job of the st century article wa published by the harvard business review hbr link to the original article is here data scientist the sexiest job of the st century the hbr make some interesting speculation about the field and think the article is good snapshot of what data science wa like back in or at least the aspiration for data science interested in revisiting this article and see what still hold true and what doesn any more year on since it publication in addition what changed about data science since then in other word what speculation did they get right and what did they get wrong
good course about ds with big data,hi all looking for course that essentially describes what is big data and shows how to use it with ml dl models have deep understanding of ml amp dl so just looking for course that add the big data puzzle piece to my tools preferably the course should demonstrate its work amp have the exercises be written in python mostly use coursera as my go to since it teaches the technical mathematical background and not only here is piece of code try it type of course so any course that cover both theoretical amp practical sides of the topic is welcome,1,good course about d with big data,hi all looking for course that essentially describes what is big data and show how to use it with ml dl model have deep understanding of ml amp dl so just looking for course that add the big data puzzle piece to my tool preferably the course should demonstrate it work amp have the exercise be written in python mostly use coursera a my go to since it teach the technical mathematical background and not only here is piece of code try it type of course so any course that cover both theoretical amp practical side of the topic is welcome
developing nonprofessional experience,am wondering how one can prove their efficiency as data scientist and their relevant skills prior to entering the work force what sort of projects certifications ect would strengthen rsum for entry level data science positions about me am currently finishing my msc in mathematics and have an undergraduate degree in physics know some programming little bit of python but mostly higher level languages like matlab and mathematica and recognize improving these skills along with maybe my statistical analysis skills would help just trying to figure out the best way to demonstrate that on resume,1,developing nonprofessional experience,am wondering how one can prove their efficiency a data scientist and their relevant skill prior to entering the work force what sort of project certification ect would strengthen rsum for entry level data science position about me am currently finishing my msc in mathematics and have an undergraduate degree in physic know some programming little bit of python but mostly higher level language like matlab and mathematica and recognize improving these skill along with maybe my statistical analysis skill would help just trying to figure out the best way to demonstrate that on resume
beginners choice,which programming language is best for starting the data science journey for someone with no programming knowledge view poll,1,beginner choice,which programming language is best for starting the data science journey for someone with no programming knowledge view poll
hey guys,iam facing an issue extracting the datasets for adni want to extract datasets related to but can seem tk find them can anyone help it will be massive favor,1,hey guy,iam facing an issue extracting the datasets for adni want to extract datasets related to but can seem tk find them can anyone help it will be massive favor
what are your goals and what is your system to reach them,have been in data science field for years now work as trainer first few years was reading amp learning lot but since covid it been hard to keep up with the changing amp constantly growing field difficulty of catching up with abundant information covid isolation has lead to me burning out and loosing motivation amp am trying to get back the mojo to dive deep in the field again haven been feeling that into ai for while years despite achieving decent amount and earning comfortable salary want to overcome the career plateau have reached and looking to shake things up want to rediscover what loved about ai in the first place and move into new subfields which are little more interesting to me my plan is redo the basics andrew ng course and some other classics in computer vision and nlp actually implement some small projects catch up with the progress made in past years start personal project that focuses on combination of nlp unreal engine for effective interactions somethng which feel would be very useful in future don know if it pheasible or what things would be better to try or learn thoughts what are you planning on doing in,1,what are your goal and what is your system to reach them,have been in data science field for year now work a trainer first few year wa reading amp learning lot but since covid it been hard to keep up with the changing amp constantly growing field difficulty of catching up with abundant information covid isolation ha lead to me burning out and loosing motivation amp am trying to get back the mojo to dive deep in the field again haven been feeling that into ai for while year despite achieving decent amount and earning comfortable salary want to overcome the career plateau have reached and looking to shake thing up want to rediscover what loved about ai in the first place and move into new subfields which are little more interesting to me my plan is redo the basic andrew ng course and some other classic in computer vision and nlp actually implement some small project catch up with the progress made in past year start personal project that focus on combination of nlp unreal engine for effective interaction somethng which feel would be very useful in future don know if it pheasible or what thing would be better to try or learn thought what are you planning on doing in
create face mask detector in min with opencv keras tensorflow python and deep learning,create face mask detector in min with opencv keras tensorflow python and deep learning,1,create face mask detector in min with opencv kera tensorflow python and deep learning,create face mask detector in min with opencv kera tensorflow python and deep learning
what the best python notebook setup for data analysis,do you use jupyter lab notebooks in browser or in an ide vscode pycharm dataspell or combination of both what are advantages of each approach any recommended extensions plugins,1,what the best python notebook setup for data analysis,do you use jupyter lab notebook in browser or in an ide vscode pycharm dataspell or combination of both what are advantage of each approach any recommended extension plugins
top object detection apis,top object detection apis,1,top object detection apis,top object detection apis
open domain question answering part blenderbot,open domain question answering part blenderbot,1,open domain question answering part blenderbot,open domain question answering part blenderbot
reading scala spark xgboost model in python,have an xgboost model trained in scala spark with xgboost4j package want to read it in python for doing some analysis it can be done in scala spack any help will be appreciated,1,reading scala spark xgboost model in python,have an xgboost model trained in scala spark with xgboost4j package want to read it in python for doing some analysis it can be done in scala spack any help will be appreciated
can anyone explain about the difference in scope of work between data analyst and market researcher,just an intern in the marketing and research division my job is to build database and doing field research just puzzled about the limitation because my users call them with two different things or are their jobdesk actually the same,1,can anyone explain about the difference in scope of work between data analyst and market researcher,just an intern in the marketing and research division my job is to build database and doing field research just puzzled about the limitation because my user call them with two different thing or are their jobdesk actually the same
where to find good social media suicide dataset,where to find good social media suicide dataset,1,where to find good social medium suicide dataset,where to find good social medium suicide dataset
has anyone ever dealt with passive aggressive bullying in the workplace,junior mid level data scientist who works in small team at startup have supervisor who also has supervisor the sense get from them is that they think not capable for the job they show it in very passive aggressive way my direct supervisor would always be keen to find faults right in the middle of what saying when explaining presenting discussing something esp with other team members these faults don even make sense most often and he let others complete what they have to say without interjecting he tends to point out my mistakes publicly and feel behind doors he must be even more severe in his criticisms cause his supervisor has started to look at me in the same way the whole think you re of inferior intellect and need to show you your place thing really shows when he talks and there such distinct difference from the way he interacts with others vs me in both what he says and the manner tone in which he says it it in effect makes me respond even dumber out of anxiety already anyways nervous because do think they re smarter also like years older and want to learn and do great work but this anxiety turns it into something ugly and uncomfortable the thing scared about is getting fired feel they talk about me behind closed doors and barely talk to me about anything just adds to the anxiety and indeed already underperforming in some areas have been applying recently and hope soon ll be out of this but if you ve had such an experience would love to know how you dealt with it,1,ha anyone ever dealt with passive aggressive bullying in the workplace,junior mid level data scientist who work in small team at startup have supervisor who also ha supervisor the sense get from them is that they think not capable for the job they show it in very passive aggressive way my direct supervisor would always be keen to find fault right in the middle of what saying when explaining presenting discussing something esp with other team member these fault don even make sense most often and he let others complete what they have to say without interjecting he tends to point out my mistake publicly and feel behind door he must be even more severe in his criticism cause his supervisor ha started to look at me in the same way the whole think you re of inferior intellect and need to show you your place thing really show when he talk and there such distinct difference from the way he interacts with others v me in both what he say and the manner tone in which he say it it in effect make me respond even dumber out of anxiety already anyways nervous because do think they re smarter also like year older and want to learn and do great work but this anxiety turn it into something ugly and uncomfortable the thing scared about is getting fired feel they talk about me behind closed door and barely talk to me about anything just add to the anxiety and indeed already underperforming in some area have been applying recently and hope soon ll be out of this but if you ve had such an experience would love to know how you dealt with it
planning for masters in mis msis master with ba bi electives,hi folks male from india am currently working as networking consultant support team at network firm plan to do ms in information systems or management information systems next fall admissions not in ivy colleges to make my career as business analyst or intelligence engineer in some mnc checked the prerequisite on linkedin glassdoor almost required mysql skills along with python tableau and sometimes excel now it been almost years since wrote the mysql query please recommend me couple of websites where can practice writing mysql queries which may be helpful for me in cracking full time jobs opportunities in the us please let me know any other profiles which may be closely related to the above bold roles as of now checked for amazon deloitte zs associates,1,planning for master in mi msis master with ba bi elective,hi folk male from india am currently working a networking consultant support team at network firm plan to do m in information system or management information system next fall admission not in ivy college to make my career a business analyst or intelligence engineer in some mnc checked the prerequisite on linkedin glassdoor almost required mysql skill along with python tableau and sometimes excel now it been almost year since wrote the mysql query please recommend me couple of website where can practice writing mysql query which may be helpful for me in cracking full time job opportunity in the u please let me know any other profile which may be closely related to the above bold role a of now checked for amazon deloitte z associate
start my new role today and am absolutely terrified,this is going to be my first pure data scientist role and am absolutely terrified and its giving me anxiety in my previous roles have done work as data analyst am familiar with python and java amongst others and have built dashboards and websites was working as business data analyst but was abruptly let go in july being let go was what broke the camels back and spent the next several months working on my mental health when had addressed my longstanding issues and was in better and healthy mental state started appling to new roles it took me little over month to get the offer from my current company have wanted to transition to data science role for few years and now that its finally happening am absolutely shitting my pants is it impostor syndrome setting in any advice would be much appriciated,1,start my new role today and am absolutely terrified,this is going to be my first pure data scientist role and am absolutely terrified and it giving me anxiety in my previous role have done work a data analyst am familiar with python and java amongst others and have built dashboard and website wa working a business data analyst but wa abruptly let go in july being let go wa what broke the camel back and spent the next several month working on my mental health when had addressed my longstanding issue and wa in better and healthy mental state started appling to new role it took me little over month to get the offer from my current company have wanted to transition to data science role for few year and now that it finally happening am absolutely shitting my pant is it impostor syndrome setting in any advice would be much appriciated
what are the common applications of ml in fintech companies,would be interested to know what are the things should focus more on as currently do not have much knowledge in finance but interested to look for job in fintech companies somehow are there any good recommendations on skills should focus more on before applying role in fintech companies cheers,1,what are the common application of ml in fintech company,would be interested to know what are the thing should focus more on a currently do not have much knowledge in finance but interested to look for job in fintech company somehow are there any good recommendation on skill should focus more on before applying role in fintech company cheer
how to validate data integrity,hi trying to set up process for validating data integrity on month to month basis can any of you experts share how you verify data integrity our team updates the database based on the information obtained from email communication and updates accordingly,1,how to validate data integrity,hi trying to set up process for validating data integrity on month to month basis can any of you expert share how you verify data integrity our team update the database based on the information obtained from email communication and update accordingly
how can best set myself up for success switching careers to data science,basically the title background have masters in financial economics from big state school and graduated with around gpa worked and currently working in unrelated high finance position at major company none of my current skills transfer in under grad took years of computer science but it was around java if looking to make the jump what recommended steps should take to best help my chances any boot camps worth doing what entry level roles are recommended where can gain applicable experience thanks for any help in advance,1,how can best set myself up for success switching career to data science,basically the title background have master in financial economics from big state school and graduated with around gpa worked and currently working in unrelated high finance position at major company none of my current skill transfer in under grad took year of computer science but it wa around java if looking to make the jump what recommended step should take to best help my chance any boot camp worth doing what entry level role are recommended where can gain applicable experience thanks for any help in advance
is slipping behind,see fewer and fewer references to python seems increasingly dominant in the ds field am wrong ps heavy user and have created several cran packages,1,is slipping behind,see fewer and fewer reference to python seems increasingly dominant in the d field am wrong p heavy user and have created several cran package
looking to enter the field have some questions,posting this again since for some reason could not view my post from my account hey everyone im in here looking for advice ve given the context at the end basically looking to enter the field as data scientist analyst don even know if those terms are interchangeable but have no idea where to begin as for why it because kinda liked programming but only when picked up basics of programming on codeacademy was not motivated to complete since didn really think it would use it later on here are some questions what exactly does data scientist analyst do and how would programming languages help them in their work ve never learned programming languages in college am mostly entering school for mba will only get job in this field if did programming in college where can learn sql and python in depth basically am looking for resource that not only teaches me the basics but also how to apply them in data science for context from india just completed my college last may in marketing and probably gonna join some mba next july so until then wanted to learn stuff know like the basics of like addition and making graphs also okay with learning this as couple of years thing where start asap and probably take job in some other field and maybe pivot into ds if am good enough,1,looking to enter the field have some question,posting this again since for some reason could not view my post from my account hey everyone im in here looking for advice ve given the context at the end basically looking to enter the field a data scientist analyst don even know if those term are interchangeable but have no idea where to begin a for why it because kinda liked programming but only when picked up basic of programming on codeacademy wa not motivated to complete since didn really think it would use it later on here are some question what exactly doe data scientist analyst do and how would programming language help them in their work ve never learned programming language in college am mostly entering school for mba will only get job in this field if did programming in college where can learn sql and python in depth basically am looking for resource that not only teach me the basic but also how to apply them in data science for context from india just completed my college last may in marketing and probably gonna join some mba next july so until then wanted to learn stuff know like the basic of like addition and making graph also okay with learning this a couple of year thing where start asap and probably take job in some other field and maybe pivot into d if am good enough
the data science field is very exhausting and consume much of our time so what else do you do beside your work or study in this domain that helps you to give your brain break from it,the data science field is very exhausting and consume much of our time so what else do you do beside your work or study in this domain that helps you to give your brain break from it,1,the data science field is very exhausting and consume much of our time so what else do you do beside your work or study in this domain that help you to give your brain break from it,the data science field is very exhausting and consume much of our time so what else do you do beside your work or study in this domain that help you to give your brain break from it
what re your favorite tools in what do you wish someone would build,what re your favorite tools in what do you wish someone would build,1,what re your favorite tool in what do you wish someone would build,what re your favorite tool in what do you wish someone would build
do you use reinforcement learning in your work as data scientist how what are the applications,do you use reinforcement learning in your work as data scientist how what are the applications,1,do you use reinforcement learning in your work a data scientist how what are the application,do you use reinforcement learning in your work a data scientist how what are the application
interrupted time series its in python,interrupted time series its in python,1,interrupted time series it in python,interrupted time series it in python
maybe someone looking for data science job in soccer as roma is looking for data analytics manager,maybe someone looking for data science job in soccer as roma is looking for data analytics manager,1,maybe someone looking for data science job in soccer a rom is looking for data analytics manager,maybe someone looking for data science job in soccer a rom is looking for data analytics manager
data science for roles in data engineering ml engineering,hi everyone currently student and looking to move into data engineering ml engineering role am in the process currently of applying to in data science programs and noticed that most of the programs state that their purpose is to develop data scientists would they not be the proper outlet for someone looking to move into an engineering role under data science my goals for example are to develop data science and machine learning applications throughout my career am concerned that if be honest about my intentions to use the degree to move into data engineering that the admissions committees may feel like my goals are not aligned with what they are looking for even though they have the exact curriculum need to meet my goals does anyone have any experience in this area have you pursued data science degree and been upfront about having the goal of moving into an engineering role after graduation should be concerned about being honest about my goals in my personal statements when university claims their program is aimed at creating data scientists do they mean strictly data scientists or would this normally include professionals in broader data science field such as data ml engineers as well thank you all for your time and perspectives they re very much appreciated,1,data science for role in data engineering ml engineering,hi everyone currently student and looking to move into data engineering ml engineering role am in the process currently of applying to in data science program and noticed that most of the program state that their purpose is to develop data scientist would they not be the proper outlet for someone looking to move into an engineering role under data science my goal for example are to develop data science and machine learning application throughout my career am concerned that if be honest about my intention to use the degree to move into data engineering that the admission committee may feel like my goal are not aligned with what they are looking for even though they have the exact curriculum need to meet my goal doe anyone have any experience in this area have you pursued data science degree and been upfront about having the goal of moving into an engineering role after graduation should be concerned about being honest about my goal in my personal statement when university claim their program is aimed at creating data scientist do they mean strictly data scientist or would this normally include professional in broader data science field such a data ml engineer a well thank you all for your time and perspective they re very much appreciated
career advice for transitioning from computer vision to data science,what would you recommend to cv engineer who wants to pursue career in data science took all the ml related grad level courses but ve only worked on computer vision problems during my studies and short career now want to work as data scientist but not sure if be able to get job and don feel confident about it would you recommend kaggle for practicing or should get junior role and learn while working,1,career advice for transitioning from computer vision to data science,what would you recommend to cv engineer who want to pursue career in data science took all the ml related grad level course but ve only worked on computer vision problem during my study and short career now want to work a data scientist but not sure if be able to get job and don feel confident about it would you recommend kaggle for practicing or should get junior role and learn while working
seeking for data science internship and mentorship,my name is lilian am starting out my career in data science have completed courses on data science specialization on coursera and machine learning specialization on udemy also did one year data science and ai fellowship am open to internship opportunity in data science and mentor ship my skills are sql data visualization using power bi tableau python and excel machine learning etc my githhub link is github com lilynaza com you can conntect with on linkedin with the name lilian ugwu,1,seeking for data science internship and mentorship,my name is lilian am starting out my career in data science have completed course on data science specialization on coursera and machine learning specialization on udemy also did one year data science and ai fellowship am open to internship opportunity in data science and mentor ship my skill are sql data visualization using power bi tableau python and excel machine learning etc my githhub link is github com lilynaza com you can conntect with on linkedin with the name lilian ugwu
is facebook prophet an awful package that led to zillow blowing up,is facebook prophet an awful package that led to zillow blowing up,1,is facebook prophet an awful package that led to zillow blowing up,is facebook prophet an awful package that led to zillow blowing up
is there lot of writing code in data science,coming from web mobile development background where write lot of code but it not so challenging mentally thinking of learning data science and especially machine learning problem is that scared ll be locked in excel data visualization hell where really care only about writing code and solving problems know that with machine learning comes with lot of math thinking how to strip data and stuff but it all seems just like problem solving always knew just don want to spend few months of my life learning it just to leave because it not my thing,1,is there lot of writing code in data science,coming from web mobile development background where write lot of code but it not so challenging mentally thinking of learning data science and especially machine learning problem is that scared ll be locked in excel data visualization hell where really care only about writing code and solving problem know that with machine learning come with lot of math thinking how to strip data and stuff but it all seems just like problem solving always knew just don want to spend few month of my life learning it just to leave because it not my thing
better refraction methods,just starting to code larger projects and starting to feel like lot of the code that write and clean is unclear and at times confusing many times it also feels like what doing in my program should be simpler or feel like should have been able to do it more concisely does anyone have advice on cleaning and refactoring code for larger projects,1,better refraction method,just starting to code larger project and starting to feel like lot of the code that write and clean is unclear and at time confusing many time it also feel like what doing in my program should be simpler or feel like should have been able to do it more concisely doe anyone have advice on cleaning and refactoring code for larger project
seeking data ops advice,hi all please let me know if there is better place to ask this question basically each month receive data set looking to have that data set be added to data warehouse in that dw there is specific column where like to use some sort of vba script to push out an email the email is triggered once one of those dates hits let just say days until renewal it needs to be all stored sent through the msft suite if you have any insight ideas on how this could be done it would be greatly appreciated again ve got the vision just unsure how to get there ideal process data set is added to live dw gt script eliminates duplicate entries gt renewal date is updated daily gt once amount of days out from today date gt email is pushed out through outlook hope everyone had nice holiday season cheers,1,seeking data ops advice,hi all please let me know if there is better place to ask this question basically each month receive data set looking to have that data set be added to data warehouse in that dw there is specific column where like to use some sort of vba script to push out an email the email is triggered once one of those date hit let just say day until renewal it need to be all stored sent through the msft suite if you have any insight idea on how this could be done it would be greatly appreciated again ve got the vision just unsure how to get there ideal process data set is added to live dw gt script eliminates duplicate entry gt renewal date is updated daily gt once amount of day out from today date gt email is pushed out through outlook hope everyone had nice holiday season cheer
recommended service for deploying model online for multiple users to add to central database,please let me know if there better place to post this question still relatively new data scientist pretty familiar with building models but we ve always handed them off to an mle for deployment and ve only ever done that in work setting so model deployment is bit out of my depth ve done fair bit of research but nothing ve found has addressed this particular use case and not sure what keywords to include to find an answer friend of mine has successfully sued to have number of documents released to the public under foia there are thousands of these documents and the only way to get all of them will be to foia few of them at time we want to compile database of the information in these documents so our plan is to spread out the work of requesting them across multiple people and combining the information on the back end the body we re requesting the documents from definitely won provide us with searchable database as evidenced by the fact that we had to sue in the first place ve put together model that can extract the information in these documents pretty accurately our ideal deployment workflow is that people who foia the documents can upload the documents run them through the extraction model manually validate and correct the output and add it to central database people can access what service should use that can do all of these are there any considerations for this use case should be aware of that haven mentioned,1,recommended service for deploying model online for multiple user to add to central database,please let me know if there better place to post this question still relatively new data scientist pretty familiar with building model but we ve always handed them off to an mle for deployment and ve only ever done that in work setting so model deployment is bit out of my depth ve done fair bit of research but nothing ve found ha addressed this particular use case and not sure what keywords to include to find an answer friend of mine ha successfully sued to have number of document released to the public under foia there are thousand of these document and the only way to get all of them will be to foia few of them at time we want to compile database of the information in these document so our plan is to spread out the work of requesting them across multiple people and combining the information on the back end the body we re requesting the document from definitely won provide u with searchable database a evidenced by the fact that we had to sue in the first place ve put together model that can extract the information in these document pretty accurately our ideal deployment workflow is that people who foia the document can upload the document run them through the extraction model manually validate and correct the output and add it to central database people can access what service should use that can do all of these are there any consideration for this use case should be aware of that haven mentioned
data science information,hi currently enrolled in data science boot camp and will soon begin masters in data science have in mathematics but no data science experience whatsoever like to connect with people to learn more about the industry as well as what should be focusing on any input or phone call email correspondence would be appreciated,1,data science information,hi currently enrolled in data science boot camp and will soon begin master in data science have in mathematics but no data science experience whatsoever like to connect with people to learn more about the industry a well a what should be focusing on any input or phone call email correspondence would be appreciated
do do fellowship for the federal government or do phd,hi year old graduate student living in boston have house here and study biostatistics and epidemiology my career goal is do data science for help solve health disparities and use big data to help inform clinics and hospital systems about cancer treatments and teach classes on the side at crossroads as to what should do next finish my degree in year and am wondering if should do fellowship or do phd in data science the pros of the phd are that love boston phd would help me with my ultimate career goal but the cons are that it would be significant investment of time and emotion the pros of the fellowship are that doing it would be really really cool and would allow me to be mentored but the cons are that not sure if can move to maryland because love my life here and not sure if can be accepted was wondering what this community thinks,1,do do fellowship for the federal government or do phd,hi year old graduate student living in boston have house here and study biostatistics and epidemiology my career goal is do data science for help solve health disparity and use big data to help inform clinic and hospital system about cancer treatment and teach class on the side at crossroad a to what should do next finish my degree in year and am wondering if should do fellowship or do phd in data science the pro of the phd are that love boston phd would help me with my ultimate career goal but the con are that it would be significant investment of time and emotion the pro of the fellowship are that doing it would be really really cool and would allow me to be mentored but the con are that not sure if can move to maryland because love my life here and not sure if can be accepted wa wondering what this community think
is vba transferable to other languages,am in the process of applying to masters program in data science and bit worried about the prospect of learning new programming languages work for huge corporate entity and there very tight it policies that do not permit me to load python on my work pc instead ve automated lot of my work using vba scripting and use this language for data cleansing as well in the process ve become pretty familiar with flow control variables data types booleans objects etc is this background information enough to readily pick up something like python if not is there good primer that you might recommend for any gaps in knowledge thanks much,1,is vba transferable to other language,am in the process of applying to master program in data science and bit worried about the prospect of learning new programming language work for huge corporate entity and there very tight it policy that do not permit me to load python on my work pc instead ve automated lot of my work using vba scripting and use this language for data cleansing a well in the process ve become pretty familiar with flow control variable data type booleans object etc is this background information enough to readily pick up something like python if not is there good primer that you might recommend for any gap in knowledge thanks much
top technical certifications of,top technical certifications of,1,top technical certification of,top technical certification of
data science roadmap suggestions,data analyst and want to switch to data science come up with some kind of roadmap for this transition and if you have any suggestions will be more than happy to listen read already good with pl sql and tableau so think it will help but other than that don think have much experience roadmap firstly want to learn more about statistics learned it at uni but don remember much from it so found this to follow if you can suggest anything else better please do secondly think about phyton if you can suggest any tutorials or udemy coursera courses it will be awesome lastly machine learning and deep learning still don know from where so any suggestions will be welcome thanks in advance,1,data science roadmap suggestion,data analyst and want to switch to data science come up with some kind of roadmap for this transition and if you have any suggestion will be more than happy to listen read already good with pl sql and tableau so think it will help but other than that don think have much experience roadmap firstly want to learn more about statistic learned it at uni but don remember much from it so found this to follow if you can suggest anything else better please do secondly think about phyton if you can suggest any tutorial or udemy coursera course it will be awesome lastly machine learning and deep learning still don know from where so any suggestion will be welcome thanks in advance
data scientist vs clinical sas statistical programmer,which one is more lucrative career which ony pays more which one has less competition more bargaining power for an employee,1,data scientist v clinical sa statistical programmer,which one is more lucrative career which ony pay more which one ha le competition more bargaining power for an employee
what are the best data science tools should learn to use if want to be an economist,ve got rudimentary understanding of matlab and stata what are the other software should look at what are the most usefull tools should look at machine learning techniques like lasso what are your most useful tools,1,what are the best data science tool should learn to use if want to be an economist,ve got rudimentary understanding of matlab and stata what are the other software should look at what are the most usefull tool should look at machine learning technique like lasso what are your most useful tool
utility vs understanding the state of machine learning entering,utility vs understanding the state of machine learning entering,1,utility v understanding the state of machine learning entering,utility v understanding the state of machine learning entering
be useful,remember to be useful also the best leaders are the ones that don want to lead not the ones trying to become leaders listen to elon and lex in terms of data science don be in love with data because that not useful be in love with system and solving its problems business social research etc,1,be useful,remember to be useful also the best leader are the one that don want to lead not the one trying to become leader listen to elon and lex in term of data science don be in love with data because that not useful be in love with system and solving it problem business social research etc
weekly entering amp transitioning thread jan jan,welcome to this week entering amp transitioning thread this thread is for any questions about getting started studying or transitioning into the data science field topics include learning resources books tutorials videos traditional education schools degrees electives alternative education online courses bootcamps job search questions resumes applying career prospects elementary questions where to start what next while you wait for answers from the community check out the faq and resources resources pages on our wiki you can also search for answers in past weekly threads restrict_sr amp sort new,1,weekly entering amp transitioning thread jan jan,welcome to this week entering amp transitioning thread this thread is for any question about getting started studying or transitioning into the data science field topic include learning resource book tutorial video traditional education school degree elective alternative education online course bootcamps job search question resume applying career prospect elementary question where to start what next while you wait for answer from the community check out the faq and resource resource page on our wiki you can also search for answer in past weekly thread restrict_sr amp sort new
what the best in between aws and data science,what if someone is stuck to choose in between aws and data science which one is the best,1,what the best in between aws and data science,what if someone is stuck to choose in between aws and data science which one is the best
venturebeat how to discover ai code know how with catalyzex,venturebeat how to discover ai code know how with catalyzex,1,venturebeat how to discover ai code know how with catalyzex,venturebeat how to discover ai code know how with catalyzex
macbook pro intel based vs macbook pro m1 based,so am planning on getting new laptop for my studies masters in data science should go for the intel based macbook pros or the new m1 macbook pros have read about not risking it with the new m1 macs but what is your opinion,1,macbook pro intel based v macbook pro m1 based,so am planning on getting new laptop for my study master in data science should go for the intel based macbook pro or the new m1 macbook pro have read about not risking it with the new m1 mac but what is your opinion
what is your team function within your organisation,as title suggests am interested whether data scientists usually sit in team that provide support to other product teams support team or are you part of the product team view poll,1,what is your team function within your organisation,a title suggests am interested whether data scientist usually sit in team that provide support to other product team support team or are you part of the product team view poll
anyone here doing cricket analytics could you give some good research papers on how to analyze the games,so have craze for cricket as game and looking for better ways to find meaningful insights from the data have so if you have research papers that could help me find better techniques that would be appreciated thanks,1,anyone here doing cricket analytics could you give some good research paper on how to analyze the game,so have craze for cricket a game and looking for better way to find meaningful insight from the data have so if you have research paper that could help me find better technique that would be appreciated thanks
is medium even proofreading submissions at all,just read this medium post about embedding layers that was emailed to me as suggestion there are so many typos spelling mistakes it hard to understand how it was published not to mentioned the questionable omission of one hot encoding as an obvious option to deal with categorical variables,1,is medium even proofreading submission at all,just read this medium post about embedding layer that wa emailed to me a suggestion there are so many typo spelling mistake it hard to understand how it wa published not to mentioned the questionable omission of one hot encoding a an obvious option to deal with categorical variable
predicting multiple targets multi class text classification,predicting multiple targets multi class text classification,1,predicting multiple target multi class text classification,predicting multiple target multi class text classification
can anyone tell me how to improve my technical writing on medium looking for feedback,so ve been writing from time to time on medium to get traction on my profile so far have been reached by few startup companies to join them looking for feedback since most of my writing is about data science it would be really helpful to learn what should work on to improve here is the link to my medium account medium app installed thank you,1,can anyone tell me how to improve my technical writing on medium looking for feedback,so ve been writing from time to time on medium to get traction on my profile so far have been reached by few startup company to join them looking for feedback since most of my writing is about data science it would be really helpful to learn what should work on to improve here is the link to my medium account medium app installed thank you
need career advice,hey guys need career advice am working as digital marketer and want to learn data science python to learn more skills for better career how should go about it from where should start and what ia your advice for me how the future in data science jobs thanks,1,need career advice,hey guy need career advice am working a digital marketer and want to learn data science python to learn more skill for better career how should go about it from where should start and what ia your advice for me how the future in data science job thanks
from data science to software engineer swe,thinking to switch from data science data analytic to software engineer anyone thinking the same,1,from data science to software engineer swe,thinking to switch from data science data analytic to software engineer anyone thinking the same
for those in the us where did you get your masters in data science and why,thinking of doing masters but can decide on which appreciate any response on this thank you,1,for those in the u where did you get your master in data science and why,thinking of doing master but can decide on which appreciate any response on this thank you
method of estimating population from video samples,helping local aquaculture firm with setting up ds ml pipeline for shrimp culture one of the tasks attempting to do is estimate the total population of shrimp in pond from video feed the problem with this is don have traditional top down view of the pond where can count all the shrimp instead have few cameras set up at the boundaries of the pond where it has view into the pond and each camera is able to capture specific area of the pond in its fov already have shrimp detector and counter built where the model counts and tracks each shrimp going in and out of view so have virtually infinite number of samples as long as the model and cameras are running the thing is how can use the output number of shrimp being detected by the cameras at any point of time over period of time to estimate the total population of shrimp reliably my initial thought was to try using mcmc but can seem to find literature supporting it would love to hear from the professionals here thanks,1,method of estimating population from video sample,helping local aquaculture firm with setting up d ml pipeline for shrimp culture one of the task attempting to do is estimate the total population of shrimp in pond from video feed the problem with this is don have traditional top down view of the pond where can count all the shrimp instead have few camera set up at the boundary of the pond where it ha view into the pond and each camera is able to capture specific area of the pond in it fov already have shrimp detector and counter built where the model count and track each shrimp going in and out of view so have virtually infinite number of sample a long a the model and camera are running the thing is how can use the output number of shrimp being detected by the camera at any point of time over period of time to estimate the total population of shrimp reliably my initial thought wa to try using mcmc but can seem to find literature supporting it would love to hear from the professional here thanks
civil engineer undergrad gt data science masters feasible,interested in getting into data science as civil engineer pay is shit going to graduate with an undergraduate degree in civil engineering so ve taken at most intro class each with matlab js etc wondering if it be feasible to get into masters program for data science and if this transitions feasible looking at gt omsa program online or usc data science masters in person as it geared towards stem people who don have experience with data science do have previous intern experience but that was civil engineering related using autocad and excel is the masters necessary to get into the ds field,1,civil engineer undergrad gt data science master feasible,interested in getting into data science a civil engineer pay is shit going to graduate with an undergraduate degree in civil engineering so ve taken at most intro class each with matlab j etc wondering if it be feasible to get into master program for data science and if this transition feasible looking at gt omsa program online or usc data science master in person a it geared towards stem people who don have experience with data science do have previous intern experience but that wa civil engineering related using autocad and excel is the master necessary to get into the d field
graduating in spring with business degree what next,as the title said am currently senior that will be graduating in the spring with degree in business analytics during my junior year was introduced to big data and became interested immediately since then have taken every course available that deals with coding through my university and worked on my own to learn sql and python in the future would like to work in the data science field but don know my plans after graduation am currently researching ms in data science specifically from syracuse and denver but have some reservations about their quality recently discovered georgia tech omscs and am currently researching this option compared to the ms in data science this option is far more affordable does anyone have any advice for what program would be good next step or have program university they would recommend for someone in my position this is my first post so apologize any naivety or general inexperience on my end thank you for your help,1,graduating in spring with business degree what next,a the title said am currently senior that will be graduating in the spring with degree in business analytics during my junior year wa introduced to big data and became interested immediately since then have taken every course available that deal with coding through my university and worked on my own to learn sql and python in the future would like to work in the data science field but don know my plan after graduation am currently researching m in data science specifically from syracuse and denver but have some reservation about their quality recently discovered georgia tech omscs and am currently researching this option compared to the m in data science this option is far more affordable doe anyone have any advice for what program would be good next step or have program university they would recommend for someone in my position this is my first post so apologize any naivety or general inexperience on my end thank you for your help
electrical engineering to data science machine learning,hi all am currently in my last year of bachelors of electrical engineering in canada have been hearing the rave about how data science machine learning ai python sql etc is the future and the job market is about to boom for it or has rather already begun never had any special inclination towards any of the electronics analogue and communications courses had to take throughout my degree but rather the few courses took in programming object oriented software development java dealing with data structures for dynamic arrays and linked lists in an imperative programming paradigm long story short ve always known deep down that would eventually be deviating from the traditional realm of all things electrical and electronics engineering and veering towards more tech savvy domain here are some of the questions have from my very preliminary research so far what understand is that people with bachelors of engineering bachelors of math statistics etc or degrees of business management etc are all somehow able to work in the field of data science am curious to know how that is possible seeing that this is quite the variety of faculties would still be applying my engineering skills and problem solving nature should choose this field seeing as it encompasses people of different majors will still be an engineer if were to pick one to study right now to optimize my spare time out of python sql and what would you suggest ask from job demand market perspective could you also guide me to the right resources free or with fee doesn matter any other skills should be developing or languages should be mastering how can make my resume stand out in this job market last but definitely not least what is data science what is machine learning would love to hear from professional in this field preferably from an engineer last thing want is to sound like the rest of the amateurs throwing flashy words around doing ai machine learning just to hop on trending sexy hype know these questions can be quite complex to answer in couple lines but would greatly appreciate any insight and direction am quite overwhelmed by the various opinions views and ideas every google search has to offer sorry if sounded ignorant and aloof quite lost and inexperienced thanks for reading if you made it till here happy new year puzzled soon to be fresh engineer graduate,1,electrical engineering to data science machine learning,hi all am currently in my last year of bachelor of electrical engineering in canada have been hearing the rave about how data science machine learning ai python sql etc is the future and the job market is about to boom for it or ha rather already begun never had any special inclination towards any of the electronics analogue and communication course had to take throughout my degree but rather the few course took in programming object oriented software development java dealing with data structure for dynamic array and linked list in an imperative programming paradigm long story short ve always known deep down that would eventually be deviating from the traditional realm of all thing electrical and electronics engineering and veering towards more tech savvy domain here are some of the question have from my very preliminary research so far what understand is that people with bachelor of engineering bachelor of math statistic etc or degree of business management etc are all somehow able to work in the field of data science am curious to know how that is possible seeing that this is quite the variety of faculty would still be applying my engineering skill and problem solving nature should choose this field seeing a it encompasses people of different major will still be an engineer if were to pick one to study right now to optimize my spare time out of python sql and what would you suggest ask from job demand market perspective could you also guide me to the right resource free or with fee doesn matter any other skill should be developing or language should be mastering how can make my resume stand out in this job market last but definitely not least what is data science what is machine learning would love to hear from professional in this field preferably from an engineer last thing want is to sound like the rest of the amateur throwing flashy word around doing ai machine learning just to hop on trending sexy hype know these question can be quite complex to answer in couple line but would greatly appreciate any insight and direction am quite overwhelmed by the various opinion view and idea every google search ha to offer sorry if sounded ignorant and aloof quite lost and inexperienced thanks for reading if you made it till here happy new year puzzled soon to be fresh engineer graduate
primer for biological statistics in rstudio anova regressions correlations bar and line graphs,primer for biological statistics in rstudio anova regressions correlations bar and line graphs,1,primer for biological statistic in rstudio anova regression correlation bar and line graph,primer for biological statistic in rstudio anova regression correlation bar and line graph
which role to apply for data analyst vs data scientist,currently work as senior data analyst for the us federal government my work involves little data engineering lot of data analysis but little data science also have ms degree which gave me ds education love that my current job gives me some exposure to data engineering but don like the lack of exposure to machine learning interviewed for few data science roles at several companies think didn get the jobs because failed the case studies happy to work my way up to data scientist role but not sure which route should take should try to start as data analyst at bigger tech company and work my way up or should apply to smaller or less technical companies as data scientist where might have better shot at getting hired also should apply to smaller companies in order to get more exposure to different roles da de ds especially since still learning,1,which role to apply for data analyst v data scientist,currently work a senior data analyst for the u federal government my work involves little data engineering lot of data analysis but little data science also have m degree which gave me d education love that my current job give me some exposure to data engineering but don like the lack of exposure to machine learning interviewed for few data science role at several company think didn get the job because failed the case study happy to work my way up to data scientist role but not sure which route should take should try to start a data analyst at bigger tech company and work my way up or should apply to smaller or le technical company a data scientist where might have better shot at getting hired also should apply to smaller company in order to get more exposure to different role da de d especially since still learning
which templates are better to create data science web portfolio on github pages jekyll or html5up,planning to create web portfolio hosted in github pages ve found two alternatives use templates from html5up or jekyll can anyone advise me on which one to use from what ve seen think that jekyll offers more features but not planning on building something complex although do want to be able to post notebooks in some projects pages,1,which template are better to create data science web portfolio on github page jekyll or html5up,planning to create web portfolio hosted in github page ve found two alternative use template from html5up or jekyll can anyone advise me on which one to use from what ve seen think that jekyll offer more feature but not planning on building something complex although do want to be able to post notebook in some project page
mle first position,got an offer for an mle position with one semester of university left ve only taken course in ml for my cs degree and don really have experience in ml beyond that what would be the best way to prepare for the role,1,mle first position,got an offer for an mle position with one semester of university left ve only taken course in ml for my c degree and don really have experience in ml beyond that what would be the best way to prepare for the role
how to approach sql questions in interview,am fairly good at sql and have been applying to roles that test sql but am always stumped by the question when practice using leetcode and statcratch it is always the phrasing of the questions that confuses me once look at the solution or watch the video answer in the case of statascratch they always make sense the concepts required in the questions are all things know but dont know how to apply them have been researching for advice and someone suggested to write down what the output table should look like by hand and then work backwards which has helped but am still failing to grasp how to solve these questions below is practice question that stumped me posted on statscartch from meta medium difficulty any advice would help amp x200b what is the overall friend acceptance rate by date your output should have the rate of acceptances by the date the request was sent order by the earliest date to latest assume that each friend request starts by user sending user id sender friend request to another user user id receiver that logged in the table with action sent if the request is accepted the table logs action accepted if the request is not accepted no record of action accepted is logged amp x200b format png amp auto webp amp a286e292e8f223d172e7e71e24485bbaed328d,1,how to approach sql question in interview,am fairly good at sql and have been applying to role that test sql but am always stumped by the question when practice using leetcode and statcratch it is always the phrasing of the question that confuses me once look at the solution or watch the video answer in the case of statascratch they always make sense the concept required in the question are all thing know but dont know how to apply them have been researching for advice and someone suggested to write down what the output table should look like by hand and then work backwards which ha helped but am still failing to grasp how to solve these question below is practice question that stumped me posted on statscartch from meta medium difficulty any advice would help amp x200b what is the overall friend acceptance rate by date your output should have the rate of acceptance by the date the request wa sent order by the earliest date to latest assume that each friend request start by user sending user id sender friend request to another user user id receiver that logged in the table with action sent if the request is accepted the table log action accepted if the request is not accepted no record of action accepted is logged amp x200b format png amp auto webp amp a286e292e8f223d172e7e71e24485bbaed328d
what are recruiting managers looking for in data scientist,live in us and got bachelor degree in cs with an emphasis in information systems from foreign country and been working in non skilled jobs for years with zero professional experience in data science would like to know what the right path to start in data science should take bootcamp get master degree or just self study btw currently self studying the data scientist path on datacamp hopefully there are some recruiting managers here could tell me if they would consider hiring someone as an entry level data scientist with foreign degree and zero professional experience in the industry,1,what are recruiting manager looking for in data scientist,live in u and got bachelor degree in c with an emphasis in information system from foreign country and been working in non skilled job for year with zero professional experience in data science would like to know what the right path to start in data science should take bootcamp get master degree or just self study btw currently self studying the data scientist path on datacamp hopefully there are some recruiting manager here could tell me if they would consider hiring someone a an entry level data scientist with foreign degree and zero professional experience in the industry
is it ethically right to pirate course,well have huge folder with video materials from very popular course provider downloaded it for free in reality it costs lot of money of course will not get the same mentorship and help as if bought it but still feel weird about it now feel conflicted if it ethically ok to study with it,1,is it ethically right to pirate course,well have huge folder with video material from very popular course provider downloaded it for free in reality it cost lot of money of course will not get the same mentorship and help a if bought it but still feel weird about it now feel conflicted if it ethically ok to study with it
partial least squares regression question,hey everyone am looking to do partial least squares regression in using the pls function understand that pls functions similar to pcr in that it reduces data dimensions and uses those reductions as covariates in regression model in is there way to save the reduced variables so that you can use them anyway you want like thinking of this in pcr kind of way in which we reduce the variables into principal components and save those and then regress whichever of them we choose is that possible in any help would be appreciated thanks,1,partial least square regression question,hey everyone am looking to do partial least square regression in using the pls function understand that pls function similar to pcr in that it reduces data dimension and us those reduction a covariates in regression model in is there way to save the reduced variable so that you can use them anyway you want like thinking of this in pcr kind of way in which we reduce the variable into principal component and save those and then regress whichever of them we choose is that possible in any help would be appreciated thanks
how to register on usgs earthexplorer and download digital elevation model dem,how to register on usgs earthexplorer and download digital elevation model dem,1,how to register on usgs earthexplorer and download digital elevation model dem,how to register on usgs earthexplorer and download digital elevation model dem
need hand beginner data analytic,hi there need an advice from someone with an experience in data analysis on my learning way got stuck with my very first project non commercial where need to create dashboard with simple visualisations line chart about historical sales data my dataset contains data about historical sales of random retail shop and contains columns date and sale amount and has rows years my target is to create dashboard with one line chart and filtering field so users will be able to see sales perfomane by week month year by checking web found so many ways and tools even templates which made me only even more confused where should start with this and what to use word or powerbi or probably tableau or studio excel google sheets additionaly would be great to have an ability to compare those sales as well as have weekends and weekdays marked on data viz thanks for reading will be happy to hear any advice and opinion about this case,1,need hand beginner data analytic,hi there need an advice from someone with an experience in data analysis on my learning way got stuck with my very first project non commercial where need to create dashboard with simple visualisation line chart about historical sale data my dataset contains data about historical sale of random retail shop and contains column date and sale amount and ha row year my target is to create dashboard with one line chart and filtering field so user will be able to see sale perfomane by week month year by checking web found so many way and tool even template which made me only even more confused where should start with this and what to use word or powerbi or probably tableau or studio excel google sheet additionaly would be great to have an ability to compare those sale a well a have weekend and weekday marked on data viz thanks for reading will be happy to hear any advice and opinion about this case
what do you think,what do you think,1,what do you think,what do you think
where can learn about github and what should upload to github,ve spent most of last year learning to switch careers into the data field feel really good about my python knowledge decent about my pandas matplotlib and seaborn skills modeling machine learning and linear regression are major holes in my knowledge looking to fix ve got quite few projects built up at this point and want to start applying to entry level positions ve read that github and linkedin are major tools to help me but kinda lost any material you recommend for learning how to operate github what should upload to github ve got few projects that are extremely guided used learning to code with fantasy football and bunch that are me should upload the guided ones or just the ones that where coded without book to guide me do need to upload the data used at well thanks so much dudes,1,where can learn about github and what should upload to github,ve spent most of last year learning to switch career into the data field feel really good about my python knowledge decent about my panda matplotlib and seaborn skill modeling machine learning and linear regression are major hole in my knowledge looking to fix ve got quite few project built up at this point and want to start applying to entry level position ve read that github and linkedin are major tool to help me but kinda lost any material you recommend for learning how to operate github what should upload to github ve got few project that are extremely guided used learning to code with fantasy football and bunch that are me should upload the guided one or just the one that where coded without book to guide me do need to upload the data used at well thanks so much dude
advice business analytics vs data science master degree,hello datascience community am looking for some advice on graduate degree here is little background stem administrator and teacher and am desiring to make transition at some point into learning analytics la or another data analysis area in education have master in educational technology but would like to pursue another degree that aligns more with my career interests my bachelor degree is in math and physics so am comfortable with the quantitative side of things have two options that am looking at at two regional universities one is the ms in business analytics and the other is an ms in data science the coursework for both looks similar although the msba has some leveling courses in business and the msds has programming courses for leveling am an alum of the school offering the msba guess my main question is this is an msba accepted as legitimate degree in the ds community should worry less about the degree and focus more on ds skills am going back and forth with this and need some insight thank you,1,advice business analytics v data science master degree,hello datascience community am looking for some advice on graduate degree here is little background stem administrator and teacher and am desiring to make transition at some point into learning analytics la or another data analysis area in education have master in educational technology but would like to pursue another degree that aligns more with my career interest my bachelor degree is in math and physic so am comfortable with the quantitative side of thing have two option that am looking at at two regional university one is the m in business analytics and the other is an m in data science the coursework for both look similar although the msba ha some leveling course in business and the msds ha programming course for leveling am an alum of the school offering the msba guess my main question is this is an msba accepted a legitimate degree in the d community should worry le about the degree and focus more on d skill am going back and forth with this and need some insight thank you
statistics resources for data scientist wannabe,hi all as part of trying to move into the data scientist career role in the near future want to review recap the essential statistics knowledge and concepts which understand from this community is experimental statistics experiment design hypothesis testing testing etc does anyone have any particular notable recommendations for online courses on udemy udacity coursera and the like or plain webpage resources that covers the material well thanks for any suggestions,1,statistic resource for data scientist wannabe,hi all a part of trying to move into the data scientist career role in the near future want to review recap the essential statistic knowledge and concept which understand from this community is experimental statistic experiment design hypothesis testing testing etc doe anyone have any particular notable recommendation for online course on udemy udacity coursera and the like or plain webpage resource that cover the material well thanks for any suggestion
people have been telling me bootcamps are scam and they out there just to get your money also believed college degrees is the only way until colleges everywhere starting to have their own bootcamps are they scamming people in doing bootcamps,people have been telling me bootcamps are scam and they out there just to get your money also believed college degrees is the only way until colleges everywhere starting to have their own bootcamps are they scamming people in doing bootcamps,1,people have been telling me bootcamps are scam and they out there just to get your money also believed college degree is the only way until college everywhere starting to have their own bootcamps are they scamming people in doing bootcamps,people have been telling me bootcamps are scam and they out there just to get your money also believed college degree is the only way until college everywhere starting to have their own bootcamps are they scamming people in doing bootcamps
random walk with fire,random walk with fire,1,random walk with fire,random walk with fire
semi snarky request write better questions,this sub has lot of people asking how do do thing this is great glad you want to learn and honestly most of these questions are more interesting than the nth how do get job as data scientist which certificate should blow my money on etc post however the best way for us to help you is to be very clear about what do you want to do sometimes you may not know entirely eg how do know if something is causal but the clearer you can be in question the more effective we the subreddit can be in providing you guidance details about the kind of data you have what you re trying to measure things you ve tried already all of this can help have some data and want to see if there relationship there not great question what tools you re comfortable with are you python pandas purist do you think the tidyverse is tops do you think anything written after assembly is blasphemous and cheating if so wow not sure why you re here you probably know more about coding than everyone else in this sub combined are you an excel wizard who shudders at the thought of the command line no judgement here everyone has to start somewhere the more you can tell us about what you re comfortable with the better our recommendations can be what constraints you have are you at faang where you have effectively infinite compute more money than god and the ability to simulate the universe in miniature to answer hard questions suggestions for you are going to be different than those for someone working locally on desktop laptop with limited compute resources in the latter case regex and logistic regression might be more useful than high powered neural network whether this is homework or class assignment if it is ask your professor ta they can answer your question better than we can and to those of you answering please stop suggesting neural networks to solve problems it probably not the best solution and certainly shouldn be the first,1,semi snarky request write better question,this sub ha lot of people asking how do do thing this is great glad you want to learn and honestly most of these question are more interesting than the nth how do get job a data scientist which certificate should blow my money on etc post however the best way for u to help you is to be very clear about what do you want to do sometimes you may not know entirely eg how do know if something is causal but the clearer you can be in question the more effective we the subreddit can be in providing you guidance detail about the kind of data you have what you re trying to measure thing you ve tried already all of this can help have some data and want to see if there relationship there not great question what tool you re comfortable with are you python panda purist do you think the tidyverse is top do you think anything written after assembly is blasphemous and cheating if so wow not sure why you re here you probably know more about coding than everyone else in this sub combined are you an excel wizard who shudder at the thought of the command line no judgement here everyone ha to start somewhere the more you can tell u about what you re comfortable with the better our recommendation can be what constraint you have are you at faang where you have effectively infinite compute more money than god and the ability to simulate the universe in miniature to answer hard question suggestion for you are going to be different than those for someone working locally on desktop laptop with limited compute resource in the latter case regex and logistic regression might be more useful than high powered neural network whether this is homework or class assignment if it is ask your professor ta they can answer your question better than we can and to those of you answering please stop suggesting neural network to solve problem it probably not the best solution and certainly shouldn be the first
normal distribution,on what factors on the data of baseball matches may apply normal distribution,1,normal distribution,on what factor on the data of baseball match may apply normal distribution
intentionally null cell values,prepared large dataset however some cell values are null which is related to some other column values therefore it something important and cannot drop them and want it also effect the results for example country ncity namecity1 namecity2 namecity3 null am planning to apply some machine learning algorithms on this dataset what would be your suggestions,1,intentionally null cell value,prepared large dataset however some cell value are null which is related to some other column value therefore it something important and cannot drop them and want it also effect the result for example country ncity namecity1 namecity2 namecity3 null am planning to apply some machine learning algorithm on this dataset what would be your suggestion
entry level years fuck off,entry level years fuck off,1,entry level year fuck off,entry level year fuck off
basics of statistics need to know to be able to understand element of statistical learning book,basics of statistics need to know to be able to understand element of statistical learning book,1,basic of statistic need to know to be able to understand element of statistical learning book,basic of statistic need to know to be able to understand element of statistical learning book
covid cases incredibly fibonaccians,amp x200b format png amp auto webp amp c9fc0bcea36c0a34fe08d5436dc59ff3d16bee76,1,covid case incredibly fibonaccians,amp x200b format png amp auto webp amp c9fc0bcea36c0a34fe08d5436dc59ff3d16bee76
has anyone here worked with analyzing server data,interested in analyzing server data metrics and would like to know if anyone here has done this before,1,ha anyone here worked with analyzing server data,interested in analyzing server data metric and would like to know if anyone here ha done this before
what does data scientist do,what does data scientist do,1,what doe data scientist do,what doe data scientist do
looking for impactful projects to put in my portfolio,hi all am looking for some projects can do to practice and hone my skills specially projects related to sql python tableau some data wrangling and maybe some ml dl as final step for clustering prediction etc let say have month to learn as much as can and is currently going through data analysis route but would love picking some ds skills on the way would appreciate any advice,1,looking for impactful project to put in my portfolio,hi all am looking for some project can do to practice and hone my skill specially project related to sql python tableau some data wrangling and maybe some ml dl a final step for clustering prediction etc let say have month to learn a much a can and is currently going through data analysis route but would love picking some d skill on the way would appreciate any advice
hello have dataset of airfare rates that are negotiated each year by my organization and structured as one way fare prices from airport to airport needing some help to make this information easy to access for others,hello have dataset of airfare rates that are negotiated each year by my organization and structured as one way fare prices from airport to airport needing some help to make this information easy to access for others,1,hello have dataset of airfare rate that are negotiated each year by my organization and structured a one way fare price from airport to airport needing some help to make this information easy to access for others,hello have dataset of airfare rate that are negotiated each year by my organization and structured a one way fare price from airport to airport needing some help to make this information easy to access for others
the best tool to store and visualize data for small not for profit,hi all work for small not for profit based in india our volunteers teach students and we are searching for tools that will help us store and visualize our data what are some tools that we can try expectation tool to store student data tool to visualize student data criteria free cheap since the not for profit is run purely on donation easy to use for the tech since we are only volunteers with any tech background we will prefer tool that does not need much development time we cannot develop deploy our own tools on our own server unless it is easy but am fine with using say grafana easy for the end user the end users will be volunteer teacher so ideally we will prefer these teachers to easily be able to input data students exam scores via simple ui than writing sql insert queries,1,the best tool to store and visualize data for small not for profit,hi all work for small not for profit based in india our volunteer teach student and we are searching for tool that will help u store and visualize our data what are some tool that we can try expectation tool to store student data tool to visualize student data criterion free cheap since the not for profit is run purely on donation easy to use for the tech since we are only volunteer with any tech background we will prefer tool that doe not need much development time we cannot develop deploy our own tool on our own server unless it is easy but am fine with using say grafana easy for the end user the end user will be volunteer teacher so ideally we will prefer these teacher to easily be able to input data student exam score via simple ui than writing sql insert query
how hard is it to land job as sas programmer,biomedical health science graduate wondering if wasting my time learning sas is there good demand in this field with just bachelor degree,1,how hard is it to land job a sa programmer,biomedical health science graduate wondering if wasting my time learning sa is there good demand in this field with just bachelor degree
post graduate certificate in data science and machine learning from iit roorkee,came across this new course on coursera the course costs which is way higher than other data science courses on offer have done google data analytics and am currently doing ibm data science both of which cost about each please suggest if the course offered by iit roorkee is worth it will it add weight to my resume should also mention that am years old don have college degree and am currently working in bpo as trainer looking to switch to data science analytics and there lot of scope for its application in my current job ll be very thankful for your insights,1,post graduate certificate in data science and machine learning from iit roorkee,came across this new course on coursera the course cost which is way higher than other data science course on offer have done google data analytics and am currently doing ibm data science both of which cost about each please suggest if the course offered by iit roorkee is worth it will it add weight to my resume should also mention that am year old don have college degree and am currently working in bpo a trainer looking to switch to data science analytics and there lot of scope for it application in my current job ll be very thankful for your insight
will this idea be useful at all to pursue further,hello community happy new year to everyone prototyped an idea called storytelling whatever you see which is chrome extension that helps users to turn any data they see on the web into sharable story in couple of clicks minute prototype demo youtube ideally it should support any data like spreadsheet excel csv json in http url or even html table unstructured data could be as creative as word counting etc this prototype covers spreadsheet only my questions is this type of extension useful at all if yes in what scenario if not any possible changes could make it useful would you like to give it try reply with an email so that will send you an access link any other thoughts to make it useful thanks for your feedback in advance,1,will this idea be useful at all to pursue further,hello community happy new year to everyone prototyped an idea called storytelling whatever you see which is chrome extension that help user to turn any data they see on the web into sharable story in couple of click minute prototype demo youtube ideally it should support any data like spreadsheet excel csv json in http url or even html table unstructured data could be a creative a word counting etc this prototype cover spreadsheet only my question is this type of extension useful at all if yes in what scenario if not any possible change could make it useful would you like to give it try reply with an email so that will send you an access link any other thought to make it useful thanks for your feedback in advance
is this subreddit for data analysts or just data scientists,is this subreddit for data analysts or just data scientists,1,is this subreddit for data analyst or just data scientist,is this subreddit for data analyst or just data scientist
how was your graduation project,hello everyone curious to know what other people went through in their graduation project to me it was pretty good learning experience cause it made me self learn lot with almost none hand holding aside from documentation my grad project was basically an image classification project about detecting deformaties it was exciting to learn about data science data mining model building some ml techniques and dl models and their architecture but it was also very exhausting was basically holding the team together there were always problems with someone not doing his job correctly ended up talking with the doctors and professors about it huge fights embarrassment in front of the judging doctors for each phase it was so stressful and tiring but we got through it in the end,1,how wa your graduation project,hello everyone curious to know what other people went through in their graduation project to me it wa pretty good learning experience cause it made me self learn lot with almost none hand holding aside from documentation my grad project wa basically an image classification project about detecting deformaties it wa exciting to learn about data science data mining model building some ml technique and dl model and their architecture but it wa also very exhausting wa basically holding the team together there were always problem with someone not doing his job correctly ended up talking with the doctor and professor about it huge fight embarrassment in front of the judging doctor for each phase it wa so stressful and tiring but we got through it in the end
created blog for developers coming to data science,hello everyone used to be software engineer since and in switched to data science nowadays even with short time in ds became lead data scientist at my organization during the last days decided to start blog to share some knowledge ve got and tricks now ve written my second blog post hope it will be helpful to anybody and if you have any suggestions or comments they re always welcome so can improve my articles,1,created blog for developer coming to data science,hello everyone used to be software engineer since and in switched to data science nowadays even with short time in d became lead data scientist at my organization during the last day decided to start blog to share some knowledge ve got and trick now ve written my second blog post hope it will be helpful to anybody and if you have any suggestion or comment they re always welcome so can improve my article
how to do data pre processing in production,am software eng traditionally but recently became data scientist looking for ideas on how to do data pre processing in production the use case is as follows new data row becomes available every few minutes it needs to be preprocessed before it can be injested by downstream models creating an oop style data pre processor class does not sit well with me because it does not make sense that there is class structure here all of the methods end up being static or non static but dont end up using the class state so practically static so really what is the point of the class at the same time python is oop language making it difficult to structure things in class less manner wondering if anyone faced similar scenarios and what are some good practices here,1,how to do data pre processing in production,am software eng traditionally but recently became data scientist looking for idea on how to do data pre processing in production the use case is a follows new data row becomes available every few minute it need to be preprocessed before it can be injested by downstream model creating an oop style data pre processor class doe not sit well with me because it doe not make sense that there is class structure here all of the method end up being static or non static but dont end up using the class state so practically static so really what is the point of the class at the same time python is oop language making it difficult to structure thing in class le manner wondering if anyone faced similar scenario and what are some good practice here
how to get jnto data science when you come from academia,recently completed phd in theoretical physics am now working at an airline doing it software maintenance as could not land single job for months got rejected from jobs which were mostly in data science some recruiters even told me that many phd graduates apply for data science jobs but they can never transfer their skills successfully to the corporate world and so most companies are reluctant to hire people who dont have experience in the industry or give them any training have years experience in programming with python and matlab but unfortunately it has all been in areas like numerical computing particularly partial differential equations and stochastic modelling dont really know how to sell these skills as nobody seems to be interested in them will staying in my current job for longer count as relevant industry experience dont really do any data analytics in it but cant think of any way to transition into the career as there are no junior positions available where live australia perhaps doing another degree like graduate certificate diploma of data science might help hear colleges like general assembly tend to have good employability score for its students,1,how to get jnto data science when you come from academia,recently completed phd in theoretical physic am now working at an airline doing it software maintenance a could not land single job for month got rejected from job which were mostly in data science some recruiter even told me that many phd graduate apply for data science job but they can never transfer their skill successfully to the corporate world and so most company are reluctant to hire people who dont have experience in the industry or give them any training have year experience in programming with python and matlab but unfortunately it ha all been in area like numerical computing particularly partial differential equation and stochastic modelling dont really know how to sell these skill a nobody seems to be interested in them will staying in my current job for longer count a relevant industry experience dont really do any data analytics in it but cant think of any way to transition into the career a there are no junior position available where live australia perhaps doing another degree like graduate certificate diploma of data science might help hear college like general assembly tend to have good employability score for it student
python style guide pep write clean python code,this article has discussed few important guidelines that we should follow and be aware of when coding more information can be found in the article reference section please do support me by following on medium thank you coding python cleancode styleguide pep8 python3 pythonprogramming pythonfordatascience,1,python style guide pep write clean python code,this article ha discussed few important guideline that we should follow and be aware of when coding more information can be found in the article reference section please do support me by following on medium thank you coding python cleancode styleguide pep8 python3 pythonprogramming pythonfordatascience
chances to find job in data science field,am computational physics quantum materials phd student at one of the top universities of germany after phd am planning to go into data science field codes can effectively use python numpy pandas sklearn matplotlib etc fortran besides bash linux slurm hpc etc am very familiar with the artificial intelligence algorithms in terms of their theoretical background and implementation what are my chances,1,chance to find job in data science field,am computational physic quantum material phd student at one of the top university of germany after phd am planning to go into data science field code can effectively use python numpy panda sklearn matplotlib etc fortran besides bash linux slurm hpc etc am very familiar with the artificial intelligence algorithm in term of their theoretical background and implementation what are my chance
chances to find job in date science field,am computational physics quantum materials phd student at one of the top universities of germany after phd am planning to go into data science field codes can effectively use python numpy pandas sklearn matplotlib etc fortran besides bash linux slurm hpc etc am very familiar with the artificial intelligence algorithms in terms of their theoretical background and implementation what are my chances,1,chance to find job in date science field,am computational physic quantum material phd student at one of the top university of germany after phd am planning to go into data science field code can effectively use python numpy panda sklearn matplotlib etc fortran besides bash linux slurm hpc etc am very familiar with the artificial intelligence algorithm in term of their theoretical background and implementation what are my chance
should try to find new job given my circumstance,in gap year between undergrad and grad school at the moment this past semester was taking some classes part time and working part time but now have no other classes or obligations so my schedule is clear my current job isn able to offer me full time hours and the work has gotten rather stale unsure wether to try and apply for different full time role just for the next months until start grad school in august or just try to stick it out with my current employer and maybe ask them for more hours have around months experience in the field and fairly strong background but unsure how difficult it would be for me to find full time month contract doing interesting work somewhere ve posted my resume below for reference please let me know what you would do in my position and if theres any other information that help you in answering this question thanks resume,1,should try to find new job given my circumstance,in gap year between undergrad and grad school at the moment this past semester wa taking some class part time and working part time but now have no other class or obligation so my schedule is clear my current job isn able to offer me full time hour and the work ha gotten rather stale unsure wether to try and apply for different full time role just for the next month until start grad school in august or just try to stick it out with my current employer and maybe ask them for more hour have around month experience in the field and fairly strong background but unsure how difficult it would be for me to find full time month contract doing interesting work somewhere ve posted my resume below for reference please let me know what you would do in my position and if there any other information that help you in answering this question thanks resume
going for another career what should learn to get month,going for another career what should learn to get month,1,going for another career what should learn to get month,going for another career what should learn to get month
self worth tied to model performance,this is probably stupid question please forgive me because came from software engineering background and recently transitioned to data analyst tried to build predictive model but realized that many models that created suck and feel that my performance is at the mercy of the data somestimes am very anxious as what to do if can find any correlation between the data at least in software engineering as long as you write good quality code and the requirement is correct you can build good product but with machine learning feel that at the mercy of the data am crazy for thinking this way thank you so much for your thoughts,1,self worth tied to model performance,this is probably stupid question please forgive me because came from software engineering background and recently transitioned to data analyst tried to build predictive model but realized that many model that created suck and feel that my performance is at the mercy of the data somestimes am very anxious a what to do if can find any correlation between the data at least in software engineering a long a you write good quality code and the requirement is correct you can build good product but with machine learning feel that at the mercy of the data am crazy for thinking this way thank you so much for your thought
classifying the political compass what method,have school nlp project that is pretty open ended am fairly new to ml so don have very good grasp of what classification method would be right for my scenario grabbed bunch of flairs off politicalcompassmemes and some comments by the users want to classify both dimensions right left authoritarian libertarian my first thought was to just do separate binary classifications am content with this idea except don want to lose the center was thinking maybe could do binary classification but treat the middle as center although not sure if this sort of thing would be configurable with the libraries end up using to make predictions alternatively could do multilabel classification for each right center left authoritarian center libertarian but not sure how much this increases complexity and not sure if up to that task,1,classifying the political compass what method,have school nlp project that is pretty open ended am fairly new to ml so don have very good grasp of what classification method would be right for my scenario grabbed bunch of flair off politicalcompassmemes and some comment by the user want to classify both dimension right left authoritarian libertarian my first thought wa to just do separate binary classification am content with this idea except don want to lose the center wa thinking maybe could do binary classification but treat the middle a center although not sure if this sort of thing would be configurable with the library end up using to make prediction alternatively could do multilabel classification for each right center left authoritarian center libertarian but not sure how much this increase complexity and not sure if up to that task
loved how simply this explains the process of analysing and understanding data,loved how simply this explains the process of analysing and understanding data,1,loved how simply this explains the process of analysing and understanding data,loved how simply this explains the process of analysing and understanding data
fraud,hi everyone am working on project of fraud detection and want to know if anyone can recommend good books or papers for this kind of task thank you,1,fraud,hi everyone am working on project of fraud detection and want to know if anyone can recommend good book or paper for this kind of task thank you
skull ct scan dataset,hi looking for dataset with the following characteristics skull ct scans as many as possible known age and demographics if possible of each corresponding patient relatively large amount of data can anyone share any public datasets that match the criteria above or recommend where to find this type of dataset thanks,1,skull ct scan dataset,hi looking for dataset with the following characteristic skull ct scan a many a possible known age and demographic if possible of each corresponding patient relatively large amount of data can anyone share any public datasets that match the criterion above or recommend where to find this type of dataset thanks
any new machine learning library in,would love to know if you guys saw something that looks promising or developing something especially in feature engineering,1,any new machine learning library in,would love to know if you guy saw something that look promising or developing something especially in feature engineering
want to join data scientist at meta,hi this is my first reddit post so pls spare me if any rules or guidelines are been broken well this october got rejected at meta for summer intern as data scientist and feel stupid as missed great opportunity few of these questions am looking for an answer once rejected can apply again for summer intern when is the idle time to apply for full time am graduating in december please if anyone can point me to resources that are required to clear meta interview would be grateful if there are reddit channel for data scientist jobs can you tag them here,1,want to join data scientist at meta,hi this is my first reddit post so pls spare me if any rule or guideline are been broken well this october got rejected at meta for summer intern a data scientist and feel stupid a missed great opportunity few of these question am looking for an answer once rejected can apply again for summer intern when is the idle time to apply for full time am graduating in december please if anyone can point me to resource that are required to clear meta interview would be grateful if there are reddit channel for data scientist job can you tag them here
beautiful and informative notebooks of,beautiful and informative notebooks of,1,beautiful and informative notebook of,beautiful and informative notebook of
dija grocery data scraping dija grocery data extraction,at foodspark we scrape dija grocery data with our online instacart grocery app scraping api and convert data into suitable informational statistics and patterns contact usa id info foodspark io mailto info foodspark io,1,dija grocery data scraping dija grocery data extraction,at foodspark we scrape dija grocery data with our online instacart grocery app scraping api and convert data into suitable informational statistic and pattern contact usa id info foodspark io mailto info foodspark io
math teacher looking to become data analyst gt data scientist looking for advice on my resume and on the projects ve done that demonstrate my ability to work in those fields,hi everyone have strong passion for math enjoy problem solving and find satisfaction in being able to help others which is what got me into teaching in the first place over the past two years ve become somewhat disillusioned with teaching searching for other fields to transition into that would allow me to continue using math and my analytical skills and data science sparked an interest in me without any previous experience know this will be tough transition to make however hoping that my strong background in math combined with my ability to explain things teach to those who may not be math people will give me leg up do not expect to land job as data scientist right away instead looking more towards beginning as data analyst gaining some experience with the goal to ultimately become data scientist through mutual friends ve interviewed few data analysts scientists who recommended that build portfolio of work to show potential employers what capable of which are linked below these people also recommended focus primarily on building skills with sql dash boarding tools like tableau and python and ve spent the last two years building proficiency in these areas the projects below focus heavily on python some tableau and little sql ve had hard time figuring out what kind of project can create to showcase sql skills through github but do have two repos on there that contain solutions to many sql puzzles in case need to demonstrate something if you could please take look over my resume and or my projects love some feedback feel as if ready to start applying for analyst jobs however with no prior experience hesitant and looking for guidance before begin also if you have any recommendations on how can showcase my sql skills all ears projects reddit accounting survey analysis analysis performed analysis of data from survey on accounting broken into three jupyter notebooks cleaned data using pandas numpy visualized data in an exploratory data analysis eda using matplotlib built and analyzed regression models using scipy sklearn crafted story in tableau which explores differences in annual salary between male and female accountants us accounting tableau viz etl craigslist web scraper tutoring scraper built web scraper to extract data on tutoring prices from craigslist transform the data according to my specifications and load the data into local postgresql database analysis of scholarly article scholarly analysis completed hypothesis testing confidence interval analysis and linear regression modeling of acupuncture treatments and their effect on hypertension using python,1,math teacher looking to become data analyst gt data scientist looking for advice on my resume and on the project ve done that demonstrate my ability to work in those field,hi everyone have strong passion for math enjoy problem solving and find satisfaction in being able to help others which is what got me into teaching in the first place over the past two year ve become somewhat disillusioned with teaching searching for other field to transition into that would allow me to continue using math and my analytical skill and data science sparked an interest in me without any previous experience know this will be tough transition to make however hoping that my strong background in math combined with my ability to explain thing teach to those who may not be math people will give me leg up do not expect to land job a data scientist right away instead looking more towards beginning a data analyst gaining some experience with the goal to ultimately become data scientist through mutual friend ve interviewed few data analyst scientist who recommended that build portfolio of work to show potential employer what capable of which are linked below these people also recommended focus primarily on building skill with sql dash boarding tool like tableau and python and ve spent the last two year building proficiency in these area the project below focus heavily on python some tableau and little sql ve had hard time figuring out what kind of project can create to showcase sql skill through github but do have two repos on there that contain solution to many sql puzzle in case need to demonstrate something if you could please take look over my resume and or my project love some feedback feel a if ready to start applying for analyst job however with no prior experience hesitant and looking for guidance before begin also if you have any recommendation on how can showcase my sql skill all ear project reddit accounting survey analysis analysis performed analysis of data from survey on accounting broken into three jupyter notebook cleaned data using panda numpy visualized data in an exploratory data analysis eda using matplotlib built and analyzed regression model using scipy sklearn crafted story in tableau which explores difference in annual salary between male and female accountant u accounting tableau viz etl craigslist web scraper tutoring scraper built web scraper to extract data on tutoring price from craigslist transform the data according to my specification and load the data into local postgresql database analysis of scholarly article scholarly analysis completed hypothesis testing confidence interval analysis and linear regression modeling of acupuncture treatment and their effect on hypertension using python
best intro to python course,looking for the best coursera edx type course to get me up to speed on python ve been doing ds for years sql and sas were my bread and butter the world has changed and need to keep up ve coded in vba vb sql sas etc recently switched from sas to looking for course that covers the basics of python and can take me to intermediate level not worried about the ds packages just basic python thanks in advance for the recommendations,1,best intro to python course,looking for the best coursera edx type course to get me up to speed on python ve been doing d for year sql and sa were my bread and butter the world ha changed and need to keep up ve coded in vba vb sql sa etc recently switched from sa to looking for course that cover the basic of python and can take me to intermediate level not worried about the d package just basic python thanks in advance for the recommendation
valuable suggestions are welcomed,come from non tech background and currently pursing masters in data science so my questions are these following other than python and mysql which language should learn how difficult it is for fresher to crack offcampus placements what skills need to develop while pursing my masters what else should do so that have better chances of cracking good companies thanks,1,valuable suggestion are welcomed,come from non tech background and currently pursing master in data science so my question are these following other than python and mysql which language should learn how difficult it is for fresher to crack offcampus placement what skill need to develop while pursing my master what else should do so that have better chance of cracking good company thanks
create data lake,to the cloud best strategy hi all general question about migrating different databases from our legacy erps to the cloud the nirvana state want to be able to connect my legacy data to new apps and or leverage low code solutions like ms power apps could even have it so that when we look at new saas we hold our api key targets and data dictionary we issue them their connection credentials and then it on them to write the api to integrate initial thoughts what we think want is data lake that we could create different data stores operational financial with the sole purpose of integrating best fit or best in class solutions per major work flow workflow use saas financial uses sage or ms dynamics etc those are the big wins the little wins are anything we are doing in excel that needs workbooks with all those macros running replaced immediately with power apps we spin up legacy framework one db is in sql other in older framework using delphi though not entirely familiar with how it is really structured dirty data between the dbs we have tons of duplicate entries years of folks changing the name just enough to be able to enter in the customer info can what we want to do even be done the way we want it how do we consolidate the duped data what approaches have you seen work or not work,1,create data lake,to the cloud best strategy hi all general question about migrating different database from our legacy erps to the cloud the nirvana state want to be able to connect my legacy data to new apps and or leverage low code solution like m power apps could even have it so that when we look at new saas we hold our api key target and data dictionary we issue them their connection credential and then it on them to write the api to integrate initial thought what we think want is data lake that we could create different data store operational financial with the sole purpose of integrating best fit or best in class solution per major work flow workflow use saas financial us sage or m dynamic etc those are the big win the little win are anything we are doing in excel that need workbook with all those macro running replaced immediately with power apps we spin up legacy framework one db is in sql other in older framework using delphi though not entirely familiar with how it is really structured dirty data between the db we have ton of duplicate entry year of folk changing the name just enough to be able to enter in the customer info can what we want to do even be done the way we want it how do we consolidate the duped data what approach have you seen work or not work
nonlinear programming in,hey everyone am interested in using to do portfolio variance minimization problem using nonlinear programming but only seem to see packages for linear programming problems does anyone know of package for nonlinear programs thanks if there is package in python for this please let me know as well,1,nonlinear programming in,hey everyone am interested in using to do portfolio variance minimization problem using nonlinear programming but only seem to see package for linear programming problem doe anyone know of package for nonlinear program thanks if there is package in python for this please let me know a well
finishing my two year analytics program what the next step,it time to start figuring out what to specialize in and perhaps seek out an additional certification or two looking for advice from professionals they are actively working in the field have beginner intermediate knowledge of the following sas sql tableau python excel and of course stats econ business logistic longest regressions etc enjoyed tableau and sql the most have concerns over burnout because as much as ve thoroughly enjoyed learning all of these tools and techniques not sure how ll adjust to working full forty each week in front of the laptop pretty social my primary background is working as an entertainer and side gigging as bartender because of the last concern strongly considering pursuing technical sales position selling some sort of analytics solution to businesses currently working as supervisor on rent assistance program in state government and our funding will be running out in approximately six to nine months so it time to start looking for other opportunities thanks in advance any advice is greatly appreciated,1,finishing my two year analytics program what the next step,it time to start figuring out what to specialize in and perhaps seek out an additional certification or two looking for advice from professional they are actively working in the field have beginner intermediate knowledge of the following sa sql tableau python excel and of course stats econ business logistic longest regression etc enjoyed tableau and sql the most have concern over burnout because a much a ve thoroughly enjoyed learning all of these tool and technique not sure how ll adjust to working full forty each week in front of the laptop pretty social my primary background is working a an entertainer and side gigging a bartender because of the last concern strongly considering pursuing technical sale position selling some sort of analytics solution to business currently working a supervisor on rent assistance program in state government and our funding will be running out in approximately six to nine month so it time to start looking for other opportunity thanks in advance any advice is greatly appreciated
siamese neural networks for semantic text similarity,repository containing comprehensive neural networks based pytorch implementations for the semantic text similarity task including architectures such as siamese lstm siamese lstm attention siamese transformer and siamese bert,1,siamese neural network for semantic text similarity,repository containing comprehensive neural network based pytorch implementation for the semantic text similarity task including architecture such a siamese lstm siamese lstm attention siamese transformer and siamese bert
including joint research project on my portfolio,during my last year of undergraduate study worked in group where we analysed the colour distribution of amp ms do need to get my group members permission to publish our work more importantly put on my github portfolio thank you,1,including joint research project on my portfolio,during my last year of undergraduate study worked in group where we analysed the colour distribution of amp m do need to get my group member permission to publish our work more importantly put on my github portfolio thank you
european unemployment rates since the,european unemployment rates since the,1,european unemployment rate since the,european unemployment rate since the
did screw up with this line of code to calculate frequency of grams,did screw up with this line of code to calculate frequency of grams,1,did screw up with this line of code to calculate frequency of gram,did screw up with this line of code to calculate frequency of gram
to the companies that send candidates hour take home test and then say their corporate policy does not permit feedback after one is rejected,your hiring process is terrible and you absolutely have terrible policy job hunting is already crappy long and unrewarding activity and at the very least feedback would be helpful to help candidates improve their chances in their job hunt for the next role they apply to it not only the hour test that stressful but even before doing the test we have to review and refresh our knowledge because we ve all been pigeonholed one way or another at our respective firms and we re trying to re learn so many things that you claim is normal day to day operation at your firm for data scientists and quite frankly call that bs that your day to day ops includes advanced statistics or measuring bayesian probability by hand just like how my firm claims the role for our job requires coding in python and statistics only to realize that daily tasks are to run reports from google analytics adobe analytics like come on rant,1,to the company that send candidate hour take home test and then say their corporate policy doe not permit feedback after one is rejected,your hiring process is terrible and you absolutely have terrible policy job hunting is already crappy long and unrewarding activity and at the very least feedback would be helpful to help candidate improve their chance in their job hunt for the next role they apply to it not only the hour test that stressful but even before doing the test we have to review and refresh our knowledge because we ve all been pigeonholed one way or another at our respective firm and we re trying to re learn so many thing that you claim is normal day to day operation at your firm for data scientist and quite frankly call that b that your day to day ops includes advanced statistic or measuring bayesian probability by hand just like how my firm claim the role for our job requires coding in python and statistic only to realize that daily task are to run report from google analytics adobe analytics like come on rant
graceful exit,member of very small team at my current organization there are loads of things like about my job but there is no room there for me to take the next step in my career looking for change and have landed few interviews don want to leave my team in the lurch and burn bridges if another opportunity presents itself have any of you been in this situation how did it end what would you do or not do again if you found yourself in similar situation,1,graceful exit,member of very small team at my current organization there are load of thing like about my job but there is no room there for me to take the next step in my career looking for change and have landed few interview don want to leave my team in the lurch and burn bridge if another opportunity present itself have any of you been in this situation how did it end what would you do or not do again if you found yourself in similar situation
do portfolios matter if you have gt years of experience,hey everyone didn see this topic in the search results data scientist with little over years of experience and an ms in stats looking at changing jobs over the next few months once have my permanent residence sorted out my question is should create portfolio of projects for when start applying for new jobs have things can point to in my cv where saved xx xxx in multiple different instances and worked on the back end for an app on the side at one of the universities projects also have publications where was data analyst at the pscyh department at the same university just nervous that none of the stuff actually sounds that cool and didn end up having to use nns or anything fancy most of the techniques involved are fairly straightforward logistic regression or tools etc,1,do portfolio matter if you have gt year of experience,hey everyone didn see this topic in the search result data scientist with little over year of experience and an m in stats looking at changing job over the next few month once have my permanent residence sorted out my question is should create portfolio of project for when start applying for new job have thing can point to in my cv where saved xx xxx in multiple different instance and worked on the back end for an app on the side at one of the university project also have publication where wa data analyst at the pscyh department at the same university just nervous that none of the stuff actually sound that cool and didn end up having to use nns or anything fancy most of the technique involved are fairly straightforward logistic regression or tool etc
what the best course for learning python and data science,hi everyone was wondering if could get any recommendations or suggestions on the best online course can take to learn python and data science ve been data analyst for years now dabbling into little bit of machine learning on past projects but certainly not the bulk of my work worked with sas for years and the past year ve been using sql although during my sas time used sql through sas want to learn python focused mostly on data analytics science so looking for course to take know there are plenty of free sources out there but need some structure to stay focused when starting out took two intro python courses about year ago and have used it sparsely here and there so not complete beginner but fairly new to it ve looked at these two so far days of code the complete python pro bootcamp for utm_medium udemyads amp utm_campaign python_v prof_la en_cc us_ti amp utm_content deal4584 amp utm_term ag_78513466559_ ad_532070164200_ kw__ de_c_ dm__ pl__ ti_dsa li_9007268_ pd__ amp matchtype amp gclid cjwkcaiaikuobhbqeiwaid_skxtoocgrb7kkvwwoes9j9uhuypxbay7keirt u2oh9xttvynxaiozhocyveqavd_bwe looks good for learning python but it looks like there lot of web development content python for data science and machine learning bootcamp utm_medium udemyads amp utm_campaign python_v prof_la en_cc us_ti amp utm_content deal4584 amp utm_term ag_78513466559_ ad_532070164200_ kw__ de_c_ dm__ pl__ ti_dsa li_9007268_ pd__ amp matchtype amp gclid cjwkcaiazrwobhbjeiwaq85qz6wkcum200feikncaczmqqu0qybugfybqsxfotamiftv_3z63hwm6roce_mqavd_bwe looks like it covers lot of the data science aspect but maybe not as good for someone with only little bit of python experience,1,what the best course for learning python and data science,hi everyone wa wondering if could get any recommendation or suggestion on the best online course can take to learn python and data science ve been data analyst for year now dabbling into little bit of machine learning on past project but certainly not the bulk of my work worked with sa for year and the past year ve been using sql although during my sa time used sql through sa want to learn python focused mostly on data analytics science so looking for course to take know there are plenty of free source out there but need some structure to stay focused when starting out took two intro python course about year ago and have used it sparsely here and there so not complete beginner but fairly new to it ve looked at these two so far day of code the complete python pro bootcamp for utm_medium udemyads amp utm_campaign python_v prof_la en_cc us_ti amp utm_content deal4584 amp utm_term ag_78513466559_ ad_532070164200_ kw__ de_c_ dm__ pl__ ti_dsa li_9007268_ pd__ amp matchtype amp gclid cjwkcaiaikuobhbqeiwaid_skxtoocgrb7kkvwwoes9j9uhuypxbay7keirt u2oh9xttvynxaiozhocyveqavd_bwe look good for learning python but it look like there lot of web development content python for data science and machine learning bootcamp utm_medium udemyads amp utm_campaign python_v prof_la en_cc us_ti amp utm_content deal4584 amp utm_term ag_78513466559_ ad_532070164200_ kw__ de_c_ dm__ pl__ ti_dsa li_9007268_ pd__ amp matchtype amp gclid cjwkcaiazrwobhbjeiwaq85qz6wkcum200feikncaczmqqu0qybugfybqsxfotamiftv_3z63hwm6roce_mqavd_bwe look like it cover lot of the data science aspect but maybe not a good for someone with only little bit of python experience
interview prep,hi data science community have an entry level interview coming up and hoping few of all could maybe point me in the right direction for few things and or confirm on the right track for others does anyone know of any simple sql tutorials that could be done with enough time in about week really just looking to get my feet wet and say yeah practiced with this on my own time and have better understanding of its utility and applicability for big data not really looking for bootcamp just fun little tutorial if possible all may have stumbled across what kind of data management tools exist within for python to me the jupyter environment is its own data management system but know that not the answer expected of me what the difference between gridded spatial data and raster friend said they were similar but different but their explanation just read like raster to me and having trouble differentiating them thanks all any guidance help would be greatly appreciated,1,interview prep,hi data science community have an entry level interview coming up and hoping few of all could maybe point me in the right direction for few thing and or confirm on the right track for others doe anyone know of any simple sql tutorial that could be done with enough time in about week really just looking to get my foot wet and say yeah practiced with this on my own time and have better understanding of it utility and applicability for big data not really looking for bootcamp just fun little tutorial if possible all may have stumbled across what kind of data management tool exist within for python to me the jupyter environment is it own data management system but know that not the answer expected of me what the difference between gridded spatial data and raster friend said they were similar but different but their explanation just read like raster to me and having trouble differentiating them thanks all any guidance help would be greatly appreciated
census api what dataset do need for this particular request,hi all out of members figure someone here must understand the census api looking at the documentation and it clear as mud need to get the income and number of children for households in philadelphia pa by zip code or block doing in python and know how to use the requests library and parse json and also got the censusdata library just don understand this particular api because there are many datasets and don know which one need should be looking at the acs year or the pep see example api calls in the documentation which get but doesn tell me how to construct the ones need,1,census api what dataset do need for this particular request,hi all out of member figure someone here must understand the census api looking at the documentation and it clear a mud need to get the income and number of child for household in philadelphia pa by zip code or block doing in python and know how to use the request library and parse json and also got the censusdata library just don understand this particular api because there are many datasets and don know which one need should be looking at the ac year or the pep see example api call in the documentation which get but doesn tell me how to construct the one need
how to deal with feature selection,im quite new to ml and have had some trouble understanding the best way to select features for regression problem understand we don want multicollinearity between features but is it just preference which method we use to choose features pearson or algorithms like lasso or random forest does it matter if we do don use the same algorithm for our model as for our feature selection and finally this is probably highly subjective and depends on the individual problem but is there certain threshold for multicollinearity not too certain understand why it best to avoid multicollinearity so if anyone has some intuition to help with that appreciate it thanks for any help,1,how to deal with feature selection,im quite new to ml and have had some trouble understanding the best way to select feature for regression problem understand we don want multicollinearity between feature but is it just preference which method we use to choose feature pearson or algorithm like lasso or random forest doe it matter if we do don use the same algorithm for our model a for our feature selection and finally this is probably highly subjective and depends on the individual problem but is there certain threshold for multicollinearity not too certain understand why it best to avoid multicollinearity so if anyone ha some intuition to help with that appreciate it thanks for any help
can get by without neural nets,after leveraging my connections and my current skillset found great transition job from engineering to data science there will still be some engineering involved but the primary job is implementing machine learning models to help predict failure or equipment from sensor data ve learned lot in my free time but haven gotten around to neural nets yet to my understanding they are just slower more accurate method to be used instead of other ml models are neural nets something should absolutely learn before starting or can use other methods for now as learn on the job ve already been told will have set amount of time per day to spend learning additionally expect the first few weeks to be lots of onboarding information won have much time between now and when start so rather relax with my family but wanted to hear from others first because will grind if it is something important to know,1,can get by without neural net,after leveraging my connection and my current skillset found great transition job from engineering to data science there will still be some engineering involved but the primary job is implementing machine learning model to help predict failure or equipment from sensor data ve learned lot in my free time but haven gotten around to neural net yet to my understanding they are just slower more accurate method to be used instead of other ml model are neural net something should absolutely learn before starting or can use other method for now a learn on the job ve already been told will have set amount of time per day to spend learning additionally expect the first few week to be lot of onboarding information won have much time between now and when start so rather relax with my family but wanted to hear from others first because will grind if it is something important to know
how can become an elite unicorn data scientist,hello am highly intelligent disciplined motivated individual this upcoming year will be starting my second semester as junior in college statistics public state university my hope is to get fellowship at my current uni for my masters while working remote part time job as statistician found research mentor that runs our data analytics center at our university so will have access to some small scale academic industry projects throughout the next seven semesters have completed coding bootcamp so my coding experience is there but not advanced also have gone through some courses on data analytics at my uni which has give me some basic understanding skills in regards to modeling regression and analytics currently spend hours day on campus on weekdays and about hours on weekends am thinking that if can spend this time efficiently effectively in pursuit of skills and working on projects then can pursue the goal of becoming an elite or unicorn data scientist have some ideas about where to start but feel like could get some better ones from more experienced people my biggest concern is lack of access to projects that actually produce value and give me the opportunity to develop showcase skills can always crank out kaggle based portfolio and run bunch of different models on data sets find online but this does not seem like it would give me any genuinely advanced experiences what skills niches or pursuits can focus on that will open up the door for me to develop into an elite data scientist,1,how can become an elite unicorn data scientist,hello am highly intelligent disciplined motivated individual this upcoming year will be starting my second semester a junior in college statistic public state university my hope is to get fellowship at my current uni for my master while working remote part time job a statistician found research mentor that run our data analytics center at our university so will have access to some small scale academic industry project throughout the next seven semester have completed coding bootcamp so my coding experience is there but not advanced also have gone through some course on data analytics at my uni which ha give me some basic understanding skill in regard to modeling regression and analytics currently spend hour day on campus on weekday and about hour on weekend am thinking that if can spend this time efficiently effectively in pursuit of skill and working on project then can pursue the goal of becoming an elite or unicorn data scientist have some idea about where to start but feel like could get some better one from more experienced people my biggest concern is lack of access to project that actually produce value and give me the opportunity to develop showcase skill can always crank out kaggle based portfolio and run bunch of different model on data set find online but this doe not seem like it would give me any genuinely advanced experience what skill niche or pursuit can focus on that will open up the door for me to develop into an elite data scientist
podcasts youtube videos to listen to on commute,chemical engineer gunning for software data roles at my current company my dream is to work for faang my commute is around hours per day what can listen to during the commute to prep me for faang interviews and swe ds roles driving so eyes need to be on the road,1,podcasts youtube video to listen to on commute,chemical engineer gunning for software data role at my current company my dream is to work for faang my commute is around hour per day what can listen to during the commute to prep me for faang interview and swe d role driving so eye need to be on the road
best podcasts or youtube channels to study during commute,chemical engineer gunning for software data roles at my current company my dream is to work for faang my commute is around hours per day what can listen to during the commute to prep me for faang interviews and swe ds roles,1,best podcasts or youtube channel to study during commute,chemical engineer gunning for software data role at my current company my dream is to work for faang my commute is around hour per day what can listen to during the commute to prep me for faang interview and swe d role
tech focused data scientist career path,hi can someone please help me telling me about their experience data engineer architect ml etc which technical skill do you think is most important what do you like about your data career what do you do on daily basis,1,tech focused data scientist career path,hi can someone please help me telling me about their experience data engineer architect ml etc which technical skill do you think is most important what do you like about your data career what do you do on daily basis
where can find dataset from fast food menu prices,where can find dataset from fast food menu prices,1,where can find dataset from fast food menu price,where can find dataset from fast food menu price
fast food menu coffee prices datasets,fast food menu coffee prices datasets,1,fast food menu coffee price datasets,fast food menu coffee price datasets
how to add publication in the data science resume,what is the right way to add relevant ml publications in one page data science resume have few projects on ds and ml and publications in reputed journals amp conferences in nlp ml deep learning should add those publications under separate section publications and just reference them or merge it with the project section projects publications and write brief summary of those published works,1,how to add publication in the data science resume,what is the right way to add relevant ml publication in one page data science resume have few project on d and ml and publication in reputed journal amp conference in nlp ml deep learning should add those publication under separate section publication and just reference them or merge it with the project section project publication and write brief summary of those published work
data collection for ml project to predict perfect career fit,data collection for ml project to predict perfect career fit,1,data collection for ml project to predict perfect career fit,data collection for ml project to predict perfect career fit
how to get thousands of survey responses to train an ml model,how to get thousands of survey responses to train an ml model,1,how to get thousand of survey response to train an ml model,how to get thousand of survey response to train an ml model
intern working on forecasting project don know how to proceed,hi everyone so an intern on small company people and working here for little more than months started working on time series forecasting project and there is people formally working on that project including me but no one have expertise in time series forecasting our task is to improve the forecasting system of our client who uses classical models and we are trying to use ml models to include various features since entered became responsible for lot of tasks in the project including presenting results to the client even though just an intern and the current implementation of our solution was pretty much done by me just worked on the original proposed solution but gave some suggestions as we went think that should feel grateful for their trust but feel the lack of guidance and to be honest just afraid of not producing good solution since it is my first project the problem is that we are not getting very good results and as studied noticed that it is indeed pretty difficult to do better than classical models but the project is already late and we need to deliver something what think we should have done since the beginning was baseline model using classical solutions so our client and ourselves could have good idea of what to expect asked about the quality of the project at various moments and no one gave satisfactory answer what plan we could follow kept doing my tasks and since they didn said anything presumed did everything right next week we need to present the performance of our system and don know what could do to improve it don think it possible right know without changing our model and for what ve read more intelligent solution would just to stick to classical models afraid the client won be satisfied and feeling responsible if the project fail another point is that thinking about just changing careers during this internship perceived that really don like to do analysis and presenting insights to clients really disliked this part of the job my experience in previous projects was programming deep learning architectures not analysis so decided to jump to engineering think ll receive an offer in the next days to full time position as software machine learning engineer and now feeling guilty about abandoning the internship in the middle of project that isn going well and was responsible for implementing big part of it but well just the intern how should deal with this situation,1,intern working on forecasting project don know how to proceed,hi everyone so an intern on small company people and working here for little more than month started working on time series forecasting project and there is people formally working on that project including me but no one have expertise in time series forecasting our task is to improve the forecasting system of our client who us classical model and we are trying to use ml model to include various feature since entered became responsible for lot of task in the project including presenting result to the client even though just an intern and the current implementation of our solution wa pretty much done by me just worked on the original proposed solution but gave some suggestion a we went think that should feel grateful for their trust but feel the lack of guidance and to be honest just afraid of not producing good solution since it is my first project the problem is that we are not getting very good result and a studied noticed that it is indeed pretty difficult to do better than classical model but the project is already late and we need to deliver something what think we should have done since the beginning wa baseline model using classical solution so our client and ourselves could have good idea of what to expect asked about the quality of the project at various moment and no one gave satisfactory answer what plan we could follow kept doing my task and since they didn said anything presumed did everything right next week we need to present the performance of our system and don know what could do to improve it don think it possible right know without changing our model and for what ve read more intelligent solution would just to stick to classical model afraid the client won be satisfied and feeling responsible if the project fail another point is that thinking about just changing career during this internship perceived that really don like to do analysis and presenting insight to client really disliked this part of the job my experience in previous project wa programming deep learning architecture not analysis so decided to jump to engineering think ll receive an offer in the next day to full time position a software machine learning engineer and now feeling guilty about abandoning the internship in the middle of project that isn going well and wa responsible for implementing big part of it but well just the intern how should deal with this situation
how to start learning data scientist from home,study economy on university and want start learn some skills useful in ds how should start what do you recommend where to start,1,how to start learning data scientist from home,study economy on university and want start learn some skill useful in d how should start what do you recommend where to start
intern working on forecasting problem don know how to proceed,hi everyone so an intern on small company people and working here for little more than months started working on time series forecasting project and there is people formally working on that project including me but no one have expertise in time series forecasting our task is to improve the forecasting system of our client who uses classical models and we are trying to use ml models to include various features since entered became responsible for lot of tasks in the project including presenting results to the client even though just an intern and the current implementation of our solution was pretty much done by me just worked on the original proposed solution but gave some suggestions as we went think that should feel grateful for their trust but feel the lack of guidance and to be honest just afraid of not producing good solution since it is my first project the problem is that we are not getting very good results and as studied noticed that it is indeed pretty difficult to do better than classical models but the project is already late and we need to deliver something what think we should have done since the beginning was baseline model using classical solutions so our client and ourselves could have good idea of what to expect asked about the quality of the project at various moments and no one gave satisfactory answer what plan we could follow kept doing my tasks and since they didn said anything presumed did everything right next week we need to present the performance of our system and don know what could do to improve it don think it possible right know without changing our model and for what ve read more intelligent solution would just to stick to classical models afraid the client won be satisfied and feeling responsible if the project fail another point is that thinking about just changing careers during this internship perceived that really don like to do analysis and presenting insights to clients really disliked this part of the job my experience in previous projects was programming deep learning architectures not analysis so decided to jump to engineering think ll receive an offer in the next days to full time position as software machine learning engineer and now feeling guilty about abandoning the internship in the middle of project that isn going well and was responsible for implementing big part of it but well just the intern how should deal with this situation,1,intern working on forecasting problem don know how to proceed,hi everyone so an intern on small company people and working here for little more than month started working on time series forecasting project and there is people formally working on that project including me but no one have expertise in time series forecasting our task is to improve the forecasting system of our client who us classical model and we are trying to use ml model to include various feature since entered became responsible for lot of task in the project including presenting result to the client even though just an intern and the current implementation of our solution wa pretty much done by me just worked on the original proposed solution but gave some suggestion a we went think that should feel grateful for their trust but feel the lack of guidance and to be honest just afraid of not producing good solution since it is my first project the problem is that we are not getting very good result and a studied noticed that it is indeed pretty difficult to do better than classical model but the project is already late and we need to deliver something what think we should have done since the beginning wa baseline model using classical solution so our client and ourselves could have good idea of what to expect asked about the quality of the project at various moment and no one gave satisfactory answer what plan we could follow kept doing my task and since they didn said anything presumed did everything right next week we need to present the performance of our system and don know what could do to improve it don think it possible right know without changing our model and for what ve read more intelligent solution would just to stick to classical model afraid the client won be satisfied and feeling responsible if the project fail another point is that thinking about just changing career during this internship perceived that really don like to do analysis and presenting insight to client really disliked this part of the job my experience in previous project wa programming deep learning architecture not analysis so decided to jump to engineering think ll receive an offer in the next day to full time position a software machine learning engineer and now feeling guilty about abandoning the internship in the middle of project that isn going well and wa responsible for implementing big part of it but well just the intern how should deal with this situation
finally got the offer before the end of year,throw away account my background is msc from ee years as ds for an energy analytics start up moved to new industry and being terminated during probation being lost and there is no proper onboarding after months of searching got the offer as analyst for another industry here the highlights of my application fintech company being rejected after round doing case study fraud prediction well the last interview is team selection and suddenly pair programming exercise is assigned out of the blue not did well and being rejected retail furniture company completed case study presentation operation research but solution is not scalable also being rejected after round due to unclear not straightforward communication generally really like their feedback and hiring an insurance company case study is super hard faulty machine probability modeling and gps motion modelling of course failed telecommunication company being coldly questioned with probability python programming ml not being told in advance at the first round of course not do well and failed an aerospace company failed at brain teaser probability question of course not impressive enough to move to next round personal care company thought did well in rounds and feeling even the manager really enjoy the interviews and my cv it turned out being rejected due to poor communication travel company case study is interesting and diversified completed them but don have good domain knowledge to explain some results not impressive enough for other rounds multimedia company case study is conceptual question domain is new to me programming question is easy can answer every question not good enough diy business company did very well in every round total completed live coding with only half of the time solved business problems well even the engineering director told my interview make his day again being rejected after the last round for no clear reason car trading company ve been asked about arvix papers read algorithms ve used told random forest maybe they wanna discuss about complex stuff like bert transformers maybe my knowledge is not deep enough to impress the interviewers shopping startup solve business well failed at poor reasoning during machine learning questions reason of rejection is due to communication search business company do not have domain knowledge only partially solve their business questions rejected others some rejected me at prescreening some make my withdrawal due not not meeting my expectation after the first interview some told passed the first round and then never heard them back really learn lot from other business after doing their case studies like beside technical metrics mse f1 there re also business metrics revenue profit where people wanna discuss also understand more people skills and politics through interviews as those skills can be searched in stackoverflow another importance is to make them like me during the talks what experience is that ds is really wide range of skills every case study is different skillset in which domain knowledge is super important also some companies don know how to hire and how to onboard,1,finally got the offer before the end of year,throw away account my background is msc from ee year a d for an energy analytics start up moved to new industry and being terminated during probation being lost and there is no proper onboarding after month of searching got the offer a analyst for another industry here the highlight of my application fintech company being rejected after round doing case study fraud prediction well the last interview is team selection and suddenly pair programming exercise is assigned out of the blue not did well and being rejected retail furniture company completed case study presentation operation research but solution is not scalable also being rejected after round due to unclear not straightforward communication generally really like their feedback and hiring an insurance company case study is super hard faulty machine probability modeling and gps motion modelling of course failed telecommunication company being coldly questioned with probability python programming ml not being told in advance at the first round of course not do well and failed an aerospace company failed at brain teaser probability question of course not impressive enough to move to next round personal care company thought did well in round and feeling even the manager really enjoy the interview and my cv it turned out being rejected due to poor communication travel company case study is interesting and diversified completed them but don have good domain knowledge to explain some result not impressive enough for other round multimedia company case study is conceptual question domain is new to me programming question is easy can answer every question not good enough diy business company did very well in every round total completed live coding with only half of the time solved business problem well even the engineering director told my interview make his day again being rejected after the last round for no clear reason car trading company ve been asked about arvix paper read algorithm ve used told random forest maybe they wanna discus about complex stuff like bert transformer maybe my knowledge is not deep enough to impress the interviewer shopping startup solve business well failed at poor reasoning during machine learning question reason of rejection is due to communication search business company do not have domain knowledge only partially solve their business question rejected others some rejected me at prescreening some make my withdrawal due not not meeting my expectation after the first interview some told passed the first round and then never heard them back really learn lot from other business after doing their case study like beside technical metric mse f1 there re also business metric revenue profit where people wanna discus also understand more people skill and politics through interview a those skill can be searched in stackoverflow another importance is to make them like me during the talk what experience is that d is really wide range of skill every case study is different skillset in which domain knowledge is super important also some company don know how to hire and how to onboard
first data scientist job after experience in other engineering field how to succeed,this felt little long to post in the weekly thread hope it okay to make stand alone post anyway here goes started as an analyst on data science team at non faang tech company several years back most of what worked on was descriptive analysis studying churn and anomaly detection never built ton of models and did not productionalize any of my work really wanted to move full on into data scientist role but didn see good path on that team due to management etc then moved into specialized area of tech engineering that love and have been in for about years continued to do analytics heavy work in this domain when could and my work mostly bordered on what call data science ve gained lot of experience both in this domain and have worked to further my data skills grad school projects etc and just landed my first actual data scientist position doing work in my area of speciality couldn be more excited but am also absolutely terrified it small company and ll be their first data scientist worried in over my head and that won be able to deliver results ve successfully built out and been tech lead for small team in my area of engineering so fairly confident in my soft skills and ability to help push projects forward but very worried won be able to wow everyone with data magic or do really complex modeling as ramping up ve come to prefer simpler models anyway but worried my skills won be up to par for being the sole data scientist has anyone been in situation like this what can do to ensure the best chances of success not only for myself but for data science as whole at this company,1,first data scientist job after experience in other engineering field how to succeed,this felt little long to post in the weekly thread hope it okay to make stand alone post anyway here go started a an analyst on data science team at non faang tech company several year back most of what worked on wa descriptive analysis studying churn and anomaly detection never built ton of model and did not productionalize any of my work really wanted to move full on into data scientist role but didn see good path on that team due to management etc then moved into specialized area of tech engineering that love and have been in for about year continued to do analytics heavy work in this domain when could and my work mostly bordered on what call data science ve gained lot of experience both in this domain and have worked to further my data skill grad school project etc and just landed my first actual data scientist position doing work in my area of speciality couldn be more excited but am also absolutely terrified it small company and ll be their first data scientist worried in over my head and that won be able to deliver result ve successfully built out and been tech lead for small team in my area of engineering so fairly confident in my soft skill and ability to help push project forward but very worried won be able to wow everyone with data magic or do really complex modeling a ramping up ve come to prefer simpler model anyway but worried my skill won be up to par for being the sole data scientist ha anyone been in situation like this what can do to ensure the best chance of success not only for myself but for data science a whole at this company
with remote work do companies still strongly prefer local applicants,with remote work do companies still strongly prefer local applicants,1,with remote work do company still strongly prefer local applicant,with remote work do company still strongly prefer local applicant
project presentation advise,hello and happy holidays have machine learning graduate position an entry level position according to the hr interviewer operations interview next year january the hr asked me to prepare for the interview this is the nd and maybe last interview need to ace to be accepted was told to present the last project have created so am here asking for advise how can possibly present my project when my the project did is more of program course recommendation coming from students based on their performance during their senior years to reduce attrition rate and program recommendation also for to students who wish to shift program amp x200b the size of the project made is kinda small in my opinion any idea how to sound technical during the interview used flask for my back end and vue for the front end any tips will be greatly appreciated thank you so much,1,project presentation advise,hello and happy holiday have machine learning graduate position an entry level position according to the hr interviewer operation interview next year january the hr asked me to prepare for the interview this is the nd and maybe last interview need to ace to be accepted wa told to present the last project have created so am here asking for advise how can possibly present my project when my the project did is more of program course recommendation coming from student based on their performance during their senior year to reduce attrition rate and program recommendation also for to student who wish to shift program amp x200b the size of the project made is kinda small in my opinion any idea how to sound technical during the interview used flask for my back end and vue for the front end any tip will be greatly appreciated thank you so much
what do you do and what type of math do you use in your day to day work can you give me any examples how complex does it get,what do you do and what type of math do you use in your day to day work can you give me any examples how complex does it get,1,what do you do and what type of math do you use in your day to day work can you give me any example how complex doe it get,what do you do and what type of math do you use in your day to day work can you give me any example how complex doe it get
pdf krishna dey wang computational topology for data analysis,pdf krishna dey wang computational topology for data analysis,1,pdf krishna dey wang computational topology for data analysis,pdf krishna dey wang computational topology for data analysis
lark or owl,dear data scientists planning some scrum activities in industry and curious about the preferred active hours for data scientists in academia was surrounded by owls what the personal preference in industry despite the expected thanks in advance view poll,1,lark or owl,dear data scientist planning some scrum activity in industry and curious about the preferred active hour for data scientist in academia wa surrounded by owl what the personal preference in industry despite the expected thanks in advance view poll
pooling in convolutional neural network,pooling in convolutional neural network,1,pooling in convolutional neural network,pooling in convolutional neural network
how do you reduce information leakage and bias when going from descriptive analytics to prescriptive analytics,cassie kozyrkov cheif decision scientist at google wrote great article about data charlatans trying to analyze data and at the same time making statistical tests to pretty much verify the phenomena already seen in the exploration the solution to this problem is of course data splitting in the best cases having subset for exploration and other for training testing etc but what happens when an organisation is far behind the analytical ladder and is doing descriptive analytics in various reports and with no intention of doing anything else but then wants to move to more diagnostic predictive and eventually prescriptive analytics the data in the analytical reports are probably showing all data in real time and several conclusions about the data has probably already been made how can you limit the information leakage when deciding to do more more statistical tests training and testing has anybody had any experience with this problem and found solution,1,how do you reduce information leakage and bias when going from descriptive analytics to prescriptive analytics,cassie kozyrkov cheif decision scientist at google wrote great article about data charlatan trying to analyze data and at the same time making statistical test to pretty much verify the phenomenon already seen in the exploration the solution to this problem is of course data splitting in the best case having subset for exploration and other for training testing etc but what happens when an organisation is far behind the analytical ladder and is doing descriptive analytics in various report and with no intention of doing anything else but then want to move to more diagnostic predictive and eventually prescriptive analytics the data in the analytical report are probably showing all data in real time and several conclusion about the data ha probably already been made how can you limit the information leakage when deciding to do more more statistical test training and testing ha anybody had any experience with this problem and found solution
how do you recude information leakage and bias when going from descriptive analytics to prescriptive analytics,cassie kozyrkov cheif decision scientist at google wrote great article about data charlatans trying to analyze data and at the same time making statistical tests to pretty much verify the phenomena already seen in the exploration the solution to this problem is of course data splitting in the best cases having subset for exploration and other for training testing etc but what happens when an organisation is far behind the analytical ladder and is doing descriptive analytics in various reports and with no intention of doing anything else but after while wants to move to more diagnostic predictive and eventually prescriptive analytics the data in the analytical reports are probably showing all data in real time and several conclusions about the data has probably already been made how can you limit the information leakage when deciding to do more more statistical tests training and testing has anybody had any experience with this problem and found solution,1,how do you recude information leakage and bias when going from descriptive analytics to prescriptive analytics,cassie kozyrkov cheif decision scientist at google wrote great article about data charlatan trying to analyze data and at the same time making statistical test to pretty much verify the phenomenon already seen in the exploration the solution to this problem is of course data splitting in the best case having subset for exploration and other for training testing etc but what happens when an organisation is far behind the analytical ladder and is doing descriptive analytics in various report and with no intention of doing anything else but after while want to move to more diagnostic predictive and eventually prescriptive analytics the data in the analytical report are probably showing all data in real time and several conclusion about the data ha probably already been made how can you limit the information leakage when deciding to do more more statistical test training and testing ha anybody had any experience with this problem and found solution
need datacamp subscription,problem don have datacamp and need to do their course on web scraping with python solution can someone here share their datacamp credentials impact will learn,1,need datacamp subscription,problem don have datacamp and need to do their course on web scraping with python solution can someone here share their datacamp credential impact will learn
job search,how can get an entry level data science job,1,job search,how can get an entry level data science job
anyone done the ibm data science last chapter applied data science capstone have been finding it difficult to find the answers for these quizes in week web scraping and collecting data and my subscription will be finished in days can anyone answer these questions,anyone done the ibm data science last chapter applied data science capstone have been finding it difficult to find the answers for these quizes in week web scraping and collecting data and my subscription will be finished in days can anyone answer these questions,1,anyone done the ibm data science last chapter applied data science capstone have been finding it difficult to find the answer for these quiz in week web scraping and collecting data and my subscription will be finished in day can anyone answer these question,anyone done the ibm data science last chapter applied data science capstone have been finding it difficult to find the answer for these quiz in week web scraping and collecting data and my subscription will be finished in day can anyone answer these question
how to find monthly google searches for specific keyword,how to find monthly google searches for specific keyword have seen people say to use google keywords planner but that needs an ad campaign am looking for something else which is free just want to make something like this for just fun side project the video has instructions on how he did it in the description but am unable to find it thanks,1,how to find monthly google search for specific keyword,how to find monthly google search for specific keyword have seen people say to use google keywords planner but that need an ad campaign am looking for something else which is free just want to make something like this for just fun side project the video ha instruction on how he did it in the description but am unable to find it thanks
advice,graduated from high school months ago am taking gap year before go to university before go to university want to if there is any future in data science will an undergrads at university so naturally will be doing computer science then major in ds would also like to know if can make enough to live comfortable life,1,advice,graduated from high school month ago am taking gap year before go to university before go to university want to if there is any future in data science will an undergrad at university so naturally will be doing computer science then major in d would also like to know if can make enough to live comfortable life
the more think of my data career the more anxious become mid career,sorry for the clickbait title but exhausted this probably isn the right sub but need some assistance also extremely lucky make good money pretty good job boss who get along with and lot of autonomy data is pretty straight forward sales finance product marketing data the part that exhausts me is reading the data labor market and trying to make sense of trends to plan for what to come it plagued me for two years now and cannot infer enough to make decision for what my next step forward should be due to the ever changing landscape this industry is tumultuous and drastically inconsistent to say the least started in bi several years ago after earning bachelors in accounting and then taking it courses at community college mainly db and sql centered classes up until or so hadn really worried about what to come because often felt like my skill set was enough so just kept going deeper with sql dbs and bi tools figured that would top out as bi director and then decide what should do next well next is now for the past two companies have worked as an individual contributor director yes in the start up technology space we unfortunately exist saas technology is the industry prefer since it aligns with my interests the fact of the matter is since everything continues to evolve fear ll eventually fall behind because don have the formal technical nor quantitative credentials when going for the next role whatever that should be ve looked into ds roles but haven ever built ml model haven put any into production either because any forecasting prediction ve come across was handled by fp amp no one in product has given damn about analytics because they swag everything everything in finance goes by their rules not by statistics sales is the same they have set of rules they abide by ve tried thinking in terms of using statistics but it seems very rare to find someone who cares about them or how it would lead to something other than nice to have on the technical side everything except for the edw is built out spend of my time on sql and of dashboarding and visualizations work on jobs in the edw work on modeling reports etc ve sped up business processes that took days down to hours from f300 companies to these start ups it seems cs and stats knowledge is on the fringes at best what am supposed to do as approach my like to have masters degree as signal that either formally educated to where if encounter ds topics in bi won just swag it feel like the cs part is the safest bet since can always fall back in bi dev or data analytics engineer type roles but lose out on statistics similarly if go statistics feel like will never use the knowledge or that it overkill for what do does anyone else especially those who work in data science encounter this kind of existence saw post the other day where there was category of data monkey and thought the director of data monkeys any help is appreciated just to help ease my mind,1,the more think of my data career the more anxious become mid career,sorry for the clickbait title but exhausted this probably isn the right sub but need some assistance also extremely lucky make good money pretty good job bos who get along with and lot of autonomy data is pretty straight forward sale finance product marketing data the part that exhaust me is reading the data labor market and trying to make sense of trend to plan for what to come it plagued me for two year now and cannot infer enough to make decision for what my next step forward should be due to the ever changing landscape this industry is tumultuous and drastically inconsistent to say the least started in bi several year ago after earning bachelor in accounting and then taking it course at community college mainly db and sql centered class up until or so hadn really worried about what to come because often felt like my skill set wa enough so just kept going deeper with sql db and bi tool figured that would top out a bi director and then decide what should do next well next is now for the past two company have worked a an individual contributor director yes in the start up technology space we unfortunately exist saas technology is the industry prefer since it aligns with my interest the fact of the matter is since everything continues to evolve fear ll eventually fall behind because don have the formal technical nor quantitative credential when going for the next role whatever that should be ve looked into d role but haven ever built ml model haven put any into production either because any forecasting prediction ve come across wa handled by fp amp no one in product ha given damn about analytics because they swag everything everything in finance go by their rule not by statistic sale is the same they have set of rule they abide by ve tried thinking in term of using statistic but it seems very rare to find someone who care about them or how it would lead to something other than nice to have on the technical side everything except for the edw is built out spend of my time on sql and of dashboarding and visualization work on job in the edw work on modeling report etc ve sped up business process that took day down to hour from f300 company to these start ups it seems c and stats knowledge is on the fringe at best what am supposed to do a approach my like to have master degree a signal that either formally educated to where if encounter d topic in bi won just swag it feel like the c part is the safest bet since can always fall back in bi dev or data analytics engineer type role but lose out on statistic similarly if go statistic feel like will never use the knowledge or that it overkill for what do doe anyone else especially those who work in data science encounter this kind of existence saw post the other day where there wa category of data monkey and thought the director of data monkey any help is appreciated just to help ease my mind
engineer vs scientist,what the difference,1,engineer v scientist,what the difference
how to start freelancing,have full time work as data scientist my am interested in doing other projects for example day per week have you experience from freelancing platforms etc that could land you project that can be done remotely,1,how to start freelancing,have full time work a data scientist my am interested in doing other project for example day per week have you experience from freelancing platform etc that could land you project that can be done remotely
is this ia program amazing or is it bullshitting me,tldr am considering applying to university program of ia in the netherlands but some of the courses sound like the public sci fi perception of ia rather than the real thing introduction to the brain cognitive psychology and philosophy of ai and cognition want to know if this is fair assesment or if am blinded by my lack of experience detailed version hello my name is alex am applying to the university of groningen to study applied mathematics noticed they also have an ia program and was intrested these are the courses now have work little in data science in the past unpaid internships related to neuroscience research here but it was pretty much installing libraries creating models to fit the data and trying to improve the performance of said models which is fine love math and like programming so enjoyed it but when looked at the courses they include stuff like introduction to the brain and cognitive psychology which is more on the fictional sci fi ia side of things also some compulsory philosophy progams like philosophy of ai and cognition my kneejerk reaction is that this is what people who don know about data science is they think its all about creating consciousness without it becoming ultron when in reality its more about creating program that can solve problem but only have little over hours of official ia experience university courses internship experience so wanted to know if this community had some perspective don know there is small part of the ia community which tries to create conscious robots but don think its close to being the majority also these are the minority and every career has its own bullshit classes so it not like this is dealbreaker,1,is this ia program amazing or is it bullshitting me,tldr am considering applying to university program of ia in the netherlands but some of the course sound like the public sci fi perception of ia rather than the real thing introduction to the brain cognitive psychology and philosophy of ai and cognition want to know if this is fair assesment or if am blinded by my lack of experience detailed version hello my name is alex am applying to the university of groningen to study applied mathematics noticed they also have an ia program and wa intrested these are the course now have work little in data science in the past unpaid internship related to neuroscience research here but it wa pretty much installing library creating model to fit the data and trying to improve the performance of said model which is fine love math and like programming so enjoyed it but when looked at the course they include stuff like introduction to the brain and cognitive psychology which is more on the fictional sci fi ia side of thing also some compulsory philosophy progams like philosophy of ai and cognition my kneejerk reaction is that this is what people who don know about data science is they think it all about creating consciousness without it becoming ultron when in reality it more about creating program that can solve problem but only have little over hour of official ia experience university course internship experience so wanted to know if this community had some perspective don know there is small part of the ia community which try to create conscious robot but don think it close to being the majority also these are the minority and every career ha it own bullshit class so it not like this is dealbreaker
what did you learn in your first ds job and why you moved to next job,what did you learn in your first ds job and why you moved to next job,1,what did you learn in your first d job and why you moved to next job,what did you learn in your first d job and why you moved to next job
is topological data analysis useful for time series data,today came across this paper claiming to use tda to predict early warning signs of market crashes although their methodology makes sense really don see any way how these results are better than simple volatility check does topological data analysis provide any useful information in this case other than just being glorified version of clustering is it still relevant today or it was just fad started by ayasdi,1,is topological data analysis useful for time series data,today came across this paper claiming to use tda to predict early warning sign of market crash although their methodology make sense really don see any way how these result are better than simple volatility check doe topological data analysis provide any useful information in this case other than just being glorified version of clustering is it still relevant today or it wa just fad started by ayasdi
does anyone know good resource for making the map in this new york times article,here the article smtyp cur amp fbclid iwar0chy3 ipskew9hutpcs ilzqbdowa5de5rdexnrs8gj0ig9ec6mfpa4 smtyp cur amp fbclid iwar0chy3_e ipskew9hutpcs ilzqbdowa5de5rdexnrs8gj0ig9ec6mfpa4 love to know if anyone has found good resource for making these kinds of maps,1,doe anyone know good resource for making the map in this new york time article,here the article smtyp cur amp fbclid iwar0chy3 ipskew9hutpcs ilzqbdowa5de5rdexnrs8gj0ig9ec6mfpa4 smtyp cur amp fbclid iwar0chy3_e ipskew9hutpcs ilzqbdowa5de5rdexnrs8gj0ig9ec6mfpa4 love to know if anyone ha found good resource for making these kind of map
healthcare analytics course,hey everyone ve been working at health clinic for about months now as data analyst work with mostly clinical data and also business operations like to improve my health literature understand the current trends in healthcare and overall improve my analytical skills are there are any healthcare related courses anyone could recommend thanks in advance,1,healthcare analytics course,hey everyone ve been working at health clinic for about month now a data analyst work with mostly clinical data and also business operation like to improve my health literature understand the current trend in healthcare and overall improve my analytical skill are there are any healthcare related course anyone could recommend thanks in advance
anyone with an ms in cognitive science,anyone with an ms in cognitive science,1,anyone with an m in cognitive science,anyone with an m in cognitive science
how do you ask someone to be your mentor,silly question but recently started my career in data analytics as an analyst want to become data scientist and have the necessary educational qualifications and skills however since just started my career need some direction to accomplish that ve seen linkedin profiles of people that look up to and want to be mentored by them just don know how to ask them that,1,how do you ask someone to be your mentor,silly question but recently started my career in data analytics a an analyst want to become data scientist and have the necessary educational qualification and skill however since just started my career need some direction to accomplish that ve seen linkedin profile of people that look up to and want to be mentored by them just don know how to ask them that
the pymc developers wrote book bayesian modeling and computation in python detailed toc screenshotted link to publisher page in first photo,the pymc developers wrote book bayesian modeling and computation in python detailed toc screenshotted link to publisher page in first photo,1,the pymc developer wrote book bayesian modeling and computation in python detailed toc screenshotted link to publisher page in first photo,the pymc developer wrote book bayesian modeling and computation in python detailed toc screenshotted link to publisher page in first photo
career advice,ve been working in higher education for years as an academic librarian have phd in political science and lot of practical experience collecting analyzing data whether it be for my own research or more directly related to my day to day duties throughout my career have been involved with and led what we call assessment efforts within higher education essentially this means collecting various data related to the things we do and student success outcomes to help better illustrate our value to stakeholders comply with various accrediting bodies and guide organizational decision making build databases dashboards create various visualizations conduct survey based satisfaction research etc over the past few years have also been involved with the development of an open source data warehouse solution for libraries and similar organizations am not programmer but more of an analyst though did minor in computer science in college and have some low level programming experience got into couple of years ago and have published couple of articles on basic machine learning solutions within the library environment using adaboost to predict library use patterns to be honest feel like my talents are wasted in libraries unfortunately am very good at administrative work meetings planning paperwork etc so have been pigeon holed into that role so of my time is spent doing what feels like rote nonsense my wife recently landed job at big company and the pay is ridiculous like twice what make and not doing bad find myself wondering why am running myself into the ground doing inconsequential work that unfulfilling know can do more and have talents that are withering on the vine so to speak understand data and statistical methods fairly well though am constantly finding that there more to learn am fairly weak at programming in the sense that don have ton of practical experience though have demonstrated an ability to teach myself quite bit and if had the time to focus on it could be much better the problem is that my job doesn require me to do that so it difficult to make time for it of course am willing to make the time in my private life which have done in the past with other pursuits earned my phd while working full time as librarian writing that page dissertation was full time job in itself with that said am very proficient at designing experiments designing deploying practical data collection tools analyzing data writing reports and presenting to small large groups of non expert and expert audiences as an academician this is simply what do so my question is what do need to do to be competitive for some of these jobs see staff data scientist at reddit do simply need more programming chops python etc would appreciate any advice you professional data scientists can send my way thanks so much for reading all of this,1,career advice,ve been working in higher education for year a an academic librarian have phd in political science and lot of practical experience collecting analyzing data whether it be for my own research or more directly related to my day to day duty throughout my career have been involved with and led what we call assessment effort within higher education essentially this mean collecting various data related to the thing we do and student success outcome to help better illustrate our value to stakeholder comply with various accrediting body and guide organizational decision making build database dashboard create various visualization conduct survey based satisfaction research etc over the past few year have also been involved with the development of an open source data warehouse solution for library and similar organization am not programmer but more of an analyst though did minor in computer science in college and have some low level programming experience got into couple of year ago and have published couple of article on basic machine learning solution within the library environment using adaboost to predict library use pattern to be honest feel like my talent are wasted in library unfortunately am very good at administrative work meeting planning paperwork etc so have been pigeon holed into that role so of my time is spent doing what feel like rote nonsense my wife recently landed job at big company and the pay is ridiculous like twice what make and not doing bad find myself wondering why am running myself into the ground doing inconsequential work that unfulfilling know can do more and have talent that are withering on the vine so to speak understand data and statistical method fairly well though am constantly finding that there more to learn am fairly weak at programming in the sense that don have ton of practical experience though have demonstrated an ability to teach myself quite bit and if had the time to focus on it could be much better the problem is that my job doesn require me to do that so it difficult to make time for it of course am willing to make the time in my private life which have done in the past with other pursuit earned my phd while working full time a librarian writing that page dissertation wa full time job in itself with that said am very proficient at designing experiment designing deploying practical data collection tool analyzing data writing report and presenting to small large group of non expert and expert audience a an academician this is simply what do so my question is what do need to do to be competitive for some of these job see staff data scientist at reddit do simply need more programming chop python etc would appreciate any advice you professional data scientist can send my way thanks so much for reading all of this
made modern data catalog tool for anyone using word document or excel sheet as data catalog curious if anyone would like to try it out,at my old job it seemed like have new project with new dataset every few weeks the hardest part of my job was understanding the data not completing the project last year built data catalog using the no code platform bubble and shared it here we ended up with quite few people testing it out and using it on personal projects in the last months took the original platform built and leveraged some open source platforms like amundsen to rebuild modern data catalog focused on making data documentation transparent collaborative and straightforward for anyone or company we have sandbox environment with dummy data that we re looking for user feedback on if anyone is interested in giving it spin please let me know we re planning to release public version for anyone to use early next year happy new year and appreciate anyone willing to give it try,1,made modern data catalog tool for anyone using word document or excel sheet a data catalog curious if anyone would like to try it out,at my old job it seemed like have new project with new dataset every few week the hardest part of my job wa understanding the data not completing the project last year built data catalog using the no code platform bubble and shared it here we ended up with quite few people testing it out and using it on personal project in the last month took the original platform built and leveraged some open source platform like amundsen to rebuild modern data catalog focused on making data documentation transparent collaborative and straightforward for anyone or company we have sandbox environment with dummy data that we re looking for user feedback on if anyone is interested in giving it spin please let me know we re planning to release public version for anyone to use early next year happy new year and appreciate anyone willing to give it try
how can this data visualization be improved,this is for an application to school think it might have little too much information does anybody have any ideas this is somebody else work not my own,1,how can this data visualization be improved,this is for an application to school think it might have little too much information doe anybody have any idea this is somebody else work not my own
recommendations on career path for data analysis data science,hi everyone am currently working as management consultant and want to specialise in the data field have graduated from bsc in business with information systems and have udacity certifications in data analysis and business analytics have knowledge of sql python statistics and visualisations using tableau and power bi however am not using them as much as want to in my day to day work would be grateful if anyone could answer the following queries have are there any further education certifications which need to pursue to break in the data field any particular masters what are the continuous learnings should do to keep my skills set up to date and or improve my skills set any books to read any project ideas to build up my portfolio how can push for more data analysis in my work my work mainly revolves around strategy formulation and digital transformation what is more relevant in the context of business data engineering or data science thanks in advance to anyone who will answer open to any further discussions,1,recommendation on career path for data analysis data science,hi everyone am currently working a management consultant and want to specialise in the data field have graduated from bsc in business with information system and have udacity certification in data analysis and business analytics have knowledge of sql python statistic and visualisation using tableau and power bi however am not using them a much a want to in my day to day work would be grateful if anyone could answer the following query have are there any further education certification which need to pursue to break in the data field any particular master what are the continuous learning should do to keep my skill set up to date and or improve my skill set any book to read any project idea to build up my portfolio how can push for more data analysis in my work my work mainly revolves around strategy formulation and digital transformation what is more relevant in the context of business data engineering or data science thanks in advance to anyone who will answer open to any further discussion
have social science phd and want data science job,my skills are good ve taken phd level comp sci courses and have published on machine learning topics but need to signal to employers that actually qualified and not someone who just reads plato or knows regression any tips,1,have social science phd and want data science job,my skill are good ve taken phd level comp sci course and have published on machine learning topic but need to signal to employer that actually qualified and not someone who just read plato or know regression any tip
learn to be math artist,numbers and greek symbols sit on page like clay learn to be math artist see applications in the math you learn in the same way an artist sees shapes in clay you can learn to sculpt math that is going to open up career in data science for you in interviews ask for an unconventional application or modification of an existing algorithm can you go from being told how to use math to creatively applying the math you have learned physics taught me practical applications of calculus that completely changed the way see math had learned uses for equations in the past there was something more visible and tangible that finally clicked these types of shifts in thinking are more important than memorization applying research to business problems requires math artist you have to see an algorithm framed in the context of the business need you have to see an implementation math can be generalized repurposed and applied the algorithms and proofs are not static treat them like clay and you will build better models,1,learn to be math artist,number and greek symbol sit on page like clay learn to be math artist see application in the math you learn in the same way an artist see shape in clay you can learn to sculpt math that is going to open up career in data science for you in interview ask for an unconventional application or modification of an existing algorithm can you go from being told how to use math to creatively applying the math you have learned physic taught me practical application of calculus that completely changed the way see math had learned us for equation in the past there wa something more visible and tangible that finally clicked these type of shift in thinking are more important than memorization applying research to business problem requires math artist you have to see an algorithm framed in the context of the business need you have to see an implementation math can be generalized repurposed and applied the algorithm and proof are not static treat them like clay and you will build better model
really feel like data science gets romanticized and people don talk about the support aspect,am not data science person by trade did it for little over years because our data analyst left and was the dba am an systems engineer didn have time to roll out pbi push reports etc did lot of my stuff as canned sps that could modify quickly we got bought and handed all of that off to real data team with the right tools and knowledge but from time to time get asked something and it is easier for me to run point so can just get the info directly from whoever and sort it out faster than back and forth with lower tier of reporting support yesterday afternoon got asked about problem where user said report was missing information and to me that just meant they expected something not that anything was missing which is huge part of ds managing expectations something don think people realize is huge part of the job going in so sift through the info and talk to the person reporting the problem between them and the reporting support tier realize the problem is they are looking at maintenance page and comparing that list to the report they were concerned the report didn have everything the maint page had needed to confirm this with the user reporting to establish baseline send them the exact link to the maint page and say you re looking here and seeing claims that aren on the claims status report on the customer side right they say no am looking at the customer side and send screenshot of the report say right but in order for you to feel like that report is missing claims you must have another list or data source you re comparing it to is it this page on the admin side no on the shipper side sends same screenshot of report me understand but what are you looking at to make you think claims are missing him joe blue confirmed on the admin side me okay where on the admin side did he look to confirm how did he confirm him sends screenshot of the page linked minutes before asking if that was where thank you all for doing the job so don have too and know there are lot of you out there over qualified not getting paid enough because people don factor in the above situations and how needy people are especially sales so they don think far enough into it to pay you to make dealing with that worth it they think pay should be based on skill set alone not an incentive to endure nonsense patient but not that ds patient,1,really feel like data science get romanticized and people don talk about the support aspect,am not data science person by trade did it for little over year because our data analyst left and wa the dba am an system engineer didn have time to roll out pbi push report etc did lot of my stuff a canned sps that could modify quickly we got bought and handed all of that off to real data team with the right tool and knowledge but from time to time get asked something and it is easier for me to run point so can just get the info directly from whoever and sort it out faster than back and forth with lower tier of reporting support yesterday afternoon got asked about problem where user said report wa missing information and to me that just meant they expected something not that anything wa missing which is huge part of d managing expectation something don think people realize is huge part of the job going in so sift through the info and talk to the person reporting the problem between them and the reporting support tier realize the problem is they are looking at maintenance page and comparing that list to the report they were concerned the report didn have everything the maint page had needed to confirm this with the user reporting to establish baseline send them the exact link to the maint page and say you re looking here and seeing claim that aren on the claim status report on the customer side right they say no am looking at the customer side and send screenshot of the report say right but in order for you to feel like that report is missing claim you must have another list or data source you re comparing it to is it this page on the admin side no on the shipper side sends same screenshot of report me understand but what are you looking at to make you think claim are missing him joe blue confirmed on the admin side me okay where on the admin side did he look to confirm how did he confirm him sends screenshot of the page linked minute before asking if that wa where thank you all for doing the job so don have too and know there are lot of you out there over qualified not getting paid enough because people don factor in the above situation and how needy people are especially sale so they don think far enough into it to pay you to make dealing with that worth it they think pay should be based on skill set alone not an incentive to endure nonsense patient but not that d patient
friendly reminder to just go ahead and set up git on everything,or whatever version control you use had small script would use for ad hoc data cleaning that didnt bother adding it to the repo well now it not so small and it needs to be added just dislike adding new local directories to the repo and wish had just set it up from the start instead,1,friendly reminder to just go ahead and set up git on everything,or whatever version control you use had small script would use for ad hoc data cleaning that didnt bother adding it to the repo well now it not so small and it need to be added just dislike adding new local directory to the repo and wish had just set it up from the start instead
pgdm in data science from jain university,hi from mumbai india planning to do my masters from germany under daad scholarship have work experience of year and graduation it seems that for daad scholarship one needs years of work experience and total of years of education came across course for data science from ims proschool which provides pgdm certification from jain university have always wanted to upskill by learning data science just want to be sure if this course can help me learn the necessary skills and also pgdm which can complete years of my education,1,pgdm in data science from jain university,hi from mumbai india planning to do my master from germany under daad scholarship have work experience of year and graduation it seems that for daad scholarship one need year of work experience and total of year of education came across course for data science from ims proschool which provides pgdm certification from jain university have always wanted to upskill by learning data science just want to be sure if this course can help me learn the necessary skill and also pgdm which can complete year of my education
does it make sense doing stats tests like ttests chi square etc to better select the predictors in supervised model,mean it makes sense to me but don see many of the machine learning tutorials on internet performing those tests when doing feature selection is it good practice or there catch thanks,1,doe it make sense doing stats test like ttests chi square etc to better select the predictor in supervised model,mean it make sense to me but don see many of the machine learning tutorial on internet performing those test when doing feature selection is it good practice or there catch thanks
simple and effective way to go from beginner to intermediate level of ml knowledge,read the scikit learn user guide from top to bottom this is not even joke it contains many examples tips and teaches you to work with their api to avoid common pitfalls actually explains part of the underlying math and links to relevant books papers by reading it you ll come into contact with ton of methods you probably never heard of as beginner like gaussian process kernel ridge regression and tons of methods in robust statistics encourage you to take notes watch video and learn about these methods you may want to start with chapter first but that up to you when you re done you can attempt to do the same thing for statsmodels but that will be considerably more painful,1,simple and effective way to go from beginner to intermediate level of ml knowledge,read the scikit learn user guide from top to bottom this is not even joke it contains many example tip and teach you to work with their api to avoid common pitfall actually explains part of the underlying math and link to relevant book paper by reading it you ll come into contact with ton of method you probably never heard of a beginner like gaussian process kernel ridge regression and ton of method in robust statistic encourage you to take note watch video and learn about these method you may want to start with chapter first but that up to you when you re done you can attempt to do the same thing for statsmodels but that will be considerably more painful
transitioning from corporate to bioinformatics research,full stack developer at us based company have completed my bachelor engineering in computer science and engineering in at that time wasn sure about what really wanted to do with my career in last year ve been trying different technologies and fields to figure out what would love to do for the rest of my life and few months back realised that want to apply machine learning and deep learning techniques in biology or healthcare field one more thing that came to my mind after thorough observation is that love exploring new things and have genuine curiosity about how things work what can we do to improve something or how can we find something or some way that could solve problem this lead me to the fact that should follow the career of bioinformatics researcher or data scientist focusing on healthcare given my current scenario ve to get master degree in healthcare informatics or bioinformatics then go for phd and then getting enrolled in some institute as research from this discussion looking to address following questions what do you think should be the path that should follow in order to make this transition is it good idea to leave highly paying profile to into research world where the pay will be bit lesser what areas should explore before making this shift what in your opinion should expect from this change,1,transitioning from corporate to bioinformatics research,full stack developer at u based company have completed my bachelor engineering in computer science and engineering in at that time wasn sure about what really wanted to do with my career in last year ve been trying different technology and field to figure out what would love to do for the rest of my life and few month back realised that want to apply machine learning and deep learning technique in biology or healthcare field one more thing that came to my mind after thorough observation is that love exploring new thing and have genuine curiosity about how thing work what can we do to improve something or how can we find something or some way that could solve problem this lead me to the fact that should follow the career of bioinformatics researcher or data scientist focusing on healthcare given my current scenario ve to get master degree in healthcare informatics or bioinformatics then go for phd and then getting enrolled in some institute a research from this discussion looking to address following question what do you think should be the path that should follow in order to make this transition is it good idea to leave highly paying profile to into research world where the pay will be bit lesser what area should explore before making this shift what in your opinion should expect from this change
forest fire prediction,hello guys need your help so the problem is we me and my friend we both are team and we have to create project so my friend just googled up for ideas and decided to make project on forest fire prediction the teacher approved this project but later my friend told me he don know anything about data science and same goes for me and there are only months left guys it will be great help if you guys guide me that what should do in this months to make this project success,1,forest fire prediction,hello guy need your help so the problem is we me and my friend we both are team and we have to create project so my friend just googled up for idea and decided to make project on forest fire prediction the teacher approved this project but later my friend told me he don know anything about data science and same go for me and there are only month left guy it will be great help if you guy guide me that what should do in this month to make this project success
trying to enter the field don know how to so gonna ask for help here,hey everyone im in here looking for advice ve given the context at the end basically looking to enter the field as data scientist analyst don even know if those terms are interchangeable but have no idea where to begin as for why it because kinda liked programming but only when picked up basics of programming on codeacademy was not motivated to complete since didn really think it would use it later on here are some questions what exactly does data scientist analyst do and how would programming languages help them in their work ve never learned programming languages in college am mostly entering school for mba will only get job in this field if did programming in college where can learn sql and python in depth basically am looking for resource that not only teaches me the basics but also how to apply them in data science for context from india just completed my college last may in marketing and probably gonna join some mba next july so until then wanted to learn stuff know like the basics of like addition and making graphs also okay with learning this as couple of years thing where start asap and probably take job in some other field and maybe pivot into ds if am good enough thank you,1,trying to enter the field don know how to so gonna ask for help here,hey everyone im in here looking for advice ve given the context at the end basically looking to enter the field a data scientist analyst don even know if those term are interchangeable but have no idea where to begin a for why it because kinda liked programming but only when picked up basic of programming on codeacademy wa not motivated to complete since didn really think it would use it later on here are some question what exactly doe data scientist analyst do and how would programming language help them in their work ve never learned programming language in college am mostly entering school for mba will only get job in this field if did programming in college where can learn sql and python in depth basically am looking for resource that not only teach me the basic but also how to apply them in data science for context from india just completed my college last may in marketing and probably gonna join some mba next july so until then wanted to learn stuff know like the basic of like addition and making graph also okay with learning this a couple of year thing where start asap and probably take job in some other field and maybe pivot into d if am good enough thank you
online data science training,online data science training,1,online data science training,online data science training
anaconda jupyter is awesome platform for file handling for windows operating system,anaconda jupyter is awesome platform for file handling for windows operating system check for examples session fbclid iwar31jaj0n7dkzfjmhordnyvyb1b0go115wiuvlcslk69lhfpqrqnvwobace session amp fbclid iwar3cyewrdk5x8_9ihjj_f4ooo02slfvhhbsfc2b5t0byipqxtcbhfply7dk any other good platforms,1,anaconda jupyter is awesome platform for file handling for window operating system,anaconda jupyter is awesome platform for file handling for window operating system check for example session fbclid iwar31jaj0n7dkzfjmhordnyvyb1b0go115wiuvlcslk69lhfpqrqnvwobace session amp fbclid iwar3cyewrdk5x8_9ihjj_f4ooo02slfvhhbsfc2b5t0byipqxtcbhfply7dk any other good platform
data scientists who are working outside their home country especially in usa canada or europe how did you get hired how did you convince them to hire an international data scientist rather than hiring local one especially if you need visa sponsorship,data scientists who are working outside their home country especially in usa canada or europe how did you get hired how did you convince them to hire an international data scientist rather than hiring local one especially if you need visa sponsorship,1,data scientist who are working outside their home country especially in usa canada or europe how did you get hired how did you convince them to hire an international data scientist rather than hiring local one especially if you need visa sponsorship,data scientist who are working outside their home country especially in usa canada or europe how did you get hired how did you convince them to hire an international data scientist rather than hiring local one especially if you need visa sponsorship
anaconda jupyter is awesome platform for file handling for windows operating system check for examples list pl6xjgh1xtyjl8yvzjv92 zksei_clmw4w amp index list pl6xjgh1xtyjl8yvzjv92 zksei_clmw4w amp index,amp x200b anaconda jupyter is awesome platform for file handling for windows operating system how many agree for it,1,anaconda jupyter is awesome platform for file handling for window operating system check for example list pl6xjgh1xtyjl8yvzjv92 zksei_clmw4w amp index list pl6xjgh1xtyjl8yvzjv92 zksei_clmw4w amp index,amp x200b anaconda jupyter is awesome platform for file handling for window operating system how many agree for it
what kind of metrics that you will look into before training an ml model,ve been learning data science from past couple of months using python and so far it very intresting so just wanted to understand in real life where do you start once you have the cleaned dataset ready for example what kind of stastitical metrics charts eda that will look into thanks for your help in advance,1,what kind of metric that you will look into before training an ml model,ve been learning data science from past couple of month using python and so far it very intresting so just wanted to understand in real life where do you start once you have the cleaned dataset ready for example what kind of stastitical metric chart eda that will look into thanks for your help in advance
lettuce pray before we start the feast,lettuce pray before we start the feast,1,lettuce pray before we start the feast,lettuce pray before we start the feast
trigonometric functions in python complete guide,trigonometric functions in python complete guide,1,trigonometric function in python complete guide,trigonometric function in python complete guide
what stopping data scientists from applying to remote only roles in high cost of living high paying locations like california and living in low cost of living location,right now remote work is more popular than ever especially due to the recent delta and omicron variants california and new york pays by far the most for data scientists but the high cost of living there offsets the high pay but if data scientist were to be working for company in california remotely with the same salary while living in state with lower cost of living his purchasing power with his income would be huge so why wouldn every data scientist be clawing to get the remote positions in such high paying companies,1,what stopping data scientist from applying to remote only role in high cost of living high paying location like california and living in low cost of living location,right now remote work is more popular than ever especially due to the recent delta and omicron variant california and new york pay by far the most for data scientist but the high cost of living there offset the high pay but if data scientist were to be working for company in california remotely with the same salary while living in state with lower cost of living his purchasing power with his income would be huge so why wouldn every data scientist be clawing to get the remote position in such high paying company
looking back on what you know now what concepts took you surprising amount of effort and time to truly understand,looking back on what you know now what concepts took you surprising amount of effort and time to truly understand,1,looking back on what you know now what concept took you surprising amount of effort and time to truly understand,looking back on what you know now what concept took you surprising amount of effort and time to truly understand
wackerly vs larsen introduction to mathematical statistics which do you prefer why,wackerly vs larsen introduction to mathematical statistics which do you prefer why,1,wackerly v larsen introduction to mathematical statistic which do you prefer why,wackerly v larsen introduction to mathematical statistic which do you prefer why
which data science ml development processes would you like to see more automated have python library for,what parts of ml projects do you find really tedious and do you think should be wrapped in python library looking for project ideas,1,which data science ml development process would you like to see more automated have python library for,what part of ml project do you find really tedious and do you think should be wrapped in python library looking for project idea
hiring data scientists and engineers for hot bay area startup,building team of data scientists and looking for advice on where to find great people ve tried recruiters linkedin and networks but need other ideas the company is well funded startup in healthcare technology we are mission driven and have solid plan to ipo in years we are using azure and likely databricks remote first it eventually will have an office in the bay area and the person will need to come meet in person our internal motto is don be jerk and someone in our team translated it into mathematical equation and it on our tshirts any tips on where can find good people would be much appreciated,1,hiring data scientist and engineer for hot bay area startup,building team of data scientist and looking for advice on where to find great people ve tried recruiter linkedin and network but need other idea the company is well funded startup in healthcare technology we are mission driven and have solid plan to ipo in year we are using azure and likely databricks remote first it eventually will have an office in the bay area and the person will need to come meet in person our internal motto is don be jerk and someone in our team translated it into mathematical equation and it on our tshirts any tip on where can find good people would be much appreciated
in masters of ds which classes are more important to focus on theory and which are more important for application,just got through my first semester was super into it and tbh killed it off of the drive to do well but for some of it was thinking will ever actually use this on the job it was stats and data analytics so seemed on point but my question is how much theory is used in day to day lot of both of the classes weren too much application which was little bit of bummer as hoped masters would be little more geared towards real world application all in all which classes should study more theory and which classes application,1,in master of d which class are more important to focus on theory and which are more important for application,just got through my first semester wa super into it and tbh killed it off of the drive to do well but for some of it wa thinking will ever actually use this on the job it wa stats and data analytics so seemed on point but my question is how much theory is used in day to day lot of both of the class weren too much application which wa little bit of bummer a hoped master would be little more geared towards real world application all in all which class should study more theory and which class application
planning on getting an internship on this resume please help,amp x200b resume format png amp auto webp amp ccd86e0dd3ca7a96b741615a09bcc04abeffb015,1,planning on getting an internship on this resume please help,amp x200b resume format png amp auto webp amp ccd86e0dd3ca7a96b741615a09bcc04abeffb015
would you say the model used was the random forest random forests,would you say the model used was the random forest random forests,1,would you say the model used wa the random forest random forest,would you say the model used wa the random forest random forest
sprg genesis recruitment is here,risen from the ashes of group thrown into chaos and ultimately destroyed new era of sprg capital arises the special projects research group will dedicate its time to furthering the knowledge and net worth of those who put the time and effort into learning the art of technical analysis and the wider crypto realm no matter your background or current situation if you are willing to learn and put in the effort you will be rewarded with strong community and true financial freedom in the future waagmi we were niche group of investors discord gg yrwft2xu,1,sprg genesis recruitment is here,risen from the ash of group thrown into chaos and ultimately destroyed new era of sprg capital arises the special project research group will dedicate it time to furthering the knowledge and net worth of those who put the time and effort into learning the art of technical analysis and the wider crypto realm no matter your background or current situation if you are willing to learn and put in the effort you will be rewarded with strong community and true financial freedom in the future waagmi we were niche group of investor discord gg yrwft2xu
boosting resume,currently in data science boot camp through butler university and an working on personal projects to supplement that because have heard that they leave commonly holes in your knowledge have laundry list of projects that have been suggested for me to work on and from what have seen they are good for my portfolio but to increase my odds of getting better job once ve finished my boot camp what are some other ways could boost my resume amp x200b does anyone know of some affordable certifications that look good for resumes or any other experience besides independent projects that look good,1,boosting resume,currently in data science boot camp through butler university and an working on personal project to supplement that because have heard that they leave commonly hole in your knowledge have laundry list of project that have been suggested for me to work on and from what have seen they are good for my portfolio but to increase my odds of getting better job once ve finished my boot camp what are some other way could boost my resume amp x200b doe anyone know of some affordable certification that look good for resume or any other experience besides independent project that look good
data science hot topics,hi all still very new to this field so was wondering what people oppinions are of the key topics of today within data science or if they have any articles books podcasts ect they think have interesting ideas or cover these so called hot topics ve seen fair bit on twitter covering machine learning ethics but be curious to see your thoughts on this,1,data science hot topic,hi all still very new to this field so wa wondering what people oppinions are of the key topic of today within data science or if they have any article book podcasts ect they think have interesting idea or cover these so called hot topic ve seen fair bit on twitter covering machine learning ethic but be curious to see your thought on this
which statistics topics are must to know before one dive into data science,want to self study data science so did some research online about the prerequisite courses found out programming acknowledge and statistics but am finding different answers on topics of statistics have to to study did python course with free code camp and bi with udemy which online resources can use for statics,1,which statistic topic are must to know before one dive into data science,want to self study data science so did some research online about the prerequisite course found out programming acknowledge and statistic but am finding different answer on topic of statistic have to to study did python course with free code camp and bi with udemy which online resource can use for static
interview data analysis on site test,hello all so have an on site interview for the position pricing analyst analytics with reputed retail focused company where would be given hour to perform data analysis on dataset present the findings in minutes presentation have to confirm with them whether the data analysis has to be done using excel or can use python etc as is the current trend the job description asked for bit of everything knowledge of data science and machine learning data visualization and proficiency in excel has anyone underwent similar data analysis test during interview can you share your experience any advise tia,1,interview data analysis on site test,hello all so have an on site interview for the position pricing analyst analytics with reputed retail focused company where would be given hour to perform data analysis on dataset present the finding in minute presentation have to confirm with them whether the data analysis ha to be done using excel or can use python etc a is the current trend the job description asked for bit of everything knowledge of data science and machine learning data visualization and proficiency in excel ha anyone underwent similar data analysis test during interview can you share your experience any advise tia
part of ds team that do not have data engineers and our datasource is datawarehouse which was not meant to be used for ds how could we create useful data model,hello community ve been working as ds in recently created team that does not have any data engineer the goal of my team is to provide insights into the usage of our products to our leads and for this we use telemetry data provided by different team but frustrated of how bad the data model is we don have data model my question is what could we do to improve this situation waste so much of my time joining tables breaking tables into more useful aggregations and this kind of tasks worked in bi team for some months before joining this team and learned about snowflake data model fact and dimension tables and found it useful at the time so thinking on how to bring that to my team think that could help my team to analyze data better but would like to know your take on this thanks,1,part of d team that do not have data engineer and our datasource is datawarehouse which wa not meant to be used for d how could we create useful data model,hello community ve been working a d in recently created team that doe not have any data engineer the goal of my team is to provide insight into the usage of our product to our lead and for this we use telemetry data provided by different team but frustrated of how bad the data model is we don have data model my question is what could we do to improve this situation waste so much of my time joining table breaking table into more useful aggregation and this kind of task worked in bi team for some month before joining this team and learned about snowflake data model fact and dimension table and found it useful at the time so thinking on how to bring that to my team think that could help my team to analyze data better but would like to know your take on this thanks
probability distribution,data science peeps suggestion needed how to know like which probability distribution to apply on the data set how can we assess that what probability distribution will generate right predictions the data set is of soccer game variables are runs difference runs allowed runs scored etc distributions know binomial poisson normal bernoulli uniform,1,probability distribution,data science peep suggestion needed how to know like which probability distribution to apply on the data set how can we ass that what probability distribution will generate right prediction the data set is of soccer game variable are run difference run allowed run scored etc distribution know binomial poisson normal bernoulli uniform
soft skills calm down unhappy client,beginner data analyst here internal department was requesting some data and when went to ask the higher ups where the data is they said they do not know then sent me bunch of tables that might have what looking for after reviewing all the tables and organizing what could sent it to the department informed them that there is decent chunk of information missing from the report after doing very thorough review of all the information had not being able to find everything is not uncommon at my job since there lot of transitions happening right now so thought they would have some understanding they re not happy about it at all and are completely surprised that the data is not as organized as they thought while can answer the additional questions they are asking how would you suggest calming them down feel like might run into this reaction again in the future so what is the general rule on how to handle this all the discussion is via email all of the people with the answers are on vacation until after new year,1,soft skill calm down unhappy client,beginner data analyst here internal department wa requesting some data and when went to ask the higher ups where the data is they said they do not know then sent me bunch of table that might have what looking for after reviewing all the table and organizing what could sent it to the department informed them that there is decent chunk of information missing from the report after doing very thorough review of all the information had not being able to find everything is not uncommon at my job since there lot of transition happening right now so thought they would have some understanding they re not happy about it at all and are completely surprised that the data is not a organized a they thought while can answer the additional question they are asking how would you suggest calming them down feel like might run into this reaction again in the future so what is the general rule on how to handle this all the discussion is via email all of the people with the answer are on vacation until after new year
principal components regression question,hello friends had quick question about principal component regression does anyone know if you can regress principal components and the raw covariates that they are calculated from in the same model for example if we are predicting wage based on education work experience state of residence race etc could we compute two principal components pc1 and pc2 and later regress wage b0 b1pc1 b2pc2 b3education b4work experience b5state of residence b6race would there be multicollinearity issues between the principal components and the original covariates would it make sense to do this have done this in stata and it doesn drop any variables for collinearity issues but it does help my predictions any help would be appreciated thanks,1,principal component regression question,hello friend had quick question about principal component regression doe anyone know if you can regress principal component and the raw covariates that they are calculated from in the same model for example if we are predicting wage based on education work experience state of residence race etc could we compute two principal component pc1 and pc2 and later regress wage b0 b1pc1 b2pc2 b3education b4work experience b5state of residence b6race would there be multicollinearity issue between the principal component and the original covariates would it make sense to do this have done this in stata and it doesn drop any variable for collinearity issue but it doe help my prediction any help would be appreciated thanks
looking for ds job to make positive impact learn lots and have decent salary netherlands,hi ve been data science consultant for the past two years first job at consulting firm mostly doing strategy financial analyses to make them more data driven awesome workplace since they only take on projects that can make positive impact nice working environment and got lot of freedom but since its core business isn data science am not learning as fast or as much as would like so would like to get some advice for companies in the netherlands preferably in the rotterdam area but not limited to where data science has prominent role can learn lot fast seniority course opportunities ds centered projects where can do projects with positive impact or at leas not projects with negative impact like working on cv model to detect drilling places for shell really curious to see what out there already looked at hospitals municipalities and the dutch railroad to give you an idea,1,looking for d job to make positive impact learn lot and have decent salary netherlands,hi ve been data science consultant for the past two year first job at consulting firm mostly doing strategy financial analysis to make them more data driven awesome workplace since they only take on project that can make positive impact nice working environment and got lot of freedom but since it core business isn data science am not learning a fast or a much a would like so would like to get some advice for company in the netherlands preferably in the rotterdam area but not limited to where data science ha prominent role can learn lot fast seniority course opportunity d centered project where can do project with positive impact or at lea not project with negative impact like working on cv model to detect drilling place for shell really curious to see what out there already looked at hospital municipality and the dutch railroad to give you an idea
work life balance on job search,work for large insurance company with great work life balance here what have at my company compressed work week work and have every friday off weeks pto hour work week tc im casually looking for job change to increase comp but only open to opportunities that maintain this level of work life especially important to me is the ve been in my role for years when should bring this up in the interview process resume also open to resume feedback,1,work life balance on job search,work for large insurance company with great work life balance here what have at my company compressed work week work and have every friday off week pto hour work week tc im casually looking for job change to increase comp but only open to opportunity that maintain this level of work life especially important to me is the ve been in my role for year when should bring this up in the interview process resume also open to resume feedback
help using metaknowledge for bibliometric analysis,am enrolled in data science course at my university and are currently writing thesis for my exam where am doing bibliometric analysis my supervisor mentioned the package called metaknowledge which should be very helpful for doing bibliometrics however according to the documentation it currently only accepts plain text files from pages like web of science scopus pubmed etc my problem is that even though am uploading plain text file with authors references titles year of publish citation count etc downloaded directly from scopus metaknowledge does not recognize it as scopus file and am therefore not able to use the functions of the package have tried reading futher in the documentation and searcing on stackoverflow but without success does anybody have experience using metaknowledge that might shine light on my problem thanks,1,help using metaknowledge for bibliometric analysis,am enrolled in data science course at my university and are currently writing thesis for my exam where am doing bibliometric analysis my supervisor mentioned the package called metaknowledge which should be very helpful for doing bibliometrics however according to the documentation it currently only accepts plain text file from page like web of science scopus pubmed etc my problem is that even though am uploading plain text file with author reference title year of publish citation count etc downloaded directly from scopus metaknowledge doe not recognize it a scopus file and am therefore not able to use the function of the package have tried reading futher in the documentation and searcing on stackoverflow but without success doe anybody have experience using metaknowledge that might shine light on my problem thanks
mac mini m1,am looking at getting mac mini m1 with gb ram and gb ssd have tb external ssd already will this be good route to go for ds computer,1,mac mini m1,am looking at getting mac mini m1 with gb ram and gb ssd have tb external ssd already will this be good route to go for d computer
help regarding career opportunities,hi currently pursuing post grad in ds offline from an institute ve completed basic python and sql module from their curriculum which is almost of their whole syllabus so in this institute there are always placements going on for some or the other fields as its very well known so right now an opening for data analyst has come and placement cell is saying that we should apply for this position as like we should not lose an opportunity to get hired in all sense definitely would like to do job but also want to study further in data science domain and freshly start as data scientist full time or internship it doesn matter as long as it is related to ds so my main question is what will be the best possible outcome regarding career growth and experience apply and possibly get hired for data analyst role while also attending lectures for my ongoing ds course and then transition to ds apply just for internship in data analyst role for months will this experience even count while applying for ds roles in future don apply just focus on your current ongoing course for ds also ve just recently started to get hands on by working on ds projects performing basic tasks from kaggle etc as my python and sql module is completed and soon we will start with python libraries such as pandas and numpy and further down the line with ml and other ml centric modules such as deep learning and ai stuff please through your knowledge help me take concrete decision really confused what will be best for future regarding experience,1,help regarding career opportunity,hi currently pursuing post grad in d offline from an institute ve completed basic python and sql module from their curriculum which is almost of their whole syllabus so in this institute there are always placement going on for some or the other field a it very well known so right now an opening for data analyst ha come and placement cell is saying that we should apply for this position a like we should not lose an opportunity to get hired in all sense definitely would like to do job but also want to study further in data science domain and freshly start a data scientist full time or internship it doesn matter a long a it is related to d so my main question is what will be the best possible outcome regarding career growth and experience apply and possibly get hired for data analyst role while also attending lecture for my ongoing d course and then transition to d apply just for internship in data analyst role for month will this experience even count while applying for d role in future don apply just focus on your current ongoing course for d also ve just recently started to get hand on by working on d project performing basic task from kaggle etc a my python and sql module is completed and soon we will start with python library such a panda and numpy and further down the line with ml and other ml centric module such a deep learning and ai stuff please through your knowledge help me take concrete decision really confused what will be best for future regarding experience
help regarding career opportunities,hi currently pursuing post grad in ds offline from an institute ve completed basic python and sql module from their curriculum which is almost of their whole syllabus so in this institute there are always placements going on for some or the other fields as its very well known so right now an opening for data analyst has come and placement cell is saying that we should apply for this position as like we should not lose an opportunity to get hired in all sense definitely would like to do job but also want to study further in data science domain and freshly start as data scientist full time or internship it doesn matter as long as it is related to ds so my main question is what will be the best possible outcome regarding career growth and experience apply and possibly get hired for data analyst role while also attending lectures for my ongoing ds course and then transition to ds apply just for internship in data analyst role for months will this experience even count while applying for ds roles in future don apply just focus on your current ongoing course for ds also ve just recently started to get hands on by working on ds projects performing basic tasks from kaggle etc as my python and sql module is completed and soon we will start with python libraries such as pandas and numpy and further down the line with ml and other ml centric modules such as deep learning and ai stuff please through your knowledge help me take concrete decision really confused what will be best for future regarding experience,1,help regarding career opportunity,hi currently pursuing post grad in d offline from an institute ve completed basic python and sql module from their curriculum which is almost of their whole syllabus so in this institute there are always placement going on for some or the other field a it very well known so right now an opening for data analyst ha come and placement cell is saying that we should apply for this position a like we should not lose an opportunity to get hired in all sense definitely would like to do job but also want to study further in data science domain and freshly start a data scientist full time or internship it doesn matter a long a it is related to d so my main question is what will be the best possible outcome regarding career growth and experience apply and possibly get hired for data analyst role while also attending lecture for my ongoing d course and then transition to d apply just for internship in data analyst role for month will this experience even count while applying for d role in future don apply just focus on your current ongoing course for d also ve just recently started to get hand on by working on d project performing basic task from kaggle etc a my python and sql module is completed and soon we will start with python library such a panda and numpy and further down the line with ml and other ml centric module such a deep learning and ai stuff please through your knowledge help me take concrete decision really confused what will be best for future regarding experience
are you too data scientist in recent times this buzz word has become reality of many in the technology industry with lot of the,are you too data scientist in recent times this buzz word has become reality of many in the technology industry with lot of the,1,are you too data scientist in recent time this buzz word ha become reality of many in the technology industry with lot of the,are you too data scientist in recent time this buzz word ha become reality of many in the technology industry with lot of the
automated notifications from sql db,hi have the following problem suppose you have database with bunch of values for simplicity suppose it something like the price of stock over time would like to have some automated mechanism that reads the last value every so often and if this read value is bigger than threshold it sends notification through an api what kind of tool would help in that context was trying to do it in retool and can do everything except for the cron job that checks periodically can do it by pressing button any idea would be appreciated,1,automated notification from sql db,hi have the following problem suppose you have database with bunch of value for simplicity suppose it something like the price of stock over time would like to have some automated mechanism that read the last value every so often and if this read value is bigger than threshold it sends notification through an api what kind of tool would help in that context wa trying to do it in retool and can do everything except for the cron job that check periodically can do it by pressing button any idea would be appreciated
check out my github account ll be grateful if you rate it and subscribe,check out my github account ll be grateful if you rate it and subscribe,1,check out my github account ll be grateful if you rate it and subscribe,check out my github account ll be grateful if you rate it and subscribe
graphics for data,hi everyone am looking for free program can use to make data look better pivot tables in excel arent the greatest aesthetically for me does anyone have any suggestions,1,graphic for data,hi everyone am looking for free program can use to make data look better pivot table in excel arent the greatest aesthetically for me doe anyone have any suggestion
is free code camp good resource,just noticed that free code camp which is known for teaching web design has added some new courses for data science the courses seem to be scientific computing with python data analysis with python machine learning in python each one seems to be about hours long has anyone tried them or heard about them and know if they are good haven found any info on them online sorry for any inconvenience and thank you for your help,1,is free code camp good resource,just noticed that free code camp which is known for teaching web design ha added some new course for data science the course seem to be scientific computing with python data analysis with python machine learning in python each one seems to be about hour long ha anyone tried them or heard about them and know if they are good haven found any info on them online sorry for any inconvenience and thank you for your help
part time projects on upwork anyone,first post hi am from india would like to ask the top data scientists here in this sub reddit that has anyone tried doing projects or working part time or full time on fiverr or upwork what are your experiences in freelancing anyone in this sub reddit won any competitions on kaggle or other online site please feel free to brag also if you won any competitions the how much practise did you people do to win that competetion would love to hear experiences of successful people here on this sub reddit also would like to give link link which think every data scientist should have look at,1,part time project on upwork anyone,first post hi am from india would like to ask the top data scientist here in this sub reddit that ha anyone tried doing project or working part time or full time on fiverr or upwork what are your experience in freelancing anyone in this sub reddit won any competition on kaggle or other online site please feel free to brag also if you won any competition the how much practise did you people do to win that competetion would love to hear experience of successful people here on this sub reddit also would like to give link link which think every data scientist should have look at
looking for an advance tableau online course any recommendations,looking for an advance tableau online course any recommendations,1,looking for an advance tableau online course any recommendation,looking for an advance tableau online course any recommendation
how much will companies want to train up candidate on the engineering aspects of data science,an undergrad stats major math minor whose taken lots of coursework with regards to statistics and theory and how to use for analysis took only two software dev courses in java and only know python for data analysis and sql my background and my future interests in getting masters in statistics would put me in the position of analysis man data science archetype someone who could do analysis for days do some modeling in rmarkdown or colab and talk about insights and at minimum put it on dashboard but no real cloud experience software engineering or anything like that ve heard that having stats background in data science is great choice but wondering as to how much coaching get on the engineering aspects of data science know for fact that even though have stats background won be only doing modeling and analysis and be expected to do data engineering and deployment to an extent but my worry is that either end up at company who doesn care enough to teach me it and just pigeonhole me in an analysis role or be expected to learn everything which prefer but the added pressure of learning everything fast despite having minimum software dev experience would my value as stats background be more from product analytics standpoint of something more product facing minimum analysis and no engineering or would be coached and trained to the engineering this may vary of course tremendously on many factors but wanted to hear what you guys had to say,1,how much will company want to train up candidate on the engineering aspect of data science,an undergrad stats major math minor whose taken lot of coursework with regard to statistic and theory and how to use for analysis took only two software dev course in java and only know python for data analysis and sql my background and my future interest in getting master in statistic would put me in the position of analysis man data science archetype someone who could do analysis for day do some modeling in rmarkdown or colab and talk about insight and at minimum put it on dashboard but no real cloud experience software engineering or anything like that ve heard that having stats background in data science is great choice but wondering a to how much coaching get on the engineering aspect of data science know for fact that even though have stats background won be only doing modeling and analysis and be expected to do data engineering and deployment to an extent but my worry is that either end up at company who doesn care enough to teach me it and just pigeonhole me in an analysis role or be expected to learn everything which prefer but the added pressure of learning everything fast despite having minimum software dev experience would my value a stats background be more from product analytics standpoint of something more product facing minimum analysis and no engineering or would be coached and trained to the engineering this may vary of course tremendously on many factor but wanted to hear what you guy had to say
search interesting topic in data quality assessment,hello do you know some interesting topics and research questions for research essay about data quality assessment am new in this field link to industrial applications would be nice but isn necessary would be really thankfully about some inspiration cheers,1,search interesting topic in data quality assessment,hello do you know some interesting topic and research question for research essay about data quality assessment am new in this field link to industrial application would be nice but isn necessary would be really thankfully about some inspiration cheer
engineering masters or data science msc,currently on meng course integrated masters for civil engineering but deciding to switch to data science is it possible to go into data science with masters in engineering and do some courses on the side or is it required that do masters in data science since don have maths or comp sci degree,1,engineering master or data science msc,currently on meng course integrated master for civil engineering but deciding to switch to data science is it possible to go into data science with master in engineering and do some course on the side or is it required that do master in data science since don have math or comp sci degree
what kind of analysis should conduct to see impact from multivariate time series data,able to spot some pattern trend seasonal from my data which has multiple series that are correlated sales order selling price however would love to hear some approach to do further analysis about impact perhaps it causal inference any resources should check,1,what kind of analysis should conduct to see impact from multivariate time series data,able to spot some pattern trend seasonal from my data which ha multiple series that are correlated sale order selling price however would love to hear some approach to do further analysis about impact perhaps it causal inference any resource should check
advice on masters,right now am student currently studying chemical engineering in nus singapore do plan on advancing my career by doing masters am interested in finance and business management however after weeks of reading up online alot of searches turn to show that should instead pursue something leaning towards data science or data analytics and even cs but honestly do find cs extremely hard am in my rd year right now and have about months worth of internship experience right now research analyst intern and have landed an internship for another months working for micron as new product life cycle intern amp x200b do think have good amount of time and plan on using it wisely havent narrowed down what exactly want to do for my masters but if there is anyone could talk to or reach out to to get abit more information about this that would be swell amp x200b thanks wishing everyone happy new year,1,advice on master,right now am student currently studying chemical engineering in nu singapore do plan on advancing my career by doing master am interested in finance and business management however after week of reading up online alot of search turn to show that should instead pursue something leaning towards data science or data analytics and even c but honestly do find c extremely hard am in my rd year right now and have about month worth of internship experience right now research analyst intern and have landed an internship for another month working for micron a new product life cycle intern amp x200b do think have good amount of time and plan on using it wisely havent narrowed down what exactly want to do for my master but if there is anyone could talk to or reach out to to get abit more information about this that would be swell amp x200b thanks wishing everyone happy new year
any advice to engineer to switch to data scientist is msc good starting point,good day everyone am an engineer who wants to start career as data scientist and am looking for some advices based on my background and my goals any comments on my decisions are more than welcome as must have overlooked some factors as an outsider of the ds field background meng in civil engineering in in uk upper second gpa and have been working in this field for years no proper training on computer science and programming except module that matlab was briefly taught but doubt that counts self taught python to beginner intermediate level recently discovered my interest in data and algorithms and have been watching andrew ng courses on youtube am aware of the course is just fraction of ds but would say that enjoyed the entire course so much and truly appreciated the idea of ml in practical perspective believe the demand on ds will continue to grow while engineers might have reached its peak only have smattering of understanding of ds but find it much more interesting and fulfilling than my current career my goals to learn competitive skills and start working in the field of ds to work in the big techs tesla faang etc probably the long term goal questions is ds msc the most direct way to switch to ds is ds msc useful if have no cs background at all is ds msc really worth the time and money what things should be aware of when deciding which course to go for should study in the us if eventually want to work in the us what skillsets are potentially advantageous for applying uni with better ranking reputation huge thanks to everyone any suggestions or some experience sharing will be much appreciated,1,any advice to engineer to switch to data scientist is msc good starting point,good day everyone am an engineer who want to start career a data scientist and am looking for some advice based on my background and my goal any comment on my decision are more than welcome a must have overlooked some factor a an outsider of the d field background meng in civil engineering in in uk upper second gpa and have been working in this field for year no proper training on computer science and programming except module that matlab wa briefly taught but doubt that count self taught python to beginner intermediate level recently discovered my interest in data and algorithm and have been watching andrew ng course on youtube am aware of the course is just fraction of d but would say that enjoyed the entire course so much and truly appreciated the idea of ml in practical perspective believe the demand on d will continue to grow while engineer might have reached it peak only have smattering of understanding of d but find it much more interesting and fulfilling than my current career my goal to learn competitive skill and start working in the field of d to work in the big tech tesla faang etc probably the long term goal question is d msc the most direct way to switch to d is d msc useful if have no c background at all is d msc really worth the time and money what thing should be aware of when deciding which course to go for should study in the u if eventually want to work in the u what skillsets are potentially advantageous for applying uni with better ranking reputation huge thanks to everyone any suggestion or some experience sharing will be much appreciated
what are some jobs available for people who have no qualifications in the field,am hoping someone can point me out in the right direction am interested in the field but have no degree or math skills am not aspiring to be data scientist older early and with no education or background in the field it will be highly improbable almost impossible however have noticed data sicence or data analytic associate or even assistant type of jobs and as this field is growing positive there will be more of these and not all of them will be filled by graduates in the field so what should study to get in this field yes older but can dedicate about hours of study in the next years on this so you will agree that is lot of study my goal is to eventually get wfh digital nomad type of job is this realistic starting point almost but this is what can highlight bachelor in business accounting major master in management quite useless waste if money and time many years of irrelevant experience civil service mostly in social policy never really worked in accounting or finance not that it matters decent excel skills not advanced but can handle some basic data visualisation simple data analysis building dashboards beginning python completed an introductory book python crash course and coded along doing automate the boring stuff now pretty poor math from data science perspective but confident at highschool level math algebra algebra have never done any calculus or linear algebra or anything remotely advanced university introductory course in statistics and some self teaching since nothing advanced but definitely comfortable with basic concepts especially descriptive statistics some basic experience with statistical software spss any advice thoughts where to take this im not sure of should or if should focus my attention on something else web development to get remote job prefer data science many times over web development and would learn more math if necessary however not sure how viable learning math is on my own at that level will need to get really conversant with calculus linear algebra and what about other stuff like relational databases advanced statistics java algorithms bi etc any self taught people here who can offer me advice should try to specialise in something and learn it really well keep in mind that not aspiring to be data scientist am good at self study taught myself foreign languages including english and have decent amount of time to study thanks for reading,1,what are some job available for people who have no qualification in the field,am hoping someone can point me out in the right direction am interested in the field but have no degree or math skill am not aspiring to be data scientist older early and with no education or background in the field it will be highly improbable almost impossible however have noticed data sicence or data analytic associate or even assistant type of job and a this field is growing positive there will be more of these and not all of them will be filled by graduate in the field so what should study to get in this field yes older but can dedicate about hour of study in the next year on this so you will agree that is lot of study my goal is to eventually get wfh digital nomad type of job is this realistic starting point almost but this is what can highlight bachelor in business accounting major master in management quite useless waste if money and time many year of irrelevant experience civil service mostly in social policy never really worked in accounting or finance not that it matter decent excel skill not advanced but can handle some basic data visualisation simple data analysis building dashboard beginning python completed an introductory book python crash course and coded along doing automate the boring stuff now pretty poor math from data science perspective but confident at highschool level math algebra algebra have never done any calculus or linear algebra or anything remotely advanced university introductory course in statistic and some self teaching since nothing advanced but definitely comfortable with basic concept especially descriptive statistic some basic experience with statistical software spss any advice thought where to take this im not sure of should or if should focus my attention on something else web development to get remote job prefer data science many time over web development and would learn more math if necessary however not sure how viable learning math is on my own at that level will need to get really conversant with calculus linear algebra and what about other stuff like relational database advanced statistic java algorithm bi etc any self taught people here who can offer me advice should try to specialise in something and learn it really well keep in mind that not aspiring to be data scientist am good at self study taught myself foreign language including english and have decent amount of time to study thanks for reading
please help,hi just dropped out of university because don have enough money to pay for the tuition then can be data analyst without university degree will some certifications from faang help appreciate all the answers have nice day,1,please help,hi just dropped out of university because don have enough money to pay for the tuition then can be data analyst without university degree will some certification from faang help appreciate all the answer have nice day
best websites paid for historical tick data for equities etfs indices,if anyone could comment below the best reliable and accurate websites for min min intervals of etfs indices equities paying is not problem since it is for work ive looked at quantquote algoseek and kibot but trying to list as many as possible thanks in advance,1,best website paid for historical tick data for equity etf index,if anyone could comment below the best reliable and accurate website for min min interval of etf index equity paying is not problem since it is for work ive looked at quantquote algoseek and kibot but trying to list a many a possible thanks in advance
hi group want to start career in data science what would you recommend to someone new in this journey,hi group want to start career in data science what would you recommend to someone new in this journey,1,hi group want to start career in data science what would you recommend to someone new in this journey,hi group want to start career in data science what would you recommend to someone new in this journey
ms data science in usa,excuse me guys what are cheapest universities provide ds ms in usa note cheapest with good quality,1,m data science in usa,excuse me guy what are cheapest university provide d m in usa note cheapest with good quality
need help to come up with an idea use case for my master thesis project big data ml web3,hey everyone am data engineer who is finishing his master in statistics data analytics and ml and am really struggling to find good idea use case for my graduation thesis project so bear with me on daily basis am working as data engineer with lots of de technologies spark with scala python sql kafka rabbitmq databricks sparkml but started my master in ml because was really interested in machine learning and have solid experience in it with tensorflow sparkml sklearn but recently ve gained lot of interest in web3 too ve started learning lot about web3 technologies like crypto nfts and smart contracts and was able to implement them in react app is there some use case where can do some data analysis on web3 data or where can implement some ml model in order to predict something know that the scope of technologies is very large but if you have any ideas please write them in the comments,1,need help to come up with an idea use case for my master thesis project big data ml web3,hey everyone am data engineer who is finishing his master in statistic data analytics and ml and am really struggling to find good idea use case for my graduation thesis project so bear with me on daily basis am working a data engineer with lot of de technology spark with scala python sql kafka rabbitmq databricks sparkml but started my master in ml because wa really interested in machine learning and have solid experience in it with tensorflow sparkml sklearn but recently ve gained lot of interest in web3 too ve started learning lot about web3 technology like crypto nfts and smart contract and wa able to implement them in react app is there some use case where can do some data analysis on web3 data or where can implement some ml model in order to predict something know that the scope of technology is very large but if you have any idea please write them in the comment
please help,hi just dropped out of university because don have enough money to pay for the tuition then can be data analyst without university degree will some certifications from faang help appreciate all the answers have nice day,1,please help,hi just dropped out of university because don have enough money to pay for the tuition then can be data analyst without university degree will some certification from faang help appreciate all the answer have nice day
how do people from linguistics law humanities and biology end up in data science and make the jobs market crowded how do they do it why,how do people from linguistics law humanities and biology end up in data science and make the jobs market crowded how do they do it why,1,how do people from linguistics law humanity and biology end up in data science and make the job market crowded how do they do it why,how do people from linguistics law humanity and biology end up in data science and make the job market crowded how do they do it why
data science hot topics,hi all still very new to this field so was wondering what people oppinions are of the key topics of today within data science or if they have any articles books podcasts ect they think have interesting ideas or cover these so called hot topics ve seen fair bit on twitter covering machine learning ethics but be curious to see your thoughts on this,1,data science hot topic,hi all still very new to this field so wa wondering what people oppinions are of the key topic of today within data science or if they have any article book podcasts ect they think have interesting idea or cover these so called hot topic ve seen fair bit on twitter covering machine learning ethic but be curious to see your thought on this
is data science boring and mega interesting,hi sorry for the catchy title amp x200b anyway am first semester ai student working part time in an insurance startup like company have been software engineer before and enjoyed it lot but then decided to go into ai because was fascinated by neural networks and now am starting with data science in my company as the first one to ever do datascience there so have lot of possibilities and freedom in work few days in to the new data science role am kind of bored from what have experienced of the work is just cleaning data which is not the most interesting work for me it is okay but it sure does not excite me on the other hand are modelling training evaluating which are absolutely fascinating in my opinion amp x200b but also troubleshooting model is more like alchemy than engineering coming from software engineering were debugging is straight forward the trouble shooting in data science is also such bad experience amp x200b from this kind of regret my choice of getting into ai data science is this general observation or do you think different had different jobs,1,is data science boring and mega interesting,hi sorry for the catchy title amp x200b anyway am first semester ai student working part time in an insurance startup like company have been software engineer before and enjoyed it lot but then decided to go into ai because wa fascinated by neural network and now am starting with data science in my company a the first one to ever do datascience there so have lot of possibility and freedom in work few day in to the new data science role am kind of bored from what have experienced of the work is just cleaning data which is not the most interesting work for me it is okay but it sure doe not excite me on the other hand are modelling training evaluating which are absolutely fascinating in my opinion amp x200b but also troubleshooting model is more like alchemy than engineering coming from software engineering were debugging is straight forward the trouble shooting in data science is also such bad experience amp x200b from this kind of regret my choice of getting into ai data science is this general observation or do you think different had different job
legalities of collection and redistribution of information from apis or web scraping,when working with data that is collected from api or web scraping where are the legal boundaries ve been working on projects where would like to store and share data from web scraping and apis but don know if its technically allowed or what problems could encounter when doing so ve seen sources saying that if it violates terms of service or user agreements the website may block your ip or ban you but can pursue legal action as long as the information is publicly available but ve also seen individuals saying that you can get in legal trouble for redistributing information from apis amp x200b does anyone have some tips advice or even good resources for the legalities of collection and redistribution of information form apis and web scraping,1,legality of collection and redistribution of information from apis or web scraping,when working with data that is collected from api or web scraping where are the legal boundary ve been working on project where would like to store and share data from web scraping and apis but don know if it technically allowed or what problem could encounter when doing so ve seen source saying that if it violates term of service or user agreement the website may block your ip or ban you but can pursue legal action a long a the information is publicly available but ve also seen individual saying that you can get in legal trouble for redistributing information from apis amp x200b doe anyone have some tip advice or even good resource for the legality of collection and redistribution of information form apis and web scraping
why is distributed computing system important necessary for big data what is one example of use of distributed system in big data,here are the notes in my college curriculum which of course understand but it doesn make clear what is the role of distributed system in big data these are some tutorials that try to explain this topic but imo fail to do so they don really explain the need of distributed system in big data already have studied subject called distributed system this was our syllabus studied it really well still have hipster pdas of this subject to reference upon,1,why is distributed computing system important necessary for big data what is one example of use of distributed system in big data,here are the note in my college curriculum which of course understand but it doesn make clear what is the role of distributed system in big data these are some tutorial that try to explain this topic but imo fail to do so they don really explain the need of distributed system in big data already have studied subject called distributed system this wa our syllabus studied it really well still have hipster pda of this subject to reference upon
which elective course should pick if want to become machine learning engineer currently student of bachelor of computer science hons artificial intelligence,which elective course should pick if want to become machine learning engineer currently student of bachelor of computer science hons artificial intelligence,1,which elective course should pick if want to become machine learning engineer currently student of bachelor of computer science hons artificial intelligence,which elective course should pick if want to become machine learning engineer currently student of bachelor of computer science hons artificial intelligence
model deployment,how can learn on model deployment productionalizations it is clear to me that at least in my current organization data scientist don do model deployment we need to hand it over to it dept or some specialized experts this is an absolute black box for me and want to learn the skill set where can start what platforms are better for beginners in this topic thanks in advance,1,model deployment,how can learn on model deployment productionalizations it is clear to me that at least in my current organization data scientist don do model deployment we need to hand it over to it dept or some specialized expert this is an absolute black box for me and want to learn the skill set where can start what platform are better for beginner in this topic thanks in advance
career change from biological field to data science in my any tips how does this plan sound,did my undergraduate degree in neuroscience biology based so little quantitative sciences besides st year calc and stats since then ve acquired experience in academic and industry settings but shift to career in data science is something aiming for as approach my mid thirties in few years don have strong quantitative background so was planning on applying to online ms degrees in data science pursue this part time while working my biotech job however there are lot of prerequisites for these programs that lacking and so wondering if it would be good idea to enroll in certificate programs or boot camps to address these knowledge gaps structured learner so don think excel in finding out everything need to know on my own especially since have full time job on my plate has anyone done anything similar or have suggestions as to how can go about this career change thank you,1,career change from biological field to data science in my any tip how doe this plan sound,did my undergraduate degree in neuroscience biology based so little quantitative science besides st year calc and stats since then ve acquired experience in academic and industry setting but shift to career in data science is something aiming for a approach my mid thirty in few year don have strong quantitative background so wa planning on applying to online m degree in data science pursue this part time while working my biotech job however there are lot of prerequisite for these program that lacking and so wondering if it would be good idea to enroll in certificate program or boot camp to address these knowledge gap structured learner so don think excel in finding out everything need to know on my own especially since have full time job on my plate ha anyone done anything similar or have suggestion a to how can go about this career change thank you
want to become data scientist where so begin have bachelors degree in business and know some basic stuff about sql and how do become data scientist,want to become data scientist where so begin have bachelors degree in business and know some basic stuff about sql and how do become data scientist,1,want to become data scientist where so begin have bachelor degree in business and know some basic stuff about sql and how do become data scientist,want to become data scientist where so begin have bachelor degree in business and know some basic stuff about sql and how do become data scientist
is there good dataset of commonly paired english words with frequency of use,is there good dataset of commonly paired english words with frequency of use,1,is there good dataset of commonly paired english word with frequency of use,is there good dataset of commonly paired english word with frequency of use
for non academic industry not research machine learning career what is the best choice between phd in machine learning and mbb analytics bcg gamma bain analytics quantumblack,ve now yoe as data scientist and was wondering which career move to make know that don want to do research in the long term but phd experience interest me for the learning and may be must have to do serious machine learning work mbb provide great business experience and clear career path what would you choose between those two option and why doing both may not be possible don know if mbb hire yoe phd,1,for non academic industry not research machine learning career what is the best choice between phd in machine learning and mbb analytics bcg gamma bain analytics quantumblack,ve now yoe a data scientist and wa wondering which career move to make know that don want to do research in the long term but phd experience interest me for the learning and may be must have to do serious machine learning work mbb provide great business experience and clear career path what would you choose between those two option and why doing both may not be possible don know if mbb hire yoe phd
what sort of things would you expect data science intern to know,what sort of things would you expect data science intern to know,1,what sort of thing would you expect data science intern to know,what sort of thing would you expect data science intern to know
this paper is little over my head but am excited about quantifying non monotonic associations between variables what your take,this paper is little over my head but am excited about quantifying non monotonic associations between variables what your take,1,this paper is little over my head but am excited about quantifying non monotonic association between variable what your take,this paper is little over my head but am excited about quantifying non monotonic association between variable what your take
need help on unsupervised learning,have survey dataset with no ground truth labels there have applied unsupervised learning techniques and took clusters have done normalization and pca dimension reduction before getting the clusters using kmeans clustering algorithm then tried to make prediction model where used supervised techniques and got accuracy in cross validation using svm now have question am doing anything wrong having the clusters using kmeans and using those for model training is there anything wrong in the process or am in the correct track any type of guidance will be helpful for me,1,need help on unsupervised learning,have survey dataset with no ground truth label there have applied unsupervised learning technique and took cluster have done normalization and pca dimension reduction before getting the cluster using kmeans clustering algorithm then tried to make prediction model where used supervised technique and got accuracy in cross validation using svm now have question am doing anything wrong having the cluster using kmeans and using those for model training is there anything wrong in the process or am in the correct track any type of guidance will be helpful for me
future of cloud data engineer scientist,am currently working with azure and gcp on cloud with etl is there vast future grow to here or will be limited to only learning pre built tools from azure or gcp,1,future of cloud data engineer scientist,am currently working with azure and gcp on cloud with etl is there vast future grow to here or will be limited to only learning pre built tool from azure or gcp
data science in europe,if you have data science masters degree from european university how much weight does the school make when applying for good paying jobs in europe if the school name does matter what universities are reputable for data science masters what do european employers think of having masters in data science from somewhat reputable american university does it have an advantage or disadvantage when comparing from european universities or is it neutral some background my ultimate goal is to live in western europe have an european citizenship but live in the us doing an online data science masters with an american university while work full time as process engineer debating whether it make more sense to quit what doing now to do masters program in europe and find job there afterwards or finish the masters here and then move to europe with job getting burned out from working full time and studying at the same time so quitting to do masters in europe sounds appealing but that mean that won be having good income for years any insight from data scientists in europe will help thanks,1,data science in europe,if you have data science master degree from european university how much weight doe the school make when applying for good paying job in europe if the school name doe matter what university are reputable for data science master what do european employer think of having master in data science from somewhat reputable american university doe it have an advantage or disadvantage when comparing from european university or is it neutral some background my ultimate goal is to live in western europe have an european citizenship but live in the u doing an online data science master with an american university while work full time a process engineer debating whether it make more sense to quit what doing now to do master program in europe and find job there afterwards or finish the master here and then move to europe with job getting burned out from working full time and studying at the same time so quitting to do master in europe sound appealing but that mean that won be having good income for year any insight from data scientist in europe will help thanks
high school advice and internship question,hi senior in high school applying to colleges as data science major was wondering if anyone has any advice on how to get exposure to the field before starting college or how to find internships at the beginner level thank you,1,high school advice and internship question,hi senior in high school applying to college a data science major wa wondering if anyone ha any advice on how to get exposure to the field before starting college or how to find internship at the beginner level thank you
college sophomore need advice for learning improving data analysis skills,hi am currently college sophomore in the us double majoring in mathematics and computer science so far have been interested in data analysis and data visualization as potential career path think am good in terms of academics but am lacking technical skills which suppose is common issue participated in summer research mentoring program which introduced me to and my mentor suggested me to go through hadley for data science book got through half of it and used some of my knowledge to create poster presentation it was pretty simple with few tables summary stats and linear aggression then the fall semester started so couldn keep up my independent study also work as research assistant but haven done much work there other than running files or storing data in excel sheets the professor work with recently introduced me to sql he said its really easy to learn so kinda know bit of that now also know some java from the cs class took this semester other than that have bunch of leadership experience but its not really relevant to the field tried applying to summer internships but didn get any responses think some of my problems are lack of technical skills experience being sophomore feel like most companies are looking for juniors seniors and being minor will be in few weeks but entered college bit early so am not yet of legal age lot of internship applications ask if you re of legal age which think also puts me at disadvantage either way decided to stop applying to internships and wait for my junior year in the meantime thought would work on my resume am currently planning to finish the hadley book and maybe ask the professor work with if he has any projects can participate in from my internship search know lot of companies also look for python but am hoping to learn that through one of my cs classes in my junior year other than that am not sure what else to do know need to learn different skills relevant to the data analysis field but then what am supposed to do after have learned the content people say to practice but don understand what that means even with feel like have forgotten lot of what learned few months ago would probably remember stuff once pick up the hadley book but how can make sure that maintain my skills know said will ask my professor but am not sure if he will have something for me if not will probably have to practice with my own projects but don get how that works like where do start how do come up with project on my own how do find way to apply my skills and not only maintain them but also improve this was long post but wanted to give enough context and what my current plans are am basically looking for any advice pertaining to the questions asked or the data analytics fields in general also if you think should try learning some other skills before applying to internships next year let me know of those as well,1,college sophomore need advice for learning improving data analysis skill,hi am currently college sophomore in the u double majoring in mathematics and computer science so far have been interested in data analysis and data visualization a potential career path think am good in term of academic but am lacking technical skill which suppose is common issue participated in summer research mentoring program which introduced me to and my mentor suggested me to go through hadley for data science book got through half of it and used some of my knowledge to create poster presentation it wa pretty simple with few table summary stats and linear aggression then the fall semester started so couldn keep up my independent study also work a research assistant but haven done much work there other than running file or storing data in excel sheet the professor work with recently introduced me to sql he said it really easy to learn so kinda know bit of that now also know some java from the c class took this semester other than that have bunch of leadership experience but it not really relevant to the field tried applying to summer internship but didn get any response think some of my problem are lack of technical skill experience being sophomore feel like most company are looking for junior senior and being minor will be in few week but entered college bit early so am not yet of legal age lot of internship application ask if you re of legal age which think also put me at disadvantage either way decided to stop applying to internship and wait for my junior year in the meantime thought would work on my resume am currently planning to finish the hadley book and maybe ask the professor work with if he ha any project can participate in from my internship search know lot of company also look for python but am hoping to learn that through one of my c class in my junior year other than that am not sure what else to do know need to learn different skill relevant to the data analysis field but then what am supposed to do after have learned the content people say to practice but don understand what that mean even with feel like have forgotten lot of what learned few month ago would probably remember stuff once pick up the hadley book but how can make sure that maintain my skill know said will ask my professor but am not sure if he will have something for me if not will probably have to practice with my own project but don get how that work like where do start how do come up with project on my own how do find way to apply my skill and not only maintain them but also improve this wa long post but wanted to give enough context and what my current plan are am basically looking for any advice pertaining to the question asked or the data analytics field in general also if you think should try learning some other skill before applying to internship next year let me know of those a well
cpu selection recommendations for running python sql and data viz tools power bi tableau,cpu selection recommendations for running python sql and data viz tools power bi tableau,1,cpu selection recommendation for running python sql and data viz tool power bi tableau,cpu selection recommendation for running python sql and data viz tool power bi tableau
need advice on pursuing data science as profession,received my undergraduate degree in informatics with concentration in data science and currently in master program for data science but not really sure how to get experience outside of the classroom to make me better candidate for internships and jobs also love to know more about working in the field but don personally know many people in data science was wondering if anyone had any insight on different ways could improve my skills or somehow prove to companies that have experience in the areas they re looking for was also hoping someone could provide more information on different specializations within the field as well as things they personally did wish they did to put themselves in good place professionally speaking thank you in advance,1,need advice on pursuing data science a profession,received my undergraduate degree in informatics with concentration in data science and currently in master program for data science but not really sure how to get experience outside of the classroom to make me better candidate for internship and job also love to know more about working in the field but don personally know many people in data science wa wondering if anyone had any insight on different way could improve my skill or somehow prove to company that have experience in the area they re looking for wa also hoping someone could provide more information on different specialization within the field a well a thing they personally did wish they did to put themselves in good place professionally speaking thank you in advance
data being split more than expected with writing to parquet using pandas,here is my simple code to write to parquet using pandas df to_parquet output_path this is what df is supposed to look like id cosine_similarity sub_id_list some more details on the columns cosine_similarity cosine similarity matrix sub_id_list list of all of the sub_id within id when read the parquet files in my directory in pyspark my cosine similarity matrix ends up splitting on auto generated index level index_level_0__ cosine_similarity sub_id id with the way it split won be able to leverage my cosine_sim matrix right now my data is being split into parquet files when start adding more data to this flow there will be well over unique id wondering if this would resolve itself if include id as partition_col,1,data being split more than expected with writing to parquet using panda,here is my simple code to write to parquet using panda df to_parquet output_path this is what df is supposed to look like id cosine_similarity sub_id_list some more detail on the column cosine_similarity cosine similarity matrix sub_id_list list of all of the sub_id within id when read the parquet file in my directory in pyspark my cosine similarity matrix end up splitting on auto generated index level index_level_0__ cosine_similarity sub_id id with the way it split won be able to leverage my cosine_sim matrix right now my data is being split into parquet file when start adding more data to this flow there will be well over unique id wondering if this would resolve itself if include id a partition_col
should reject hold out on job offers to do phd,currently working as research assistant data scientist on project at my alma mater pay is ok but was mostly in it because the project could have positive policy implications and it also just nice thing to have on my resum aside from that working on publication that should be done before june which is around when my current contract ends my idea was always to leverage the connections ve made and the publication to land an interesting phd position as plan had been applying to strong mle data science positions in industry too my strategy here was to draw out the interview process as long as possible to more or less keep the position open for me so could buy time to see what interesting phd positions come up one of the best positions was interviewing for probably caught wind of this unexpectedly reduced the amount of interviews from to and made formal offer recently with week max to sign the dotted line as for job content would say it pretty good all round actual data science position there varied amount of projects ranging from standard uplift modelling to computer vision and nlp the company has full data analyst business intelligence division so dashboarding would not be an immediate part of my responsibilities this was my priority when applying nothing wrong with dashboarding but generally bad dislike story telling with data or anything frontend related total comp is strong too say it is less than phd in the beginning in terms of raw salary once you factor in the bonusses it more in addition to the fact that there structure of periodic raises and phd salary remains static for years it is very enticing to say the least both in content and salary the thing is that if would have an interesting phd position come up right now would accept that even if it financially worse decision in the short term my biggest problem right now is that think it quite weird and unreasonable to reject job offer based on position offer that may or may not even come what do you think reddit,1,should reject hold out on job offer to do phd,currently working a research assistant data scientist on project at my alma mater pay is ok but wa mostly in it because the project could have positive policy implication and it also just nice thing to have on my resum aside from that working on publication that should be done before june which is around when my current contract end my idea wa always to leverage the connection ve made and the publication to land an interesting phd position a plan had been applying to strong mle data science position in industry too my strategy here wa to draw out the interview process a long a possible to more or le keep the position open for me so could buy time to see what interesting phd position come up one of the best position wa interviewing for probably caught wind of this unexpectedly reduced the amount of interview from to and made formal offer recently with week max to sign the dotted line a for job content would say it pretty good all round actual data science position there varied amount of project ranging from standard uplift modelling to computer vision and nlp the company ha full data analyst business intelligence division so dashboarding would not be an immediate part of my responsibility this wa my priority when applying nothing wrong with dashboarding but generally bad dislike story telling with data or anything frontend related total comp is strong too say it is le than phd in the beginning in term of raw salary once you factor in the bonus it more in addition to the fact that there structure of periodic raise and phd salary remains static for year it is very enticing to say the least both in content and salary the thing is that if would have an interesting phd position come up right now would accept that even if it financially worse decision in the short term my biggest problem right now is that think it quite weird and unreasonable to reject job offer based on position offer that may or may not even come what do you think reddit
is decision science really thing or is it more marketing buzz,work in bi and have read about decision sciences especially the chief decision scientist at google go figure and was wondering if this is growing function seems to me that what bi and analytics are meant to cover anyone with experience or insight into decision science,1,is decision science really thing or is it more marketing buzz,work in bi and have read about decision science especially the chief decision scientist at google go figure and wa wondering if this is growing function seems to me that what bi and analytics are meant to cover anyone with experience or insight into decision science
resume review yrs experience feeling curious,resume review yrs experience feeling curious,1,resume review yr experience feeling curious,resume review yr experience feeling curious
how was that science used in law enforcement,does anyone have any links to blogs videos or podcasts that may be useful in in determining to roll data science in law enforcement,1,how wa that science used in law enforcement,doe anyone have any link to blog video or podcasts that may be useful in in determining to roll data science in law enforcement
statistics full university course on data science basics over hours,statistics full university course on data science basics over hours,1,statistic full university course on data science basic over hour,statistic full university course on data science basic over hour
mfge major interested in data science,hello everyone so quarters away from graduating with manufacturing engineering degree from cal poly slo recently have been looking into career in data science as it involves statistics which really enjoy as some of you may know manufacturing engineering is subfield of industrial engineering which have heard is viable way of entering the field of data science have some free time these next two quarters as only need one class for winter quarter and classes for spring purchased the course the data science course complete data science bootcamp by careers team on udemy to get some knowledge experience the course is broken down into the following parts probability statistics python advanced statistical methods linear algebra deep learning the only two sections which weren covered in my curriculum in college were advanced statistical methods and deep learning besides completing this course what else can do in the next months as get closer to graduation to make myself more likely to get hired as data scientist thanks,1,mfge major interested in data science,hello everyone so quarter away from graduating with manufacturing engineering degree from cal poly slo recently have been looking into career in data science a it involves statistic which really enjoy a some of you may know manufacturing engineering is subfield of industrial engineering which have heard is viable way of entering the field of data science have some free time these next two quarter a only need one class for winter quarter and class for spring purchased the course the data science course complete data science bootcamp by career team on udemy to get some knowledge experience the course is broken down into the following part probability statistic python advanced statistical method linear algebra deep learning the only two section which weren covered in my curriculum in college were advanced statistical method and deep learning besides completing this course what else can do in the next month a get closer to graduation to make myself more likely to get hired a data scientist thanks
santa gift for the data science world free gan nfts,santa gift for the data science world free gan nfts,1,santa gift for the data science world free gan nfts,santa gift for the data science world free gan nfts
covid case data and us airline travel data together in panel regression,covid case data and us airline travel data together in panel regression,1,covid case data and u airline travel data together in panel regression,covid case data and u airline travel data together in panel regression
confusion and indecisiveness about data science data analysis,hello everyone hope you can help me with this have graduated computer science for months now my graduation project is what made me so interested in ds because of the deep learning models and image classification had to self study just to complete it ever since graduated was looking for job in data science found out the being data scientist requires masters degree in most cases which can afford right now so found out that can start as junior data analyst then climb my way up to data scientist unfortunately can find job in that either because every job apply to requires year or two of experience which really don understand how to get as fresh graduate who can even find an internship to get that said experience anyhow am currently improving my skill set as much as possible by learning sql tableau data wrangling using python and it associated libraries playing with some deep learning models and machine learning techniques from time to time but still can find job please tell me what am doing wrong how can improve my hiring chance ps have reached point where no one interviews me or even calls me to give reply am desperate and don know if should just change careers and go away from data science just to try to make living,1,confusion and indecisiveness about data science data analysis,hello everyone hope you can help me with this have graduated computer science for month now my graduation project is what made me so interested in d because of the deep learning model and image classification had to self study just to complete it ever since graduated wa looking for job in data science found out the being data scientist requires master degree in most case which can afford right now so found out that can start a junior data analyst then climb my way up to data scientist unfortunately can find job in that either because every job apply to requires year or two of experience which really don understand how to get a fresh graduate who can even find an internship to get that said experience anyhow am currently improving my skill set a much a possible by learning sql tableau data wrangling using python and it associated library playing with some deep learning model and machine learning technique from time to time but still can find job please tell me what am doing wrong how can improve my hiring chance p have reached point where no one interview me or even call me to give reply am desperate and don know if should just change career and go away from data science just to try to make living
data analyst position interview,hello good data enthausiasts on my final year as business intelligence amp erp engineering student have got myself an interview for talent acquisition program for data analysts where will do my required internship and spend just over year learning then working with company not sure whether can mention the name here farly good with sql and selection of tools used during my studies ssms talend powerbi ve also had basic practice with sap bw and abap the interview is minutes then minutes for case study then another for debating my solution and this is where bit lost is this case study some sort of counseling like thing or is it more technical and coding oriented need help on how to prepare for this and examples of how interviews like this happen plus if you have any more advice or ressources please write them below or link me to them would appreciate any help,1,data analyst position interview,hello good data enthausiasts on my final year a business intelligence amp erp engineering student have got myself an interview for talent acquisition program for data analyst where will do my required internship and spend just over year learning then working with company not sure whether can mention the name here farly good with sql and selection of tool used during my study ssms talend powerbi ve also had basic practice with sap bw and abap the interview is minute then minute for case study then another for debating my solution and this is where bit lost is this case study some sort of counseling like thing or is it more technical and coding oriented need help on how to prepare for this and example of how interview like this happen plus if you have any more advice or ressources please write them below or link me to them would appreciate any help
tips for career transition into ds,hello friend of mine has over years experience in sales and marketing mostly in the entertainment industry but also spent some time in the medical and real estate sectors he has always been exceptional in math and is experienced in business and he wants to get into ds he took cal berkeley ds certificate program but his undergrad is in music business he doesn have specific sector in mind but knows he does not want to be in music entertainment ever again what are some tips you suggest for career transitioner like him as opposed to new grad trying to get his first ds job,1,tip for career transition into d,hello friend of mine ha over year experience in sale and marketing mostly in the entertainment industry but also spent some time in the medical and real estate sector he ha always been exceptional in math and is experienced in business and he want to get into d he took cal berkeley d certificate program but his undergrad is in music business he doesn have specific sector in mind but know he doe not want to be in music entertainment ever again what are some tip you suggest for career transitioner like him a opposed to new grad trying to get his first d job
changing data culture in an organization,what strategies have you found successful in changing data culture within you organization my current role has involved porting legacy department projects that were excel access based and moving them to python sas and or sql while we ve gotten handle on these legacy department projects the wider organization still has the same problems for example our budget team takes output from variety of sources and hand enters it into new spreadsheet to develop set of what if budget projection scenarios within excel basically what have you found migrates subject matter experts from requesting the raw data they need to requesting the final product they needs,1,changing data culture in an organization,what strategy have you found successful in changing data culture within you organization my current role ha involved porting legacy department project that were excel access based and moving them to python sa and or sql while we ve gotten handle on these legacy department project the wider organization still ha the same problem for example our budget team take output from variety of source and hand enters it into new spreadsheet to develop set of what if budget projection scenario within excel basically what have you found migrates subject matter expert from requesting the raw data they need to requesting the final product they need
books on model deployment,hi everyone was reading ds book suggestions recommendations megathread although model deployment is crucial skill for data scientist all books in the thread are about statistics and machine learning then did quick research on past posts could only find the practical mlops by reilly therefore would like to create post about model deployment books please post model deployment books that you have found particularly interesting or helpful for learning during your career include the title with either an author or link thank you very much,1,book on model deployment,hi everyone wa reading d book suggestion recommendation megathread although model deployment is crucial skill for data scientist all book in the thread are about statistic and machine learning then did quick research on past post could only find the practical mlops by reilly therefore would like to create post about model deployment book please post model deployment book that you have found particularly interesting or helpful for learning during your career include the title with either an author or link thank you very much
resume review yrs experience feeling curious,resume review yrs experience feeling curious,1,resume review yr experience feeling curious,resume review yr experience feeling curious
what is better bsc in data science or masters in data science,hi everybody have year general bsc concertation in math and chem want to pursue data science career and have two options online masters program us or bsc in data science canada both will take years to complete however the bsc has year co op program so do not know what to do which option will better prepare me for landing job in the field by the way the bsc is year program but am able to transfer years form my first degree bit more about myself that could be relevant was self employed for years after my bsc so do not have any work experience since didn work for anybody years old so kinda feel like don have time to waste or do the wrong move the bsc in data science program is new so technically will be the first to graduate from it english is not my first language so my apologies for all the grammatical mistakes thank you for your time,1,what is better bsc in data science or master in data science,hi everybody have year general bsc concertation in math and chem want to pursue data science career and have two option online master program u or bsc in data science canada both will take year to complete however the bsc ha year co op program so do not know what to do which option will better prepare me for landing job in the field by the way the bsc is year program but am able to transfer year form my first degree bit more about myself that could be relevant wa self employed for year after my bsc so do not have any work experience since didn work for anybody year old so kinda feel like don have time to waste or do the wrong move the bsc in data science program is new so technically will be the first to graduate from it english is not my first language so my apology for all the grammatical mistake thank you for your time
creating an optimization algorithm for cost function for nn,is possible to find an article or an example of new optimization algorithm for cost function for nn,1,creating an optimization algorithm for cost function for nn,is possible to find an article or an example of new optimization algorithm for cost function for nn
android tablet for data scince,hi am data scientist and spend my time with numpy scipy scikit pandas and tensorflow and million other packages am searching for something light and cheap top do some work on the side mostly develop some code do some beta testing and let the server do the major work so do not wnat to the latest and greatest machine only something light and portable to do work on te go so here the question can use an android table with keyboard and pydroid or something close to anaconda does it work love spyder as ide and absolutely hate jupiter what ide can use,1,android tablet for data scince,hi am data scientist and spend my time with numpy scipy scikit panda and tensorflow and million other package am searching for something light and cheap top do some work on the side mostly develop some code do some beta testing and let the server do the major work so do not wnat to the latest and greatest machine only something light and portable to do work on te go so here the question can use an android table with keyboard and pydroid or something close to anaconda doe it work love spyder a ide and absolutely hate jupiter what ide can use
model agnostic feature importance,hi all wanted to check if there is technique to estimate feature importance which is model agnostic thanks,1,model agnostic feature importance,hi all wanted to check if there is technique to estimate feature importance which is model agnostic thanks
optimising and minimising ml models,give me two ways to minimize ml model error how would optimize linear regression for is this question about error metrics or about techniques to improve accuracy for is this to do with gradient descent or something else,1,optimising and minimising ml model,give me two way to minimize ml model error how would optimize linear regression for is this question about error metric or about technique to improve accuracy for is this to do with gradient descent or something else
machine learning use cases in telecom industry,apart from churn prediction customer segmentation and anomaly detection how is data science used in the telecommunications sector are you aware of industry research use cases in that field,1,machine learning use case in telecom industry,apart from churn prediction customer segmentation and anomaly detection how is data science used in the telecommunication sector are you aware of industry research use case in that field
what do you do with your crypto art nft after you have bought them,what do you do with your crypto art nft after you have bought them,1,what do you do with your crypto art nft after you have bought them,what do you do with your crypto art nft after you have bought them
advice needed how to learn data science,have not been able to pin down one correct way or order of studying the various areas of data science for instance keep spiralling from one term to the next and from one course leaving it in between lot of times to the next know should stick to one thing first and then go on to the next but in between there always something or the other such as notebook on kaggle or person github projects that get me distracted and then well ofc don have proper pathway to study all of this in an orderly fashion just today thought resume the udemy course was doing earlier left due to other academic commitments and then came across someone kaggle github profile saying they did notebook on descriptive stats hence making me wonder if should fist do stats course and the most important and confusing point of it all is what areas out of all these should focus on to secure data science internship job in an established brand as well as what kind of projects or achievements or profiles should do to showcase what ve learnt if someone could advice me with this it would be damn appreciated help me out please,1,advice needed how to learn data science,have not been able to pin down one correct way or order of studying the various area of data science for instance keep spiralling from one term to the next and from one course leaving it in between lot of time to the next know should stick to one thing first and then go on to the next but in between there always something or the other such a notebook on kaggle or person github project that get me distracted and then well ofc don have proper pathway to study all of this in an orderly fashion just today thought resume the udemy course wa doing earlier left due to other academic commitment and then came across someone kaggle github profile saying they did notebook on descriptive stats hence making me wonder if should fist do stats course and the most important and confusing point of it all is what area out of all these should focus on to secure data science internship job in an established brand a well a what kind of project or achievement or profile should do to showcase what ve learnt if someone could advice me with this it would be damn appreciated help me out please
tips for college senior last semester,hello ds community am senior undergrad in his last semester on cycle will be joining as ds at manga faang company wondering if you got any tips on what should do my last semester to kickoff my career since interested in doing product in the near term planning to join startup as intern during the semester instead of taking challenging classes already done with major courses stat thoughts on this also any advice is welcome,1,tip for college senior last semester,hello d community am senior undergrad in his last semester on cycle will be joining a d at manga faang company wondering if you got any tip on what should do my last semester to kickoff my career since interested in doing product in the near term planning to join startup a intern during the semester instead of taking challenging class already done with major course stat thought on this also any advice is welcome
is tableau good software to become data visualization designer without learning to code,brand designer for almost years and had client from vc backed data company that needed help to redesign its charts and graphs to their new brand it was simple task for me however they pay great this got me into data visualization design and stuff and it would be interesting to learn new skill for the entire to all professionals here how your career with this software do you think this skill is great combo for brand graphic designers without learning code,1,is tableau good software to become data visualization designer without learning to code,brand designer for almost year and had client from vc backed data company that needed help to redesign it chart and graph to their new brand it wa simple task for me however they pay great this got me into data visualization design and stuff and it would be interesting to learn new skill for the entire to all professional here how your career with this software do you think this skill is great combo for brand graphic designer without learning code
like to find large sample of accounts on twitter that tweet around certain subject how would you approach this without paying too much for api access,could start by taking some known users and then checking who they are following and so on but it quickly gets out of hand some of them follow people so if then check each of them it will be huge operation is there better way,1,like to find large sample of account on twitter that tweet around certain subject how would you approach this without paying too much for api access,could start by taking some known user and then checking who they are following and so on but it quickly get out of hand some of them follow people so if then check each of them it will be huge operation is there better way
advice for data science beginner,hi all fairly new to the data science machine learning world taught myself some stuff during my phd and published paper using some basic ml feature engineering hyperparameter optimization etc at my job charged with analyzing some plant data and my boss says he ll help me get into more data science roles next year but my degree is in chemical engineering and my job title is research engineer so data isn all do in my free time ve been getting into some kaggle got top in my first competition and top in my second so improving also took an ml class so familiar with the basic algorithms taught myself enough python for kaggle and my paper but don know much about data structures classes etc some questions what is the dream job of someone in this career what career path pays the most was looking at faang salaries and they look to only be around that seems quite low for bay area get paid more now when adjusting for col at less famous company would it look good on my resume to side hustle in data ve been looking for some gigs on freelancer does this look good is it realistic to aim for faang if didn get my degree in computers and am not at top company didn graduate at the top of my class want to stay an engineer rather than scientist how do make sure stay on an engineer track in this field thanks in advance,1,advice for data science beginner,hi all fairly new to the data science machine learning world taught myself some stuff during my phd and published paper using some basic ml feature engineering hyperparameter optimization etc at my job charged with analyzing some plant data and my bos say he ll help me get into more data science role next year but my degree is in chemical engineering and my job title is research engineer so data isn all do in my free time ve been getting into some kaggle got top in my first competition and top in my second so improving also took an ml class so familiar with the basic algorithm taught myself enough python for kaggle and my paper but don know much about data structure class etc some question what is the dream job of someone in this career what career path pay the most wa looking at faang salary and they look to only be around that seems quite low for bay area get paid more now when adjusting for col at le famous company would it look good on my resume to side hustle in data ve been looking for some gig on freelancer doe this look good is it realistic to aim for faang if didn get my degree in computer and am not at top company didn graduate at the top of my class want to stay an engineer rather than scientist how do make sure stay on an engineer track in this field thanks in advance
elitemini hx90 for data science any thought,am going to buy computer for data science analysis work ran into the elitemini hx90 which has good specs given the price has anybody used this for data work or else any thoughts and experience would be much appreciated here is the link to the product,1,elitemini hx90 for data science any thought,am going to buy computer for data science analysis work ran into the elitemini hx90 which ha good spec given the price ha anybody used this for data work or else any thought and experience would be much appreciated here is the link to the product
how did you advance your nlp career,dear all if there are any nlp ml engineers ds or researchers out there could really use some advice am graduating from my ms in economics with full time job lined job as ds at well known fintech company however it is driving me crazy to find clear path forward to pursue more nlp involved job down the line here is what currently have that can be classified as nlp experiences past internships have done anything from product management intern for data products powered by nlp to management consultant doing research on the data collection strategies that client could take to improve their nlp classification outcome research am writing paper with researchers from nlp for applying nlp techniques to public policy related documents and is due to publish in the next couple of months current job the team that am currently on and hired into that have been interning on uses lot of nlp for insights discovery we also plan on launching large scale nlp product down the line which will be very involved in given our very lean corporate structure why think will have hard time advancing in the field do not have cs undergrad or ms in cs my background in economics dictated that am good at math but not at linguistics do not come from hyper prestigious school like stanford or mit but mid tier school in the east coast us feel everyone in the field is so overqualified for what they are doing granted people may just be very good imposters have no clue what to do should go get an mscs to compete down the line how does moving up in nlp careers work can any folks shine some light on very confused young person will literally take any suggestions or advice haha thank all,1,how did you advance your nlp career,dear all if there are any nlp ml engineer d or researcher out there could really use some advice am graduating from my m in economics with full time job lined job a d at well known fintech company however it is driving me crazy to find clear path forward to pursue more nlp involved job down the line here is what currently have that can be classified a nlp experience past internship have done anything from product management intern for data product powered by nlp to management consultant doing research on the data collection strategy that client could take to improve their nlp classification outcome research am writing paper with researcher from nlp for applying nlp technique to public policy related document and is due to publish in the next couple of month current job the team that am currently on and hired into that have been interning on us lot of nlp for insight discovery we also plan on launching large scale nlp product down the line which will be very involved in given our very lean corporate structure why think will have hard time advancing in the field do not have c undergrad or m in c my background in economics dictated that am good at math but not at linguistics do not come from hyper prestigious school like stanford or mit but mid tier school in the east coast u feel everyone in the field is so overqualified for what they are doing granted people may just be very good imposter have no clue what to do should go get an msc to compete down the line how doe moving up in nlp career work can any folk shine some light on very confused young person will literally take any suggestion or advice haha thank all
questions about starting my data career,just completed my master in statistics also have bachelor in statistics most of my studies were very mathematical and theoretical didn apply lot of the concepts studied in real world setting and am not super proficient in several programming languages or software am fairly good at and intermediate in sql my question is what kind of software programming languages do need to familiarize myself with so that can be competitive when look for job there are several things but asking about the major stuff not looking for any big data engineering or data science job just the tools needed for an entry level data analyst job thank you very much,1,question about starting my data career,just completed my master in statistic also have bachelor in statistic most of my study were very mathematical and theoretical didn apply lot of the concept studied in real world setting and am not super proficient in several programming language or software am fairly good at and intermediate in sql my question is what kind of software programming language do need to familiarize myself with so that can be competitive when look for job there are several thing but asking about the major stuff not looking for any big data engineering or data science job just the tool needed for an entry level data analyst job thank you very much
advice for math econ music student,hi all recently joined this community because am preparing for the job market and am looking into analytics or data science as career path currently completing bm in piano performance ba in economics and math minor although may switch the last two programs to math econ degree ve done few small research papers for class projects that required me to use econometric models in and stata to predict some value tourism activity obesity rates etc using various regression methods am quite comfortable performing basic visualization and regression ols lasso but did pretty poorly in my calc and classes and linear algebra bs and cs while doing well in econometrics statistics and big data classes all as have basic experience in stata sql and excel can really tell how far have to go to be good data scientist and am planning to post some projects here over the next few months in the meanntime hoping to get some advice have three main questions what is the best way to improve my data analytics science skills should do certification course or just focus on doing projects are there are any interesting specializations of data science in music music industry any way to bring music into this should focus on becoming proficient in or should jump to learning python python seems to be the industry standard at this point thanks for any help,1,advice for math econ music student,hi all recently joined this community because am preparing for the job market and am looking into analytics or data science a career path currently completing bm in piano performance ba in economics and math minor although may switch the last two program to math econ degree ve done few small research paper for class project that required me to use econometric model in and stata to predict some value tourism activity obesity rate etc using various regression method am quite comfortable performing basic visualization and regression ols lasso but did pretty poorly in my calc and class and linear algebra b and c while doing well in econometrics statistic and big data class all a have basic experience in stata sql and excel can really tell how far have to go to be good data scientist and am planning to post some project here over the next few month in the meanntime hoping to get some advice have three main question what is the best way to improve my data analytics science skill should do certification course or just focus on doing project are there are any interesting specialization of data science in music music industry any way to bring music into this should focus on becoming proficient in or should jump to learning python python seems to be the industry standard at this point thanks for any help
how to use cumulative distribution function to detect performance bottlenecks explained using sports example statistics and data science,how to use cumulative distribution function to detect performance bottlenecks explained using sports example statistics and data science,1,how to use cumulative distribution function to detect performance bottleneck explained using sport example statistic and data science,how to use cumulative distribution function to detect performance bottleneck explained using sport example statistic and data science
advice needed how to learn data science,have not been able to pin down one correct way or order of studying the various areas of data science for instance keep spiralling from one term to the next and from one course leaving it in between lot of times to the next know should stick to one thing first and then go on to the next but in between there always something or the other such as notebook on kaggle or person github projects that get me distracted and then well ofc don have proper pathway to study all of this in an orderly fashion just today thought resume the udemy course was doing earlier left due to other academic commitments and then came across someone kaggle github profile saying they did notebook on descriptive stats hence making me wonder if should fist do stats course and the most important and confusing point of it all is what areas out of all these should focus on to secure data science internship job in an established brand if someone could advice me with this it would be damn appreciated help me out please,1,advice needed how to learn data science,have not been able to pin down one correct way or order of studying the various area of data science for instance keep spiralling from one term to the next and from one course leaving it in between lot of time to the next know should stick to one thing first and then go on to the next but in between there always something or the other such a notebook on kaggle or person github project that get me distracted and then well ofc don have proper pathway to study all of this in an orderly fashion just today thought resume the udemy course wa doing earlier left due to other academic commitment and then came across someone kaggle github profile saying they did notebook on descriptive stats hence making me wonder if should fist do stats course and the most important and confusing point of it all is what area out of all these should focus on to secure data science internship job in an established brand if someone could advice me with this it would be damn appreciated help me out please
what iq do you need to have in order to be data scientist,so am looking for serious answers have heard that data science is hard obviously and iq plays big factor into it the average iq of data scientist is around but it has been told there other factors into it such as conscientiousness but that is about of success in data science what is required as someone who doesn have extremely high intelligence what should do,1,what iq do you need to have in order to be data scientist,so am looking for serious answer have heard that data science is hard obviously and iq play big factor into it the average iq of data scientist is around but it ha been told there other factor into it such a conscientiousness but that is about of success in data science what is required a someone who doesn have extremely high intelligence what should do
golang or rust do they have place in data science,am curious what this communities opinion is about either golang or rust for data science is anyone using them does anyone anticipate either language overtaking python or in the future what opinion does anyone have of either language in this practice if were to choose one of these languages to learn which would be better choice,1,golang or rust do they have place in data science,am curious what this community opinion is about either golang or rust for data science is anyone using them doe anyone anticipate either language overtaking python or in the future what opinion doe anyone have of either language in this practice if were to choose one of these language to learn which would be better choice
what algorithm am looking for here,have fixed bunch of cluster centers and bunch of points want to assign to these clusters however each cluster has certain limit for of points that can be assigned to it this is not hard limit with more points over the limit assigned to the cluster it should be increasingly difficult this is modified nearest neighbor search problem or kmeans cluster with fixed center however can think of any way dealing with the constrain part except iteratively recalculate the distances between points and center with some sort of distance penalty function applied any tips,1,what algorithm am looking for here,have fixed bunch of cluster center and bunch of point want to assign to these cluster however each cluster ha certain limit for of point that can be assigned to it this is not hard limit with more point over the limit assigned to the cluster it should be increasingly difficult this is modified nearest neighbor search problem or kmeans cluster with fixed center however can think of any way dealing with the constrain part except iteratively recalculate the distance between point and center with some sort of distance penalty function applied any tip
how great do you need to be at coding to become data scientist,hello everyone am physics undergrad considering getting master in statistics to one day become data scientist have very basic level of skill in python coding don really love to code but can do it for work if need to curious about ds jobs because like math and stats should stay away from data science unless love to code thank you,1,how great do you need to be at coding to become data scientist,hello everyone am physic undergrad considering getting master in statistic to one day become data scientist have very basic level of skill in python coding don really love to code but can do it for work if need to curious about d job because like math and stats should stay away from data science unless love to code thank you
data science practice sites,hackerrank gt great for sql and python practice www hackerrank com kaggle gt ml competitions and tutorials www kaggle com datacamp aceai data science interview prep www aceainow com coursera udacity courses,1,data science practice site,hackerrank gt great for sql and python practice www hackerrank com kaggle gt ml competition and tutorial www kaggle com datacamp aceai data science interview prep www aceainow com coursera udacity course
what companies think ai looks like vs what actually it is,what companies think ai looks like vs what actually it is,1,what company think ai look like v what actually it is,what company think ai look like v what actually it is
how do you import google maps listings into google sheets with importhtml,how do you import google maps listings into google sheets with importhtml,1,how do you import google map listing into google sheet with importhtml,how do you import google map listing into google sheet with importhtml
need help with importing list of businesses from the kohler website into google sheets using the importhtml function any advice is appreciated,url dealer locator kohler power need specific list for location clarksville md,1,need help with importing list of business from the kohler website into google sheet using the importhtml function any advice is appreciated,url dealer locator kohler power need specific list for location clarksville md
where to start what should read study,study now sociology and had very interesting lecture about networks etc the teacher is data scientist and thinking about that maybe want to be data scientist soooo the question is where should start,1,where to start what should read study,study now sociology and had very interesting lecture about network etc the teacher is data scientist and thinking about that maybe want to be data scientist soooo the question is where should start
can be data analyst without degree,hi just dropped out of university because don have enough money to pay for the tuition so then can be data analyst without university degree will some certifications from faang help appreciate all the answers have nice day,1,can be data analyst without degree,hi just dropped out of university because don have enough money to pay for the tuition so then can be data analyst without university degree will some certification from faang help appreciate all the answer have nice day
what are employers looking for internships,so just finished my masters in data science and im looking to get internships in amsterdam however lack projects to do know to avoid titanic and iris data sets but don want to waste my time on projects as dont have much free time amp x200b would something along the lines of this be enough get data from and api store it on cloud clean eda machine learning model power bi to demonstrate amp x200b is it better to do many or just big one many thanks,1,what are employer looking for internship,so just finished my master in data science and im looking to get internship in amsterdam however lack project to do know to avoid titanic and iris data set but don want to waste my time on project a dont have much free time amp x200b would something along the line of this be enough get data from and api store it on cloud clean eda machine learning model power bi to demonstrate amp x200b is it better to do many or just big one many thanks
weekly entering amp transitioning thread dec jan,welcome to this week entering amp transitioning thread this thread is for any questions about getting started studying or transitioning into the data science field topics include learning resources books tutorials videos traditional education schools degrees electives alternative education online courses bootcamps job search questions resumes applying career prospects elementary questions where to start what next while you wait for answers from the community check out the faq and resources resources pages on our wiki you can also search for answers in past weekly threads restrict_sr amp sort new,1,weekly entering amp transitioning thread dec jan,welcome to this week entering amp transitioning thread this thread is for any question about getting started studying or transitioning into the data science field topic include learning resource book tutorial video traditional education school degree elective alternative education online course bootcamps job search question resume applying career prospect elementary question where to start what next while you wait for answer from the community check out the faq and resource resource page on our wiki you can also search for answer in past weekly thread restrict_sr amp sort new
seeking to learn data analysis,hello everyone im looking to learn data science dont have experience in programming that much just basic python so need to know how can start studying is there any courses for beginners that might help me any recommendations for good courses or where shall start excuse me because english is not my first language,1,seeking to learn data analysis,hello everyone im looking to learn data science dont have experience in programming that much just basic python so need to know how can start studying is there any course for beginner that might help me any recommendation for good course or where shall start excuse me because english is not my first language
finally figured out nearest neighbors,they are and,1,finally figured out nearest neighbor,they are and
ml coding,is it important to remember each codebase of every algorithm and every step or is it fine to use google in real time am getting problems in coding is it normal or how can improve it,1,ml coding,is it important to remember each codebase of every algorithm and every step or is it fine to use google in real time am getting problem in coding is it normal or how can improve it
need mentor for becoming better data scientist,hi am currently working in the analytics domain for one year and have one year of experience in doing research credited my master thesis however still feel bit lost when it comes to deciding which courses do need to do and which skills should develop so that can get an upgrade from my current job feel that the internet has all the resources for learning and working on problems however lack sense of direction how can find some mentorship for growth in my professional life,1,need mentor for becoming better data scientist,hi am currently working in the analytics domain for one year and have one year of experience in doing research credited my master thesis however still feel bit lost when it come to deciding which course do need to do and which skill should develop so that can get an upgrade from my current job feel that the internet ha all the resource for learning and working on problem however lack sense of direction how can find some mentorship for growth in my professional life
need help deciding between two offers,hello everyone so am in the fortunate position of deciding between two quite different offers quick background have been working on contract as junior data scientist for the last year or so since graduating college at big name fortune company in designated artificial intelligence lab ve loved working in this field so far great work life balance at my current company and great culture just received an offer for permanent role as data scientist at current company at the same time received an offer to join the analytics department of professional sports organization the role is pretty decent drop in pay vs and is going to be less pure research of my time on day to day team operations and ad hoc analysis on longer term research but comes with all the perks of working for professional sports team free games interaction with players and personnel behind the scenes look at draft trades etc with the ability to actually weigh in on team decisions basically the kind of role any sports fan would geek out over so anyways couple more specific questions if left my current role with only year of experience in data science and became more of sql analyst type for the next few years to work in sports would it be easy to transition back into true data science roles or would likely get type casted as business analyst type for people who have moved from data science roles to more analyst type roles did you find yourself less satisfied in your job overall any advice is appreciated thank you,1,need help deciding between two offer,hello everyone so am in the fortunate position of deciding between two quite different offer quick background have been working on contract a junior data scientist for the last year or so since graduating college at big name fortune company in designated artificial intelligence lab ve loved working in this field so far great work life balance at my current company and great culture just received an offer for permanent role a data scientist at current company at the same time received an offer to join the analytics department of professional sport organization the role is pretty decent drop in pay v and is going to be le pure research of my time on day to day team operation and ad hoc analysis on longer term research but come with all the perk of working for professional sport team free game interaction with player and personnel behind the scene look at draft trade etc with the ability to actually weigh in on team decision basically the kind of role any sport fan would geek out over so anyways couple more specific question if left my current role with only year of experience in data science and became more of sql analyst type for the next few year to work in sport would it be easy to transition back into true data science role or would likely get type casted a business analyst type for people who have moved from data science role to more analyst type role did you find yourself le satisfied in your job overall any advice is appreciated thank you
blue light glasses,silly question but any experience with recommendations for blue lights glasses finding myself having trouble falling asleep and experiencing some eye strain when on laptop for hours day,1,blue light glass,silly question but any experience with recommendation for blue light glass finding myself having trouble falling asleep and experiencing some eye strain when on laptop for hour day
sql training week,hey happy christmas starting with week of training on sql from th dec st dec with microsoft sql server from am pm est online training through zoom live training with real data sets and scenarios recording shared after session only for to learn for hrs of session,1,sql training week,hey happy christmas starting with week of training on sql from th dec st dec with microsoft sql server from am pm est online training through zoom live training with real data set and scenario recording shared after session only for to learn for hr of session
azure certifications for students,hello am masters in analytics student who is trying to get his certifications in ai fundamentals and data fundamentals will sitting down and earning those certifications mean something or will companies just ignore extra effort have connections and all but knowing hr am not sure if hr will even understand the value of the work or they will rather select bs compsci over me bcom amp it in other worlds still not get recognized just because know the work and can do it but for some reason am still not considered not technical enough or taken seriously despite doing the work and knowing it through practical application and being in top schools in the world am just really concerned that time and resources that are spent ontop of all the projects and networking will go to nothing and still have net value like would they even look at my resume even after all the effort put mean am learning this for genuine value because think this will really help me succeed but have reached the point of asking will they even look at my resume,1,azure certification for student,hello am master in analytics student who is trying to get his certification in ai fundamental and data fundamental will sitting down and earning those certification mean something or will company just ignore extra effort have connection and all but knowing hr am not sure if hr will even understand the value of the work or they will rather select b compsci over me bcom amp it in other world still not get recognized just because know the work and can do it but for some reason am still not considered not technical enough or taken seriously despite doing the work and knowing it through practical application and being in top school in the world am just really concerned that time and resource that are spent ontop of all the project and networking will go to nothing and still have net value like would they even look at my resume even after all the effort put mean am learning this for genuine value because think this will really help me succeed but have reached the point of asking will they even look at my resume
how smart do you have to be to do data science data engineering,am in my third year of college and am studying business which dislike due to the poor job prospects am interested in putting my hand in data analysis because like the idea of helping business making better decisions however am slow learner and it very possible that have low iq however am willing to give something shot than rather be quitter any advice on how to get started,1,how smart do you have to be to do data science data engineering,am in my third year of college and am studying business which dislike due to the poor job prospect am interested in putting my hand in data analysis because like the idea of helping business making better decision however am slow learner and it very possible that have low iq however am willing to give something shot than rather be quitter any advice on how to get started
good way to segment test results for insight or narrative,hey all ve been using heterogenous modeling for tests to find which users respond better to the treatments cate what wondering is if anyone here has good way to find decent density segments that you could highlight to non ds partners even though have scores for every user want to say something like android users in south america and users who play mobile games between the ages of responded really well to this change and make up of the uplift could probably come at this by fitting simple uplift decision tree but trying to get something that isn as hierarchical based,1,good way to segment test result for insight or narrative,hey all ve been using heterogenous modeling for test to find which user respond better to the treatment cate what wondering is if anyone here ha good way to find decent density segment that you could highlight to non d partner even though have score for every user want to say something like android user in south america and user who play mobile game between the age of responded really well to this change and make up of the uplift could probably come at this by fitting simple uplift decision tree but trying to get something that isn a hierarchical based
how important are internships,hello everybody am third year statistics major and economics minor at university in california with focus in data science have been looking for quite while and have yet to get single internship over my college career getting quite discouraged especially since lot of positions can even apply to in the first place since they explicitly want ms or phd students and that context leads me into my question how important really is getting an internship for data science career will me lacking any internship experience be severe detriment to my future job prospects thank you for your time,1,how important are internship,hello everybody am third year statistic major and economics minor at university in california with focus in data science have been looking for quite while and have yet to get single internship over my college career getting quite discouraged especially since lot of position can even apply to in the first place since they explicitly want m or phd student and that context lead me into my question how important really is getting an internship for data science career will me lacking any internship experience be severe detriment to my future job prospect thank you for your time
altruism and data science,so just saw the social dilemma on netflix although know netflix likes to dramatise lot of things in documentaries do believe there is lot of truth in the topics covered in this particular documentary the documentary is about how the ai behind social media platforms is essentially harming the society and health of its users for profit maximisation self interest are the engineers that created these social media algorithms ai to get us addicted to our screens also considered data scientists if so what are some examples of data science companies projects that do not exploit others for individual interest only with my knowledge on data science jobs the top tier jobs are always at the big companies like google facebook netflix amazon people will say you have made it when you get into such company am actually looking to get into data science ai after my current studies however am not excited to get job at any of these big companies at all feel like will just help them obtain analyse even more data to exploit even more people exploit people even more what are some more altruistic jobs for data scientist that still pays decently what is your opinion on this anyway,1,altruism and data science,so just saw the social dilemma on netflix although know netflix like to dramatise lot of thing in documentary do believe there is lot of truth in the topic covered in this particular documentary the documentary is about how the ai behind social medium platform is essentially harming the society and health of it user for profit maximisation self interest are the engineer that created these social medium algorithm ai to get u addicted to our screen also considered data scientist if so what are some example of data science company project that do not exploit others for individual interest only with my knowledge on data science job the top tier job are always at the big company like google facebook netflix amazon people will say you have made it when you get into such company am actually looking to get into data science ai after my current study however am not excited to get job at any of these big company at all feel like will just help them obtain analyse even more data to exploit even more people exploit people even more what are some more altruistic job for data scientist that still pay decently what is your opinion on this anyway
how do prepare to manage lead teams little experience doing this,been an ic for years now will be analytics manager in new firm with folks eventually reporting under me what resources would best prepare me,1,how do prepare to manage lead team little experience doing this,been an ic for year now will be analytics manager in new firm with folk eventually reporting under me what resource would best prepare me
question about new york times dataset,hi am student working with nyt dataset that contains information on front page nyt articles from with most information coming from am trying to find some interesting questions to answer based on this dataset some information this dataset contains is title of the front page author of the front page article content snippet of some of the writing month the piece was released and if the article was print or web version do not have any knowledge of journalism or news consumption so my question is this if you had access to this information what kinds of questions would you want answered thanks for any help,1,question about new york time dataset,hi am student working with nyt dataset that contains information on front page nyt article from with most information coming from am trying to find some interesting question to answer based on this dataset some information this dataset contains is title of the front page author of the front page article content snippet of some of the writing month the piece wa released and if the article wa print or web version do not have any knowledge of journalism or news consumption so my question is this if you had access to this information what kind of question would you want answered thanks for any help
peoplesanspeople unity human centric synthetic data generator better data than imagenet pre training for human centric computer vision tasks comes with coco compliant json annotations,webpage paper source code papers with code demo video dudb70dc summary peoplesanspeople is human centric data generator provided by unity technologies that contains highly parametric and simulation ready human assets parameterized lighting and camera system parameterized environment generators and fully manipulable and extensible domain randomizers peoplesanspeople can generate rgb images with sub pixel perfect bounding box coco compliant human keypoints and semantic instance segmentation masks in json annotation files all packaged in macos and linux executable binaries capable of generating datasets in addition we release template unity environment for lowering the barrier of entry and getting you started with creating your own highly parameterized human centric synth data generator we affectionately named our synthetic data generator peoplesanspeople as it is data generator aimed at human centric computer vision without using human data which bears serious privacy safety ethical bias and legal concerns benchmarks the domain randomization we used for our benchmarks are nave brute forced sweeps through the pre chosen range of parameters as such we end up generating psychedelic looking scenes which turned out to train more performant models for human centric computer vision using peoplesanspeople we benchmarked detectron2 keypoint cnn variant results indicate synthetic pre training with our data outperforms results of training on real data alone or pre training with imagenet both in limited and abundant data regimes we envisage that this freely available data generator should enable wide range of research into the emerging field of simulation to real transfer learning in the critical area of human centric computer vision,1,peoplesanspeople unity human centric synthetic data generator better data than imagenet pre training for human centric computer vision task come with coco compliant json annotation,webpage paper source code paper with code demo video dudb70dc summary peoplesanspeople is human centric data generator provided by unity technology that contains highly parametric and simulation ready human asset parameterized lighting and camera system parameterized environment generator and fully manipulable and extensible domain randomizers peoplesanspeople can generate rgb image with sub pixel perfect bounding box coco compliant human keypoints and semantic instance segmentation mask in json annotation file all packaged in macos and linux executable binary capable of generating datasets in addition we release template unity environment for lowering the barrier of entry and getting you started with creating your own highly parameterized human centric synth data generator we affectionately named our synthetic data generator peoplesanspeople a it is data generator aimed at human centric computer vision without using human data which bear serious privacy safety ethical bias and legal concern benchmark the domain randomization we used for our benchmark are nave brute forced sweep through the pre chosen range of parameter a such we end up generating psychedelic looking scene which turned out to train more performant model for human centric computer vision using peoplesanspeople we benchmarked detectron2 keypoint cnn variant result indicate synthetic pre training with our data outperforms result of training on real data alone or pre training with imagenet both in limited and abundant data regime we envisage that this freely available data generator should enable wide range of research into the emerging field of simulation to real transfer learning in the critical area of human centric computer vision
peoplesanspeople unity human centric synthetic data generator better data than imagenet pre training for human centric computer vision tasks comes with coco formatted json data,webpage paper source code papers with code demo video dudb70dc summary peoplesanspeople is human centric data generator provided by unity technologies that contains highly parametric and simulation ready human assets parameterized lighting and camera system parameterized environment generators and fully manipulable and extensible domain randomizers peoplesanspeople can generate rgb images with sub pixel perfect bounding box coco compliant human keypoints and semantic instance segmentation masks in json annotation files all packaged in macos and linux executable binaries capable of generating datasets in addition we release template unity environment for lowering the barrier of entry and getting you started with creating your own highly parameterized human centric synth data generator we affectionately named our synthetic data generator peoplesanspeople as it is data generator aimed at human centric computer vision without using human data which bears serious privacy safety ethical bias and legal concerns benchmarks the domain randomization we used for our benchmarks are nave brute forced sweeps through the pre chosen range of parameters as such we end up generating psychedelic looking scenes which turned out to train more performant models for human centric computer vision using peoplesanspeople we benchmarked detectron2 keypoint cnn variant results indicate synthetic pre training with our data outperforms results of training on real data alone or pre training with imagenet both in limited and abundant data regimes we envisage that this freely available data generator should enable wide range of research into the emerging field of simulation to real transfer learning in the critical area of human centric computer vision,1,peoplesanspeople unity human centric synthetic data generator better data than imagenet pre training for human centric computer vision task come with coco formatted json data,webpage paper source code paper with code demo video dudb70dc summary peoplesanspeople is human centric data generator provided by unity technology that contains highly parametric and simulation ready human asset parameterized lighting and camera system parameterized environment generator and fully manipulable and extensible domain randomizers peoplesanspeople can generate rgb image with sub pixel perfect bounding box coco compliant human keypoints and semantic instance segmentation mask in json annotation file all packaged in macos and linux executable binary capable of generating datasets in addition we release template unity environment for lowering the barrier of entry and getting you started with creating your own highly parameterized human centric synth data generator we affectionately named our synthetic data generator peoplesanspeople a it is data generator aimed at human centric computer vision without using human data which bear serious privacy safety ethical bias and legal concern benchmark the domain randomization we used for our benchmark are nave brute forced sweep through the pre chosen range of parameter a such we end up generating psychedelic looking scene which turned out to train more performant model for human centric computer vision using peoplesanspeople we benchmarked detectron2 keypoint cnn variant result indicate synthetic pre training with our data outperforms result of training on real data alone or pre training with imagenet both in limited and abundant data regime we envisage that this freely available data generator should enable wide range of research into the emerging field of simulation to real transfer learning in the critical area of human centric computer vision
it vs is for data careers,is major in information systems or information technology better to work in data related careers,1,it v is for data career,is major in information system or information technology better to work in data related career
instagram influencers,does anyone know of good source to get instagram influencers in field we currently pay person to use their tiktok mobile api which has been fantastic however the instagram api is rate limited unstable and only shows top posts for each hashtag search amp x200b input hashtag output instagram post or username signature of followers,1,instagram influencers,doe anyone know of good source to get instagram influencers in field we currently pay person to use their tiktok mobile api which ha been fantastic however the instagram api is rate limited unstable and only show top post for each hashtag search amp x200b input hashtag output instagram post or username signature of follower
low hour w2 ds jobs,currently have remote ds job with no benefits what are some low hour options for ds jobs where can get w2 income doesn really matter plus benefits healthcare life insurance,1,low hour w2 d job,currently have remote d job with no benefit what are some low hour option for d job where can get w2 income doesn really matter plus benefit healthcare life insurance
am years old is it late to chase career in the data science field,hello everybody am years old and don have much working experience tbh have finished masters degree from university computer science department introducing me to data science in am currently living with my mother together unfortunately my mother had stroke in september and it took time to recover almost years the good thing is my mother completely recovered and she is back in her routine during the period of felt that what learned from the master degree in computer science specialized in data science was not enough to understand the whole concept of data science so started to learn from online platforms and tried to build portfolio of projects and expand my data science skills from udacity and kaggle platforms mainly focused on data analysis with python pandas building models with machine learning and deep learning tried to apply for jobs between the period of but couldn pass the introductory interviews or coding interviews and had many rejections almost months from today stopped searching for jobs cause got disappointed with so many rejections had and lost my focus on becoming data scientist want to make change to myself this year don want to stay unemployed anymore so would like to ask the community if it is late for years old guy like me to pursue career in data science want to return back to the job market due to the pandemic want to search for remote jobs and am currently revisiting back my old projects to refresh and remember again the coding analytical and machine learning skills gained in the past need to thank you all in advance merry xmas everyone and happy holidays looking forward to the data science community feedback any advice will be more than welcomed,1,am year old is it late to chase career in the data science field,hello everybody am year old and don have much working experience tbh have finished master degree from university computer science department introducing me to data science in am currently living with my mother together unfortunately my mother had stroke in september and it took time to recover almost year the good thing is my mother completely recovered and she is back in her routine during the period of felt that what learned from the master degree in computer science specialized in data science wa not enough to understand the whole concept of data science so started to learn from online platform and tried to build portfolio of project and expand my data science skill from udacity and kaggle platform mainly focused on data analysis with python panda building model with machine learning and deep learning tried to apply for job between the period of but couldn pas the introductory interview or coding interview and had many rejection almost month from today stopped searching for job cause got disappointed with so many rejection had and lost my focus on becoming data scientist want to make change to myself this year don want to stay unemployed anymore so would like to ask the community if it is late for year old guy like me to pursue career in data science want to return back to the job market due to the pandemic want to search for remote job and am currently revisiting back my old project to refresh and remember again the coding analytical and machine learning skill gained in the past need to thank you all in advance merry xmas everyone and happy holiday looking forward to the data science community feedback any advice will be more than welcomed
is it worth taking master in ds amp ai in,hello there in summary just finished my bachelors degree in economics and have always been in love with analyzing data creating great visualization and so on so forth now have the opportunity to take master degree in uk this uni is university in uk so am going to invest at least for this the only field that have been interested so far is data science regardless it is sometimes considered buzzword or bubble even though we wont hear about dt anymore in the future is it useful to have degree in stem subject from your point of view is it worth taking master degree in ds amp ai nowadays what other close options could have fields closely related to ds amp ai it would be pretty miserable to invest so important amount of money when could take bootcamp or watch tutorials online what would you do if you were me tysm guys,1,is it worth taking master in d amp ai in,hello there in summary just finished my bachelor degree in economics and have always been in love with analyzing data creating great visualization and so on so forth now have the opportunity to take master degree in uk this uni is university in uk so am going to invest at least for this the only field that have been interested so far is data science regardless it is sometimes considered buzzword or bubble even though we wont hear about dt anymore in the future is it useful to have degree in stem subject from your point of view is it worth taking master degree in d amp ai nowadays what other close option could have field closely related to d amp ai it would be pretty miserable to invest so important amount of money when could take bootcamp or watch tutorial online what would you do if you were me tysm guy
any final word of advice,hi folks am about to start applying for colleges in ms ds programs bit tight on deadlines but it is what it is would also like to tell you that have chosen universities dropping gre as an evaluation criterion for fall can someone help me confirm if should go with the following universities and if not then what should replace each one with uw umich gsu neu columbia umn twin cities suny buffalo usf usc,1,any final word of advice,hi folk am about to start applying for college in m d program bit tight on deadline but it is what it is would also like to tell you that have chosen university dropping gre a an evaluation criterion for fall can someone help me confirm if should go with the following university and if not then what should replace each one with uw umich gsu neu columbia umn twin city suny buffalo usf usc
sampling from the posterior be like,sampling from the posterior be like,1,sampling from the posterior be like,sampling from the posterior be like
data career pivot advice requested,hello currently work at ohio state university and want to use my tuition assistance in order to move into data analytics from an unrelated social science field ultimately want to become data scientist in the healthcare industry after reviewing options planning on completing the certificate of practice in data analytics then completing the masters of translational data analytics what feedback do you have on the plan or the programs advice is encouraged notes in my masters want to focus on data as tool to increase health outcomes quality of life for historically disadvantaged communities theres certificate program in biomedical informatics that would like to complete recently completed coding bootcamp and will continue to study to strengthen my programming and technical skills,1,data career pivot advice requested,hello currently work at ohio state university and want to use my tuition assistance in order to move into data analytics from an unrelated social science field ultimately want to become data scientist in the healthcare industry after reviewing option planning on completing the certificate of practice in data analytics then completing the master of translational data analytics what feedback do you have on the plan or the program advice is encouraged note in my master want to focus on data a tool to increase health outcome quality of life for historically disadvantaged community there certificate program in biomedical informatics that would like to complete recently completed coding bootcamp and will continue to study to strengthen my programming and technical skill
remote work during graduate school possible,hello am junior at college will graduate in spring with degree in statistics and will continue with graduate school at my current university with fellowship in statistics or data science my research mentor says graduate program will open soon but not sure if there will be fellowships available for those during graduate school will have more than enough time to work full time job am about minutes from st louis and if could find job there might be able to commute but am really hoping might be able to find part time remote job as statistician during my graduate school years is it realistic goal for me to have part time job while doing graduate school,1,remote work during graduate school possible,hello am junior at college will graduate in spring with degree in statistic and will continue with graduate school at my current university with fellowship in statistic or data science my research mentor say graduate program will open soon but not sure if there will be fellowship available for those during graduate school will have more than enough time to work full time job am about minute from st louis and if could find job there might be able to commute but am really hoping might be able to find part time remote job a statistician during my graduate school year is it realistic goal for me to have part time job while doing graduate school
the best way to make decisions about the accuracy of data,am currently working on project and have found two different data sources for my research would like to do comparative analysis between the two of them to determine which one of them is more accurate before continue can you suggest me what would be the best way to do this thanks in advance,1,the best way to make decision about the accuracy of data,am currently working on project and have found two different data source for my research would like to do comparative analysis between the two of them to determine which one of them is more accurate before continue can you suggest me what would be the best way to do this thanks in advance
the no free lunch theorem,the no free lunch theorem,1,the no free lunch theorem,the no free lunch theorem
availability of remote work,hi thinking of transitioning to ds career year manufacturing quality engineer had been looking to move into quality then operations management but am dealing with some family health issues ve got year old daughter and my wife has newly diagnosed autoimmune disease it not life threatening right now but not confident that my wife will make it to see our daughter graduate high school if that were to happen remote position would help me take the best care of my daughter are remote or mostly remote positions common in the field so thinking of making the transition think it would be good fit for me ve had some projects over my career that intersect with programming in general and they reliably bring me into my flow state,1,availability of remote work,hi thinking of transitioning to d career year manufacturing quality engineer had been looking to move into quality then operation management but am dealing with some family health issue ve got year old daughter and my wife ha newly diagnosed autoimmune disease it not life threatening right now but not confident that my wife will make it to see our daughter graduate high school if that were to happen remote position would help me take the best care of my daughter are remote or mostly remote position common in the field so thinking of making the transition think it would be good fit for me ve had some project over my career that intersect with programming in general and they reliably bring me into my flow state
msc in data science versus online certifications,hello new here and thinking of career change have background in clinical research and was thinking of going into the medical data field do any of your have experience with employers preferring an msc versus online certifications in data science python machine learning would appreciate any advice,1,msc in data science versus online certification,hello new here and thinking of career change have background in clinical research and wa thinking of going into the medical data field do any of your have experience with employer preferring an msc versus online certification in data science python machine learning would appreciate any advice
any free data analytics bootcamp,hello everyone is there any free data analytics bootcamp for certification that are funded by the government need to learn microsoft sql and tableau specifically as am studying health care information thank you,1,any free data analytics bootcamp,hello everyone is there any free data analytics bootcamp for certification that are funded by the government need to learn microsoft sql and tableau specifically a am studying health care information thank you
what can do with computer and data science economics degree,what title says also how lucrative is this joint and double degree compared to the likes of finance data science major or business data science what kind of jobs can find with all of the degrees listed where at thank you so much for your input,1,what can do with computer and data science economics degree,what title say also how lucrative is this joint and double degree compared to the like of finance data science major or business data science what kind of job can find with all of the degree listed where at thank you so much for your input
what are the top places to work as new grad data scientist wlb,hi guys for some background doing bs in industrial engineering cs minor ms in analytics at georgia tech ve read lots of things about how some ds positions are positions where you write sql queries all day or be dashboard guy this kind of work is extremely boring to me and want to avoid companies where ll have to do this stuff as new grad still have more internships left before looking for ft roles need some guidance on what companies should be targeting given my experience what companies will be able to do meaningful ds work what are the top companies for wlb faang what would starting salaries look like at these places,1,what are the top place to work a new grad data scientist wlb,hi guy for some background doing b in industrial engineering c minor m in analytics at georgia tech ve read lot of thing about how some d position are position where you write sql query all day or be dashboard guy this kind of work is extremely boring to me and want to avoid company where ll have to do this stuff a new grad still have more internship left before looking for ft role need some guidance on what company should be targeting given my experience what company will be able to do meaningful d work what are the top company for wlb faang what would starting salary look like at these place
seeking honesty about future in this field,did double major in computer science and information systems from not so prestigious university as compared to those locally then proceeded to do bootcamp in data science from the more well known institutions due to the lack of substantial projects back in uni as of right now am on job hunt and while doing so have discovered so much about how lack the skills to even take up an entry level data scientist job refuse to believe any data scientist who convinces me that you don need in depth knowledge of statistics to succeed in the field have not done mathematics and statistics in long time however am also not one that is afraid of hard work could put in lot of effort to learn the math and stats to be better data scientist but will ever be better data scientist despite all that have been thinking about data engineering instead not because it the second best option but because am interested in programming engineering in building something and being part of something know how difficult it is like said am not afraid of the hard work the true dilemma have right now is all the time that were to spend on learning math and stats and all the other stuff to be data scientist and be better at it could channel that time and energy towards become data engineer and being better at that instead could also leverage the knowledge from my degree and data engineers do know some level of ml which believe is at the level of the bootcamp did feel that the bootcamp has put this false expectation in my head that being data scientist is easy and possible in months when it truly not am seeking advice from professionals in this field for long time and to confirm this suspicion have that would indeed need much more than just what bootcamp could offer and that perhaps data engineering is more realistic path,1,seeking honesty about future in this field,did double major in computer science and information system from not so prestigious university a compared to those locally then proceeded to do bootcamp in data science from the more well known institution due to the lack of substantial project back in uni a of right now am on job hunt and while doing so have discovered so much about how lack the skill to even take up an entry level data scientist job refuse to believe any data scientist who convinces me that you don need in depth knowledge of statistic to succeed in the field have not done mathematics and statistic in long time however am also not one that is afraid of hard work could put in lot of effort to learn the math and stats to be better data scientist but will ever be better data scientist despite all that have been thinking about data engineering instead not because it the second best option but because am interested in programming engineering in building something and being part of something know how difficult it is like said am not afraid of the hard work the true dilemma have right now is all the time that were to spend on learning math and stats and all the other stuff to be data scientist and be better at it could channel that time and energy towards become data engineer and being better at that instead could also leverage the knowledge from my degree and data engineer do know some level of ml which believe is at the level of the bootcamp did feel that the bootcamp ha put this false expectation in my head that being data scientist is easy and possible in month when it truly not am seeking advice from professional in this field for long time and to confirm this suspicion have that would indeed need much more than just what bootcamp could offer and that perhaps data engineering is more realistic path
best boot camp,looking at starting the data science boot camp at flatiron school for career change from education current math teacher with years experience to data science any advice,1,best boot camp,looking at starting the data science boot camp at flatiron school for career change from education current math teacher with year experience to data science any advice
mind uploading,mind uploading,1,mind uploading,mind uploading
discussion this org won submit their linked report to real science journal wild salmon populations are under serious threat and bc salmon farmers independent paid report is trying to dilute that can anyone help poke holes in the data sea lice in the discovery islands,discussion this org won submit their linked report to real science journal wild salmon populations are under serious threat and bc salmon farmers independent paid report is trying to dilute that can anyone help poke holes in the data sea lice in the discovery islands,1,discussion this org won submit their linked report to real science journal wild salmon population are under serious threat and bc salmon farmer independent paid report is trying to dilute that can anyone help poke hole in the data sea louse in the discovery island,discussion this org won submit their linked report to real science journal wild salmon population are under serious threat and bc salmon farmer independent paid report is trying to dilute that can anyone help poke hole in the data sea louse in the discovery island
project vs certification,hi everyone am data science aspirant and new dad to month old am currently working as bi professional planning to change my career to ds am getting ready to attend interviews am debating whether to focus now on getting aws certification or just fully focus on projects and interview prep instead of spending time on aws certification any suggestion would be appreciated,1,project v certification,hi everyone am data science aspirant and new dad to month old am currently working a bi professional planning to change my career to d am getting ready to attend interview am debating whether to focus now on getting aws certification or just fully focus on project and interview prep instead of spending time on aws certification any suggestion would be appreciated
carrer recommendations for someone who loves ml mle ds or something else,carrer recommendations for someone who loves ml mle ds or something else,1,carrer recommendation for someone who love ml mle d or something else,carrer recommendation for someone who love ml mle d or something else
how to transition to manager,ve got phd and have been at data science for years professionally in contributor role feel ready to try to get into management but it doesn seem likely will get people to report to me within my current org there are informal mentorship teaching and project management opportunities as contributor but is this the same as having people reporting to you how does one successfully transition from contributor to manager get paid very comfortably and just started months ago here don want to leave for few more years,1,how to transition to manager,ve got phd and have been at data science for year professionally in contributor role feel ready to try to get into management but it doesn seem likely will get people to report to me within my current org there are informal mentorship teaching and project management opportunity a contributor but is this the same a having people reporting to you how doe one successfully transition from contributor to manager get paid very comfortably and just started month ago here don want to leave for few more year
apple laptop for data science course,planning on doing some data science courses and it time to replace my old mac prefer to stick with apple for hobby music editing reasons but will that work for programming also specific recommendations are welcome thanks,1,apple laptop for data science course,planning on doing some data science course and it time to replace my old mac prefer to stick with apple for hobby music editing reason but will that work for programming also specific recommendation are welcome thanks
you are assigned to work for young company that organizes and markets events what are the first steps you take,you re looking to work for events and music companies their data so far just consists of social media engagements and ticket sales info and they just hand that to you what can be done to get these companies to start using data wisely,1,you are assigned to work for young company that organizes and market event what are the first step you take,you re looking to work for event and music company their data so far just consists of social medium engagement and ticket sale info and they just hand that to you what can be done to get these company to start using data wisely
what are some useful tools to learn for research,hi people am an early biology student with some free time in my hands due to the holidays school is over in my country was looking to expand my knowledge by learning to code but figured out it would be more worthwhile to combine learning new programming computer tool with what will be doing in the future that is to say research was thinking of something like spss for data analysis but open to anything useful thanks,1,what are some useful tool to learn for research,hi people am an early biology student with some free time in my hand due to the holiday school is over in my country wa looking to expand my knowledge by learning to code but figured out it would be more worthwhile to combine learning new programming computer tool with what will be doing in the future that is to say research wa thinking of something like spss for data analysis but open to anything useful thanks
will passing the exam first actuarial exam affect my job search in data science,will passing the exam first actuarial exam affect my job search in data science,1,will passing the exam first actuarial exam affect my job search in data science,will passing the exam first actuarial exam affect my job search in data science
quick primer players in the hiring process,lot of people here are early in their careers and may not understand who all fits into the hiring process and how so here quick primer the hiring manager is the person in charge of deciding who ultimately gets hired of the time this will be the person who manages the candidate being hired the hiring manager is responsible for defining what they want in candidate what they will be doing etc the way they do this is by writing job description which outlines the skills and experience person should have as well as what they will be doing the hr comp department will then take job description and grade it decide how much someone in that role should be paid this is the team that says if you want someone with years of experience and grad degree they should get xk year then comes your hr business partner as hiring manager you don normally talk to the comp team directly your hr bp is the one who coordinates everything and they can also be the one to give you insights into how jd may influence comp example they may tell you if you make grad degree optional the max they will approve is if you want to go higher than that you ll need to require grad degree then there the recruiter aka talent acquisition these are the people that find candidates and do the initial screening on them reading resumes and conducting an initial interview important callout data science recruiter likely has no experience with data science however if they are good recruiters they understand what data scientists do and how they talk about it having said that they re only screening they are mostly making sure that what you said you did is legit and that it generally matches what the hiring manager wants most of the rest of the process is handled by the hiring manager with hr primarily helping administratively setting up meetings sending emails etc hr doesn make decisions other than screening and grading hr is there purely to carry out their process points of conflict between hiring managers and hr bad grading you put together reasonable job description and the grading comes under what you expected normally this happens when companies use bad comparison points to find comparable salaries to ds they use it professionals or instead of using data in your high col city using state wide data that can lead to job description padding when you make the job description much demanding than what you re willing to hire so you can pay what you want bad screenings your recruiter filters out people because they have ms in biostats instead of stats or they just don know that operations research is relevant discipline or they will let people through that don have the right experience lazy recruiters recruiters do two things to find candidates look at who applied and actively look for candidates who may be passive lazy ones don do the second part,1,quick primer player in the hiring process,lot of people here are early in their career and may not understand who all fit into the hiring process and how so here quick primer the hiring manager is the person in charge of deciding who ultimately get hired of the time this will be the person who manages the candidate being hired the hiring manager is responsible for defining what they want in candidate what they will be doing etc the way they do this is by writing job description which outline the skill and experience person should have a well a what they will be doing the hr comp department will then take job description and grade it decide how much someone in that role should be paid this is the team that say if you want someone with year of experience and grad degree they should get xk year then come your hr business partner a hiring manager you don normally talk to the comp team directly your hr bp is the one who coordinate everything and they can also be the one to give you insight into how jd may influence comp example they may tell you if you make grad degree optional the max they will approve is if you want to go higher than that you ll need to require grad degree then there the recruiter aka talent acquisition these are the people that find candidate and do the initial screening on them reading resume and conducting an initial interview important callout data science recruiter likely ha no experience with data science however if they are good recruiter they understand what data scientist do and how they talk about it having said that they re only screening they are mostly making sure that what you said you did is legit and that it generally match what the hiring manager want most of the rest of the process is handled by the hiring manager with hr primarily helping administratively setting up meeting sending email etc hr doesn make decision other than screening and grading hr is there purely to carry out their process point of conflict between hiring manager and hr bad grading you put together reasonable job description and the grading come under what you expected normally this happens when company use bad comparison point to find comparable salary to d they use it professional or instead of using data in your high col city using state wide data that can lead to job description padding when you make the job description much demanding than what you re willing to hire so you can pay what you want bad screening your recruiter filter out people because they have m in biostats instead of stats or they just don know that operation research is relevant discipline or they will let people through that don have the right experience lazy recruiter recruiter do two thing to find candidate look at who applied and actively look for candidate who may be passive lazy one don do the second part
rnn for time series forecasting,hi everyone we have been asked to implement rnn in order to forecast time series what are some useful architectures which are state of the art we could try for this task,1,rnn for time series forecasting,hi everyone we have been asked to implement rnn in order to forecast time series what are some useful architecture which are state of the art we could try for this task
specialisms within the data science sphere catalogue,ve read lot of discussions about the splintering of ds into different specialisms but there seems to be general misunderstanding or lack of understanding and agreement about what these specialisms are what they do what backgrounds are generally suited for entry into these roles talking specifically about job titles and roles not more vague descriptions like to have thread where these different specialisms can be listed with concise descriptions about what each one actually is does and what backgrounds are most suited to entry into these areas ll create starting list in the op of different job titles within ds and update it with details from posts in the thread please feel free to add any more roles haven included note not necessarily saying all of these roles come under the umbrella of data science per se but there all related amp x200b data scientist data engineer machine learning engineer machine learning scientist applied scientist research scientist research engineer,1,specialism within the data science sphere catalogue,ve read lot of discussion about the splintering of d into different specialism but there seems to be general misunderstanding or lack of understanding and agreement about what these specialism are what they do what background are generally suited for entry into these role talking specifically about job title and role not more vague description like to have thread where these different specialism can be listed with concise description about what each one actually is doe and what background are most suited to entry into these area ll create starting list in the op of different job title within d and update it with detail from post in the thread please feel free to add any more role haven included note not necessarily saying all of these role come under the umbrella of data science per se but there all related amp x200b data scientist data engineer machine learning engineer machine learning scientist applied scientist research scientist research engineer
transitioning careers,heyyyy currently an accounting and finance student in my final year however after doing placement year realised do not want to go into this industry as career path and want to get into data analysis of data science so ve decided to do online data courses before graduate in summer to improve my skill and knowledge and just wanted to know would you recommend grow with google or datacamp as they both come at fee subscription as student can only choose one or do you recommend any other sources will still be using other free resources online,1,transitioning career,heyyyy currently an accounting and finance student in my final year however after doing placement year realised do not want to go into this industry a career path and want to get into data analysis of data science so ve decided to do online data course before graduate in summer to improve my skill and knowledge and just wanted to know would you recommend grow with google or datacamp a they both come at fee subscription a student can only choose one or do you recommend any other source will still be using other free resource online
started self learning data science years ago and this where ve gotten advice for beginners,compensation wise about more than was being paid before started actually have what most high achieving people would consider good job was already at fairly good job before if you re wondering why only increase future outlook lot better certainly feel more respected at work and more confident in my career the industry is still at it birth so if you study the right things there are lot of opportunities to accomplish what you want compared to most fields industries advice for beginners the first months are the hardest you re really new the space opportunities will not come easily then just keep learning consider applying to other jobs that are easier to get but have the opportunities to interact with data people like internships data entry jobs volunteer work etc heck ve interacted frequently at work with people from customer support sales product management etc whom we were able to get setup with their own data environment because they were interested in learning and pulling the data they need if you re not sure where to start there are great blogs quora posts cheap online platforms etc it may seem like an endless amount of information but ve found that most information is useful and can lead you to other information,1,started self learning data science year ago and this where ve gotten advice for beginner,compensation wise about more than wa being paid before started actually have what most high achieving people would consider good job wa already at fairly good job before if you re wondering why only increase future outlook lot better certainly feel more respected at work and more confident in my career the industry is still at it birth so if you study the right thing there are lot of opportunity to accomplish what you want compared to most field industry advice for beginner the first month are the hardest you re really new the space opportunity will not come easily then just keep learning consider applying to other job that are easier to get but have the opportunity to interact with data people like internship data entry job volunteer work etc heck ve interacted frequently at work with people from customer support sale product management etc whom we were able to get setup with their own data environment because they were interested in learning and pulling the data they need if you re not sure where to start there are great blog quora post cheap online platform etc it may seem like an endless amount of information but ve found that most information is useful and can lead you to other information
operations research on resume,hi all have degree in operations research and am interested in transitioning into data science unfortunately ve found that very few people know what operations research is and concerned this will hurt my job prospects would anyone have any recommendations on remedying this issue on my resume thanks,1,operation research on resume,hi all have degree in operation research and am interested in transitioning into data science unfortunately ve found that very few people know what operation research is and concerned this will hurt my job prospect would anyone have any recommendation on remedying this issue on my resume thanks
tips amp tricks of deploying deep learning webapp on heroku cloud kdnuggets,tips amp tricks of deploying deep learning webapp on heroku cloud kdnuggets,1,tip amp trick of deploying deep learning webapp on heroku cloud kdnuggets,tip amp trick of deploying deep learning webapp on heroku cloud kdnuggets
next steps from clinical informatics analyst,hello am trying to figure out my next step in my career am finishing masters in bioinformatics but more importantly have spent the last year as data analyst for clinical research team my role extends across the whole data analysis pipeline from data acquirement and cleaning to regression analysis and other statistical tests am skilled in and sas and know python to passable degree have foundation in mathematics statistics from my masters and have been spending time developing background in ml beyond the few projects completed in my masters would like to step up to the next pay bracket stuck in the mid at the moment the goal would be close to six figure somewhere in the healthcare space have sent out few round of applications but have not had any callbacks does anyone have thoughts on how to go about making this next step,1,next step from clinical informatics analyst,hello am trying to figure out my next step in my career am finishing master in bioinformatics but more importantly have spent the last year a data analyst for clinical research team my role extends across the whole data analysis pipeline from data acquirement and cleaning to regression analysis and other statistical test am skilled in and sa and know python to passable degree have foundation in mathematics statistic from my master and have been spending time developing background in ml beyond the few project completed in my master would like to step up to the next pay bracket stuck in the mid at the moment the goal would be close to six figure somewhere in the healthcare space have sent out few round of application but have not had any callback doe anyone have thought on how to go about making this next step
how to start learning data science,am nurse by profession and really interested in data science that is applicable to my profession but don know how to start can you recommend any courses or topics should start studying in order to be in this field or if there is actually degree sorry really have no idea,1,how to start learning data science,am nurse by profession and really interested in data science that is applicable to my profession but don know how to start can you recommend any course or topic should start studying in order to be in this field or if there is actually degree sorry really have no idea
how to search for internships effectively non us,hi the hunt for internships can be really overwhelming especially if you don know how to effectively find new ones most offers ve seen were through random linkedin posts random telegram posts in channels ve been subscribed to is there any place that can search for internships for mle or ds positions worldwide my latest impression was that most internships are rnd and research based is this true if so can university internships be good too,1,how to search for internship effectively non u,hi the hunt for internship can be really overwhelming especially if you don know how to effectively find new one most offer ve seen were through random linkedin post random telegram post in channel ve been subscribed to is there any place that can search for internship for mle or d position worldwide my latest impression wa that most internship are rnd and research based is this true if so can university internship be good too
which job develops ml models more ds or ml engineer,perhaps the industry blends titles too much can tell who works with ml models more or who gets to develop them do you just have to look at the job description and talk with the company seems like the roles are not well defined,1,which job develops ml model more d or ml engineer,perhaps the industry blend title too much can tell who work with ml model more or who get to develop them do you just have to look at the job description and talk with the company seems like the role are not well defined
christmas is time of giving,christmas is time of giving,1,christmas is time of giving,christmas is time of giving
for data tools do you prefer desktop or web version,hi folks happy upcoming holidays my team is building data transformer and we wonder are you used to having desktop or web versions of the tools for data operations it will help up lot so thank you in advance,1,for data tool do you prefer desktop or web version,hi folk happy upcoming holiday my team is building data transformer and we wonder are you used to having desktop or web version of the tool for data operation it will help up lot so thank you in advance
can use standard deviation to turn predicted value into range,have maybe naive question about the predictive quality of given ml regression algorithm can you take the standard deviation of the difference error_pred y_pred y_test from your testing data and use it to turn your predicted number into range say you predict the material property of new compound based on your trained algorithm you get predicted value and you get the standard deviation from your testing data value sigma could you give your result as value sigma and claim that based on the available data you have probability of the result being in this range is this meaningful are there problems with this thinking am missing something this feels too simplistic and my gut tells me that there probably are issues with it but can put my finger on what it is exactly appreciate any pointers you could give me many thanks and merry christmas,1,can use standard deviation to turn predicted value into range,have maybe naive question about the predictive quality of given ml regression algorithm can you take the standard deviation of the difference error_pred y_pred y_test from your testing data and use it to turn your predicted number into range say you predict the material property of new compound based on your trained algorithm you get predicted value and you get the standard deviation from your testing data value sigma could you give your result a value sigma and claim that based on the available data you have probability of the result being in this range is this meaningful are there problem with this thinking am missing something this feel too simplistic and my gut tell me that there probably are issue with it but can put my finger on what it is exactly appreciate any pointer you could give me many thanks and merry christmas
should stick to the same company,to preface currently data scientist at reasonably successful and fast growing mnc have masters degree that finished recently and this is my first job wondering if should look into other careers and places because the work do for my current company is extremely specialized it still data science but very little of it is building skills that are transferable my question to the more experienced ones amongst you is it bad to get bogged into career that too specialized to the point that it an industry of one company or is it better to move around and experience more roles might get promoted to senior data scientist within the next few years but afraid that might be too early and then won get senior data scientist role anywhere else any advice is helpful,1,should stick to the same company,to preface currently data scientist at reasonably successful and fast growing mnc have master degree that finished recently and this is my first job wondering if should look into other career and place because the work do for my current company is extremely specialized it still data science but very little of it is building skill that are transferable my question to the more experienced one amongst you is it bad to get bogged into career that too specialized to the point that it an industry of one company or is it better to move around and experience more role might get promoted to senior data scientist within the next few year but afraid that might be too early and then won get senior data scientist role anywhere else any advice is helpful
please give some suggestions,have some scanned documents in the form of pdf and word files how can check if the text in documents is not cut off from the edges sides without essentially reading the content we can consider the page as an image as it is scanned have tried bounding boxes to detect if the image is touching the sides what else can try,1,please give some suggestion,have some scanned document in the form of pdf and word file how can check if the text in document is not cut off from the edge side without essentially reading the content we can consider the page a an image a it is scanned have tried bounding box to detect if the image is touching the side what else can try
new way to estimate the median and other percentiles based on the exponential distribution,it is well known that for finite sample sizes the estimators for most percentiles are biased this includes the median unless the underlying distribution has the same mean and median the standard way to estimate them is to first find the two order statistics that bracket the percentile then linearly interpolate between them but there is nothing special about linear interpolation perhaps it can be improved here is one strategy based on an exponential distribution that shows very promising results rohitpandey576 hear me out found better way to estimate the median c4971be4278 rohitpandey576 hear me out found better way to estimate the median c4971be4278,1,new way to estimate the median and other percentile based on the exponential distribution,it is well known that for finite sample size the estimator for most percentile are biased this includes the median unless the underlying distribution ha the same mean and median the standard way to estimate them is to first find the two order statistic that bracket the percentile then linearly interpolate between them but there is nothing special about linear interpolation perhaps it can be improved here is one strategy based on an exponential distribution that show very promising result rohitpandey576 hear me out found better way to estimate the median c4971be4278 rohitpandey576 hear me out found better way to estimate the median c4971be4278
how to represent data for neural networks,how to represent data for neural networks,1,how to represent data for neural network,how to represent data for neural network
how to get cloud experience for solution architect role,thinking of career change towards solution architect role at aws or microsoft my current day to day consists of creating reports queries in sql excel and power bi to support day to day operations lot of these solution architect roles require cloud experience such as aws or azure currently am having hard time finding these types of project experiences at work any advice how could find cloud experience solutions architect experience to put my best foot forward when applying for those roles elsewhere,1,how to get cloud experience for solution architect role,thinking of career change towards solution architect role at aws or microsoft my current day to day consists of creating report query in sql excel and power bi to support day to day operation lot of these solution architect role require cloud experience such a aws or azure currently am having hard time finding these type of project experience at work any advice how could find cloud experience solution architect experience to put my best foot forward when applying for those role elsewhere
data science interview participation,good evening among current circumstances hope this message finds everyone well am current high school senior student in the state of illinois seeking potential data science professionals or prospective data scientists willing to participate in an interview for my ap research course to provide general overview my institution is currently partnering with college board ap capstone diploma diploma program that develops student skills in research analysis evidence based arguments collaboration writing and presenting skills based on two long year courses ap seminar and ap research as student currently enrolled in the ap research course and an expected requirement am tasked with the year long process of exploring an individual area of interest that may be an academic topic of choice idea or circumstantial issue this year am centering my research on the effects traditional mathematics subjects retain in minority students academic success primarily latino students and students of hispanic origin as well as assessing the measure of academic success of collegiate students or professionals in attaining post secondary education degree and or career it is worth noting the state of illinois does not offer any data science education within its public school districts and is an objective would like to have implemented in my community have tried to establish contact with potential participants but have had no success therefore have decided to post my objective here in hopes to gain participants though am willing to take participants who are interested am seeking those who have been previously enrolled in data science course in their secondary high school career or post secondary if you are interested in participating or know of those who may be interested please do not hesitate to contact me for further information am more than willing to set up date time through either platform zoom and google meets and address any questions or concerns thank you for reading this lengthy post and happy holidays,1,data science interview participation,good evening among current circumstance hope this message find everyone well am current high school senior student in the state of illinois seeking potential data science professional or prospective data scientist willing to participate in an interview for my ap research course to provide general overview my institution is currently partnering with college board ap capstone diploma diploma program that develops student skill in research analysis evidence based argument collaboration writing and presenting skill based on two long year course ap seminar and ap research a student currently enrolled in the ap research course and an expected requirement am tasked with the year long process of exploring an individual area of interest that may be an academic topic of choice idea or circumstantial issue this year am centering my research on the effect traditional mathematics subject retain in minority student academic success primarily latino student and student of hispanic origin a well a assessing the measure of academic success of collegiate student or professional in attaining post secondary education degree and or career it is worth noting the state of illinois doe not offer any data science education within it public school district and is an objective would like to have implemented in my community have tried to establish contact with potential participant but have had no success therefore have decided to post my objective here in hope to gain participant though am willing to take participant who are interested am seeking those who have been previously enrolled in data science course in their secondary high school career or post secondary if you are interested in participating or know of those who may be interested please do not hesitate to contact me for further information am more than willing to set up date time through either platform zoom and google meet and address any question or concern thank you for reading this lengthy post and happy holiday
dream content datasets,hope this isn out of place here but am looking for datasets of people reported dreams ve looked on kaggle and google datasets but am at loss dreams fascinate me and would like to find any publically available data to analyse really just for fun and interest any ideas where might start looking if really can find any would consider creating web app and requesting that people donate their dream reports to me but that would be longer term goal or alternatively may be able to scrape websites where people report dreams though the ethics of that may be problematic do you know of any website where people report their dreams or any publically available datasets,1,dream content datasets,hope this isn out of place here but am looking for datasets of people reported dream ve looked on kaggle and google datasets but am at loss dream fascinate me and would like to find any publically available data to analyse really just for fun and interest any idea where might start looking if really can find any would consider creating web app and requesting that people donate their dream report to me but that would be longer term goal or alternatively may be able to scrape website where people report dream though the ethic of that may be problematic do you know of any website where people report their dream or any publically available datasets
need career advice imposter syndrome kicking after years,graduated from an analytics masters program about years ago and got my first figure salary as ds level in non tech industry fast forward to now still at the same position and level and ready for change of scenery the problem is that in the last years here as data scientist ve mainly worked on creating dashboards ad hoc reports and very minimal data analysis projects ve became complacent and comfortable with this light work load that it catching up to me like creating dashboards and visualizations that like to move to more bi product business analytics focused position my nd problem is after several years of applying to jobs on and off have not received any offers even though think qualified if not over qualified for some of these positions my questions is what is the best way to move forward while trying to keep my compensation and past experience relevant also am not getting any offers because too expensive in ramping up my sql python communications skills planning on applying to more job but hoping to be more strategic and better prepared also asking for feedback if get rejected,1,need career advice imposter syndrome kicking after year,graduated from an analytics master program about year ago and got my first figure salary a d level in non tech industry fast forward to now still at the same position and level and ready for change of scenery the problem is that in the last year here a data scientist ve mainly worked on creating dashboard ad hoc report and very minimal data analysis project ve became complacent and comfortable with this light work load that it catching up to me like creating dashboard and visualization that like to move to more bi product business analytics focused position my nd problem is after several year of applying to job on and off have not received any offer even though think qualified if not over qualified for some of these position my question is what is the best way to move forward while trying to keep my compensation and past experience relevant also am not getting any offer because too expensive in ramping up my sql python communication skill planning on applying to more job but hoping to be more strategic and better prepared also asking for feedback if get rejected
it was either this or the flowers,it was either this or the flowers,1,it wa either this or the flower,it wa either this or the flower
where to start with statistical learning,many thanks to anyone who responds am looking for recommendations re books online education etc from which can learn about statistical learning in particular am trying to build tool that provides value estimates given set of values for predictor variables programming is handled but haven looked at statistical learning in the last years and could use some guidance re modeling happy holidays,1,where to start with statistical learning,many thanks to anyone who responds am looking for recommendation re book online education etc from which can learn about statistical learning in particular am trying to build tool that provides value estimate given set of value for predictor variable programming is handled but haven looked at statistical learning in the last year and could use some guidance re modeling happy holiday
ds books on kindle,are there any worthwhile books to read on data science or statistics or data engineering that are well suited to kindle what mean here is that there not too many color graphics or plots math formulas long code snippets or pictures as none of these tend to show up very well on my kindle am thinking of well written narrative on how to choose model or how to approach working on new problem or anything high level like this like to read in bed after my wife falls asleep and this is the least disturbing option to level up for minutes before doze off,1,d book on kindle,are there any worthwhile book to read on data science or statistic or data engineering that are well suited to kindle what mean here is that there not too many color graphic or plot math formula long code snippet or picture a none of these tend to show up very well on my kindle am thinking of well written narrative on how to choose model or how to approach working on new problem or anything high level like this like to read in bed after my wife fall asleep and this is the least disturbing option to level up for minute before doze off
resources for retired engineer,my father expressed his interest in ai particularly he is retired chemical engineer so he has good grasp of though rusty of the mathematics side of things with hardly any programming skills any introductory materials to ai that might suite him,1,resource for retired engineer,my father expressed his interest in ai particularly he is retired chemical engineer so he ha good grasp of though rusty of the mathematics side of thing with hardly any programming skill any introductory material to ai that might suite him
better democracy through technology,better democracy through technology,1,better democracy through technology,better democracy through technology
does anyone know what up with census data,ve had lot of requests for simple demographic type stuff recently am missing where data is available or has census just not gotten around to releasing it yet see some basic state level stuff but nowhere near all variables and you can drill down to census tract or other geographies that would be useful for me,1,doe anyone know what up with census data,ve had lot of request for simple demographic type stuff recently am missing where data is available or ha census just not gotten around to releasing it yet see some basic state level stuff but nowhere near all variable and you can drill down to census tract or other geography that would be useful for me
mit applied data science program,am looking at the online applied data science program mit is offering anyone has experience taking this or any review would be very much appreciated thanks in advance,1,mit applied data science program,am looking at the online applied data science program mit is offering anyone ha experience taking this or any review would be very much appreciated thanks in advance
jupyter notebook not giving any output,beginner here reinstalled the notebook bunch of times changed locations changed browsers kernal is infinite and interuppting or stopping kernal does not work what the solution,1,jupyter notebook not giving any output,beginner here reinstalled the notebook bunch of time changed location changed browser kernal is infinite and interuppting or stopping kernal doe not work what the solution
linear regression vs correlation,can anyone tell what the intuitive difference between linear regression vs correlation thanks for all of you,1,linear regression v correlation,can anyone tell what the intuitive difference between linear regression v correlation thanks for all of you
how to prepare and interview with the cco,am trying to land on my first data science job there is one company where have already passed the initial interview with hr and the technical interview with the lead data scientist now they want me to interview with the commercial director but ve got no idea how to prepare this any advice,1,how to prepare and interview with the cco,am trying to land on my first data science job there is one company where have already passed the initial interview with hr and the technical interview with the lead data scientist now they want me to interview with the commercial director but ve got no idea how to prepare this any advice
ms in cs or an ms in statistics,currently math statistics and cs major and wanna do master before jumping into the industry should do one in statistics or in cs feel like bs in cs is sufficient background in computing and programming and that an ms in statistics would help top it off with heavier theory mathematical emphasis however the benefit of an ms in cs is that its highly flexible in fact can take up to half my courses from other departments like statistics and applied math the end result is both stronger cs background and stronger knowledge of statistics in limited number of topics,1,m in c or an m in statistic,currently math statistic and c major and wanna do master before jumping into the industry should do one in statistic or in c feel like b in c is sufficient background in computing and programming and that an m in statistic would help top it off with heavier theory mathematical emphasis however the benefit of an m in c is that it highly flexible in fact can take up to half my course from other department like statistic and applied math the end result is both stronger c background and stronger knowledge of statistic in limited number of topic
what skills would full stack data scientist have,the other day at work we were talking about the difference between our development and data science teams we are in the process of hiring people for both and some of the people applying for our development team describe themselves as full stack programmers on the ds team we don really have term for someone who could take project from inception to production is their term for that and what skills would you expect full stack data scientist to have we aren looking for for full stack person it just an interesting discussion also not going to be posting anything about the positions not the hiring manager,1,what skill would full stack data scientist have,the other day at work we were talking about the difference between our development and data science team we are in the process of hiring people for both and some of the people applying for our development team describe themselves a full stack programmer on the d team we don really have term for someone who could take project from inception to production is their term for that and what skill would you expect full stack data scientist to have we aren looking for for full stack person it just an interesting discussion also not going to be posting anything about the position not the hiring manager
am too dumb for data science,hey guys gals am with no cs related background currently on third semester in my bsc degree in business amp cs business informatics starting on my third semester got huge interessed in data science thanks to module called business analytics in the past months effectively learned how code with java and python and also built my first simple ml model with sklearn for predictive maintenance based on datasets we got during our exercise lessons despite that am really struggling in statistics and fields like deep learning consider myself as slow learner and it is frustrating for me that take so long to understand the math and implementation of statistics and deep learning like tensorflow etc am just too dumb or am trying to rush things too hard any suggestions on my road to ds what do need for the job market,1,am too dumb for data science,hey guy gal am with no c related background currently on third semester in my bsc degree in business amp c business informatics starting on my third semester got huge interessed in data science thanks to module called business analytics in the past month effectively learned how code with java and python and also built my first simple ml model with sklearn for predictive maintenance based on datasets we got during our exercise lesson despite that am really struggling in statistic and field like deep learning consider myself a slow learner and it is frustrating for me that take so long to understand the math and implementation of statistic and deep learning like tensorflow etc am just too dumb or am trying to rush thing too hard any suggestion on my road to d what do need for the job market
hi washington post journalist philip bump use data and graphs to cover the news ask me anything,hi washington post journalist philip bump use data and graphs to cover the news ask me anything,1,hi washington post journalist philip bump use data and graph to cover the news ask me anything,hi washington post journalist philip bump use data and graph to cover the news ask me anything
pursuing career in data,hello am recent sc hons psychology graduate with focus on organizational behavior and am currently enrolled in certificate program for data analytics big data and predictive analytics in the certificate program will learn sql python numpy pandas seaborn and matplotlib some weka ibm watson and nosql amp hadoop have heard that power bi is extremely beneficial to know so will try to learn it as well technically have over seven plus years of work experience in the service sector working as an employee at customer service sales positions and providing different services as self employee such as graphic design tutoring etc am interested in pursuing career in data and was wondering what would be good starting point as an entry level job the soonest can apply to an sc is for the winter term live in toronto if that helps any input suggestions or advice is welcome and greatly appreciated,1,pursuing career in data,hello am recent sc hons psychology graduate with focus on organizational behavior and am currently enrolled in certificate program for data analytics big data and predictive analytics in the certificate program will learn sql python numpy panda seaborn and matplotlib some weka ibm watson and nosql amp hadoop have heard that power bi is extremely beneficial to know so will try to learn it a well technically have over seven plus year of work experience in the service sector working a an employee at customer service sale position and providing different service a self employee such a graphic design tutoring etc am interested in pursuing career in data and wa wondering what would be good starting point a an entry level job the soonest can apply to an sc is for the winter term live in toronto if that help any input suggestion or advice is welcome and greatly appreciated
till where can go on statistical data science by using only excel amp sql as tool,hi friends its career question now agree without hesitation best tool is python alteryx not doubt on this but you see not all are data scientist may be because their skill is not matching or they are not interested in ds now with me am interested in ds however am comfortable only in sql excel statistics concepts would like to mention that yes many of you find python really easy its not that didn tried but my brain can simply get hand of it using prog language like or python however my brain is very much friendly to sql gt excel gt stats gt dax gt power query style of construct before deep dive into world of ds ml would like to take any helpful opinion of senior folks here on their experience can achieve something in world if ds with excel sql stats concept alone if so what can achieve eg or what would be my limitation,1,till where can go on statistical data science by using only excel amp sql a tool,hi friend it career question now agree without hesitation best tool is python alteryx not doubt on this but you see not all are data scientist may be because their skill is not matching or they are not interested in d now with me am interested in d however am comfortable only in sql excel statistic concept would like to mention that yes many of you find python really easy it not that didn tried but my brain can simply get hand of it using prog language like or python however my brain is very much friendly to sql gt excel gt stats gt dax gt power query style of construct before deep dive into world of d ml would like to take any helpful opinion of senior folk here on their experience can achieve something in world if d with excel sql stats concept alone if so what can achieve eg or what would be my limitation
resume advice for grad student hoping to break into ds,hey datascience quick background on me finishing phd soon in lab based cancer research project have an ms also in cancer research phd from top university ms from top so fairly strong academic background recently become interested in the digital revolution in oncology and began self teaching loved it and did some courses on data science picking up python and sql along the way as well as some basic guided ml projects would like to create strong resume to help get hired but obviously don have industry experience in ds or any education in computer science or stats outside of the courses took coursera provided by various universities and ibm if you have any advice on this be delighted to hear it and if it helps can dm link to my linked in really appreciate anything you guys have,1,resume advice for grad student hoping to break into d,hey datascience quick background on me finishing phd soon in lab based cancer research project have an m also in cancer research phd from top university m from top so fairly strong academic background recently become interested in the digital revolution in oncology and began self teaching loved it and did some course on data science picking up python and sql along the way a well a some basic guided ml project would like to create strong resume to help get hired but obviously don have industry experience in d or any education in computer science or stats outside of the course took coursera provided by various university and ibm if you have any advice on this be delighted to hear it and if it help can dm link to my linked in really appreciate anything you guy have
top keyword extraction apis,top keyword extraction apis,1,top keyword extraction apis,top keyword extraction apis
looking for co founder for start up,for my start up www makershands nl we are looking for co founder who specializes in data analysis we are aiming to use data on where we should scale next we are located in amsterdam makers hands has been founded to bring conscious consumerism back we solve the following problems creatives usually do not have time to focus on the business side of their company we work as an intermediaire and take care of their finances business development transport marketing and rapports we source sustainable materials that creatives can work with leather made of plants seaweed wood made from mushrooms we want to make sure that you as consumer do not have trow out your furniture or other possessions because they have been worn out we will restore the furniture and you are able to get your original personal piece back if you are bored with it we will sell this for you on our platform so people can get an unique piece of their own sustainable transportation we use sail ships to distribute products over the atlantic and receive materials in return give creative makers in africa and south america the chance to set trends that will influence buyers and creators our first project will be with lebanese creatives makers hands does not merely facilitate consumerism but also contribute to public goals and socially accepted innovations if you are interested send me dm or reply to this message thank you in advance,1,looking for co founder for start up,for my start up www makershands nl we are looking for co founder who specializes in data analysis we are aiming to use data on where we should scale next we are located in amsterdam maker hand ha been founded to bring conscious consumerism back we solve the following problem creatives usually do not have time to focus on the business side of their company we work a an intermediaire and take care of their finance business development transport marketing and rapport we source sustainable material that creatives can work with leather made of plant seaweed wood made from mushroom we want to make sure that you a consumer do not have trow out your furniture or other possession because they have been worn out we will restore the furniture and you are able to get your original personal piece back if you are bored with it we will sell this for you on our platform so people can get an unique piece of their own sustainable transportation we use sail ship to distribute product over the atlantic and receive material in return give creative maker in africa and south america the chance to set trend that will influence buyer and creator our first project will be with lebanese creatives maker hand doe not merely facilitate consumerism but also contribute to public goal and socially accepted innovation if you are interested send me dm or reply to this message thank you in advance
anyone else work in agriculture,ve recently been hired as data scientist at an agricultural firm specialising in cattle ranching feel like the only person working in this field don do any fancy modelling just regression models anyone else work in weird fields,1,anyone else work in agriculture,ve recently been hired a data scientist at an agricultural firm specialising in cattle ranching feel like the only person working in this field don do any fancy modelling just regression model anyone else work in weird field
whats the best country to work,think the us and the uk are two countries where to work as data scientist is the most beneficial seemingly salaries are the highest over there considering living costs ofc what do you think maybe some other countries,1,whats the best country to work,think the u and the uk are two country where to work a data scientist is the most beneficial seemingly salary are the highest over there considering living cost ofc what do you think maybe some other country
data visualization,does anyone know of any visualization programs that are entirely free and work well with sql and mac os,1,data visualization,doe anyone know of any visualization program that are entirely free and work well with sql and mac o
what tools do data scientists use to manage their time,hi there not data scientist develop time management productivity software that aims to reduce meeting load focusing on data scientists because it seems that data scientists get too many requests for meetings or other requests on their time than what they can realistically fulfil does that observation ring true to you why or why not how do you deal with meeting requests or generally manage your time specifically do you use any tools or methods have you tried any that didn work out for you thanks for your answers,1,what tool do data scientist use to manage their time,hi there not data scientist develop time management productivity software that aim to reduce meeting load focusing on data scientist because it seems that data scientist get too many request for meeting or other request on their time than what they can realistically fulfil doe that observation ring true to you why or why not how do you deal with meeting request or generally manage your time specifically do you use any tool or method have you tried any that didn work out for you thanks for your answer
is there any online courses books articles for helping me learn the basics of testing,is there any online courses books articles for helping me learn the basics of testing,1,is there any online course book article for helping me learn the basic of testing,is there any online course book article for helping me learn the basic of testing
survey for those working in the software industry teamwork,hi everyone at the vrije universiteit amsterdam we are conducting survey on teamwork in the software industry and some of its challenges if you are working in the software industry could you help us out and fill out our survey it takes less than minutes to complete here is the link your responses will remain completely anonymous thank you stay safe,1,survey for those working in the software industry teamwork,hi everyone at the vrije universiteit amsterdam we are conducting survey on teamwork in the software industry and some of it challenge if you are working in the software industry could you help u out and fill out our survey it take le than minute to complete here is the link your response will remain completely anonymous thank you stay safe
what is best to study at masters,next term have to pick two of the following modules and not sure which one to pick am maths ug and monte carlo interested me however not too sure if whole module in it is necessary image processing advanced natural language processing monte carlo simulations don really know what field of ds want to get into just looking for some advice thank you,1,what is best to study at master,next term have to pick two of the following module and not sure which one to pick am math ug and monte carlo interested me however not too sure if whole module in it is necessary image processing advanced natural language processing monte carlo simulation don really know what field of d want to get into just looking for some advice thank you
would data science from scratch by joel grus nd edition be good book to learn data science,hey guys like to give data science try see the book uses python which is now eol would this book still be valuable resource to learn data science even if python is eol,1,would data science from scratch by joel grus nd edition be good book to learn data science,hey guy like to give data science try see the book us python which is now eol would this book still be valuable resource to learn data science even if python is eol
is something like google data analytics professional cert good starting point for someone with zero background,looking to deep dive into data analytics have no coding knowledge and only very mild excel experience what do for living is effectively type of data analysis clear international freight through customs which means have to rapidly visually process large amounts of data on scanned documents classify everything in it accurately determine what other government agencies may apply recognize what sort of additional information special handling may be required think could transition to more proper data analysis type field or at least just find it an interesting hobby my free time is extremely limited ve worked over hours of ot in just the past months so something like google self paced course is very attractive when working hour days and pursuing an unrelated degree,1,is something like google data analytics professional cert good starting point for someone with zero background,looking to deep dive into data analytics have no coding knowledge and only very mild excel experience what do for living is effectively type of data analysis clear international freight through custom which mean have to rapidly visually process large amount of data on scanned document classify everything in it accurately determine what other government agency may apply recognize what sort of additional information special handling may be required think could transition to more proper data analysis type field or at least just find it an interesting hobby my free time is extremely limited ve worked over hour of ot in just the past month so something like google self paced course is very attractive when working hour day and pursuing an unrelated degree
can you help me with my resume trying to score an internship in the ds ml field been getting too many rejections in initial screening itself,can you help me with my resume trying to score an internship in the ds ml field been getting too many rejections in initial screening itself,1,can you help me with my resume trying to score an internship in the d ml field been getting too many rejection in initial screening itself,can you help me with my resume trying to score an internship in the d ml field been getting too many rejection in initial screening itself
which course would you suggest to opt for first machine learning or data mining,which course would you suggest to opt for first machine learning or data mining,1,which course would you suggest to opt for first machine learning or data mining,which course would you suggest to opt for first machine learning or data mining
biggest data hurdles,talking data at scale knowledge layers distributed file systems keeping elasticsearch alive learning hdfs and all the java crap parallel processing network issues timeouts indexing speeds data status amp state etc go,1,biggest data hurdle,talking data at scale knowledge layer distributed file system keeping elasticsearch alive learning hdfs and all the java crap parallel processing network issue timeouts indexing speed data status amp state etc go
student wanting to learn data science,hello biotechnology student in polytechnic in singapore love biotechnology but ngl steering towards coding and programming over time don really want to lose that life science part and completely switch career paths ve heard that in biotechnology data scientists are needed too which courses and certification can help shape my portfolio early is it possible to get job in data science with just certifications,1,student wanting to learn data science,hello biotechnology student in polytechnic in singapore love biotechnology but ngl steering towards coding and programming over time don really want to lose that life science part and completely switch career path ve heard that in biotechnology data scientist are needed too which course and certification can help shape my portfolio early is it possible to get job in data science with just certification
what are some misconceptions of being data scientist,for an average person like me it sounds like cool sexy and unsaturated job although pretty that it not what think it is what are some common misconceptions of being data scientist,1,what are some misconception of being data scientist,for an average person like me it sound like cool sexy and unsaturated job although pretty that it not what think it is what are some common misconception of being data scientist
coursera certification wes,has anyone got certification they completed online on sites like coursera approved by the world education services wes,1,coursera certification wes,ha anyone got certification they completed online on site like coursera approved by the world education service wes
how to deal with non technical manager approach that disagree with,first time working with non technical manager whose domain is industry knowledge and not data statistics most days they know what they don know but sometimes they want to drive some analysis in questionable ways to me for example they asked that remove of the random sampled data so that we could build perfectly smooth curve fit for only lt of the data by eyeballing the visuals explained to them that these data points though not fitting well visually on their ideal curve where the majority of the dats is are not definitionally outliers they were randomly sampled by our chief ds there was no data entry errors and they belong to our target population by removing them we make the data seem more predictable than it actually is and invalidates conclusions to no avail in similar veins another time they asked for another perfect curve based only on averaged data which removes all variability in our raw data found post describing exactly what they wanted to do here they want to do chart instead of utm_medium ios_app amp utm_name iossmf which explains consequences of doing so ecological fallacy ran both analyses showed them that the first approach generates curve that is closer to what chief ds expects but they say the curve through all individual data points must surely be skewed by the of data points that don fit the curve as perfectly closely have lt years of experience top undergrad in econometrics and am completing my master in ds they have in industry within my first year working with them so at loss of what to do for more context junior so my model isn the primary moneymaker here we produce one out of many inputs that go into business decisions guess in that sense their approaches don automatically endanger the business their or my job but they will present this to upper people at some point their priority is to show clean and easy to understand model as the audience is also non technical even so if it the way it currently is feel that shouldn have my name on it thoughts on how to deal with this situation diplomatically thanks,1,how to deal with non technical manager approach that disagree with,first time working with non technical manager whose domain is industry knowledge and not data statistic most day they know what they don know but sometimes they want to drive some analysis in questionable way to me for example they asked that remove of the random sampled data so that we could build perfectly smooth curve fit for only lt of the data by eyeballing the visuals explained to them that these data point though not fitting well visually on their ideal curve where the majority of the dat is are not definitionally outlier they were randomly sampled by our chief d there wa no data entry error and they belong to our target population by removing them we make the data seem more predictable than it actually is and invalidates conclusion to no avail in similar vein another time they asked for another perfect curve based only on averaged data which remove all variability in our raw data found post describing exactly what they wanted to do here they want to do chart instead of utm_medium ios_app amp utm_name iossmf which explains consequence of doing so ecological fallacy ran both analysis showed them that the first approach generates curve that is closer to what chief d expects but they say the curve through all individual data point must surely be skewed by the of data point that don fit the curve a perfectly closely have lt year of experience top undergrad in econometrics and am completing my master in d they have in industry within my first year working with them so at loss of what to do for more context junior so my model isn the primary moneymaker here we produce one out of many input that go into business decision guess in that sense their approach don automatically endanger the business their or my job but they will present this to upper people at some point their priority is to show clean and easy to understand model a the audience is also non technical even so if it the way it currently is feel that shouldn have my name on it thought on how to deal with this situation diplomatically thanks
how to deal with non technical manager approach that disagree with,first time working with non technical manager whose domain is industry knowledge and not data statistics most days they know what they don know but sometimes they want to drive some analysis in questionable ways to me for example they asked that remove of the random sampled data so that we could build perfectly smooth curve fit for only lt of the data by eyeballing the visuals explained to them that these data points though not fitting well visually on their ideal curve where the majority of the dats is are not definitionally outliers they were randomly sampled by our chief ds there was no data entry errors and they belong to our target population by doing so we make the data seem more predictable than it actually is and invalidates conclusions to no avail in similar veins another time they asked for another perfect curve based only on averaged data which removes all variability in our raw data found post describing exactly what they wanted to do here they want to do chart instead of utm_medium ios_app amp utm_name iossmf which explains consequences of doing so ecological fallacy ran both analyses showed them that the first approach generates curve that is closer to what chief ds expects but they say the curve through all individual data points must surely be skewed by the of data points that don fit the curve as perfectly closely have lt years of experience top undergrad in econometrics and am completing my master in ds they have in industry within my first year working with them so at loss of what to do for more context junior so my model isn the primary moneymaker here we produce one out of many inputs that go into business decisions guess in that sense their approaches don automatically endanger the business their or my job but they will present this to upper people at some point their priority is to show clean and easy to understand model as the audience is also non technical even so if it the way it currently is feel that shouldn have my name on it thoughts on how to deal with this situation diplomatically thanks,1,how to deal with non technical manager approach that disagree with,first time working with non technical manager whose domain is industry knowledge and not data statistic most day they know what they don know but sometimes they want to drive some analysis in questionable way to me for example they asked that remove of the random sampled data so that we could build perfectly smooth curve fit for only lt of the data by eyeballing the visuals explained to them that these data point though not fitting well visually on their ideal curve where the majority of the dat is are not definitionally outlier they were randomly sampled by our chief d there wa no data entry error and they belong to our target population by doing so we make the data seem more predictable than it actually is and invalidates conclusion to no avail in similar vein another time they asked for another perfect curve based only on averaged data which remove all variability in our raw data found post describing exactly what they wanted to do here they want to do chart instead of utm_medium ios_app amp utm_name iossmf which explains consequence of doing so ecological fallacy ran both analysis showed them that the first approach generates curve that is closer to what chief d expects but they say the curve through all individual data point must surely be skewed by the of data point that don fit the curve a perfectly closely have lt year of experience top undergrad in econometrics and am completing my master in d they have in industry within my first year working with them so at loss of what to do for more context junior so my model isn the primary moneymaker here we produce one out of many input that go into business decision guess in that sense their approach don automatically endanger the business their or my job but they will present this to upper people at some point their priority is to show clean and easy to understand model a the audience is also non technical even so if it the way it currently is feel that shouldn have my name on it thought on how to deal with this situation diplomatically thanks
how to deal with non technical manager approach that disagree with,first time working with non technical manager whose domain is industry knowledge and not data statistics most days they know what they don know but sometimes they want to drive some analysis in questionable ways to me for example they asked that remove of the random sampled data so that we could build perfectly smooth curve fit for only lt of the data by eyeballing the visuals explained to them that these data points though not fitting well visually on his ideal curve are not definitionally outliers they were randomly sampled by our chief ds there was no data entry errors and they belong to our target population by doing so we make the data seem more predictable than it actually is and invalidates conclusions to no avail in similar veins another time he asked for another perfect curve on averaged data which removes all variability in our raw data found post describing exactly what he wants to do here he wants to do chart instead of utm_medium ios_app amp utm_name iossmf which explains consequences of doing so ecological fallacy have lt years of experience top undergrad in econometrics and am completing my master in ds he has in industry within my first year working with him so at loss of what to do for more context junior so my model isn the primary moneymaker here we produce one out of many inputs that go into business decisions guess in that sense his approaches don automatically endanger the business his or my job but he will present this to upper people at some point his priority is to show clean and easy to understand model as the audience is also non technical even so if it the way it currently is feel that shouldn have my name on it thoughts on how to deal with this situation diplomatically thanks,1,how to deal with non technical manager approach that disagree with,first time working with non technical manager whose domain is industry knowledge and not data statistic most day they know what they don know but sometimes they want to drive some analysis in questionable way to me for example they asked that remove of the random sampled data so that we could build perfectly smooth curve fit for only lt of the data by eyeballing the visuals explained to them that these data point though not fitting well visually on his ideal curve are not definitionally outlier they were randomly sampled by our chief d there wa no data entry error and they belong to our target population by doing so we make the data seem more predictable than it actually is and invalidates conclusion to no avail in similar vein another time he asked for another perfect curve on averaged data which remove all variability in our raw data found post describing exactly what he want to do here he want to do chart instead of utm_medium ios_app amp utm_name iossmf which explains consequence of doing so ecological fallacy have lt year of experience top undergrad in econometrics and am completing my master in d he ha in industry within my first year working with him so at loss of what to do for more context junior so my model isn the primary moneymaker here we produce one out of many input that go into business decision guess in that sense his approach don automatically endanger the business his or my job but he will present this to upper people at some point his priority is to show clean and easy to understand model a the audience is also non technical even so if it the way it currently is feel that shouldn have my name on it thought on how to deal with this situation diplomatically thanks
data modeling for gps location,hello curious if anyone can direct me towards resources course names or tutorials for learning the type of data schemas and modeling that go into location tracking thinking apps like bike sharing you show where record ex bike or scooter is physically located at specific time and it can change the stations are set locations love to learn more about the data structures involved in an app like that thanks,1,data modeling for gps location,hello curious if anyone can direct me towards resource course name or tutorial for learning the type of data schema and modeling that go into location tracking thinking apps like bike sharing you show where record ex bike or scooter is physically located at specific time and it can change the station are set location love to learn more about the data structure involved in an app like that thanks
how likely is it for someone to become data scientist without majoring in cs or stat,im currently senior at university who is set to graduate in spring and my major is business analytics and it am pursuing data science undergraduate certificate offered by my uni as well hear stories of how it is pretty difficult to get into without masters or cs or stat background doubt my undergrad certificate as well as my particular major can compete with most other applicants so ve been trying to apply for data analyst roles hoping for interviews is there anything that can try to improve my chances of landing job as data scientist greatly appreciate any feedback,1,how likely is it for someone to become data scientist without majoring in c or stat,im currently senior at university who is set to graduate in spring and my major is business analytics and it am pursuing data science undergraduate certificate offered by my uni a well hear story of how it is pretty difficult to get into without master or c or stat background doubt my undergrad certificate a well a my particular major can compete with most other applicant so ve been trying to apply for data analyst role hoping for interview is there anything that can try to improve my chance of landing job a data scientist greatly appreciate any feedback
what are some cool random forest ml applications,have seen some jaw dropping examples of neural networks and deep learning deep fakes am looking for similarly awesome examples of what random forests can do please share,1,what are some cool random forest ml application,have seen some jaw dropping example of neural network and deep learning deep fake am looking for similarly awesome example of what random forest can do please share
need advice on managing data cleaning projects,tldr any suggestion for how to manage collaborative data cleaning projects specially when similar cleaning will be necessary for future cleaning long version work with conversational transcript data saved as csv our research interests are always evolving for example in june we want to answer particular question from our data and we do cleaning and other required tasks such as annotating certain properties from the speech in december we want to perform new analysis we do this again please note we hire research interns assistants for this tasks because of our domain its very hard to automate the data cleaning process based on the experience so far the data repository which is simply folder and files becomes mess we end up with mix of xlsx and csv column names are inconsistent redundant columns from previous iteration are also there and worst of all we end up with files that were created in macos and windows causing unicode related errors suggestions will be welcomed thanks,1,need advice on managing data cleaning project,tldr any suggestion for how to manage collaborative data cleaning project specially when similar cleaning will be necessary for future cleaning long version work with conversational transcript data saved a csv our research interest are always evolving for example in june we want to answer particular question from our data and we do cleaning and other required task such a annotating certain property from the speech in december we want to perform new analysis we do this again please note we hire research intern assistant for this task because of our domain it very hard to automate the data cleaning process based on the experience so far the data repository which is simply folder and file becomes mess we end up with mix of xlsx and csv column name are inconsistent redundant column from previous iteration are also there and worst of all we end up with file that were created in macos and window causing unicode related error suggestion will be welcomed thanks
help selecting gift,my partner is data scientist and is incredibly type he enjoys building things lego puzzles watching the stock market and is always learning more about his job field of work am quite the opposite and am having hard time with part of his gift do design work industrial floriculture so would love to be able to design it or get inspiration from something know lot of people may say galton board but not sure if he would actually enjoy that or not he is super minimalist and doesn buy things just to have them it has to be something he really wants anyway if you have made it this far thank you so much and would love to hear any ideas not for christmas just planning for his birthday in the coming months happy holidays,1,help selecting gift,my partner is data scientist and is incredibly type he enjoys building thing lego puzzle watching the stock market and is always learning more about his job field of work am quite the opposite and am having hard time with part of his gift do design work industrial floriculture so would love to be able to design it or get inspiration from something know lot of people may say galton board but not sure if he would actually enjoy that or not he is super minimalist and doesn buy thing just to have them it ha to be something he really want anyway if you have made it this far thank you so much and would love to hear any idea not for christmas just planning for his birthday in the coming month happy holiday
would getting job with an as degree in data science,considering getting degree in data science my local college offers an as degree looking for entry level work with the degree then going back to school for better work opportunities would it be difficult with an as,1,would getting job with an a degree in data science,considering getting degree in data science my local college offer an a degree looking for entry level work with the degree then going back to school for better work opportunity would it be difficult with an a
text classification via keywords of news articles via google news api bayes vs logistic regression vs,hello everyone based on set of keywords am using the google news api to collect news articles the newspaper3k python lib then gives me summaries and keywords for those articles this works fairly well but am of course getting false positives for example one of my keywords is pi as in raspberry pi and get hits on magnum pi the tv show another is docker and get hits on docker street which think is in australia also football team have added the idea of anti keywords where if an article has my keyword python but also has the anti keyword phrase reticulated python like the snake ignore it this also works pretty well but like to further decrease my false positives and maybe learn something in the process what is good way to do this ve been trying to research bayes and logistic regression but don quite have my head wrapped around it think its just text classification think want to drop stopwords lemmitize and then pass the summary keywords url to an algo perhaps along with the keyword am matching against then maybe get score back then decide based on the score ve got redis docker container ready to go for data persistence don think this is just simple spam ham issue of group of articles with python might want some but not others based on the context can anyone provide guidance tia our sole,1,text classification via keywords of news article via google news api bayes v logistic regression v,hello everyone based on set of keywords am using the google news api to collect news article the newspaper3k python lib then give me summary and keywords for those article this work fairly well but am of course getting false positive for example one of my keywords is pi a in raspberry pi and get hit on magnum pi the tv show another is docker and get hit on docker street which think is in australia also football team have added the idea of anti keywords where if an article ha my keyword python but also ha the anti keyword phrase reticulated python like the snake ignore it this also work pretty well but like to further decrease my false positive and maybe learn something in the process what is good way to do this ve been trying to research bayes and logistic regression but don quite have my head wrapped around it think it just text classification think want to drop stopwords lemmitize and then pas the summary keywords url to an algo perhaps along with the keyword am matching against then maybe get score back then decide based on the score ve got redis docker container ready to go for data persistence don think this is just simple spam ham issue of group of article with python might want some but not others based on the context can anyone provide guidance tia our sole
what are your thoughts on azure automl or any other auto ml platform,apologies probably been asked plenty of times being relatively new to ml can really see reason why auto ml isn viewed as anything other than really useful tool just from my impression online it seems most people involved in ml don rate it surely being able to test loads of models is time saving thing and the ability to tune hyper parameters and cross validate automatically personally don yet have the ability to tell which model would be most appropriate so ve enjoyed using it and seeing what models work well haven deployed model from there yet though see one downside it being bit like black box and it doesn feel very science to use appreciate any thoughts on this,1,what are your thought on azure automl or any other auto ml platform,apology probably been asked plenty of time being relatively new to ml can really see reason why auto ml isn viewed a anything other than really useful tool just from my impression online it seems most people involved in ml don rate it surely being able to test load of model is time saving thing and the ability to tune hyper parameter and cross validate automatically personally don yet have the ability to tell which model would be most appropriate so ve enjoyed using it and seeing what model work well haven deployed model from there yet though see one downside it being bit like black box and it doesn feel very science to use appreciate any thought on this
text analysis for survey data,looking for text analysis application that uses machine learning for one off survey project the number of responses records could be up to to open ended questions like to have analysts code of responses and then have the machine do the rest with the ability for the analyst to then perform quality control we have an existing application that uses but it can handle this type of volume due to the software setup looking at monkeylearn but interested in other solutions it can be too technical as the coding capability of the researchers is limited,1,text analysis for survey data,looking for text analysis application that us machine learning for one off survey project the number of response record could be up to to open ended question like to have analyst code of response and then have the machine do the rest with the ability for the analyst to then perform quality control we have an existing application that us but it can handle this type of volume due to the software setup looking at monkeylearn but interested in other solution it can be too technical a the coding capability of the researcher is limited
is there place for non professionals to ask questions about data science in society,is there place for non professionals to ask questions about data science in society see that this is mostly for pros to talk to pros and there are other places like learndatascience learndatasci learnpython etc for students to talk to everyone but is there somewhere for randos to talk to pros maybe my best bet is to find community based on the particular data science application wondering about ask in biology community if have question about data science in biology etc,1,is there place for non professional to ask question about data science in society,is there place for non professional to ask question about data science in society see that this is mostly for pro to talk to pro and there are other place like learndatascience learndatasci learnpython etc for student to talk to everyone but is there somewhere for randos to talk to pro maybe my best bet is to find community based on the particular data science application wondering about ask in biology community if have question about data science in biology etc
where do you get your news on advancements in ds ml,where do you get news on advancements in ai ml that aren focused on the major buzz topics like transformers,1,where do you get your news on advancement in d ml,where do you get news on advancement in ai ml that aren focused on the major buzz topic like transformer
deep learning with unordered datapoints,have project coming up over the holidays where will probably need to use neural network to find certain patterns in dataset of sports tracking data and coordinate per player some patterns are easy matchable with rule based approach others are quite complicated as patterns are not only still but multiple frames of positions of players will probably need to use lstm layer the part am currently struggling is how to pass the input the right way as can not order players by strong qualitative measurement position or number of player is not meaningful,1,deep learning with unordered datapoints,have project coming up over the holiday where will probably need to use neural network to find certain pattern in dataset of sport tracking data and coordinate per player some pattern are easy matchable with rule based approach others are quite complicated a pattern are not only still but multiple frame of position of player will probably need to use lstm layer the part am currently struggling is how to pas the input the right way a can not order player by strong qualitative measurement position or number of player is not meaningful
building an edge api gateway with fauna and securing it with auth0,in this tutorial we ll explore architecting rest apis in fully serverless manner by leveraging fastly compute edge fauna and auth0 read more utm_medium sc amp utm_campaign fauna,1,building an edge api gateway with fauna and securing it with auth0,in this tutorial we ll explore architecting rest apis in fully serverless manner by leveraging fastly compute edge fauna and auth0 read more utm_medium sc amp utm_campaign fauna
notes from stats class,notes from stats class,1,note from stats class,note from stats class
hbr says that data cleaning is not time consuming to acquire and not useful,hbr says that data cleaning is not time consuming to acquire and not useful,1,hbr say that data cleaning is not time consuming to acquire and not useful,hbr say that data cleaning is not time consuming to acquire and not useful
job interview,job interview,1,job interview,job interview
looking to interview folks that participated in kaggle competition,paying bucks to chat with me for minutes about your experience on kaggle and participating in competition how was that experience moderators not sure if this is an ok post apologies if not,1,looking to interview folk that participated in kaggle competition,paying buck to chat with me for minute about your experience on kaggle and participating in competition how wa that experience moderator not sure if this is an ok post apology if not
data science graduate for business analyst role,am current data science msc graduate in the uk with few internships under my belt all of them mostly data analytics projects with previous background in geoinformatics spatial data analytics if you want am currently applying and being interviewed for several positions and although my goal is to be technical data scientist or maybe ml engineer am now in front of the offer in the company energy industry like salary is alright and felt good about the interview itself but it is for the role of business analyst although overall written as da position when they want sb with python sql skills also need and want to start asap so cannot wait more months for some imaginary dream position but should be scared about being business analyst in the early stage of my career although am already in my late can it be taken as disadvantage for the future job hunt for ds positions do care too much and it is just wordplay for the role as said am quite confident will be doing data heavy stuff there especially when want to am more concert about the title itself if that is something that can be looked upon in the future anyone with similar experience,1,data science graduate for business analyst role,am current data science msc graduate in the uk with few internship under my belt all of them mostly data analytics project with previous background in geoinformatics spatial data analytics if you want am currently applying and being interviewed for several position and although my goal is to be technical data scientist or maybe ml engineer am now in front of the offer in the company energy industry like salary is alright and felt good about the interview itself but it is for the role of business analyst although overall written a da position when they want sb with python sql skill also need and want to start asap so cannot wait more month for some imaginary dream position but should be scared about being business analyst in the early stage of my career although am already in my late can it be taken a disadvantage for the future job hunt for d position do care too much and it is just wordplay for the role a said am quite confident will be doing data heavy stuff there especially when want to am more concert about the title itself if that is something that can be looked upon in the future anyone with similar experience
encoder decoder sequence to sequence models,encoder decoder sequence to sequence models,1,encoder decoder sequence to sequence model,encoder decoder sequence to sequence model
which course should take to start dsa,starting data structures and analysis ve basic knowledge in programming haven learnt oop so which course should take in udemy coursera youtube so that have firm grip on the concept want to do project as well so ask for advice,1,which course should take to start dsa,starting data structure and analysis ve basic knowledge in programming haven learnt oop so which course should take in udemy coursera youtube so that have firm grip on the concept want to do project a well so ask for advice
toronto canada salary expectations for ds yrs experience and masters,my partner just completed his master in data science amp ai from waterloo but prior to enrolling in the program he worked as data scientist for just over years he beginning to interview this week and is unsure what realistic salary expectation would be any insight would be appreciated,1,toronto canada salary expectation for d yr experience and master,my partner just completed his master in data science amp ai from waterloo but prior to enrolling in the program he worked a data scientist for just over year he beginning to interview this week and is unsure what realistic salary expectation would be any insight would be appreciated
magazine blog or subscription,like to find website magazine or newsletter that offers business news related to data science please send me your recommendations,1,magazine blog or subscription,like to find website magazine or newsletter that offer business news related to data science please send me your recommendation
need to learn some data science for work out of these books what would you guys recommend,background electrical engineer working with large datasets learning python and sql learned in school so this is going pretty quick weak foundation in statistics which ve recognized and am trying to correct see statistics book recommendations for an engineer three books have caught my eye they are python for data analysis by wes mckinney python data science by vanderplas data science from scratch by grus am thinking of going with python for data science by mckinney and pairing it with practical statistics for data scientists by peter bruce although given my limited knowledge was thinking data science from scratch may be more appropriate what do you guys think,1,need to learn some data science for work out of these book what would you guy recommend,background electrical engineer working with large datasets learning python and sql learned in school so this is going pretty quick weak foundation in statistic which ve recognized and am trying to correct see statistic book recommendation for an engineer three book have caught my eye they are python for data analysis by wes mckinney python data science by vanderplas data science from scratch by grus am thinking of going with python for data science by mckinney and pairing it with practical statistic for data scientist by peter bruce although given my limited knowledge wa thinking data science from scratch may be more appropriate what do you guy think
arithmetic operations with numpy,arithmetic operations with numpy,1,arithmetic operation with numpy,arithmetic operation with numpy
supply chain data scientists how do you all predict lead time promise times for your online products,looking to start discussion on different models and methods people use to predict lead times for products this is more of an online retailer question as caveat but am wondering what predictors you all use the level of accuracy scaling it out to your respective product website what re some of the biggest roadblocks you have faced where can read up on models regarding this business problem,1,supply chain data scientist how do you all predict lead time promise time for your online product,looking to start discussion on different model and method people use to predict lead time for product this is more of an online retailer question a caveat but am wondering what predictor you all use the level of accuracy scaling it out to your respective product website what re some of the biggest roadblock you have faced where can read up on model regarding this business problem
my ds job is limiting my learning where can go online to improve my knowledge that isn too easy or basic or am just being paranoid,ve been working in my first ds job for months now and for the first two months was loving the process of learning something new everyday in the first couple months was learning lot of python because previously had only used and also learned to use power bi created report in my first two months in power bi and one of my colleagues who was guiding me told me that my report was much better than reports she had seen elsewhere by others who had lot more experience than did was so please with that feedback and wanted to keep learning but it has very much plateaud since then feel like in the last two months just doing the same stuff with different data not learning creating new functions or learning new software like power bi simply applying the knowledge that learned in the first two months of my job and from uni to apply it to new data and it getting boring and killing my passion just couple weeks ago requested access to new ide because it looked really cool and was keen to learn something new but my boss rejected the request and said that should focus on what on my plate instead haven even done any work with api before and feel like that something really simple perhaps just being paranoid that falling behind don want to stop the learning process especially because don want to stay in this job forever my biggest worry is that in months ll leave my job and struggle getting new one because graduate schemes will be too low of salary and more experienced roles will be too difficult for me is this valid worry where can go to learn more ds online in my free time that isn for beginners,1,my d job is limiting my learning where can go online to improve my knowledge that isn too easy or basic or am just being paranoid,ve been working in my first d job for month now and for the first two month wa loving the process of learning something new everyday in the first couple month wa learning lot of python because previously had only used and also learned to use power bi created report in my first two month in power bi and one of my colleague who wa guiding me told me that my report wa much better than report she had seen elsewhere by others who had lot more experience than did wa so please with that feedback and wanted to keep learning but it ha very much plateaud since then feel like in the last two month just doing the same stuff with different data not learning creating new function or learning new software like power bi simply applying the knowledge that learned in the first two month of my job and from uni to apply it to new data and it getting boring and killing my passion just couple week ago requested access to new ide because it looked really cool and wa keen to learn something new but my bos rejected the request and said that should focus on what on my plate instead haven even done any work with api before and feel like that something really simple perhaps just being paranoid that falling behind don want to stop the learning process especially because don want to stay in this job forever my biggest worry is that in month ll leave my job and struggle getting new one because graduate scheme will be too low of salary and more experienced role will be too difficult for me is this valid worry where can go to learn more d online in my free time that isn for beginner
cheat code for breaking into any field,lot of people are trying to get into data science related fields and frequently ask similar questions along the lines of what do need to know or doing xyz does that make sense that backwards way to think about it the way to do it is to look up few dozen job postings for the role you want from those postings narrow it down to only the jobs you re interested in data science is such wide and non standardized field that not all postings are applicable to you with the postings you re left with identify which skills are common to most of those posts of those skills some you will already have so play them up in the experience of your resume the ones that you don have are ones that you should go learn this is personalized process because of the breadth of the field nobody in the world has expertise in the laundry list of skills people claim you need in medium or towardsdatascience articles,1,cheat code for breaking into any field,lot of people are trying to get into data science related field and frequently ask similar question along the line of what do need to know or doing xyz doe that make sense that backwards way to think about it the way to do it is to look up few dozen job posting for the role you want from those posting narrow it down to only the job you re interested in data science is such wide and non standardized field that not all posting are applicable to you with the posting you re left with identify which skill are common to most of those post of those skill some you will already have so play them up in the experience of your resume the one that you don have are one that you should go learn this is personalized process because of the breadth of the field nobody in the world ha expertise in the laundry list of skill people claim you need in medium or towardsdatascience article
hi any advice for math teacher trying to break into data science,ve been teaching for years have bfa in graphic design ma in special education ms in science of instruction and ma in administration am certified to teach secondary math which ve been teaching exclusively since currently teach ap stats and honors pre calculus am also math resource teacher data coach but over teaching ve lost my passion completed full stack bootcamp few months ago and quickly realized hated front end dev because it felt too practical however excelled in the mongodb courses one of my instructors suggested data science and now love it currently taking harvardx data science professional certification program and still within my week refund period is this the proper way to go also do not have degree in math just valid teaching certification is this realistic path for me or should consider returning to school for ph or proper certification program tldr what path would you suggest for seasoned math stats teacher without math degree to break into data science,1,hi any advice for math teacher trying to break into data science,ve been teaching for year have bfa in graphic design ma in special education m in science of instruction and ma in administration am certified to teach secondary math which ve been teaching exclusively since currently teach ap stats and honor pre calculus am also math resource teacher data coach but over teaching ve lost my passion completed full stack bootcamp few month ago and quickly realized hated front end dev because it felt too practical however excelled in the mongodb course one of my instructor suggested data science and now love it currently taking harvardx data science professional certification program and still within my week refund period is this the proper way to go also do not have degree in math just valid teaching certification is this realistic path for me or should consider returning to school for ph or proper certification program tldr what path would you suggest for seasoned math stats teacher without math degree to break into data science
dataiku vs azure ml,hi at my company we are currently looking for new tooling regarding our strategy for management of ds projects including experiment tracking unit testing of the published model creating audit trail and mlops strategy currently we have only few algorithms in production but the intention is there to make some big leaps in the future currently we are looking at dataiku and azure ml in combination with azure devops for me it feels like both options give the same features to me azure ml feels little bit more professional and is cost wise more scale able could someone explain to me which tool is the better option considering project management testing mlops but also the costs thnx,1,dataiku v azure ml,hi at my company we are currently looking for new tooling regarding our strategy for management of d project including experiment tracking unit testing of the published model creating audit trail and mlops strategy currently we have only few algorithm in production but the intention is there to make some big leap in the future currently we are looking at dataiku and azure ml in combination with azure devops for me it feel like both option give the same feature to me azure ml feel little bit more professional and is cost wise more scale able could someone explain to me which tool is the better option considering project management testing mlops but also the cost thnx
our round up of customer insight leader content amp guests in,our round up of customer insight leader content amp guests in,1,our round up of customer insight leader content amp guest in,our round up of customer insight leader content amp guest in
how can prepare for ibm cognos analytics v11 r2 certification exam plz help,how can prepare for ibm cognos analytics v11 r2 certification exam plz help,1,how can prepare for ibm cognos analytics v11 r2 certification exam plz help,how can prepare for ibm cognos analytics v11 r2 certification exam plz help
running collaborative machine learning experiments with git and cloud guide,the following guide explains how team of ml engineers can bundle your data and code changes for each ml experiment and push those to remote for somebody else using google drive folder to check out using dvc data version control tool running collaborative experiments it shows how setting up dvc remotes in addition to your git remotes lets you share all of the data code and hyperparameters associated with each experiment so anyone can pick up where you left off in the training process,1,running collaborative machine learning experiment with git and cloud guide,the following guide explains how team of ml engineer can bundle your data and code change for each ml experiment and push those to remote for somebody else using google drive folder to check out using dvc data version control tool running collaborative experiment it show how setting up dvc remote in addition to your git remote let you share all of the data code and hyperparameters associated with each experiment so anyone can pick up where you left off in the training process
milvus vector database is now cloud scalable,milvus vector database is now cloud scalable,1,milvus vector database is now cloud scalable,milvus vector database is now cloud scalable
have detailed records of my every purchases on excel what value can extract from that,something like purchase habits what would be great for personal uses,1,have detailed record of my every purchase on excel what value can extract from that,something like purchase habit what would be great for personal us
advice on learning vs python,posting here hoping for insight from people who have used both extensively for background work in academia in the healthcare sector took some undergraduate courses in coding but haven touched coding in years looking to relearn coding partly just to pursue personal projects like apps and websites for fun and partly for facilitate some research projects my most immediate need is to take an excel file full of relatively tidy data clean it up and runs some basic stats and graphics on it could power through just using excel but get the impression some work upfront to get the data into something like would make it easier to revisit the data with new questions in the future like to learn to automate more of the process like pulling data from an emr or other database cleaning it and putting it into research database like redcap far future think it would be interesting to learn more advanced topics like ai ml and big data at least to the level of understanding it and collaborating with actual experts initially was going to learn since it free relatively robust and seems tailored for my more immediate needs but wondering if python with some of the stats libraries would be better investment my thought is with python being more versatile as someone not employed full time to be data science or programming expert maybe be better off just learning to accomplish my immediate goals with python and then having foundation to build off of for other uses any thoughts sorry for the somewhat rambling question,1,advice on learning v python,posting here hoping for insight from people who have used both extensively for background work in academia in the healthcare sector took some undergraduate course in coding but haven touched coding in year looking to relearn coding partly just to pursue personal project like apps and website for fun and partly for facilitate some research project my most immediate need is to take an excel file full of relatively tidy data clean it up and run some basic stats and graphic on it could power through just using excel but get the impression some work upfront to get the data into something like would make it easier to revisit the data with new question in the future like to learn to automate more of the process like pulling data from an emr or other database cleaning it and putting it into research database like redcap far future think it would be interesting to learn more advanced topic like ai ml and big data at least to the level of understanding it and collaborating with actual expert initially wa going to learn since it free relatively robust and seems tailored for my more immediate need but wondering if python with some of the stats library would be better investment my thought is with python being more versatile a someone not employed full time to be data science or programming expert maybe be better off just learning to accomplish my immediate goal with python and then having foundation to build off of for other us any thought sorry for the somewhat rambling question
making up for no math stats background,do not have math stats background am currently doing datacamp data science with python career track am unable to grasp courses with stats background like statistical thinking in python part and part how can learn enough of the theory to get through these courses and get the certification have tried studying statistics from khan academy the course is pretty basic also tried using statsquest youtube channel but the videos went over my head,1,making up for no math stats background,do not have math stats background am currently doing datacamp data science with python career track am unable to grasp course with stats background like statistical thinking in python part and part how can learn enough of the theory to get through these course and get the certification have tried studying statistic from khan academy the course is pretty basic also tried using statsquest youtube channel but the video went over my head
how iot is changing our lives,how iot is changing our lives,1,how iot is changing our life,how iot is changing our life
forbes article from stated we just passed the peak of inflated expectations with data science and we are about to enter the trough of disillusionment do you believe this to be true or not,forbes article in seeing more random google articles with headlines like the data science bubble etc are these sentiments true or just headline grabbers,1,forbes article from stated we just passed the peak of inflated expectation with data science and we are about to enter the trough of disillusionment do you believe this to be true or not,forbes article in seeing more random google article with headline like the data science bubble etc are these sentiment true or just headline grabber
what kind of fun dataset meme dataset or scenario can use for my image similarity search app,what kind of fun dataset meme dataset or scenario can use for my image similarity search app,1,what kind of fun dataset meme dataset or scenario can use for my image similarity search app,what kind of fun dataset meme dataset or scenario can use for my image similarity search app
non job ways to gain experience,hello all college student majoring in data analytics here as embark on my internship search for next summer like to do bit more in the meanwhile to improve my chances of scoring good internship what are some things can do outside of class and personal projects to help improve my chances of getting good job and building up some experience are there any good data related open source projects or something,1,non job way to gain experience,hello all college student majoring in data analytics here a embark on my internship search for next summer like to do bit more in the meanwhile to improve my chance of scoring good internship what are some thing can do outside of class and personal project to help improve my chance of getting good job and building up some experience are there any good data related open source project or something
funky logit model output,working on project featuring logistical regression that indicates difficulty on paying mortgage when look at kde showing estimate vs actual my target is way over estimated for couple features but looks good when target my squared is negative which is puzzling me my confusion matrix is heavy on the false negatives can quite figure out where am throwing things off the model seems like nonsense but it is still about accurate tried removing outliers hope you don mind the screenshots thanks,1,funky logit model output,working on project featuring logistical regression that indicates difficulty on paying mortgage when look at kde showing estimate v actual my target is way over estimated for couple feature but look good when target my squared is negative which is puzzling me my confusion matrix is heavy on the false negative can quite figure out where am throwing thing off the model seems like nonsense but it is still about accurate tried removing outlier hope you don mind the screenshots thanks
promoting data driven culture in startups,am curious to hear what are some of the common methods used by startups to foster data driven culture currently we have good set of dashboards and we schedule weekly reports however think we are not great at sharing insights we do lot of analysis but it is very siloed and only focussed on certain teams am looking for some creative ways to get stakeholders interested in data perhaps news letter scheduling analysis,1,promoting data driven culture in startup,am curious to hear what are some of the common method used by startup to foster data driven culture currently we have good set of dashboard and we schedule weekly report however think we are not great at sharing insight we do lot of analysis but it is very siloed and only focussed on certain team am looking for some creative way to get stakeholder interested in data perhaps news letter scheduling analysis
programming languages for data roles big tech,programming languages for data roles big tech,1,programming language for data role big tech,programming language for data role big tech
worth going to low tier data science program in germany to save money,hello everyone currently am an analyst and think the next stage in my life is to become data scientist am strongly considering georgia tech online analytics master program unfortunately don have the best undergrad gpa since school was not my top priority at the time so highly doubt will get in my backup school is iu international university of applied science since it affordable and has easy admissions assuming don get into georgia tech most likely due to low gpa is it dumb idea to apply to iu international university of applied sciences all online classes keep in mind did look at other european university but international university is the most affordable and will most likely admit me thanks in advance,1,worth going to low tier data science program in germany to save money,hello everyone currently am an analyst and think the next stage in my life is to become data scientist am strongly considering georgia tech online analytics master program unfortunately don have the best undergrad gpa since school wa not my top priority at the time so highly doubt will get in my backup school is iu international university of applied science since it affordable and ha easy admission assuming don get into georgia tech most likely due to low gpa is it dumb idea to apply to iu international university of applied science all online class keep in mind did look at other european university but international university is the most affordable and will most likely admit me thanks in advance
drug testing in this field,odd question some backstory my fiance and have never done any drugs in our life mostly due to professional reasons ve been in my position for about years and honestly don remember my drug test or if even had to do one did in my jobs prior she started her own private practice year and half ago we like to try out some edibles to see what it like my question what were the drug tests like for your company if you had to take one,1,drug testing in this field,odd question some backstory my fiance and have never done any drug in our life mostly due to professional reason ve been in my position for about year and honestly don remember my drug test or if even had to do one did in my job prior she started her own private practice year and half ago we like to try out some edible to see what it like my question what were the drug test like for your company if you had to take one
advice for public policy work,graduating in may with my masters in applied math half of my course load was statistics courses pretty sure want to do public policy statistical work does anyone have any advice on how to get there something happened to my previous post,1,advice for public policy work,graduating in may with my master in applied math half of my course load wa statistic course pretty sure want to do public policy statistical work doe anyone have any advice on how to get there something happened to my previous post
ms or phd to gain deeper footing in the field of data science,am young faculty with phd in applied sciences field of food science am really interested in applying data science principles to food research prod development life cycle analysis nutrition supply chain optimization curious to hear your thoughts on whether diving deep with ms phd makes it quicker to learn am struggling to pick skills on my own with out dedicated plan or focus,1,m or phd to gain deeper footing in the field of data science,am young faculty with phd in applied science field of food science am really interested in applying data science principle to food research prod development life cycle analysis nutrition supply chain optimization curious to hear your thought on whether diving deep with m phd make it quicker to learn am struggling to pick skill on my own with out dedicated plan or focus
using data science as force for good environmental modeling workshop,developing weekend workshop marketed to data scientists that want to work or volunteer for environmental causes my goal is to help data scientists without background in environmental science or conservation biology learn what databases are out there and more importantly how to build connections to find work with non profits and academic scientists biologist that uses statistics pretty extensively in sustainability and conservation research ve been impressed with what my friends who went into data science data engineering and machine learning can do there are some big questions in the world of conservation and the environment that simply do not have the skills available to come up with answers but convinced many data scientists do nobody gets rich doing this work but can attest its rewarding and fascinating would love some tips on where to market such workshop as said my target audience is not other biologists know plenty of those the workshop would be hybrid and hosted at the border of upstate new york and connecticut usa approximately hours from nyc hope this doesn break the mod rules am not doing this for profit just to help an organization work with,1,using data science a force for good environmental modeling workshop,developing weekend workshop marketed to data scientist that want to work or volunteer for environmental cause my goal is to help data scientist without background in environmental science or conservation biology learn what database are out there and more importantly how to build connection to find work with non profit and academic scientist biologist that us statistic pretty extensively in sustainability and conservation research ve been impressed with what my friend who went into data science data engineering and machine learning can do there are some big question in the world of conservation and the environment that simply do not have the skill available to come up with answer but convinced many data scientist do nobody get rich doing this work but can attest it rewarding and fascinating would love some tip on where to market such workshop a said my target audience is not other biologist know plenty of those the workshop would be hybrid and hosted at the border of upstate new york and connecticut usa approximately hour from nyc hope this doesn break the mod rule am not doing this for profit just to help an organization work with
best way to learn display sql,between jobs right now after leaving my role as consultant would like to learn sql and use that as springboard to become an analyst as it would dovetail nicely with my mba what are the best ways to learn this and showcase the knowledge so that employers will hire me thanks,1,best way to learn display sql,between job right now after leaving my role a consultant would like to learn sql and use that a springboard to become an analyst a it would dovetail nicely with my mba what are the best way to learn this and showcase the knowledge so that employer will hire me thanks
entry career question,am currently enrolled in master program in data science the program focus is nearly entirely in and was wondering what should do to prepare myself for job as ll finish by spring my focus is in microeconomic analysis and am going to get an internship this summer hopefully in something related to data consulting how do set myself up for success here add the projects ve worked on and created to my github attempt to do freelance work data cleaning transformation etc also what type of roles does one apply for when my knowledge is entirely in thank you,1,entry career question,am currently enrolled in master program in data science the program focus is nearly entirely in and wa wondering what should do to prepare myself for job a ll finish by spring my focus is in microeconomic analysis and am going to get an internship this summer hopefully in something related to data consulting how do set myself up for success here add the project ve worked on and created to my github attempt to do freelance work data cleaning transformation etc also what type of role doe one apply for when my knowledge is entirely in thank you
modern data stack everything that you need to know about building and operating modern data stack,we are live on product hunt say hello to mds on product hunt modern data stack platform for everything you need to know about the modern data stack companies amp categories shaping the modern data stack create share your data stacks amp explore data stacks of the world top companies resources to get updates on the latest in this space jobs in data engineering amp more we are live on product hunt it big day for us we love to get your support and hear your feedback in the comments,1,modern data stack everything that you need to know about building and operating modern data stack,we are live on product hunt say hello to md on product hunt modern data stack platform for everything you need to know about the modern data stack company amp category shaping the modern data stack create share your data stack amp explore data stack of the world top company resource to get update on the latest in this space job in data engineering amp more we are live on product hunt it big day for u we love to get your support and hear your feedback in the comment
do you tend to forget your fundamentals,do you ever get the feeling that your fundamentals are either trash or shaky or just need revision which is often very fast when you learned well this is currently personally bugging me lot in the domain of statistics and to lesser degree most math if you experience something similar occasionally what do you do about it and how do you tend to think about it,1,do you tend to forget your fundamental,do you ever get the feeling that your fundamental are either trash or shaky or just need revision which is often very fast when you learned well this is currently personally bugging me lot in the domain of statistic and to lesser degree most math if you experience something similar occasionally what do you do about it and how do you tend to think about it
beginning to think that the data science job want doesn actually exist,beginning to think that the data science job want doesn actually exist ve had three roles in in the ds industry and ve come to believe that there are three ds archetypes the researcher these are the guys gals who invented fb prophet for example their work is all about amp to bring tool that could support many hypothetically infinite downstream problems and performance on benchmark tasks is emphasized without phd it hard to get considered for these sorts of roles the engineer these are the folks who seldom invent truly novel models but more often train and deploy established models as service they re not inventing the next world class language model they re using transfer learning on bert to build an mlaas product ve had these roles before but find them unsatisfying the answer is almost always more data or better hyperparameter tuning but understanding the inner mechanics of the model takes back seat to knowing how to integrate model into production environment cs heavy which isn my background the statistician these are the data scientists who are interested in understanding the user product ecosystem their output isn deployed model but articulating who what where when amp why their output is tangible and unblocks executives when decision needs to be made my current role by far is most aligned with the statistician archetype ds product analytics at meta my personal interest area is in bayesian statistics find hypothesis testing to be reductionist and view bayesian methods as holistic models that better answer who what where when amp why yet meta has very fast paced impact oriented culture sure could do regression analysis to infer the interaction of age and gender on product usage but it an unspoken rule that discouraged from doing so it preferred that visualize relationships formulate hypothesis and generate some confidence intervals using our experimentation platform nobody cares about quantifying the magnitude of the interaction they re much more binary significant or not decide the cadence is so fast that there isn an appetite for fully bayesian approach feel that my dream job might not exist love to take real problem draw up diagram of relevant variables define model in pymc3 or stan use mcmc simulations and visualize the posterior to better understand who what any thoughts on this do you agree with my archetypes would you add remove modify any and any hints on where might look for the flavor of ds more aligned with my pitch very close to saying it making cash stocks at growth company my dream job isn worth giving up good deal,1,beginning to think that the data science job want doesn actually exist,beginning to think that the data science job want doesn actually exist ve had three role in in the d industry and ve come to believe that there are three d archetype the researcher these are the guy gal who invented fb prophet for example their work is all about amp to bring tool that could support many hypothetically infinite downstream problem and performance on benchmark task is emphasized without phd it hard to get considered for these sort of role the engineer these are the folk who seldom invent truly novel model but more often train and deploy established model a service they re not inventing the next world class language model they re using transfer learning on bert to build an mlaas product ve had these role before but find them unsatisfying the answer is almost always more data or better hyperparameter tuning but understanding the inner mechanic of the model take back seat to knowing how to integrate model into production environment c heavy which isn my background the statistician these are the data scientist who are interested in understanding the user product ecosystem their output isn deployed model but articulating who what where when amp why their output is tangible and unblocks executive when decision need to be made my current role by far is most aligned with the statistician archetype d product analytics at meta my personal interest area is in bayesian statistic find hypothesis testing to be reductionist and view bayesian method a holistic model that better answer who what where when amp why yet meta ha very fast paced impact oriented culture sure could do regression analysis to infer the interaction of age and gender on product usage but it an unspoken rule that discouraged from doing so it preferred that visualize relationship formulate hypothesis and generate some confidence interval using our experimentation platform nobody care about quantifying the magnitude of the interaction they re much more binary significant or not decide the cadence is so fast that there isn an appetite for fully bayesian approach feel that my dream job might not exist love to take real problem draw up diagram of relevant variable define model in pymc3 or stan use mcmc simulation and visualize the posterior to better understand who what any thought on this do you agree with my archetype would you add remove modify any and any hint on where might look for the flavor of d more aligned with my pitch very close to saying it making cash stock at growth company my dream job isn worth giving up good deal
becoming data scientist without degree,am years old have prior experience in the music industry assisting management companies with marketing rollouts and engagement analysis of their artists also have experience in the fashion industry having had brand of my own that was fairly successful always loved programming and the potential to be able to fuse that with critical analyses to strategize new ideas for company and or myself is promising to me have completed the ibm data science professional certificate andrew yang machine learning stanford course amp deep learning course and am currently learning tensorflow am also in the process of building projects of my own to start my resume with all of that being said is it possible in to get position as data scientist without degree have asked several people within the industry if it is possible and they have said yes understand few years ago it didn seem too feasible but now it seems like the narrative has changed and things have progressed to be more accepting of people without degrees just want other honest opinions don want to be naive with solid portfolio and experience under my belt think it is definitely possible thank you for your time and opinions,1,becoming data scientist without degree,am year old have prior experience in the music industry assisting management company with marketing rollouts and engagement analysis of their artist also have experience in the fashion industry having had brand of my own that wa fairly successful always loved programming and the potential to be able to fuse that with critical analysis to strategize new idea for company and or myself is promising to me have completed the ibm data science professional certificate andrew yang machine learning stanford course amp deep learning course and am currently learning tensorflow am also in the process of building project of my own to start my resume with all of that being said is it possible in to get position a data scientist without degree have asked several people within the industry if it is possible and they have said yes understand few year ago it didn seem too feasible but now it seems like the narrative ha changed and thing have progressed to be more accepting of people without degree just want other honest opinion don want to be naive with solid portfolio and experience under my belt think it is definitely possible thank you for your time and opinion
extending pandas,extending pandas,1,extending panda,extending panda
using asterank api,hey trying to use the api on asterank to retrieve data on asteroids but get lt response gt after making request to query amp limit limit should change something how do retrieve their entire asteroid dataset would really appreciate any help,1,using asterank api,hey trying to use the api on asterank to retrieve data on asteroid but get lt response gt after making request to query amp limit limit should change something how do retrieve their entire asteroid dataset would really appreciate any help
ve had similar experience to this in what is simultaneously the most in demand and oversaturated field on earth,ve had similar experience to this in what is simultaneously the most in demand and oversaturated field on earth,1,ve had similar experience to this in what is simultaneously the most in demand and oversaturated field on earth,ve had similar experience to this in what is simultaneously the most in demand and oversaturated field on earth
what are core components of neural networks,what are core components of neural networks,1,what are core component of neural network,what are core component of neural network
how important is numerical analysis for data science,already signed up for numerical methods class from the cs department which is very application focused and was originally planning on doing sequence of graduate numerical analysis from the math department next year which is very theory focused but now wondering if better off taking some other class instead of the sequence ie graduate algorithms class and graduate real analysis class is that good idea is an upper division undergrad numerical methods class sufficient aiming for phd in statistics after undergrad,1,how important is numerical analysis for data science,already signed up for numerical method class from the c department which is very application focused and wa originally planning on doing sequence of graduate numerical analysis from the math department next year which is very theory focused but now wondering if better off taking some other class instead of the sequence ie graduate algorithm class and graduate real analysis class is that good idea is an upper division undergrad numerical method class sufficient aiming for phd in statistic after undergrad
how to know convolutional neural networks,how to know convolutional neural networks,1,how to know convolutional neural network,how to know convolutional neural network
how to avoid overfitting in deep learning neural networks,how to avoid overfitting in deep learning neural networks,1,how to avoid overfitting in deep learning neural network,how to avoid overfitting in deep learning neural network
amazon ads api,need to extract data from amazon ads and dsp campaigns the business has the reports in amazon platform but they would like to create more in detail reports and cross compare so need to take the underlying data of said reports or at least download and extract the data from the reports does anyone have any experience with this these api link to api doc below am reading it but it slow and painful also google did not help there are articles advertising rd party solutions of extracting data so know it is doable at least apis documentation,0,amazon ad api,need to extract data from amazon ad and dsp campaign the business ha the report in amazon platform but they would like to create more in detail report and cross compare so need to take the underlying data of said report or at least download and extract the data from the report doe anyone have any experience with this these api link to api doc below am reading it but it slow and painful also google did not help there are article advertising rd party solution of extracting data so know it is doable at least apis documentation
castled io is the fastest reverse etl platform till date,castled is to faster than other reverse etl platforms like census and hightouch the project is open source census published blog month ago in which they claimed to be the fastest reverse etl solution they even encouraged other solutions to publish their respective performance benchmarks using their dataset while castled has different take on the need for speed in data integration solution we thought it would be interesting to take up this challenge check out this blog to know more,0,castled io is the fastest reverse etl platform till date,castled is to faster than other reverse etl platform like census and hightouch the project is open source census published blog month ago in which they claimed to be the fastest reverse etl solution they even encouraged other solution to publish their respective performance benchmark using their dataset while castled ha different take on the need for speed in data integration solution we thought it would be interesting to take up this challenge check out this blog to know more
how to avoid jdbc in azure synapse analytics using spark pool for sql updates and inserts,hi currently am doing etl process using synapse analytics we are reading delta files from data lake and doing incremental load on sql table using spark pool jdbc throughout is very low and want to avoid it any suggestions using spark dataframe because transformation are very complex and everything is parameterized,0,how to avoid jdbc in azure synapse analytics using spark pool for sql update and insert,hi currently am doing etl process using synapse analytics we are reading delta file from data lake and doing incremental load on sql table using spark pool jdbc throughout is very low and want to avoid it any suggestion using spark dataframe because transformation are very complex and everything is parameterized
help for project data source,hi am trying to build project for my uni fyp and decided that will go all in so that it can double up as my personal portfolio as well my idea is to build modern batch end to end data pipeline with containers orchestrated with airflow from ingestion to visualization originally was hoping to collaborate with company for the data source to develop poc for the pipeline but as time goes on began to feel that it is incredibly difficult for companies to share their data to student therefore began to look into the possibility of using open source data or live api the problem is am facing difficulties of choosing the most appropriate data sources hoped my project could accomplish and demonstrate these characteristics below integration from multiple diverse data sources batch import every time means the data should be timestamped and constantly updating data sources rich enough for few visualizations possibly ml can anybody advise me what data source that can potentially use to accomplish this thankss,0,help for project data source,hi am trying to build project for my uni fyp and decided that will go all in so that it can double up a my personal portfolio a well my idea is to build modern batch end to end data pipeline with container orchestrated with airflow from ingestion to visualization originally wa hoping to collaborate with company for the data source to develop poc for the pipeline but a time go on began to feel that it is incredibly difficult for company to share their data to student therefore began to look into the possibility of using open source data or live api the problem is am facing difficulty of choosing the most appropriate data source hoped my project could accomplish and demonstrate these characteristic below integration from multiple diverse data source batch import every time mean the data should be timestamped and constantly updating data source rich enough for few visualization possibly ml can anybody advise me what data source that can potentially use to accomplish this thanks
forums for asking homework help for big data,am ofc not asking to do my homework but sometimes get confused and need to ask confusions what forums are good for it,0,forum for asking homework help for big data,am ofc not asking to do my homework but sometimes get confused and need to ask confusion what forum are good for it
how to level up my data engineering game,hey there work at smallish company in the ecom business use azure synapse and power bi for all of my reporting needs originally had used dataflows in power bi to report on data loaded from traditional sql database but as our datasets have grown we ve quickly outscaled that solution as any report over gb failed to refresh in microsoft bi service appropriately scaling was nightmare and quickly asked salsa to cool it on this feature today load data from various sources then generate kpis and dimensions in synapse to load into power bi this makes the refresh times much quicker as am refreshing data already prepared and aggregated this also means can scale my solution out to multiple clients with relative ease one thing to note is do most of my work through the synapse ui create all of our dataflows and pipelines through the synapse ui it is very low code and not sure if this is optimal or even how to make it better solution when look at posts on here everyone is honing their craft with python notebooks or using some programming solution to get it done my questions to my fellow redditors is it even possible to get data engineering job with some sql bi and azure synapse knowledge ve built our data warehouse from scratch but worry because learning to float and being asked to swim don really have mentor and kinda making it up as go what are some good resources to help up my game and give me the best opportunity possible to build cool things in this space as said am building everything from scratch myself am being asked to lead this front and have no clue what doing desperate for resources to learn from thanks in advance,0,how to level up my data engineering game,hey there work at smallish company in the ecom business use azure synapse and power bi for all of my reporting need originally had used dataflows in power bi to report on data loaded from traditional sql database but a our datasets have grown we ve quickly outscaled that solution a any report over gb failed to refresh in microsoft bi service appropriately scaling wa nightmare and quickly asked salsa to cool it on this feature today load data from various source then generate kpis and dimension in synapse to load into power bi this make the refresh time much quicker a am refreshing data already prepared and aggregated this also mean can scale my solution out to multiple client with relative ease one thing to note is do most of my work through the synapse ui create all of our dataflows and pipeline through the synapse ui it is very low code and not sure if this is optimal or even how to make it better solution when look at post on here everyone is honing their craft with python notebook or using some programming solution to get it done my question to my fellow redditors is it even possible to get data engineering job with some sql bi and azure synapse knowledge ve built our data warehouse from scratch but worry because learning to float and being asked to swim don really have mentor and kinda making it up a go what are some good resource to help up my game and give me the best opportunity possible to build cool thing in this space a said am building everything from scratch myself am being asked to lead this front and have no clue what doing desperate for resource to learn from thanks in advance
event traits best practices segment rudderstack,hey everyone trying to get grasp over our event traits being sent to segment and soon rudderstack ve been auditing our event data and it seems like our tracking plan basically mashes as much data into traits that don really have anything to do with the event here an example track video_liked videoid videoauthorusertype premium member our product person claims they need the data but we already have video author information in mixpanel it seems to me like mixpanel should be able to derive the videoauthorusertype no is the above bad practice any tips on best practices for shaping event data it seems to be some of this data is being flattened together to compensate for lack of feature on certain destinations but that should probably be fixed with transformation to the destination no appreciate any advice or tips,0,event trait best practice segment rudderstack,hey everyone trying to get grasp over our event trait being sent to segment and soon rudderstack ve been auditing our event data and it seems like our tracking plan basically mash a much data into trait that don really have anything to do with the event here an example track video_liked videoid videoauthorusertype premium member our product person claim they need the data but we already have video author information in mixpanel it seems to me like mixpanel should be able to derive the videoauthorusertype no is the above bad practice any tip on best practice for shaping event data it seems to be some of this data is being flattened together to compensate for lack of feature on certain destination but that should probably be fixed with transformation to the destination no appreciate any advice or tip
merge sort part,merge sort part,0,merge sort part,merge sort part
does this architecture make any sense,trying to teach myself data engineering learn better doing projects so thought about doing rather complex project that involves streaming data and it might be too ambitious for total noob like myself what trying to do is to create an architecture that grabs tweets news and the prices of several cryptocurrencies and then try to give forecast of their prices taking the sentiment of the tweets and classified texts as vectors from the news as predicting variables this forecast gets updated with new predictions in near real time and thought something like this but to be honest not sure whether it makes sense or not wonder if it would be possible to do forecasting in the same process of doing the sentiment analysis and text classification vectorization with spark or if need to store the vectors and the value of the sentiment analysis in nosql database and then doing the forecasting taking the information from this database really appreciate any insights or recommendations,0,doe this architecture make any sense,trying to teach myself data engineering learn better doing project so thought about doing rather complex project that involves streaming data and it might be too ambitious for total noob like myself what trying to do is to create an architecture that grab tweet news and the price of several cryptocurrencies and then try to give forecast of their price taking the sentiment of the tweet and classified text a vector from the news a predicting variable this forecast get updated with new prediction in near real time and thought something like this but to be honest not sure whether it make sense or not wonder if it would be possible to do forecasting in the same process of doing the sentiment analysis and text classification vectorization with spark or if need to store the vector and the value of the sentiment analysis in nosql database and then doing the forecasting taking the information from this database really appreciate any insight or recommendation
what software architecture in an etl oriented project is working out for you,working on monolithic pyspark project and have now an opportunity to refactor and redesign things bit so was wondering what your approach for code organization and composition do you introduce any additional abstractions for your data transformers it seems like this topic in the context of pure data oriented projects is quite poor there is ton of books and talks around software design but it often seems not applicable when building regular data transformers started to dig into some potential functional programming approaches and came across polylith but also no success with data context examples there do you prefer an oop class based approach or fp,0,what software architecture in an etl oriented project is working out for you,working on monolithic pyspark project and have now an opportunity to refactor and redesign thing bit so wa wondering what your approach for code organization and composition do you introduce any additional abstraction for your data transformer it seems like this topic in the context of pure data oriented project is quite poor there is ton of book and talk around software design but it often seems not applicable when building regular data transformer started to dig into some potential functional programming approach and came across polylith but also no success with data context example there do you prefer an oop class based approach or fp
data lakes and lake houses conversation with vinoth chandar creator of the apache hudi project,had an amazing conversation with vinoth about data lake and lakehouse technologies he gave one of the clearest explanations of what lakehouse is vinoth is an amazing guy and has crazy experience in large scale data systems and he was kind enough to share part of this experience with me you can listen to the conversation here data warehouses data lakes lakehouses and large scale data systems with vinoth from apache hudi,0,data lake and lake house conversation with vinoth chandar creator of the apache hudi project,had an amazing conversation with vinoth about data lake and lakehouse technology he gave one of the clearest explanation of what lakehouse is vinoth is an amazing guy and ha crazy experience in large scale data system and he wa kind enough to share part of this experience with me you can listen to the conversation here data warehouse data lake lakehouses and large scale data system with vinoth from apache hudi
introduce an open source project in data engineering,introduce this open source project under apache license hope it useful and interesting to someone this project connects to streaming data from streaming engines like kafka or cloud storage like aws s3 and gives you built in ui to analyze data into graphics it easy to run either in kubernetes or hand crafted clusters with vms if interested check out more details on the project page,0,introduce an open source project in data engineering,introduce this open source project under apache license hope it useful and interesting to someone this project connects to streaming data from streaming engine like kafka or cloud storage like aws s3 and give you built in ui to analyze data into graphic it easy to run either in kubernetes or hand crafted cluster with vms if interested check out more detail on the project page
are files stored in docker container secure,created docker image to pick some data from an api and then store it in our data lake but as an intermediate step it stores and processes the data locally before uploading in this particular case the data is not really sensitive but it still made me think is the data stored in docker container secure is it immediately destroyed after the container is done can it be accessed by the service running it on azure container registry in this case or persist after the container is done in case of processing ppi are there any considerations when processing and storing the data even temporarily in container,0,are file stored in docker container secure,created docker image to pick some data from an api and then store it in our data lake but a an intermediate step it store and process the data locally before uploading in this particular case the data is not really sensitive but it still made me think is the data stored in docker container secure is it immediately destroyed after the container is done can it be accessed by the service running it on azure container registry in this case or persist after the container is done in case of processing ppi are there any consideration when processing and storing the data even temporarily in container
introducing revision history via git,secoda can now be synced with git repository letting you customize how you develop and deploy secoda and help your team adhere to application development lifecycle best practices sync secoda to git repository so you can manage secoda workspace as code data teams have been speaking about managing data as code and treating data as product lots of conversations in circulated around bringing software development best practices into data with this new feature we allow data teams to manage their data catalogue the same way software engineering teams manage products although this is v1 of the feature we think it monumental improvement in the workflow in secoda in the future we re hoping to build the ability to create full dev staging production secoda states to bring the best practices for version control and knowledge management to your data knowledge with this approach data engineers can manage the version history of secoda as well as manage their own data in their own instance teams can start monitoring the changes across secoda and retract any changes before they impact the master branch in the future this will allow data teams to run tests amp perform qa on the staging instance while end users can access the application on the production instance here the full article on this feature,0,introducing revision history via git,secoda can now be synced with git repository letting you customize how you develop and deploy secoda and help your team adhere to application development lifecycle best practice sync secoda to git repository so you can manage secoda workspace a code data team have been speaking about managing data a code and treating data a product lot of conversation in circulated around bringing software development best practice into data with this new feature we allow data team to manage their data catalogue the same way software engineering team manage product although this is v1 of the feature we think it monumental improvement in the workflow in secoda in the future we re hoping to build the ability to create full dev staging production secoda state to bring the best practice for version control and knowledge management to your data knowledge with this approach data engineer can manage the version history of secoda a well a manage their own data in their own instance team can start monitoring the change across secoda and retract any change before they impact the master branch in the future this will allow data team to run test amp perform qa on the staging instance while end user can access the application on the production instance here the full article on this feature
life as data engineer in spotify,hi all hope my post is relevant to the group am thinking of applying for position ay spotify and would like to ask if anyone has any experience of what working there is like keep in mind that am referring for the emea region and am really familiar with the dutch working condions work life balance etc every input will be highly appreciated and happy amp healthy new year,0,life a data engineer in spotify,hi all hope my post is relevant to the group am thinking of applying for position ay spotify and would like to ask if anyone ha any experience of what working there is like keep in mind that am referring for the emea region and am really familiar with the dutch working condions work life balance etc every input will be highly appreciated and happy amp healthy new year
has anyone worked with scality here,recently joined team that using scality for storage what the difference between scality and aws and what the advantage of using one over the other,0,ha anyone worked with scality here,recently joined team that using scality for storage what the difference between scality and aws and what the advantage of using one over the other
is there learning resource that explains what the current enterprise open source solutions do,feel like when someone is explaining their data stack it is word salad of brand names have hard time lining up particular technology snowflake aws glue databricks with its function within the enterprise as bonus would love some primer that explains what all the functional areas are and how they work together not sure that exists though much appreciated,0,is there learning resource that explains what the current enterprise open source solution do,feel like when someone is explaining their data stack it is word salad of brand name have hard time lining up particular technology snowflake aws glue databricks with it function within the enterprise a bonus would love some primer that explains what all the functional area are and how they work together not sure that exists though much appreciated
setting up cicd for azure data factory using azure devops pipelines,setting up cicd for azure data factory using azure devops pipelines,0,setting up cicd for azure data factory using azure devops pipeline,setting up cicd for azure data factory using azure devops pipeline
what do you think about our series deck at airbyte,at airbyte we just raised series we also openly shared our investor deck and pitch in our deck we share how being open source and community powered is key for our growth we want to solve the long tail of integrations problem with our participative model remain non opinionated elt tool to address everyone needs and provide fair compute based pricing in airbyte cloud we are curious to know what the data engineering community thinks about our strategy moving forward what are you most excited about what are we missing,0,what do you think about our series deck at airbyte,at airbyte we just raised series we also openly shared our investor deck and pitch in our deck we share how being open source and community powered is key for our growth we want to solve the long tail of integration problem with our participative model remain non opinionated elt tool to address everyone need and provide fair compute based pricing in airbyte cloud we are curious to know what the data engineering community think about our strategy moving forward what are you most excited about what are we missing
modeling relationship in accumulating snapshot table,struggling little with building dimensional model around an accumulating snapshot fact table modeling process for project timelines so the fact table would have start date funding date end date etc and the main dimension table would be project details which would include information like title the problem is project only goes through this process one time so the fact and dimension table would always have relationship which feels wrong what would be best practice,0,modeling relationship in accumulating snapshot table,struggling little with building dimensional model around an accumulating snapshot fact table modeling process for project timeline so the fact table would have start date funding date end date etc and the main dimension table would be project detail which would include information like title the problem is project only go through this process one time so the fact and dimension table would always have relationship which feel wrong what would be best practice
databricks pyspark is there way to apply the schema from complete json object to new objects to stop null values from defaulting to string,like the title says ve got json pulling from an api read that json into databricks and convert to parquet they append that to our parquet reporting tables however there conditional array with bool if every entry for that batch is null then databricks is unable to infer what type is ought to be defaults to string this breaks our downstream appending of the single batches to the larger dataset ve found lots of suggestions for turning everything into string and that works but prefer to pickle schema object from happy path sample of the json and apply it to all incoming batches is that thing,0,databricks pyspark is there way to apply the schema from complete json object to new object to stop null value from defaulting to string,like the title say ve got json pulling from an api read that json into databricks and convert to parquet they append that to our parquet reporting table however there conditional array with bool if every entry for that batch is null then databricks is unable to infer what type is ought to be default to string this break our downstream appending of the single batch to the larger dataset ve found lot of suggestion for turning everything into string and that work but prefer to pickle schema object from happy path sample of the json and apply it to all incoming batch is that thing
who does data engineering for top sports teams,on the job hunt and would love to be working with sports data had look on linkedin and some of the top sports teams in the uk and europe eg liverpool mercedes f1 etc don have any data engineeri employees do these companies go direct to the likes of aws and microsoft and work with their best engineers to build their solutions or do they do work with specific consultancies any tips for trying to get into the sports industry would be greatly appreciated,0,who doe data engineering for top sport team,on the job hunt and would love to be working with sport data had look on linkedin and some of the top sport team in the uk and europe eg liverpool mercedes f1 etc don have any data engineeri employee do these company go direct to the like of aws and microsoft and work with their best engineer to build their solution or do they do work with specific consultancy any tip for trying to get into the sport industry would be greatly appreciated
my first google cloud data pipeline,have batch data pipeline project requires the data on near realtime minutes to minutes interval basis this pipeline will move the data from one system to another using rest apis with minimal transformation required the volume of the data ll be processing was very trivial at the moment we are receiving max rows per day was expected to increase for about rows per day the next coming months lastly will be storing my data into backup storage for recovery analytics etc after uploading it to the target system am using python to develop my pipelines but this is the first time for me to utilize cloud services so with that in mind there are solutions which was thinking use cloud functions cloud storage amp cloud scheduler stack gcf will request the data from system then dump it into gcs gcf will fetch the data from staging gcs then upload it into both system and backup gcs same with but will utilize our existing big query for backing up the data gcf will request the data from system then dump it into gcs gcf will fetch the data from staging gcs then upload it into both system and big query same as or but replacing the scheduler with an existing staled airflow instance also planning to use dbt in the future if that would affect the solution what aiming with my infrastructure are low cost and ease of maintainability amp monitoring feel free to suggest another gcloud service if that can also solve the problem any advice is very much appreciated,0,my first google cloud data pipeline,have batch data pipeline project requires the data on near realtime minute to minute interval basis this pipeline will move the data from one system to another using rest apis with minimal transformation required the volume of the data ll be processing wa very trivial at the moment we are receiving max row per day wa expected to increase for about row per day the next coming month lastly will be storing my data into backup storage for recovery analytics etc after uploading it to the target system am using python to develop my pipeline but this is the first time for me to utilize cloud service so with that in mind there are solution which wa thinking use cloud function cloud storage amp cloud scheduler stack gcf will request the data from system then dump it into gc gcf will fetch the data from staging gc then upload it into both system and backup gc same with but will utilize our existing big query for backing up the data gcf will request the data from system then dump it into gc gcf will fetch the data from staging gc then upload it into both system and big query same a or but replacing the scheduler with an existing staled airflow instance also planning to use dbt in the future if that would affect the solution what aiming with my infrastructure are low cost and ease of maintainability amp monitoring feel free to suggest another gcloud service if that can also solve the problem any advice is very much appreciated
creating my first google cloud based data pipeline,have batch data pipeline project requires the data on near realtime basis this pipeline will move the data from one system to another with minimal processing power required the volume of the data ll be processing was very trivial at the moment we are receiving rows per day was expected to increase for about rows per day,0,creating my first google cloud based data pipeline,have batch data pipeline project requires the data on near realtime basis this pipeline will move the data from one system to another with minimal processing power required the volume of the data ll be processing wa very trivial at the moment we are receiving row per day wa expected to increase for about row per day
anyone moved jobs in the last months uk,hi everyone ve been at my current company aws gcp for around months and it pretty fun although it more analytical engineering than core data engineering still learning lot and gaining domain knowledge however because worked at an early adopter of the ms azure stack get daily job offers in the region of this is substantially higher than what on for reference years ago was working as an ops analyst earning wanted to ask has anyone jumped shipped due to the crazy market demand and taken on the salary increase,0,anyone moved job in the last month uk,hi everyone ve been at my current company aws gcp for around month and it pretty fun although it more analytical engineering than core data engineering still learning lot and gaining domain knowledge however because worked at an early adopter of the m azure stack get daily job offer in the region of this is substantially higher than what on for reference year ago wa working a an ops analyst earning wanted to ask ha anyone jumped shipped due to the crazy market demand and taken on the salary increase
adf multiple file dependencies,need to trigger my adf pipeline when three files arrives in paths container folder1 file1 parquet container folder2 file2 parquet container folder3 file3 parquet only when these subfolders gets new files files will be overwritten should the adf pipeline trigger how can we achieve this,0,adf multiple file dependency,need to trigger my adf pipeline when three file arrives in path container folder1 file1 parquet container folder2 file2 parquet container folder3 file3 parquet only when these subfolders get new file file will be overwritten should the adf pipeline trigger how can we achieve this
apache dolphinscheduler release announcement workflowascode is launched,amp x200b format png amp auto webp amp ec070bff5a5cfe5c8a90a19f180e16d4104de1e0 amp x200b in the long awaited workflowascode function is finally launched in version as promised bringing good news to users who need to dynamically create and update workflows in batches in addition the new version also adds the wecom alarm group chat message push simplifies the metadata initialization process and fixes issues that existed in the former version such as failure of service restart after forced termination and the failure to add hive data source new function workflowascode first of all in terms of new functions version released pythongatewayserver which is workflow as code server started in the same way as apiserver and other services when pythongatewayserver is enabled all python api requests are sent to pythongatewayserver workflow as code lets users create workflows through the python api which is great news for users who need to create and update workflows dynamically and in batches workflows created with workflow as code can be viewed in the web ui just like other workflows the following is workflow as code test case define workflow properties including name scheduling period start time tenant etc with processdefinition name tutorial schedule start_time tenant tenant_exists as pd define tasks which are all shell tasks the required parameters of shell tasks are task name command information here are all the shell commands of echo task_parent shell name task_parent command echo hello pydolphinscheduler task_child_one shell name task_child_one command echo child one task_child_two shell name task_child_two command echo child two task_union shell name task_union command echo union define dependencies between tasks here task_child_one and task_child_two are first declared as task group through python list task_group task_child_one task_child_two use the set_downstream method to declare the task group task_group as the downstream of task_parent and declare the upstream through set_upstream task_parent set_downstream task_group use the bit operator lt lt to declare the task_union as the downstream of the task_group and support declaration through the bit operator gt gt task_union lt lt task_group when the above code runs you can see workflow in the web ui as follows gt task_child_one task_parent gt gt task_union gt task_child_two wecom alarm mode supports group chat message push in the previous version the wechat alarm only supported the message notification in version when the user uses the wecom alarm it supports pushing the group chat message in the app to the user optimization simplified metadata initialization process when apache dolphinscheduler is first installed running create dolphinscheduler sh requires step by step upgrade from the oldest version to the current version in order to initialize the metadata process more conveniently and quickly version allows users to directly install the current version of the database script which improves the installation speed remove days in complement dates removed the day in the complement date to avoid user confusion when the ui date always displays when the complement is added bug fixes fix logger memory leak in worker compatible with historical version data source connection information memory constraints cause errors when upgrading from to service restart fails after forced termination process definition version create time is wrong failed to execute procedure node add default configuration of quartz and zookeeper in common configuration items in the dependency node an error is reported when there is an option that does not belong to the current project workflow replication error workflow is always running when worker sendresult succeeds but the master does not receive error report h2 in standalone server will automatically restart after few minutes resulting in abnormal data loss error reported when executing mysql table creation statement dependent node retry delay does not work failed to add hive data source download release note thanks as always we would like to thank all the contributors in no particular order who have worked to polish apache dolphinscheduler as better platform it is your wisdom and efforts to make it more in line with the needs of users amp x200b format png amp auto webp amp ea74ced2845b5933d4c6492798f6af4772f4 the way to join there are many ways to participate and contribute to the dolphinscheduler community including documents translation amp tests codes articles keynote speeches etc we assume the first pr document code to contribute to be simple and should be used to familiarize yourself with the submission process and community collaboration style so the community has compiled the following list of issues suitable for novices list of non newbie issues is aissue label volunteer wanted is aissue label volunteer wanted how to participate in the contribution community official website github code repository your star for the project is important don hesitate to lighten star for apache dolphinscheduler,0,apache dolphinscheduler release announcement workflowascode is launched,amp x200b format png amp auto webp amp ec070bff5a5cfe5c8a90a19f180e16d4104de1e0 amp x200b in the long awaited workflowascode function is finally launched in version a promised bringing good news to user who need to dynamically create and update workflow in batch in addition the new version also add the wecom alarm group chat message push simplifies the metadata initialization process and fix issue that existed in the former version such a failure of service restart after forced termination and the failure to add hive data source new function workflowascode first of all in term of new function version released pythongatewayserver which is workflow a code server started in the same way a apiserver and other service when pythongatewayserver is enabled all python api request are sent to pythongatewayserver workflow a code let user create workflow through the python api which is great news for user who need to create and update workflow dynamically and in batch workflow created with workflow a code can be viewed in the web ui just like other workflow the following is workflow a code test case define workflow property including name scheduling period start time tenant etc with processdefinition name tutorial schedule start_time tenant tenant_exists a pd define task which are all shell task the required parameter of shell task are task name command information here are all the shell command of echo task_parent shell name task_parent command echo hello pydolphinscheduler task_child_one shell name task_child_one command echo child one task_child_two shell name task_child_two command echo child two task_union shell name task_union command echo union define dependency between task here task_child_one and task_child_two are first declared a task group through python list task_group task_child_one task_child_two use the set_downstream method to declare the task group task_group a the downstream of task_parent and declare the upstream through set_upstream task_parent set_downstream task_group use the bit operator lt lt to declare the task_union a the downstream of the task_group and support declaration through the bit operator gt gt task_union lt lt task_group when the above code run you can see workflow in the web ui a follows gt task_child_one task_parent gt gt task_union gt task_child_two wecom alarm mode support group chat message push in the previous version the wechat alarm only supported the message notification in version when the user us the wecom alarm it support pushing the group chat message in the app to the user optimization simplified metadata initialization process when apache dolphinscheduler is first installed running create dolphinscheduler sh requires step by step upgrade from the oldest version to the current version in order to initialize the metadata process more conveniently and quickly version allows user to directly install the current version of the database script which improves the installation speed remove day in complement date removed the day in the complement date to avoid user confusion when the ui date always display when the complement is added bug fix fix logger memory leak in worker compatible with historical version data source connection information memory constraint cause error when upgrading from to service restart fails after forced termination process definition version create time is wrong failed to execute procedure node add default configuration of quartz and zookeeper in common configuration item in the dependency node an error is reported when there is an option that doe not belong to the current project workflow replication error workflow is always running when worker sendresult succeeds but the master doe not receive error report h2 in standalone server will automatically restart after few minute resulting in abnormal data loss error reported when executing mysql table creation statement dependent node retry delay doe not work failed to add hive data source download release note thanks a always we would like to thank all the contributor in no particular order who have worked to polish apache dolphinscheduler a better platform it is your wisdom and effort to make it more in line with the need of user amp x200b format png amp auto webp amp ea74ced2845b5933d4c6492798f6af4772f4 the way to join there are many way to participate and contribute to the dolphinscheduler community including document translation amp test code article keynote speech etc we assume the first pr document code to contribute to be simple and should be used to familiarize yourself with the submission process and community collaboration style so the community ha compiled the following list of issue suitable for novice list of non newbie issue is aissue label volunteer wanted is aissue label volunteer wanted how to participate in the contribution community official website github code repository your star for the project is important don hesitate to lighten star for apache dolphinscheduler
here is survey for my data viz project please fill the form,here is survey for my data viz project please fill the form,0,here is survey for my data viz project please fill the form,here is survey for my data viz project please fill the form
databricks certified professional data engineer,hello everyone has anyone here passed the data engineer certification on databricks would like to pass the test myself and would appreciate any tips or recources to check out thanks in advance,0,databricks certified professional data engineer,hello everyone ha anyone here passed the data engineer certification on databricks would like to pas the test myself and would appreciate any tip or recources to check out thanks in advance
skills to learn to transition from data analysis to data engineering,hi all so recently started job in data analysis about months ago at small company after finishing my msc bioinformatics thinking about starting some freelancing projects if can but if not then just some courses and personal projects to try and make the transition to more of data engineering role in the future of my career it seems to be both more interesting in terms of handling data and more lucrative career path although in no rush to get there and have time to learn more skills to become more hireable am confident in python numpy and pandas and use lot of sas for work at the moment but was wondering what people would suggest for skillset to make this transition thank you,0,skill to learn to transition from data analysis to data engineering,hi all so recently started job in data analysis about month ago at small company after finishing my msc bioinformatics thinking about starting some freelancing project if can but if not then just some course and personal project to try and make the transition to more of data engineering role in the future of my career it seems to be both more interesting in term of handling data and more lucrative career path although in no rush to get there and have time to learn more skill to become more hireable am confident in python numpy and panda and use lot of sa for work at the moment but wa wondering what people would suggest for skillset to make this transition thank you
looking for an example project which is an etl workflow utilising python sqalchemy and extracting from an api,ve written new project recently which focuses around these main things want to compare my project to that of one which utilises these tools particularly interested in the project structure so github repo would be perfect does anyone have any great examples,0,looking for an example project which is an etl workflow utilising python sqalchemy and extracting from an api,ve written new project recently which focus around these main thing want to compare my project to that of one which utilises these tool particularly interested in the project structure so github repo would be perfect doe anyone have any great example
employees of faang am doing survey can you help me please,have few questions what is your designation in your company what technologies do you often work interact with what exactly do the big tech faang companies look for in candidates when hiring what are some red flags specific to the faang companies,0,employee of faang am doing survey can you help me please,have few question what is your designation in your company what technology do you often work interact with what exactly do the big tech faang company look for in candidate when hiring what are some red flag specific to the faang company
redhat data engineering intern,has anyone interviewed for redhat data engineering intern role could you share your experience and how to prepare thank you,0,redhat data engineering intern,ha anyone interviewed for redhat data engineering intern role could you share your experience and how to prepare thank you
transitioning to blockchain,hi have around yrs of experience into software developer data engineering am pretty interested in blockchain and would like to transition to that domain how easy or difficult it is to transition at this time are there any work involved of using blockchains in big data,0,transitioning to blockchain,hi have around yr of experience into software developer data engineering am pretty interested in blockchain and would like to transition to that domain how easy or difficult it is to transition at this time are there any work involved of using blockchains in big data
example tutorial for new tool,hello all we are very close to releasing pipelining tool to open source since our team is more oriented towards ml or ds the tutorial we currently have is ml focused and considered very popular in that space it the titanic dataset though there are certain data transformations that can demo using pipeline was curious if data engineering community has some hallmark pipeline tutorials or examples that can implement using our software the ones found on airflow bash date or prefect airline radius are bit too simple thanks for your help cheers,0,example tutorial for new tool,hello all we are very close to releasing pipelining tool to open source since our team is more oriented towards ml or d the tutorial we currently have is ml focused and considered very popular in that space it the titanic dataset though there are certain data transformation that can demo using pipeline wa curious if data engineering community ha some hallmark pipeline tutorial or example that can implement using our software the one found on airflow bash date or prefect airline radius are bit too simple thanks for your help cheer
what is your day to day look like,starting my first de job next month just wanted to know what your day to day was like how do you get work what do you use to get your job done and any advice you might have thanks,0,what is your day to day look like,starting my first de job next month just wanted to know what your day to day wa like how do you get work what do you use to get your job done and any advice you might have thanks
please roast my resume data analyst intern to data engineering intern,please roast my resume data analyst intern to data engineering intern,0,please roast my resume data analyst intern to data engineering intern,please roast my resume data analyst intern to data engineering intern
please roast my resume data analyst intern to data engineering intern,inspired by the previous data analyst post about transitioning into fulltime de role was hoping to get some criticism on my resume as begin searching for summer internship opportunities should include more personal projects club experience in lieu of professional experience have few more projects in my github repo that are data science data engineering related was wondering if those would be more relevant than the instructional assistant role thanks in advance,0,please roast my resume data analyst intern to data engineering intern,inspired by the previous data analyst post about transitioning into fulltime de role wa hoping to get some criticism on my resume a begin searching for summer internship opportunity should include more personal project club experience in lieu of professional experience have few more project in my github repo that are data science data engineering related wa wondering if those would be more relevant than the instructional assistant role thanks in advance
pandas coding convention for column naming,hi all interested in hearing the community opinion on some pandas coding convention my problem have to make new columns based on other columns the rules are basically the same so created one function that does this that simple however the new columns need new names that replace the previous suffix for example given df column1_value ll want to name the new column df column1_result simple enough but have ish columns which all would look like df column1_result funct df column1_value except the names are slightly different so in theory can write function that takes the name of the string and returns it with the ending being changed which means can loop instead of declaring each column my intuition is saying don do this anyone have good alternative or real reason why wouldn do this,0,panda coding convention for column naming,hi all interested in hearing the community opinion on some panda coding convention my problem have to make new column based on other column the rule are basically the same so created one function that doe this that simple however the new column need new name that replace the previous suffix for example given df column1_value ll want to name the new column df column1_result simple enough but have ish column which all would look like df column1_result funct df column1_value except the name are slightly different so in theory can write function that take the name of the string and return it with the ending being changed which mean can loop instead of declaring each column my intuition is saying don do this anyone have good alternative or real reason why wouldn do this
merge sort,merge sort,0,merge sort,merge sort
merge sort algorithm,merge sort algorithm,0,merge sort algorithm,merge sort algorithm
did arrive at dead end with my position in current company,hello was first bi hire in company at least for our country but we are very independent because of type of the business usage and agile thinking chose the database of choice postgresql on premise database is scaling amazingly well for sme there are literally problems small to none maintenance needed after properly setting the config now because of this boom in hiring decided to try job market and horrifying discovered that job positions are requiring some cloud experience what are my options for keeping up to date with cloud given that am keeping same position still fail to see how for exactly our type of business would db and etl process on cloud be advantage very predictable and stable usage and processes hosted on computer with specs far higher than needed far less unplanned shortages as aws us east1 for other reasons we have our own generators independent reliable isps high end recovery mechanisms only thing can think of is to create some bullshit ml side project where we need cloud for burst computing power any suggestions have theoretically full power as am in charge of everything bi related tl dr love my on premise postgres job market wants me to switch to cloud how to get best of both amp x200b also is it hard to lie about your cloud experience mean everybody is saying that its maintenance free and things just work so in theory it should be super easy to learn,0,did arrive at dead end with my position in current company,hello wa first bi hire in company at least for our country but we are very independent because of type of the business usage and agile thinking chose the database of choice postgresql on premise database is scaling amazingly well for sme there are literally problem small to none maintenance needed after properly setting the config now because of this boom in hiring decided to try job market and horrifying discovered that job position are requiring some cloud experience what are my option for keeping up to date with cloud given that am keeping same position still fail to see how for exactly our type of business would db and etl process on cloud be advantage very predictable and stable usage and process hosted on computer with spec far higher than needed far le unplanned shortage a aws u east1 for other reason we have our own generator independent reliable isps high end recovery mechanism only thing can think of is to create some bullshit ml side project where we need cloud for burst computing power any suggestion have theoretically full power a am in charge of everything bi related tl dr love my on premise postgres job market want me to switch to cloud how to get best of both amp x200b also is it hard to lie about your cloud experience mean everybody is saying that it maintenance free and thing just work so in theory it should be super easy to learn
me as an etl engineer watching people build data tables with no regard to what goes in them,me as an etl engineer watching people build data tables with no regard to what goes in them,0,me a an etl engineer watching people build data table with no regard to what go in them,me a an etl engineer watching people build data table with no regard to what go in them
bigquery gt fivetran gt big data possible,hi how do people get around the fact that fivetran cannot read data from bigquery or other bigdata sources am missing something,0,bigquery gt fivetran gt big data possible,hi how do people get around the fact that fivetran cannot read data from bigquery or other bigdata source am missing something
what should learn as data engineer,following cs conversion masters was hired as technical account manager few months ago at startup started automating some processes for them which my boss took an interest in and after few conversations now in charge of building etl pipelines with python for internal data jira tickets and the like into sql databases analysing the data and developing powerbi dashboards it early days but if make some good progress with it over the next couple of months ll likely switch to being an actual full time data engineer at the company and not tam with some engineering responsibilities this is obviously great learning opportunity but as the company has no formal data engineers working there and the dev team are all product focused have nobody directly above me for guidance on this specific project while it exciting to be managing this project end to end at such an early point in my career but want to make sure setting myself up for success later down the line so figured ask here what are some technical skills technologies that should aim to build up while working on this project it not big data so things like spark have limited applications what are some organisational skills that would be worth practising documentation if you work with data engineers and analysts what do you wish they knew did differently thank you,0,what should learn a data engineer,following c conversion master wa hired a technical account manager few month ago at startup started automating some process for them which my bos took an interest in and after few conversation now in charge of building etl pipeline with python for internal data jira ticket and the like into sql database analysing the data and developing powerbi dashboard it early day but if make some good progress with it over the next couple of month ll likely switch to being an actual full time data engineer at the company and not tam with some engineering responsibility this is obviously great learning opportunity but a the company ha no formal data engineer working there and the dev team are all product focused have nobody directly above me for guidance on this specific project while it exciting to be managing this project end to end at such an early point in my career but want to make sure setting myself up for success later down the line so figured ask here what are some technical skill technology that should aim to build up while working on this project it not big data so thing like spark have limited application what are some organisational skill that would be worth practising documentation if you work with data engineer and analyst what do you wish they knew did differently thank you
please critique my resume data analyst transitioning to data engineer,please critique my resume data analyst transitioning to data engineer,0,please critique my resume data analyst transitioning to data engineer,please critique my resume data analyst transitioning to data engineer
data engineers what do you actually do and what your tech stack,trying to decide if the pivot from appdev to data engineering is in my future and would like to get sense of what it involves in the real world hoping for broad brush breakdown of job responsibilities and what languages technologies systems are the most important to get your job done bonus points if you re willing to give rough swag of your experience and compensation thanks people appreciate you taking the time to share your expertise,0,data engineer what do you actually do and what your tech stack,trying to decide if the pivot from appdev to data engineering is in my future and would like to get sense of what it involves in the real world hoping for broad brush breakdown of job responsibility and what language technology system are the most important to get your job done bonus point if you re willing to give rough swag of your experience and compensation thanks people appreciate you taking the time to share your expertise
installing airflow error,hello everyone trying to install airflow to do some practice in it have ubuntu lts server when installing airflow can see there are errors error openapi schema validator has requirement jsonschema lt gt but you ll have jsonschema which is incompatible error flask appbuilder has requirement sqlalchemy lt but you ll have sqlalchemy which is incompatible however can use airflow in the terminal when try to use the db init got another error attached picture same error when try to run the webserver too amp x200b ve been struggling with it the last hours but no clue amp x200b any help thanks,0,installing airflow error,hello everyone trying to install airflow to do some practice in it have ubuntu lts server when installing airflow can see there are error error openapi schema validator ha requirement jsonschema lt gt but you ll have jsonschema which is incompatible error flask appbuilder ha requirement sqlalchemy lt but you ll have sqlalchemy which is incompatible however can use airflow in the terminal when try to use the db init got another error attached picture same error when try to run the webserver too amp x200b ve been struggling with it the last hour but no clue amp x200b any help thanks
got my first job as junior de what next,hi somehow got hired as junior de for the past few months everything did was focused on getting the job now that have received one feel little bit lost how to develop my career now about me it my very first job in the field not even analytics position before that the employer is f500 consulting company grind stratascratch and feel more and more comfortable with hard questions don have any degree yet studying cs part time beginner in python doing pythoninstitute org courses at the moment as am not able to solve even the easiest questions on leetcode in the eu the money isn great but ok with that for now amp x200b this is what am thinking for the next months continue learning python and get certificate from pythoninstitute then maybe move to some projects do hard sql question day get the databricks spark certificate here my questions what should my next objective be do pause responding to recruiters on linkedin or not when do start interviewing again thank you for your kind help,0,got my first job a junior de what next,hi somehow got hired a junior de for the past few month everything did wa focused on getting the job now that have received one feel little bit lost how to develop my career now about me it my very first job in the field not even analytics position before that the employer is f500 consulting company grind stratascratch and feel more and more comfortable with hard question don have any degree yet studying c part time beginner in python doing pythoninstitute org course at the moment a am not able to solve even the easiest question on leetcode in the eu the money isn great but ok with that for now amp x200b this is what am thinking for the next month continue learning python and get certificate from pythoninstitute then maybe move to some project do hard sql question day get the databricks spark certificate here my question what should my next objective be do pause responding to recruiter on linkedin or not when do start interviewing again thank you for your kind help
azure dp guide,hey guys currently in the process of studying for dp the azure data engineer certification however the ms resources as usual are too detailed and overwhelming since have no prior technical experience at all can anyone recommend sources to study from any courses out there for beginners that provide simple explanation to things any resources would be much appreciated,0,azure dp guide,hey guy currently in the process of studying for dp the azure data engineer certification however the m resource a usual are too detailed and overwhelming since have no prior technical experience at all can anyone recommend source to study from any course out there for beginner that provide simple explanation to thing any resource would be much appreciated
which messaging broker should use for my use case,we have client sdk that sends logs to elasticsearch sometimes the sdk might send batch of millions of logs to elastic want to enrich those logs want to do this after inserting and asynchronously to not block the insert as solution thought about sending the logs also to messaging broker want other microservice to pull from the broker enrich the logs and update elastic want the pull to be of batch of thousands of logs to perform the enrichment faster thought about using postgres as broker because we already use pg it performant and the learning curve is zero we use the same pg to store all of our data thought about implementing table named logs that the sdk will insert into and the microservice will pull batch from it every minutes am concerned about the performance deterioration that ll happen to pg as whole because of the large volume of reads and inserts that ll be in the logs table amp x200b what do you think about the idea of using postgres and my concern which alternative will suit my use case better kafka redis streams rabbitmq or another one,0,which messaging broker should use for my use case,we have client sdk that sends log to elasticsearch sometimes the sdk might send batch of million of log to elastic want to enrich those log want to do this after inserting and asynchronously to not block the insert a solution thought about sending the log also to messaging broker want other microservice to pull from the broker enrich the log and update elastic want the pull to be of batch of thousand of log to perform the enrichment faster thought about using postgres a broker because we already use pg it performant and the learning curve is zero we use the same pg to store all of our data thought about implementing table named log that the sdk will insert into and the microservice will pull batch from it every minute am concerned about the performance deterioration that ll happen to pg a whole because of the large volume of read and insert that ll be in the log table amp x200b what do you think about the idea of using postgres and my concern which alternative will suit my use case better kafka redis stream rabbitmq or another one
fight or flight response for new position,this will be long so ll tl dr at the bottom am trying to get some outside perspective on my career from the sub have been feeling the pressure of my position and it has been triggering flight or fight response from me half of me says to dig in and learn and another part of me says to get out asap and find new path for starters work for the fed began my career in mid shortly after graduating college was hired into rotational management program that lasted months after completing the program took my first official position within marketing but in reporting role this was my first exposure to sql the position had great potential for me to learn how to manipulate data for reporting purposes under great boss but that never happened reorg later and was basically report jockey would run some basic stuff and generate some pre made reports and then send out emails it was extremely easy but was bored and wanted to get into job with real skills that led to me getting job for the data warehousing org was not qualified by any means and they knew that but they were willing to bring me on initially they started me in one of the data marts they told me they wanted me to work on few things but that the primary goal was to get trained and learn development now there was essentially no onboarding no training no real guidance at all sat at my desk for weeks months with nothing was trying to get grasp but the world of development has so many facets and with the layer of government red tape over every process was at loss after few months and another reorg later land on different team within the same org now am handling audits and agile pm work no development after the better part of year of doing that end up in temporary assignment as developer on different team now this is related to the first position had but very different in terms of what tools they used and the processes they used to promote the code changes however have manager who knows my situation and was great with working with me and giving me projects could do early on wasn treated as full on dev since the position was temporary but was promoting code and learning well that lasted handful of months and fell off to not having much work well what do you know another reorg big one our whole organization was wiped and rebranded full on top down reorg was not selected early on to lateral into the equivalent position in the new org from march sept was unassigned still had to sign on join some meetings but had no work and nothing to do the manager of my team was also in the same position so he was checked out everything was leading to me being laid off in october at this point haven really had much contact with my previous team in while wasn technically on their team and they had new managers was on an island trying to find job in the last round of interviews before being laid off got several interviews before could even attend them all get call from the new head of the new version of our organization no interview just says you got the job if you want it but need to know within few hours so right then have to decide to stay in the role but take the new position which ended up being promotion for me or risk it on the interview had just done and the more that week ended up accepting the position because found it interesting and didn want to change again have now been on this team since oct have roughly months of actual dev work and am in full on data engineer role have inherited applications that originated in the early and have also been through reorgs and conversions and am on on call rotation every month or so there have been several major changes through out the reorg process to how code is promoted and there is very poor little onboarding or training material the manager was working with previously did not get rehired into the same role new manager with no experience in what we do is now the manager this is so long but typing this has been cathartic tl dr am not qualified for the role am in have essentially no support from team members or my manager any more and don have much means of training at work feel like am just waiting for something to fail or project to come to me and blow up in my face find this field interesting but just don know if can get there part of me says to dig in and try but another part of me says too far behind and can ever be great at this job don even know how to begin to be good at the job for those that are interested in what tools am using ab initio unix putty sql oracle teradata some other things too but that is the primary etl stuff,0,fight or flight response for new position,this will be long so ll tl dr at the bottom am trying to get some outside perspective on my career from the sub have been feeling the pressure of my position and it ha been triggering flight or fight response from me half of me say to dig in and learn and another part of me say to get out asap and find new path for starter work for the fed began my career in mid shortly after graduating college wa hired into rotational management program that lasted month after completing the program took my first official position within marketing but in reporting role this wa my first exposure to sql the position had great potential for me to learn how to manipulate data for reporting purpose under great bos but that never happened reorg later and wa basically report jockey would run some basic stuff and generate some pre made report and then send out email it wa extremely easy but wa bored and wanted to get into job with real skill that led to me getting job for the data warehousing org wa not qualified by any mean and they knew that but they were willing to bring me on initially they started me in one of the data mart they told me they wanted me to work on few thing but that the primary goal wa to get trained and learn development now there wa essentially no onboarding no training no real guidance at all sat at my desk for week month with nothing wa trying to get grasp but the world of development ha so many facet and with the layer of government red tape over every process wa at loss after few month and another reorg later land on different team within the same org now am handling audit and agile pm work no development after the better part of year of doing that end up in temporary assignment a developer on different team now this is related to the first position had but very different in term of what tool they used and the process they used to promote the code change however have manager who know my situation and wa great with working with me and giving me project could do early on wasn treated a full on dev since the position wa temporary but wa promoting code and learning well that lasted handful of month and fell off to not having much work well what do you know another reorg big one our whole organization wa wiped and rebranded full on top down reorg wa not selected early on to lateral into the equivalent position in the new org from march sept wa unassigned still had to sign on join some meeting but had no work and nothing to do the manager of my team wa also in the same position so he wa checked out everything wa leading to me being laid off in october at this point haven really had much contact with my previous team in while wasn technically on their team and they had new manager wa on an island trying to find job in the last round of interview before being laid off got several interview before could even attend them all get call from the new head of the new version of our organization no interview just say you got the job if you want it but need to know within few hour so right then have to decide to stay in the role but take the new position which ended up being promotion for me or risk it on the interview had just done and the more that week ended up accepting the position because found it interesting and didn want to change again have now been on this team since oct have roughly month of actual dev work and am in full on data engineer role have inherited application that originated in the early and have also been through reorgs and conversion and am on on call rotation every month or so there have been several major change through out the reorg process to how code is promoted and there is very poor little onboarding or training material the manager wa working with previously did not get rehired into the same role new manager with no experience in what we do is now the manager this is so long but typing this ha been cathartic tl dr am not qualified for the role am in have essentially no support from team member or my manager any more and don have much mean of training at work feel like am just waiting for something to fail or project to come to me and blow up in my face find this field interesting but just don know if can get there part of me say to dig in and try but another part of me say too far behind and can ever be great at this job don even know how to begin to be good at the job for those that are interested in what tool am using ab initio unix putty sql oracle teradata some other thing too but that is the primary etl stuff
please evaluate my resume for job search,hey everyone need your help on evaluating my resume recently switched to job which is more of ops than data engineering but want go back to being de have experience in designing and developing batch and streaming apps stateless and stateful event sourced apps but since made few short switches recently guess my resume looks bad as in not stable to hiring managers and my applications are getting rejected even through referrals though am skeptical on this because have seen many people moving to faang with more frequent switches your thoughts on this so believe there is something wrong in my resume please take look and share your suggestions on what needs to be added removed to make it better for job searches would be very helpful and greatly appreciate it amp x200b had intentionally removed most of my previous orgs experience because it would make my resume longer thought my recent work experience would make more sense and removed my education details and redacted the recent org work with thanks everyone,0,please evaluate my resume for job search,hey everyone need your help on evaluating my resume recently switched to job which is more of ops than data engineering but want go back to being de have experience in designing and developing batch and streaming apps stateless and stateful event sourced apps but since made few short switch recently guess my resume look bad a in not stable to hiring manager and my application are getting rejected even through referral though am skeptical on this because have seen many people moving to faang with more frequent switch your thought on this so believe there is something wrong in my resume please take look and share your suggestion on what need to be added removed to make it better for job search would be very helpful and greatly appreciate it amp x200b had intentionally removed most of my previous orgs experience because it would make my resume longer thought my recent work experience would make more sense and removed my education detail and redacted the recent org work with thanks everyone
pancakedb new event ingestion solution for streaming writes and batch reads,pancakedb new event ingestion solution for streaming writes and batch reads,0,pancakedb new event ingestion solution for streaming writes and batch read,pancakedb new event ingestion solution for streaming writes and batch read
hour data engineer assessment at,has anyone here appeared for the technical round at for the data engineer position they ve told me on call that it would be hr technical round which would involve taking test simulating day to day work of de has anyone appeared for it or has any tips for it thanks,0,hour data engineer assessment at,ha anyone here appeared for the technical round at for the data engineer position they ve told me on call that it would be hr technical round which would involve taking test simulating day to day work of de ha anyone appeared for it or ha any tip for it thanks
architecture design problem,is there place resource for data engineering specific design patterns specific problem here trying to build system that can launch different versions commits of pipeline at will mostly for testing of various commits or for releases these different pipelines would need to spin up associated services and then spin down after completed or upon request in case of failure curious if design pattern for this existed already leads me to the general question of resolved for de design patterns exist thanks,0,architecture design problem,is there place resource for data engineering specific design pattern specific problem here trying to build system that can launch different version commits of pipeline at will mostly for testing of various commits or for release these different pipeline would need to spin up associated service and then spin down after completed or upon request in case of failure curious if design pattern for this existed already lead me to the general question of resolved for de design pattern exist thanks
best practice to alert users if databricks finished loading data,hello was wondering about the best practice to notify users via email or slack about successful unsuccessful notebook run read about job alerts but is there way to send notifications to slack thanks in advance,0,best practice to alert user if databricks finished loading data,hello wa wondering about the best practice to notify user via email or slack about successful unsuccessful notebook run read about job alert but is there way to send notification to slack thanks in advance
using debit card with lt balance in aws,planning to play with aws little meaning there alot of uncertainties and lot of unexpected charges that might occur as far as my research goes aws budget could only alert you when the threshold for specific service has been breached but it not really going to do much more planning to use debit card with very low amount of balance so when things go wrong they couldn charge me anymore as it doesn contain enough money ve tried searching but haven seen anyone who done the same now getting paranoid whether what wrong with my idea can anyone point it out,0,using debit card with lt balance in aws,planning to play with aws little meaning there alot of uncertainty and lot of unexpected charge that might occur a far a my research go aws budget could only alert you when the threshold for specific service ha been breached but it not really going to do much more planning to use debit card with very low amount of balance so when thing go wrong they couldn charge me anymore a it doesn contain enough money ve tried searching but haven seen anyone who done the same now getting paranoid whether what wrong with my idea can anyone point it out
snowplow product office hours january,learn how to use snowplow micro to implement end to end behavioral data tracking testing in your development practices gmt format pjpg amp auto webp amp c44b8329600a95943eb676b445ed2c1d8b643a8 as usual you ll have an opportunity to ask questions at the end look forward to seeing you on jan th gmt,0,snowplow product office hour january,learn how to use snowplow micro to implement end to end behavioral data tracking testing in your development practice gmt format pjpg amp auto webp amp c44b8329600a95943eb676b445ed2c1d8b643a8 a usual you ll have an opportunity to ask question at the end look forward to seeing you on jan th gmt
snowplow product office hours january,snowplow product office hours january,0,snowplow product office hour january,snowplow product office hour january
modern dwh vs kimball,so lot of kimball principles like strict normalisation and star schemas are being replaced by wide tables which take advantage of cheap storage columnar querying and are faster to query compared to multiple joins however over time wouldn wide tables be disaster if certain dimensions which were initially considered type get crammed into the the wide tables only for them to later have to change how do you guys decide between the efficiency and speed of wide tables vs the strict data validation and robustness of star schema setup,0,modern dwh v kimball,so lot of kimball principle like strict normalisation and star schema are being replaced by wide table which take advantage of cheap storage columnar querying and are faster to query compared to multiple join however over time wouldn wide table be disaster if certain dimension which were initially considered type get crammed into the the wide table only for them to later have to change how do you guy decide between the efficiency and speed of wide table v the strict data validation and robustness of star schema setup
business intelligence engineer interview at amazon,what should expect was told it is going to be sql some python scripting and datawarehousing logic questions not familiar with amazon cloud products how many questions will get and how many should get right thanks,0,business intelligence engineer interview at amazon,what should expect wa told it is going to be sql some python scripting and datawarehousing logic question not familiar with amazon cloud product how many question will get and how many should get right thanks
help with aws lambda functions,help with aws lambda functions,0,help with aws lambda function,help with aws lambda function
job market for international applicants in usa eu,hello folks looking to the seniors of this industry or anyone from similar international background for some advice on my path from sea country got my bachelor in actuarial stats in the us lost opt job when covid struck have since been working in my home country ve held relevant roles data analyst consultant months and data scientist months to date the data field as whole is really new where at and both the positions have been at startups so ve been the only person on the team so ve had the chance to wear the full stack data guy hat so to speak engineering wise ve helped build out some clients pipelines using gcp and services like stitch fivetran at my current role built the full analytics pipeline moving data into sql connecting it to dashboards for analysis etc off their production nosql database most of the work ve done is in python scripts and sql with the orchestration being achieved via airflow cron jobs and the like ve really fallen in love with the engineering side of data over the heavy math and stats side of things so right now like to move overseas since the field isn established here along with other reasons of course but that not for this sub loved my time in the us and really felt at home there so would like to return there know it going to be tough and somewhat familiar with the difficulties of getting h1b guess my question is what the outlook in this particular field for people like me are companies considering international applicants non cs background seems to be big hurdle in the us what would be my best option to break into the field do really need to invest in masters and hope to land something off of opt other international applicants seniors familiar with overseas visa situations how you guys do it what can do with my experience tldr how can break into the us eu market as an applicant from sea country,0,job market for international applicant in usa eu,hello folk looking to the senior of this industry or anyone from similar international background for some advice on my path from sea country got my bachelor in actuarial stats in the u lost opt job when covid struck have since been working in my home country ve held relevant role data analyst consultant month and data scientist month to date the data field a whole is really new where at and both the position have been at startup so ve been the only person on the team so ve had the chance to wear the full stack data guy hat so to speak engineering wise ve helped build out some client pipeline using gcp and service like stitch fivetran at my current role built the full analytics pipeline moving data into sql connecting it to dashboard for analysis etc off their production nosql database most of the work ve done is in python script and sql with the orchestration being achieved via airflow cron job and the like ve really fallen in love with the engineering side of data over the heavy math and stats side of thing so right now like to move overseas since the field isn established here along with other reason of course but that not for this sub loved my time in the u and really felt at home there so would like to return there know it going to be tough and somewhat familiar with the difficulty of getting h1b guess my question is what the outlook in this particular field for people like me are company considering international applicant non c background seems to be big hurdle in the u what would be my best option to break into the field do really need to invest in master and hope to land something off of opt other international applicant senior familiar with overseas visa situation how you guy do it what can do with my experience tldr how can break into the u eu market a an applicant from sea country
weighing meta de offer,just got an offer for de position at meta and was interested in knowing if others could shed some light on the role to help in my decision making process and check few assumptions ve made briefly what have managed to understand is meta de work is mostly closer to the role of an analytics engineer involving lot less development and more sql dashboard building work life balance can be hard to achieve for the first point less worried do enjoy that kindof work and am comfortable in that kind of role my main concern here is work life balance reviews on blind and glassdoor are kind of all over the place with regards to this but many mention long hours and high pressure get that this can vary lot by team but appreciate any insights any of you might have,0,weighing meta de offer,just got an offer for de position at meta and wa interested in knowing if others could shed some light on the role to help in my decision making process and check few assumption ve made briefly what have managed to understand is meta de work is mostly closer to the role of an analytics engineer involving lot le development and more sql dashboard building work life balance can be hard to achieve for the first point le worried do enjoy that kindof work and am comfortable in that kind of role my main concern here is work life balance review on blind and glassdoor are kind of all over the place with regard to this but many mention long hour and high pressure get that this can vary lot by team but appreciate any insight any of you might have
transition to data architect roles,hi sr data engineer with years of experience looking to transition into the realm of data architecture ve found passion in building scalable systems and cloud architectural components that support data engineers data scientists and mles perform their duties better how do make an active transition into architecture driven roles data architect roles are sparse with not many options for less than yoe what kind of jobs do look for and how do let recruiters reaching out to me for de roles that more interested in architecture would love to hear the community advice,0,transition to data architect role,hi sr data engineer with year of experience looking to transition into the realm of data architecture ve found passion in building scalable system and cloud architectural component that support data engineer data scientist and mles perform their duty better how do make an active transition into architecture driven role data architect role are sparse with not many option for le than yoe what kind of job do look for and how do let recruiter reaching out to me for de role that more interested in architecture would love to hear the community advice
help everything is model,recently started working in computer vision robotics company as ml ops we re building some in house tracking tools dvc has become bottleneck and wandb is unreasonably expensive part of the plan is to have django api servicing dataset retrieval creation model reuse pipeline introspection etc this was actually going well until started drafting up the models my django models now reference my research teams models which in turn depend on various other data models remnants of an ooo filesystem management type approach and its starting to drive me nuts someone on my team casually suggested namespacing ml stuff as aimodel or mlmode but still hate how this looks class aimodel models model model_name charfield anyone have any tips on naming stuff in this day and age,0,help everything is model,recently started working in computer vision robotics company a ml ops we re building some in house tracking tool dvc ha become bottleneck and wandb is unreasonably expensive part of the plan is to have django api servicing dataset retrieval creation model reuse pipeline introspection etc this wa actually going well until started drafting up the model my django model now reference my research team model which in turn depend on various other data model remnant of an ooo filesystem management type approach and it starting to drive me nut someone on my team casually suggested namespacing ml stuff a aimodel or mlmode but still hate how this look class aimodel model model model_name charfield anyone have any tip on naming stuff in this day and age
being constantly shut down by more senior team members when mention adding some qa in our work,so am consultant still junior that is put on project where we are building dashboard application for our client we are team of data engineers data architect data strategy consultant don know actual role name data modellers analysts solution owner on the engineering side we are people engineers data architect strategy guy that are building data pipelines according to data modeller requirements for their consumption am quite concerned with quality of work we are producing had to push so we can investigate and then set up local aws glue environment so we dont have to actually use the gui on aws to manually run the pipelines every time we are developing them now feel like am pushing for having some sort of testing standards in place so we can test our pipelines ve been drawn in so many meetings where data architect needs to debug his code and he asks me to help to rewrite his spark logic in sql so he can check where are the mistakes like isn this the reason you want to write automated tests where you can test for crap like this using some representative dummy data everytime mention something about writing black box tests for our pipeline am being dismissed that this would nice to have and they appreciate my idea but we should focus on development work first and getting pipelines going as this is our priority the problem with this approach is that once we finish somewhat worki pipeline we jump onto next pipeline development task and this is considered done now ve been given task to write pipeline from s3 to redshift where need to parse some nested jsons at the moment only have dummy data of records asked if we can get file that specifies what sort of data we are expecting in records because many fields contain nulls so want to know whether it will contain arrays dictionaries strings was dismissed and told that this is not important when more data comes in we will just infer schema from that and should focus on task at hand and should work with what have records like am being too anal by asking if we can somehow find out from data providers what data formats will be there so know what table format should create in redshift don know have only yoe and feel this is definitely not how data engineering should be done am worried about the quality of work we are producing as well as am not learning the best engineering practices here know solution owner is on the same page with me and the other data engineer as well but any talks about quality checks following more agile practices are being shut down by data architect and big data strategy consultant will try my best to lead by example already showed how testing can be done locally will develop my pipeline with appropriate unit tests and black box tests for my pipeline hopefully this will convince more senior guys that this is not that difficult to do and it adds value my rant is over just wanted to write this somewhere so can get it off my chest,0,being constantly shut down by more senior team member when mention adding some qa in our work,so am consultant still junior that is put on project where we are building dashboard application for our client we are team of data engineer data architect data strategy consultant don know actual role name data modeller analyst solution owner on the engineering side we are people engineer data architect strategy guy that are building data pipeline according to data modeller requirement for their consumption am quite concerned with quality of work we are producing had to push so we can investigate and then set up local aws glue environment so we dont have to actually use the gui on aws to manually run the pipeline every time we are developing them now feel like am pushing for having some sort of testing standard in place so we can test our pipeline ve been drawn in so many meeting where data architect need to debug his code and he asks me to help to rewrite his spark logic in sql so he can check where are the mistake like isn this the reason you want to write automated test where you can test for crap like this using some representative dummy data everytime mention something about writing black box test for our pipeline am being dismissed that this would nice to have and they appreciate my idea but we should focus on development work first and getting pipeline going a this is our priority the problem with this approach is that once we finish somewhat worki pipeline we jump onto next pipeline development task and this is considered done now ve been given task to write pipeline from s3 to redshift where need to parse some nested jsons at the moment only have dummy data of record asked if we can get file that specifies what sort of data we are expecting in record because many field contain null so want to know whether it will contain array dictionary string wa dismissed and told that this is not important when more data come in we will just infer schema from that and should focus on task at hand and should work with what have record like am being too anal by asking if we can somehow find out from data provider what data format will be there so know what table format should create in redshift don know have only yoe and feel this is definitely not how data engineering should be done am worried about the quality of work we are producing a well a am not learning the best engineering practice here know solution owner is on the same page with me and the other data engineer a well but any talk about quality check following more agile practice are being shut down by data architect and big data strategy consultant will try my best to lead by example already showed how testing can be done locally will develop my pipeline with appropriate unit test and black box test for my pipeline hopefully this will convince more senior guy that this is not that difficult to do and it add value my rant is over just wanted to write this somewhere so can get it off my chest
part series demystifying data warehouses data lakes lake houses,part series demystifying data warehouses data lakes lake houses,0,part series demystifying data warehouse data lake lake house,part series demystifying data warehouse data lake lake house
what technology will give me the most chance to get job,hi all an intern in an hardware company would like to find another internship for summer in data engineering what technology tool will give me the most chance to get an internship in data engineering as newbie thanks for the help,0,what technology will give me the most chance to get job,hi all an intern in an hardware company would like to find another internship for summer in data engineering what technology tool will give me the most chance to get an internship in data engineering a newbie thanks for the help
can we have monthly discussion thread,wanted to post that just did the datastax cassandra certification but don think it warrants its own thread was happy with it and wanted to share with the community anyway yeah put in good weeks in preparing had bit of cassandra experience but wanted to learn the datastax site is great and all of the contributors are really cool so thanks guys,0,can we have monthly discussion thread,wanted to post that just did the datastax cassandra certification but don think it warrant it own thread wa happy with it and wanted to share with the community anyway yeah put in good week in preparing had bit of cassandra experience but wanted to learn the datastax site is great and all of the contributor are really cool so thanks guy
airflow alternatives look at prefect and dagster,airflow alternatives look at prefect and dagster,0,airflow alternative look at prefect and dagster,airflow alternative look at prefect and dagster
data architects of reddit how do you keep up with the latest trends,recently ve been more involved with data architecture at my job used to be data engineer up to this point am mostly designing things based on my experience but haven really been following any people or blogs about data architecture yet wonder if you could suggest some valuable resources to me,0,data architect of reddit how do you keep up with the latest trend,recently ve been more involved with data architecture at my job used to be data engineer up to this point am mostly designing thing based on my experience but haven really been following any people or blog about data architecture yet wonder if you could suggest some valuable resource to me
should work on snowflake as fresher,hello peeps am trained as data engineer in my company have been an role of the snowflake developer for project should take decision to work in that or wait for another one trained on azure etl basics scala python data bricks spark sql linux and currently training on aws thanks for answering,0,should work on snowflake a fresher,hello peep am trained a data engineer in my company have been an role of the snowflake developer for project should take decision to work in that or wait for another one trained on azure etl basic scala python data brick spark sql linux and currently training on aws thanks for answering
pipebase released,pipebase is low code data integration framework in general the framework allow developer customize data pipeline through manifest definition and wire variety of system through pipeware plugins to sync transform data here is tutorial as quick start build your first hello world app timer with cli and here is list of example demonstrate how to compose manifest pipe yml and wire external system ex kafka rabbitmq mysql cassandra rocksdb aws s3 mqtt etc have fun,0,pipebase released,pipebase is low code data integration framework in general the framework allow developer customize data pipeline through manifest definition and wire variety of system through pipeware plugins to sync transform data here is tutorial a quick start build your first hello world app timer with cli and here is list of example demonstrate how to compose manifest pipe yml and wire external system ex kafka rabbitmq mysql cassandra rocksdb aws s3 mqtt etc have fun
exporting data at production scale,hi all pretty seasoned data engineer can move and transform data pretty optimally but have new use case am working at company thats product will be exporting large swaths of data to clients externally traditionally ve built dws that plug into bi tools or can serve simple exports but wondering about scaling of exports if need to serve gb day to external clients what is the ultimate tool to do that with env we are using aws our raw and stg layers are in parquet files and our dw will be in some sort of db very open to the db at the moment and am familiar with row vs columnar and can optimize based on desired use case do use spark to build exports via parquet files do put in dw and query and send out from there what are thoughts know using something like aurora or snowflake to export will be very expensive as it will be cloud egress but idk how that holds up to cost of just running exports off data lake thoughts,0,exporting data at production scale,hi all pretty seasoned data engineer can move and transform data pretty optimally but have new use case am working at company thats product will be exporting large swath of data to client externally traditionally ve built dws that plug into bi tool or can serve simple export but wondering about scaling of export if need to serve gb day to external client what is the ultimate tool to do that with env we are using aws our raw and stg layer are in parquet file and our dw will be in some sort of db very open to the db at the moment and am familiar with row v columnar and can optimize based on desired use case do use spark to build export via parquet file do put in dw and query and send out from there what are thought know using something like aurora or snowflake to export will be very expensive a it will be cloud egress but idk how that hold up to cost of just running export off data lake thought
what skill or tools you need to know in order to be data engineer,do you need to be good at analytics in order to be good in data engineering,0,what skill or tool you need to know in order to be data engineer,do you need to be good at analytics in order to be good in data engineering
transition from sap bw to data engineering,hi all am working as sap bw consultant for years now trying to get into hard core data engineering there is very minimal data engineering taking place in sap framework as it very old and boring after spending years feel don see myself growing this technology it did help me understand the basics of data warehouse and it existence have been learning ml in parallel as well but hardly can implement it in sap environment is there ways can grow in sap or if need to change my technology how do start preparing have good basics of python and sql,0,transition from sap bw to data engineering,hi all am working a sap bw consultant for year now trying to get into hard core data engineering there is very minimal data engineering taking place in sap framework a it very old and boring after spending year feel don see myself growing this technology it did help me understand the basic of data warehouse and it existence have been learning ml in parallel a well but hardly can implement it in sap environment is there way can grow in sap or if need to change my technology how do start preparing have good basic of python and sql
stuck in finding job in uk need advice,hi and currently live in the uk swansea and it about months looking for job new immigrant here but have no visa problem to was net back end freelancer developer for about years design systems and databases built web apis windows applications and familiar with python wide ranges of databases such as sqlserver mysql postgresql sqlite and also nosqls recently months ve started to learn about data engineering and started to learn about etl and cloud base databases so here is the problem started to apply for paid internships or junior roles on linkedin reed glassdoor almost everywhere but the companies reject my application sometimes in under minutes could this community give me some advice about what should do,0,stuck in finding job in uk need advice,hi and currently live in the uk swansea and it about month looking for job new immigrant here but have no visa problem to wa net back end freelancer developer for about year design system and database built web apis window application and familiar with python wide range of database such a sqlserver mysql postgresql sqlite and also nosqls recently month ve started to learn about data engineering and started to learn about etl and cloud base database so here is the problem started to apply for paid internship or junior role on linkedin reed glassdoor almost everywhere but the company reject my application sometimes in under minute could this community give me some advice about what should do
comparing results between airflow runs,comparing results between airflow runs,0,comparing result between airflow run,comparing result between airflow run
any case for using snowflake streams if dbt incremental models provide same functionality,snowflake is promoting streams and tasks lot but dbt already provides incremental model and scheduling capability directly or via airflow am curious if anyone using dbt snowflake stack found use case for streams and tasks,0,any case for using snowflake stream if dbt incremental model provide same functionality,snowflake is promoting stream and task lot but dbt already provides incremental model and scheduling capability directly or via airflow am curious if anyone using dbt snowflake stack found use case for stream and task
please critique my business from the data engineering perspective,my customers are online retailers and the business model is subscription service where my customers pay monthly fee for me to maintain the data analytics power bi daily weekly pdfs potential ml currently see it working in aws where hook up to their erp system and bring it into my own database redshift on schedule from there can automate analytics pdfs in lambda and connect to power bi to maintain their dashboards is handling this many different customers paying per month viable or is this completely impractical,0,please critique my business from the data engineering perspective,my customer are online retailer and the business model is subscription service where my customer pay monthly fee for me to maintain the data analytics power bi daily weekly pdfs potential ml currently see it working in aws where hook up to their erp system and bring it into my own database redshift on schedule from there can automate analytics pdfs in lambda and connect to power bi to maintain their dashboard is handling this many different customer paying per month viable or is this completely impractical
congratulations apache dolphinscheduler has been approved as twos candidate member,amp x200b format png amp auto webp amp cd1fc4ef2746bd46c14a7e55ada9d89431 recently twos officially announced the approval of full members and candidate members apache dolphinscheduler cloud native distributed big data scheduler was listed by twos apache dolphinscheduler is new generation workflow scheduling platform that is distributed and easy to expand it is committed to solving the intricate dependencies among big data tasks and visualizing the entire data processing its powerful visual dag interface greatly improves the user experience and can configure workflow without complex code since it was officially open sourced in april apache dolphinscheduler formerly known as easyscheduler has undergone several architectural evolutions so far the relevant open source codes have accumulated stars with experienced code contributors non code contributors participating in the project which includes pmcs and committers of other apache top level projects the apache dolphinscheduler open source community continues to grow and the wechat user group has reached people and companies and institutions have adopted apache dolphinscheduler in their production environment twos at the oscar open source industry conference china academy of telecommunication research of miit caict officially established twos twos is composed of open source projects and open source communities which aims to guide the establishment of healthy credible sustainable open source community and build communication platform providing complete set of open source risk monitoring and ecological monitoring services to help enterprises reduce the risk of using open source software and promote the establishment of credible open source ecosystem caict has created credible open source standard system which carries authoritative evaluation on enterprise open source governance capabilities open source project compliance open source community maturity open source tool detection capabilities open source risk management capabilities of commercial products after being screened by twos evaluation criteria apache dolphinscheduler was approved to be candidate member which shows its recognition of apache dolphinscheduler way of open source operation maturity and contribution and encourages the community to keep active on september the first batch of members joined twos including full members such as openeuler opengauss mindspore openlookeng etc and candidate members like apache rocketmq dcloud fluid fastreid etc with total of members amp x200b format png amp auto webp amp d72c73ccd2f6f4e5c6789e4deaeb27368db4572e only two communities were selected for the second batch of candidate members apache dolphinscheduler and polardb an open source cloud native ecological distributed database contributed by alibaba cloud the apache dolphinscheduler community is very honored to be selected as candidate member of twos which is an affirmation and incentive for the entire industry to build the community better place the community will make persistent efforts and strive to become full member as soon as possible and provide more value for china open source ecological construction together with all the twos members,0,congratulation apache dolphinscheduler ha been approved a two candidate member,amp x200b format png amp auto webp amp cd1fc4ef2746bd46c14a7e55ada9d89431 recently two officially announced the approval of full member and candidate member apache dolphinscheduler cloud native distributed big data scheduler wa listed by two apache dolphinscheduler is new generation workflow scheduling platform that is distributed and easy to expand it is committed to solving the intricate dependency among big data task and visualizing the entire data processing it powerful visual dag interface greatly improves the user experience and can configure workflow without complex code since it wa officially open sourced in april apache dolphinscheduler formerly known a easyscheduler ha undergone several architectural evolution so far the relevant open source code have accumulated star with experienced code contributor non code contributor participating in the project which includes pmcs and committers of other apache top level project the apache dolphinscheduler open source community continues to grow and the wechat user group ha reached people and company and institution have adopted apache dolphinscheduler in their production environment two at the oscar open source industry conference china academy of telecommunication research of miit caict officially established two two is composed of open source project and open source community which aim to guide the establishment of healthy credible sustainable open source community and build communication platform providing complete set of open source risk monitoring and ecological monitoring service to help enterprise reduce the risk of using open source software and promote the establishment of credible open source ecosystem caict ha created credible open source standard system which carry authoritative evaluation on enterprise open source governance capability open source project compliance open source community maturity open source tool detection capability open source risk management capability of commercial product after being screened by two evaluation criterion apache dolphinscheduler wa approved to be candidate member which show it recognition of apache dolphinscheduler way of open source operation maturity and contribution and encourages the community to keep active on september the first batch of member joined two including full member such a openeuler opengauss mindspore openlookeng etc and candidate member like apache rocketmq dcloud fluid fastreid etc with total of member amp x200b format png amp auto webp amp d72c73ccd2f6f4e5c6789e4deaeb27368db4572e only two community were selected for the second batch of candidate member apache dolphinscheduler and polardb an open source cloud native ecological distributed database contributed by alibaba cloud the apache dolphinscheduler community is very honored to be selected a candidate member of two which is an affirmation and incentive for the entire industry to build the community better place the community will make persistent effort and strive to become full member a soon a possible and provide more value for china open source ecological construction together with all the two member
convert lat and long to city name in bigquery,hi guys the data that am working on is in shitty state and have to transform it more consumable form is there way to convert lat and long to city directly on bq or should write cron job in python to transform the data,0,convert lat and long to city name in bigquery,hi guy the data that am working on is in shitty state and have to transform it more consumable form is there way to convert lat and long to city directly on bq or should write cron job in python to transform the data
the th edition of data engineering weekly highlights one year of dbt lakehouse architecture reference airflow to apache dolphin comparison and more,the th edition of data engineering weekly highlights one year of dbt lakehouse architecture reference airflow to apache dolphin comparison and more,0,the th edition of data engineering weekly highlight one year of dbt lakehouse architecture reference airflow to apache dolphin comparison and more,the th edition of data engineering weekly highlight one year of dbt lakehouse architecture reference airflow to apache dolphin comparison and more
any reasons to avoid m1 macs for data engineering,took new job and ve been asked to choose new machine was disappointed to see that apple doesn sell intel chips at all anymore because would ve clone my mbp know when m1 came out there were number of compatibility issues with data science related packages have those been resolved do they not really impact your day to day,0,any reason to avoid m1 mac for data engineering,took new job and ve been asked to choose new machine wa disappointed to see that apple doesn sell intel chip at all anymore because would ve clone my mbp know when m1 came out there were number of compatibility issue with data science related package have those been resolved do they not really impact your day to day
aws community builder program free usd credits benefits how to,aws community builder program free usd credits benefits how to,0,aws community builder program free usd credit benefit how to,aws community builder program free usd credit benefit how to
mood,mood,0,mood,mood
for those that came from da bi what were your motivations,personally am finding myself less interested in the business and more interested in the technology and data is that similar motivation or perspective for those who switched out of da bi,0,for those that came from da bi what were your motivation,personally am finding myself le interested in the business and more interested in the technology and data is that similar motivation or perspective for those who switched out of da bi
how would you select the architecture and design pipeline for this type of problem included below,this is not some assignment question dealing with this at work and want to get some opinions on how others would handle it ingest unstructured data from single rd party data source let say million rows documents different tables collections some preliminary transformation like mapping custom field names to consistent internal ones date time formatting and so on and then putting it into db data store couple of potentially long running jobs depends on querying external apis which may have their own throttling limits to be run on every single document from above but only on of the tables collections so about million rows this is run only once every week or so new fields are added from the result of the api queries job where some pretrained dl models are ran against the rows for tables say about new fields are added per row after the inferences at the end some aggregation happens and new table is created where every row document is collection of sub documents from other collections this point is important another job where the newly created table is processed and bunch of new fields are created for every single subdoc upto per subdocument some of this processing also involves running ml dl models on the docs there are also training steps in the middle where some models need to train if there are enough data or enough time has passed let just say monthly training at the end of this job the contents of this new table is again aggregated based on one of the fields let say doc_owner and put into another table this is the actual data that is useful for the customer this data maybe fields wide job where the finally created data is exported to the customer preferred warehouse db store eg firebase snowflake bigtable my question is how would approach such problem what tools would you use for each step,0,how would you select the architecture and design pipeline for this type of problem included below,this is not some assignment question dealing with this at work and want to get some opinion on how others would handle it ingest unstructured data from single rd party data source let say million row document different table collection some preliminary transformation like mapping custom field name to consistent internal one date time formatting and so on and then putting it into db data store couple of potentially long running job depends on querying external apis which may have their own throttling limit to be run on every single document from above but only on of the table collection so about million row this is run only once every week or so new field are added from the result of the api query job where some pretrained dl model are ran against the row for table say about new field are added per row after the inference at the end some aggregation happens and new table is created where every row document is collection of sub document from other collection this point is important another job where the newly created table is processed and bunch of new field are created for every single subdoc upto per subdocument some of this processing also involves running ml dl model on the doc there are also training step in the middle where some model need to train if there are enough data or enough time ha passed let just say monthly training at the end of this job the content of this new table is again aggregated based on one of the field let say doc_owner and put into another table this is the actual data that is useful for the customer this data maybe field wide job where the finally created data is exported to the customer preferred warehouse db store eg firebase snowflake bigtable my question is how would approach such problem what tool would you use for each step
jobs deployment on apache flink,jobs deployment on apache flink,0,job deployment on apache flink,job deployment on apache flink
new opensource elt tool,was looking for some declarative elt tool for creating my analytics solutions and dbt was the closest ve found liked its concept but came across quite few limitations when wanted to use it couldn specify and create basic things like data types indexes primary foreign keys etc in the end decided to implement my own more straightforward and more flexible ve published the result dbd on github perhaps you can find it helpful your feedback is greatly appreciated,0,new opensource elt tool,wa looking for some declarative elt tool for creating my analytics solution and dbt wa the closest ve found liked it concept but came across quite few limitation when wanted to use it couldn specify and create basic thing like data type index primary foreign key etc in the end decided to implement my own more straightforward and more flexible ve published the result dbd on github perhaps you can find it helpful your feedback is greatly appreciated
missed connection to the guy from confluent who messaged me offering to chat about my post where does apache kafka fit into this map of the data stack further question in comments please message again,mods sorry know this is stupid post recently posted thread about where kafka fits into the modern data stack and someone offered to speak with me accidentally ignored the chat and am upset because was excited and this is rare opportunity for me amp x200b will take down this garbage post as soon as they see this and message me know this is silly but please let me keep this up thanks,0,missed connection to the guy from confluent who messaged me offering to chat about my post where doe apache kafka fit into this map of the data stack further question in comment please message again,mod sorry know this is stupid post recently posted thread about where kafka fit into the modern data stack and someone offered to speak with me accidentally ignored the chat and am upset because wa excited and this is rare opportunity for me amp x200b will take down this garbage post a soon a they see this and message me know this is silly but please let me keep this up thanks
opinionated guides engineering,opinionated guides engineering,0,opinionated guide engineering,opinionated guide engineering
cloud data warehouse guide,cloud data warehouse guide,0,cloud data warehouse guide,cloud data warehouse guide
javascript data types,javascript data types,0,javascript data type,javascript data type
how do you persist historic data when your company doesn store events,whether it new startup or an established company with legacy systems many companies still don do event sourcing or if they do it often quite limited there are numerous approaches to addressing this problem when event logs are not available change data capture cdc this is when each database operation gets written out to an event log directly from database snapshotting periodically taking the value of particular rows tables entire databases mining application logs mean if you have to whining about it to engineering until they finally stop overwriting important data how do you approach this problem as data engineer when you don have event streams,0,how do you persist historic data when your company doesn store event,whether it new startup or an established company with legacy system many company still don do event sourcing or if they do it often quite limited there are numerous approach to addressing this problem when event log are not available change data capture cdc this is when each database operation get written out to an event log directly from database snapshotting periodically taking the value of particular row table entire database mining application log mean if you have to whining about it to engineering until they finally stop overwriting important data how do you approach this problem a data engineer when you don have event stream
based on real near catastrophe,based on real near catastrophe,0,based on real near catastrophe,based on real near catastrophe
where does apache kafka fit into this map of the data stack further question in comments,where does apache kafka fit into this map of the data stack further question in comments,0,where doe apache kafka fit into this map of the data stack further question in comment,where doe apache kafka fit into this map of the data stack further question in comment
am at crossroad and cannot postpone the decision any longer,little background graduated almost years ago and joined an early stage startup as software developer learned lot there about software development but just after few months as my area of interest was more towards data engineering started taking those responsibilities and projects to became the data guy in the small tech team learned apache airflow postgresql and aws services and honed them while was there due to covid and serious budget cuts shifted to more settled mid sized company which had data engineering team but in the months that have been here my productivity is lower than it has ever been the work is just plain boring everything working extensively on sql the fun of engineering is completely gone and it feels like have not learned anything new since have joined the requirements that usually come towards the team are either making some structure changes in the database table or in some report to better align with business don know what should do should give myself more time to adjust to this work style or look for other opportunities again think with so early in my career should focus more on job which would help me grow and expand my area of expertise now since don have any hands on experience in big data technologies don qualify for openings in companies with promising de teams and fear if don go to one which is using them then will miss out again and the cycle will repeat the next time wish to switch the make matter worse don know if this is what data engineering is like and even if learn big data technologies and get decent job then won get bored as well there really need some guidance so can take the best path forward any help is greatly appreciated,0,am at crossroad and cannot postpone the decision any longer,little background graduated almost year ago and joined an early stage startup a software developer learned lot there about software development but just after few month a my area of interest wa more towards data engineering started taking those responsibility and project to became the data guy in the small tech team learned apache airflow postgresql and aws service and honed them while wa there due to covid and serious budget cut shifted to more settled mid sized company which had data engineering team but in the month that have been here my productivity is lower than it ha ever been the work is just plain boring everything working extensively on sql the fun of engineering is completely gone and it feel like have not learned anything new since have joined the requirement that usually come towards the team are either making some structure change in the database table or in some report to better align with business don know what should do should give myself more time to adjust to this work style or look for other opportunity again think with so early in my career should focus more on job which would help me grow and expand my area of expertise now since don have any hand on experience in big data technology don qualify for opening in company with promising de team and fear if don go to one which is using them then will miss out again and the cycle will repeat the next time wish to switch the make matter worse don know if this is what data engineering is like and even if learn big data technology and get decent job then won get bored a well there really need some guidance so can take the best path forward any help is greatly appreciated
beginner recommend me python sql project,currently doing week part time python and sql boot camp we have the choice to work on one portfolio project and would like to do something data engineering related but don know where to start any recommendations,0,beginner recommend me python sql project,currently doing week part time python and sql boot camp we have the choice to work on one portfolio project and would like to do something data engineering related but don know where to start any recommendation
can you guys give an example of how you optimized spark workloads for performance,trying to understand ways in which can improve spark workflow in my daily work work with datasets gb mostly transforming existing columns based on business requirements trying to see situations in which could improve the workflow and am looking for examples from you guys when and how you had to do something similar,0,can you guy give an example of how you optimized spark workload for performance,trying to understand way in which can improve spark workflow in my daily work work with datasets gb mostly transforming existing column based on business requirement trying to see situation in which could improve the workflow and am looking for example from you guy when and how you had to do something similar
want to learn aws over the next year will it be expensive to practice,want to start learning aws and doing data engineering on aws am about to start doing couple of tutorials that require starting the aws free trial however see some parts of the trial only last couple of months like redshift if that runs out will pay lot to practice am busy and slow learner so was hoping to learn slowly over the next year and ease into it the two months thing makes me question if should even start this right now if am not going to have lot of time to do it over the next two months are there other longer trial cloud services should try or is there way to extend the trial not going to be heavily using the services only when want to practice things so would it not even be that expensive if don have heavy usage anyone have any experience practicing aws,0,want to learn aws over the next year will it be expensive to practice,want to start learning aws and doing data engineering on aws am about to start doing couple of tutorial that require starting the aws free trial however see some part of the trial only last couple of month like redshift if that run out will pay lot to practice am busy and slow learner so wa hoping to learn slowly over the next year and ease into it the two month thing make me question if should even start this right now if am not going to have lot of time to do it over the next two month are there other longer trial cloud service should try or is there way to extend the trial not going to be heavily using the service only when want to practice thing so would it not even be that expensive if don have heavy usage anyone have any experience practicing aws
google treats sql like code and you should too,ve been with google for just over years and trying to create as many teachable lessons as can come from diverse background of low tech and healthcare and hope that some of these principals that google employs can be helpful for others enjoy,0,google treat sql like code and you should too,ve been with google for just over year and trying to create a many teachable lesson a can come from diverse background of low tech and healthcare and hope that some of these principal that google employ can be helpful for others enjoy
naming conventions when doing the transformations cleaning and wrangling on the data,once the data is loaded into the data warehouse let say we talk about an elt use case how do you evolve the data and how do you name the different stages for example raw the raw data loaded into the dw lake from the sources with zero transformation staging transformations that are not ready to be served to bi tools yet refined aggregated tables for data marts bi tools etc,0,naming convention when doing the transformation cleaning and wrangling on the data,once the data is loaded into the data warehouse let say we talk about an elt use case how do you evolve the data and how do you name the different stage for example raw the raw data loaded into the dw lake from the source with zero transformation staging transformation that are not ready to be served to bi tool yet refined aggregated table for data mart bi tool etc
how and why are macs preferred for data engineering,preface this is not windows vs mac debate it question out of curiosity context ve been de for over years and used mssql azuredf python over the microsoft tech stack of ssms vs github powershell and jupyter and bit of redshift company only provided dell windows laptops with decent specs recently made switch where the tech stack includes python oracle snowflake and nosql dbs over docker datagrip pycharm studio and github company provides macbook pros which are amped out question why is it preferable to use macs for data engineering all this gibberish just to rewrite the post title what specifically to de is the benefit of using mac closed system which costs more with support only from the manufacturer especially in this day and age where almost all the data resides on remote servers or cloud and you login into those through rdps or clis do get the overall mac benefits and friggin love my mbp as machine wayyy better than the dells had,0,how and why are mac preferred for data engineering,preface this is not window v mac debate it question out of curiosity context ve been de for over year and used mssql azuredf python over the microsoft tech stack of ssms v github powershell and jupyter and bit of redshift company only provided dell window laptop with decent spec recently made switch where the tech stack includes python oracle snowflake and nosql db over docker datagrip pycharm studio and github company provides macbook pro which are amped out question why is it preferable to use mac for data engineering all this gibberish just to rewrite the post title what specifically to de is the benefit of using mac closed system which cost more with support only from the manufacturer especially in this day and age where almost all the data resides on remote server or cloud and you login into those through rdps or cli do get the overall mac benefit and friggin love my mbp a machine wayyy better than the dell had
what kind of data engineering job is this,feel like this is combination of data science data engineering and machine learning have applied to software engineering position but they offered me this when they saw my resume am math cs grad with short ml internship also will be able to transition to java backend from this if work for year or two,0,what kind of data engineering job is this,feel like this is combination of data science data engineering and machine learning have applied to software engineering position but they offered me this when they saw my resume am math c grad with short ml internship also will be able to transition to java backend from this if work for year or two
what are the advantages of using nifi and kafka for data ingestion,was looking for architectures to do sentiment analysis in streaming with spark and came across this architecture format png amp auto webp amp ea3dcc106f84d8ac125f302fa9e1fb10b1a293d9 was wondering what are the advantages of using nifi wifi with the twitter api instead of directly connecting spark to it assume it would be more fault tolerant like this but really just don know and appreciate some insights,0,what are the advantage of using nifi and kafka for data ingestion,wa looking for architecture to do sentiment analysis in streaming with spark and came across this architecture format png amp auto webp amp ea3dcc106f84d8ac125f302fa9e1fb10b1a293d9 wa wondering what are the advantage of using nifi wifi with the twitter api instead of directly connecting spark to it assume it would be more fault tolerant like this but really just don know and appreciate some insight
read chapters of kimball what now,hey all am on my journey of preparing for data engineering interviews and data warehousing and dimensional modelling was one thing that always felt intimidated by before getting started but this time took the leap after seeing lot of people recommending kimball as great book to get started read chapters and learnt good deal about what is data warehousing dimensional modelling facts dimensions scds and stuff however completing the entire book is still colossal task and think it would not be necessary to crack the interviews correct me if am wrong am still very new to this area and would want to maybe practice more by creating dimensional models now rather than just read read read so here am asking the most helpful de community on the entire internet what should be my next steps how can bolster my learning of dw dm what are some important topics that are asked in the interviews is there anything beyond the star schema is it important where can find more practice material and more,0,read chapter of kimball what now,hey all am on my journey of preparing for data engineering interview and data warehousing and dimensional modelling wa one thing that always felt intimidated by before getting started but this time took the leap after seeing lot of people recommending kimball a great book to get started read chapter and learnt good deal about what is data warehousing dimensional modelling fact dimension scd and stuff however completing the entire book is still colossal task and think it would not be necessary to crack the interview correct me if am wrong am still very new to this area and would want to maybe practice more by creating dimensional model now rather than just read read read so here am asking the most helpful de community on the entire internet what should be my next step how can bolster my learning of dw dm what are some important topic that are asked in the interview is there anything beyond the star schema is it important where can find more practice material and more
data engineering what is data warehouse,,0,data engineering what is data warehouse,
home lab data lake ideas,hello wanted to reach out to see if anyone had any good data lake data pipeline ideas for home lab or links to some labs someone has already put together thanks,0,home lab data lake idea,hello wanted to reach out to see if anyone had any good data lake data pipeline idea for home lab or link to some lab someone ha already put together thanks
key skills for data engineer and career path advices,hi everyone im looking for some advices about what to do next steps of my career path right now working at company as data engineer months but it looks just in name because in my day just work coding in python no etl no cloud no pipelines or other skills that data engineer should have came from bi analyst rol months working in power bi reporting sql queries and starting to get worried about not developing the right skills to compete in market what advices would you do to me to develop competitive profile in this market bi analyst data engineer know that eventually where work going into etl on premise and they re looking for migrate into cloud services and wondering if it normal that at the start of my new job just working in new data architecture in python months for batch part of etl process thanks for reading,0,key skill for data engineer and career path advice,hi everyone im looking for some advice about what to do next step of my career path right now working at company a data engineer month but it look just in name because in my day just work coding in python no etl no cloud no pipeline or other skill that data engineer should have came from bi analyst rol month working in power bi reporting sql query and starting to get worried about not developing the right skill to compete in market what advice would you do to me to develop competitive profile in this market bi analyst data engineer know that eventually where work going into etl on premise and they re looking for migrate into cloud service and wondering if it normal that at the start of my new job just working in new data architecture in python month for batch part of etl process thanks for reading
free aws courses on amazon com,free aws courses on amazon com,0,free aws course on amazon com,free aws course on amazon com
looking for an easy setup for blockchain data,hey everyone am not de so am little unsure of what need to do am comfortable with learning it all on my own mostly but would like some general steps like to access blockchain data btc or eth for now and load this into warehouse what options do have to do this for free ultimately like to model some stuff using dbt and create an output into self hosted bi tool,0,looking for an easy setup for blockchain data,hey everyone am not de so am little unsure of what need to do am comfortable with learning it all on my own mostly but would like some general step like to access blockchain data btc or eth for now and load this into warehouse what option do have to do this for free ultimately like to model some stuff using dbt and create an output into self hosted bi tool
how do all manage warehouse schema and models between multiple environments,at company with two environments for the data warehouse at prior companies we ve really just had prod warehouse so curious about best practices of keeping the structures in sync have used django alembic and flyway for transaction db management for backend projects and am wondering if combination of similar db migration tooling with something like dbt is the way to go fellow data nerds what are your thoughts,0,how do all manage warehouse schema and model between multiple environment,at company with two environment for the data warehouse at prior company we ve really just had prod warehouse so curious about best practice of keeping the structure in sync have used django alembic and flyway for transaction db management for backend project and am wondering if combination of similar db migration tooling with something like dbt is the way to go fellow data nerd what are your thought
from data analyst to data engineer apply for internal positions or look for opportunities outside,have been data analyst for the past months at famous fast paced tech and have complete ownership of etl pipelines that build for other analysts in my org data engineering stuff use spark hadoop airflow python sql and have good understanding of data modeling and data warehousing concepts also have worked as software engineer for year and used java extensively as the title suggests am looking to completely transition into core data engineering focused roles don want to get hung up on job title but the compensation difference is too big for me to not reconsider things it has been great learning experience but definitely feel under compensated and exploited transitioning internally will be slow process and am sure my manager will not be too happy about me leaving the team also not to forget the opportunity cost of continuing with my current salary should still push for this transition internally to gain more experience and look outside after or should try to get de position outside preferably in another big tech with my current data analyst title really appreciate any advice,0,from data analyst to data engineer apply for internal position or look for opportunity outside,have been data analyst for the past month at famous fast paced tech and have complete ownership of etl pipeline that build for other analyst in my org data engineering stuff use spark hadoop airflow python sql and have good understanding of data modeling and data warehousing concept also have worked a software engineer for year and used java extensively a the title suggests am looking to completely transition into core data engineering focused role don want to get hung up on job title but the compensation difference is too big for me to not reconsider thing it ha been great learning experience but definitely feel under compensated and exploited transitioning internally will be slow process and am sure my manager will not be too happy about me leaving the team also not to forget the opportunity cost of continuing with my current salary should still push for this transition internally to gain more experience and look outside after or should try to get de position outside preferably in another big tech with my current data analyst title really appreciate any advice
m1 macs are still riddled with compatibility issues,posted while back asking about macs for data engineering but neglected to specify that it is an m1 mac after spending my first week troubleshooting tons of errors due to mismatching architecture in my packages would say to avoid m1 macs for any local development unless you like spending weeks fiddling with every package,0,m1 mac are still riddled with compatibility issue,posted while back asking about mac for data engineering but neglected to specify that it is an m1 mac after spending my first week troubleshooting ton of error due to mismatching architecture in my package would say to avoid m1 mac for any local development unless you like spending week fiddling with every package
airflow sensor vs scheduler,an de noob trying to setup our first pipeline we have weekly flow of hundreds to thousands of hour long videos into our s3 bucket for every video that comes in need to run it through shell script that converts the video to constant frame rate already have the script requires ffmpeg and cuda store the output back into s3 we re ok with somewhat delayed processing day delay is fine but have quite bit of volume in my head using the airflow s3 key sensor makes sense but reading around it seems like lot of people recommend just sticking with the cron like scheduling capability of airflow are there cons that not thinking of with the sensor idea,0,airflow sensor v scheduler,an de noob trying to setup our first pipeline we have weekly flow of hundred to thousand of hour long video into our s3 bucket for every video that come in need to run it through shell script that convert the video to constant frame rate already have the script requires ffmpeg and cuda store the output back into s3 we re ok with somewhat delayed processing day delay is fine but have quite bit of volume in my head using the airflow s3 key sensor make sense but reading around it seems like lot of people recommend just sticking with the cron like scheduling capability of airflow are there con that not thinking of with the sensor idea
alternatives to database table reinsertion for large post processed dataset,have dataset of millions of transactions that am pulling from multiple data tables handle the data query merge cleaning amp processing in python when run ml script now also need way to have the processed data accessible for bi dashboard the direct way can think of is inserting the data back into data table but wondering if there are better ways to do this not sure whether dashboards like tableau can be set up to read in pickle file from remote server directory but if it possible this would be easy since my current process exports the data to pickle file not data engineer by trade but my hunch was there should also be way to run the python data processing code in data pipeline that would then store the data in data table that is accessible to my ml code or the dashboard connection the benefit being wouldn have to run my ml script to get the processed data,0,alternative to database table reinsertion for large post processed dataset,have dataset of million of transaction that am pulling from multiple data table handle the data query merge cleaning amp processing in python when run ml script now also need way to have the processed data accessible for bi dashboard the direct way can think of is inserting the data back into data table but wondering if there are better way to do this not sure whether dashboard like tableau can be set up to read in pickle file from remote server directory but if it possible this would be easy since my current process export the data to pickle file not data engineer by trade but my hunch wa there should also be way to run the python data processing code in data pipeline that would then store the data in data table that is accessible to my ml code or the dashboard connection the benefit being wouldn have to run my ml script to get the processed data
recommended linkedin learning courses,my company is paying for linkedin learning and have decided to take full advantage of this which courses should prioritize do hiring manager even care about any of them,0,recommended linkedin learning course,my company is paying for linkedin learning and have decided to take full advantage of this which course should prioritize do hiring manager even care about any of them
de coding interview at meta facebook,have coding interview at meta facebook for de role next week and was hoping if anyone can help me out on the recent questions they might be asking leetcode and glassdoor seem to have lot of questions asked in but not any new recent ones my interview will be based on sql and python anything would help to prepare for the interview thank you so much,0,de coding interview at meta facebook,have coding interview at meta facebook for de role next week and wa hoping if anyone can help me out on the recent question they might be asking leetcode and glassdoor seem to have lot of question asked in but not any new recent one my interview will be based on sql and python anything would help to prepare for the interview thank you so much
curated list of docker compose files prepared for testing data engineering tools databases and open source libraries,curated list of docker compose files prepared for testing data engineering tools databases and open source libraries,0,curated list of docker compose file prepared for testing data engineering tool database and open source library,curated list of docker compose file prepared for testing data engineering tool database and open source library
asking for tips for my internship,so joined this start up last week and have worked for days now most of the time if don talk to my colleagues seriously have no clue what am supposed to do with all the assignments they give me what advices can you give me feel bad for not really doing any work how can understand all the tools repo that they have built should read the repo like book,0,asking for tip for my internship,so joined this start up last week and have worked for day now most of the time if don talk to my colleague seriously have no clue what am supposed to do with all the assignment they give me what advice can you give me feel bad for not really doing any work how can understand all the tool repo that they have built should read the repo like book
does anybody use aws lambda containerized apps using ecr at enterprise scale,aws recently in late added support to deploy ecr images in conjunction with aws lambda functions which is awesome but wondering if anyone uses this architecture in prod at enterprise level how scalable is this,0,doe anybody use aws lambda containerized apps using ecr at enterprise scale,aws recently in late added support to deploy ecr image in conjunction with aws lambda function which is awesome but wondering if anyone us this architecture in prod at enterprise level how scalable is this
predictions for the modern data stack in,we wrote this article with our predictions for the modern data stack in in summary these are the main trends that we think we will see in the next year the metrics layer is here on oct of drew banin shook up the data world with his pr titled dbt should know about metrics with that announcement dbt became the front runner to win the race towards the data os the implementation of the metric layer will be major factor in making data accessible to more types of users this is very exciting announcement and will make it easier for companies to leverage data for business intelligence by improving the ease of creating queries and dashboards the new data workspace will emerge the way we read share and consume information has changed drastically over the years and it has the potential to continue to do so in the future therefore an all in one data workspace tool may be solution that responds not only to the needs of data teams today but also to how stakeholders consume data tomorrow the reverse etl race heats up stakeholders outside of the data team are becoming more data literate and in doing so are starting to require different set of tools to work with data this is partially why the reverse etl space has become one of the fastest growing data categories one of the primary predictions is that open source reverse etl will reach the same adoption as both hightouch and census in in the reverse etl space this may seem like bold claim but one that we feel is backed by substantial evidence based on what has taken shape in the etl space in addition to the increased competition in the reverse etl space we believe that we could see major acquisition in this space from larger company like twilio or fivetran the synergies between reverse etl and etl are predictive models are coming extremely excited about how predictions are going to start improving the accuracy of metrics in the modern data stack continual is one company that is making it easy to maintain predictions from customer churn to inventory forecasts directly in your cloud data warehouse this approach makes me feel confident that predictive analytics and machine learning will continue to grow in popularity within the modern data stack as it continues to become easier to implement data operations takes shape ironically data teams frequently don have the information to help us to make decisions and take action in data driven way we need data about the data we provide to make decisions about this data also called metadata for example which tables are being relied upon the most by end users what is the business definition of this metric are any etl pipelines delayed answers to these sorts of questions are increasingly important as data is becoming product used beyond simple reporting to power wide surface area of applications the operations around data are almost as important as the data itself but most teams today are building products without process documentation monitoring or analytics in summary we believe the future of the modern data stack will continue to make it easier than ever to extract even more value from your data it an exciting time to be data geek and happy to be riding this wave anything that you think we missed you can read the entire article here,0,prediction for the modern data stack in,we wrote this article with our prediction for the modern data stack in in summary these are the main trend that we think we will see in the next year the metric layer is here on oct of drew banin shook up the data world with his pr titled dbt should know about metric with that announcement dbt became the front runner to win the race towards the data o the implementation of the metric layer will be major factor in making data accessible to more type of user this is very exciting announcement and will make it easier for company to leverage data for business intelligence by improving the ease of creating query and dashboard the new data workspace will emerge the way we read share and consume information ha changed drastically over the year and it ha the potential to continue to do so in the future therefore an all in one data workspace tool may be solution that responds not only to the need of data team today but also to how stakeholder consume data tomorrow the reverse etl race heat up stakeholder outside of the data team are becoming more data literate and in doing so are starting to require different set of tool to work with data this is partially why the reverse etl space ha become one of the fastest growing data category one of the primary prediction is that open source reverse etl will reach the same adoption a both hightouch and census in in the reverse etl space this may seem like bold claim but one that we feel is backed by substantial evidence based on what ha taken shape in the etl space in addition to the increased competition in the reverse etl space we believe that we could see major acquisition in this space from larger company like twilio or fivetran the synergy between reverse etl and etl are predictive model are coming extremely excited about how prediction are going to start improving the accuracy of metric in the modern data stack continual is one company that is making it easy to maintain prediction from customer churn to inventory forecast directly in your cloud data warehouse this approach make me feel confident that predictive analytics and machine learning will continue to grow in popularity within the modern data stack a it continues to become easier to implement data operation take shape ironically data team frequently don have the information to help u to make decision and take action in data driven way we need data about the data we provide to make decision about this data also called metadata for example which table are being relied upon the most by end user what is the business definition of this metric are any etl pipeline delayed answer to these sort of question are increasingly important a data is becoming product used beyond simple reporting to power wide surface area of application the operation around data are almost a important a the data itself but most team today are building product without process documentation monitoring or analytics in summary we believe the future of the modern data stack will continue to make it easier than ever to extract even more value from your data it an exciting time to be data geek and happy to be riding this wave anything that you think we missed you can read the entire article here
how to deal with number type in javascript,hello everyone in our company the backend teams use combination of nodejs and mongodb apparently in js there is data type named number and this data type can be both doubletype and integertype and they write data in mongodb the problem is we in data engineer team when we want to sync the data from mongodb to hdfs using spark we sometimes face different data types and change of schema in fields that their types are number because in spark doubletype and integertype are two different things but in js and mongodb there is no difference between them don know what is the best approach to this problem but came up with solution that we consider all number types as doubletype when syncing data so we can avoid such changes in the future want to know pros and cons of this solution and if you know any better approach to this problem please let me know thank you all in advance,0,how to deal with number type in javascript,hello everyone in our company the backend team use combination of nodejs and mongodb apparently in j there is data type named number and this data type can be both doubletype and integertype and they write data in mongodb the problem is we in data engineer team when we want to sync the data from mongodb to hdfs using spark we sometimes face different data type and change of schema in field that their type are number because in spark doubletype and integertype are two different thing but in j and mongodb there is no difference between them don know what is the best approach to this problem but came up with solution that we consider all number type a doubletype when syncing data so we can avoid such change in the future want to know pro and con of this solution and if you know any better approach to this problem please let me know thank you all in advance
advice for how to hire de intern,fairly new de yoe and received permission to bring on an intern this summer the main goal would be getting me experience teaching and managing people and hopefully to find someone that we could hire full time next year the only person in the firm wearing the de swe hat right now have some idea of what traits would want in hire namely curious enjoys programming and understanding how things work under the hood do not expect an intern to know jack shit and will probably just ask candidates to do fizzbuzz and talk about projects they ve done in the interview amp x200b while hardly seasoned veteran ve been growing into the role quite well have working data warehouse with solid logging and exception handling that hasn broken after months of daily pipeline runs and expect further growth in the months between now and when the intern would start thinking ll probably spin up cloned warehouse environment with the pii stripped out this is an investment firm so replacing client names with list of baby names or just numbers won affect data quality in meaningful way give them copy of ddia to read over the summer and have them work through some of the problems ve worked to solve that would give them real experience and provide way to familiarize them with the stack and databases basically going to treat this as learning experience for them rather than try to shove grunt work or busywork at them and feel good about myself amp x200b so with that in mind and the fact that ideally we could recruit some talent into the firm afterwards what do you think should be looking for asking for in the posting and interview process also hate the way most technical roles are hired currently and will fucking die on this hill if have to in order to make it better amp x200b my basic requirements are amp x200b cs major minor could entertain da majors but having an msba myself have pretty low opinion of the usefulness of such programs experience with any oop language the foundation of learning your first language is what matters not intimate knowledge of the idiosyncrasies of python ability to pass fizzbuzz with numbers other than and it would be nice if they knew sql but let be honest it not that hard to learn and the only way to actually get good at it is to use it rising senior smart person who gets things done amp x200b yeah that pretty much it we re finance firm with low data maturity and lot of room to grow we aren using the shiniest new tools but data people here have lot of upside and value to deliver as result any advice on how to find and evaluate candidates for de internship at small finance company what would you require from candidates what traits would you select for,0,advice for how to hire de intern,fairly new de yoe and received permission to bring on an intern this summer the main goal would be getting me experience teaching and managing people and hopefully to find someone that we could hire full time next year the only person in the firm wearing the de swe hat right now have some idea of what trait would want in hire namely curious enjoys programming and understanding how thing work under the hood do not expect an intern to know jack shit and will probably just ask candidate to do fizzbuzz and talk about project they ve done in the interview amp x200b while hardly seasoned veteran ve been growing into the role quite well have working data warehouse with solid logging and exception handling that hasn broken after month of daily pipeline run and expect further growth in the month between now and when the intern would start thinking ll probably spin up cloned warehouse environment with the pii stripped out this is an investment firm so replacing client name with list of baby name or just number won affect data quality in meaningful way give them copy of ddia to read over the summer and have them work through some of the problem ve worked to solve that would give them real experience and provide way to familiarize them with the stack and database basically going to treat this a learning experience for them rather than try to shove grunt work or busywork at them and feel good about myself amp x200b so with that in mind and the fact that ideally we could recruit some talent into the firm afterwards what do you think should be looking for asking for in the posting and interview process also hate the way most technical role are hired currently and will fucking die on this hill if have to in order to make it better amp x200b my basic requirement are amp x200b c major minor could entertain da major but having an msba myself have pretty low opinion of the usefulness of such program experience with any oop language the foundation of learning your first language is what matter not intimate knowledge of the idiosyncrasy of python ability to pas fizzbuzz with number other than and it would be nice if they knew sql but let be honest it not that hard to learn and the only way to actually get good at it is to use it rising senior smart person who get thing done amp x200b yeah that pretty much it we re finance firm with low data maturity and lot of room to grow we aren using the shiniest new tool but data people here have lot of upside and value to deliver a result any advice on how to find and evaluate candidate for de internship at small finance company what would you require from candidate what trait would you select for
efficient way in spark to compute rows read and rows written in spark jobs processing multiple datasets,we are having explict count actions in our spark jobs which is taking lot of resources imstead can we leverage spark internal metrics sparklisteners eventlogs,0,efficient way in spark to compute row read and row written in spark job processing multiple datasets,we are having explict count action in our spark job which is taking lot of resource imstead can we leverage spark internal metric sparklisteners eventlogs
dataset augmentation for deep learning,dataset augmentation for deep learning,0,dataset augmentation for deep learning,dataset augmentation for deep learning
my job title is data engineer but don think doing any engineering,currently working as data engineer in team which builds and maintains etl pipelines for internal data joined this team years ago just after college and from the little experience that have it seems like de solutions don involve much engineering for instance most of our pipelines pull data via apis from third party saas systems and this too is now being taken care of by low no code ingestion tools etl steps are pretty much the same across projects the data model involves work but they are pretty much set during project inception and are pretty much the usual facts scd amp the issues challenges that we usually face is third party source apis systems not behaving as advertised and we basically just have to wait until their team fixes it new projects should usually be exciting but in this case dread it due to the of fields that have to be looked at and types format determined on manual basis new requests from business are usually around can we add this new field to this table can we create this view to match the existing report so overall feel that the project my team handles is basically to add reporting ability to third party systems which feel should have been present in the third party systems in the first place yet to work on any serious optimization tasks yet to write any code that isn the usual etl steps not sure if this is what is involved in de or maybe just not data person,0,my job title is data engineer but don think doing any engineering,currently working a data engineer in team which build and maintains etl pipeline for internal data joined this team year ago just after college and from the little experience that have it seems like de solution don involve much engineering for instance most of our pipeline pull data via apis from third party saas system and this too is now being taken care of by low no code ingestion tool etl step are pretty much the same across project the data model involves work but they are pretty much set during project inception and are pretty much the usual fact scd amp the issue challenge that we usually face is third party source apis system not behaving a advertised and we basically just have to wait until their team fix it new project should usually be exciting but in this case dread it due to the of field that have to be looked at and type format determined on manual basis new request from business are usually around can we add this new field to this table can we create this view to match the existing report so overall feel that the project my team handle is basically to add reporting ability to third party system which feel should have been present in the third party system in the first place yet to work on any serious optimization task yet to write any code that isn the usual etl step not sure if this is what is involved in de or maybe just not data person
am good fit for data engineering,recently had an amazon recruiter reach out to me and say they think be great fit for their de position as mgis major and senior in university my guess is due to the fact had some sql experience listed on there but frankly feel terribly under qualified for the position did sql on azure like semesters ago as homework and got basic mta cert but all that stuff haven picked up since then know everything in can be learned and am confident in my abilities to develop skills so applied anyway despite the intimidating description any tips or advice on approaching this position,0,am good fit for data engineering,recently had an amazon recruiter reach out to me and say they think be great fit for their de position a mgis major and senior in university my guess is due to the fact had some sql experience listed on there but frankly feel terribly under qualified for the position did sql on azure like semester ago a homework and got basic mta cert but all that stuff haven picked up since then know everything in can be learned and am confident in my ability to develop skill so applied anyway despite the intimidating description any tip or advice on approaching this position
need advice for data engineering portfolio project,am trying to build de portfolio project and my idea is to build small scale cloud native pipeline can you guys recommend me approaches that might be suitable for this am thinking of deploying containers that contain data ingestion from live api scripts and dbt jobs for transformation and orchestrating it with airflow is this an acceptable solution what are the most suitable cloud providers or services that can use to accomplish this thank you am more than willing to invest money to pay for the cloud costs,0,need advice for data engineering portfolio project,am trying to build de portfolio project and my idea is to build small scale cloud native pipeline can you guy recommend me approach that might be suitable for this am thinking of deploying container that contain data ingestion from live api script and dbt job for transformation and orchestrating it with airflow is this an acceptable solution what are the most suitable cloud provider or service that can use to accomplish this thank you am more than willing to invest money to pay for the cloud cost
need help with career choices from etl dev to de,my background have been working as etl developer for the past years this was my first real data related job the company is in the healthcare industry and uses microsoft stack am quite familiar with sql server and ssis but don see ssis as being great career option in the long run and the job is getting boring for me with repeated tasks like many others am considering move towards data engineering or data science position my company is transitioning into azure and cloud computing and offering to give us training some time soon have few questions know python is crucial have started getting back into learning python but have not had any hands on experience with it since finishing my degree what is good place to find more etl related python knowledge given that the current company is moving to azure is it better that stay during the transition and learn everything azure related or move to using python as primary etl tool to land new job in other words would the azure knowledge be as important as getting better at python for new position have friends that want to refer me to ds da focused position have taken some related courses for masters degree but have little to no hands on experience have heard from friends that you don have to learn as many tools in the future and can fully rely on python for data science compared to de but am unsure if what have done in the past few years going to be helpful for me to land job thanks in advance for any help,0,need help with career choice from etl dev to de,my background have been working a etl developer for the past year this wa my first real data related job the company is in the healthcare industry and us microsoft stack am quite familiar with sql server and ssis but don see ssis a being great career option in the long run and the job is getting boring for me with repeated task like many others am considering move towards data engineering or data science position my company is transitioning into azure and cloud computing and offering to give u training some time soon have few question know python is crucial have started getting back into learning python but have not had any hand on experience with it since finishing my degree what is good place to find more etl related python knowledge given that the current company is moving to azure is it better that stay during the transition and learn everything azure related or move to using python a primary etl tool to land new job in other word would the azure knowledge be a important a getting better at python for new position have friend that want to refer me to d da focused position have taken some related course for master degree but have little to no hand on experience have heard from friend that you don have to learn a many tool in the future and can fully rely on python for data science compared to de but am unsure if what have done in the past few year going to be helpful for me to land job thanks in advance for any help
snowflake with data model tools need info,hey guys am looking for forward engineering data model tool for snowflake did some research and narrowed down two out of bunch dbm sql dbvisualizer do anyone have experience working with any of these tools how good are they do they cover all features to be utilized for snowflake db modeling any pros and cons of these tool in long term use case did miss any other tool out of these two,0,snowflake with data model tool need info,hey guy am looking for forward engineering data model tool for snowflake did some research and narrowed down two out of bunch dbm sql dbvisualizer do anyone have experience working with any of these tool how good are they do they cover all feature to be utilized for snowflake db modeling any pro and con of these tool in long term use case did miss any other tool out of these two
what is the best way to manage users and permissions in my data warehouse,preferably programmable with version control,0,what is the best way to manage user and permission in my data warehouse,preferably programmable with version control
how hard is it to get job in tech,going to be joining consulting firm as junior data engineer this year when graduate from college how difficult is it to break into tech also how many yoe should you have when applying would have good chance with only yoe at this job,0,how hard is it to get job in tech,going to be joining consulting firm a junior data engineer this year when graduate from college how difficult is it to break into tech also how many yoe should you have when applying would have good chance with only yoe at this job
unsure about moving to data engineering,ve been in my first role out of college kind of hodgepodge but could charitably be described as dev ops for almost years that definitely been too long and ve felt bit stagnant so ve been looking for swe positions where can focus more on programming by chance ve ended up with an offer for data engineer position focused on writing etl and torn about what to do with it the tech stack is all things that be interested in working with java ruby spark nosql aws there pay bump the company doing well and like the mission and the people like working with data but the field is new to me so worried if don end up enjoying it would the skills be transferable or would the programming experience be applicable to other backend swe roles ve been doing some reading and see some people moving from swe to data engineering but how viable is the other way,0,unsure about moving to data engineering,ve been in my first role out of college kind of hodgepodge but could charitably be described a dev ops for almost year that definitely been too long and ve felt bit stagnant so ve been looking for swe position where can focus more on programming by chance ve ended up with an offer for data engineer position focused on writing etl and torn about what to do with it the tech stack is all thing that be interested in working with java ruby spark nosql aws there pay bump the company doing well and like the mission and the people like working with data but the field is new to me so worried if don end up enjoying it would the skill be transferable or would the programming experience be applicable to other backend swe role ve been doing some reading and see some people moving from swe to data engineering but how viable is the other way
wanting honest critique of my writing,ve been watching seattledataguys youtube videos and he calls out that one of his passive income sources is writing on medium ve seen few articles on medium over the years but have never really considered writing spent about an hour putting together quick article and love some harsh criticism on my writing style galenbusch things all data engineers should learn from google a8fe917597a galenbusch things all data engineers should learn from google a8fe917597a,0,wanting honest critique of my writing,ve been watching seattledataguys youtube video and he call out that one of his passive income source is writing on medium ve seen few article on medium over the year but have never really considered writing spent about an hour putting together quick article and love some harsh criticism on my writing style galenbusch thing all data engineer should learn from google a8fe917597a galenbusch thing all data engineer should learn from google a8fe917597a
apache nifi how to video course,hei everyone am bi developer who likes to learn new things want to share what am doing created video on how to install and configure the apache nifi with some easy examples to test the sw here is also my it blog where the process is written down in depth please like and sub if you think that it ok content thank you,0,apache nifi how to video course,hei everyone am bi developer who like to learn new thing want to share what am doing created video on how to install and configure the apache nifi with some easy example to test the sw here is also my it blog where the process is written down in depth please like and sub if you think that it ok content thank you
sql in de,know it deepends but how well do you need to know sql engine and more administration commands for example do you need to know very well how to have healthy server or it handled by admins,0,sql in de,know it deepends but how well do you need to know sql engine and more administration command for example do you need to know very well how to have healthy server or it handled by admins
databricks has terrible monitoring observability or am missing something,using azure databricks and we have some massive issues trying to instrument any kind of monitoring or metrics from it the best we seem to get is ganglia page from we are currently implementing some hackery prometheus metric exporting from it using shim outside of databricks because they are trying to proxy spark into their system anyone else ran into issues trying to troubleshoot vm usage and such not sure how anyone gets good observability on this,0,databricks ha terrible monitoring observability or am missing something,using azure databricks and we have some massive issue trying to instrument any kind of monitoring or metric from it the best we seem to get is ganglion page from we are currently implementing some hackery prometheus metric exporting from it using shim outside of databricks because they are trying to proxy spark into their system anyone else ran into issue trying to troubleshoot vm usage and such not sure how anyone get good observability on this
question as de do you have access to all production in storage container,howdy my role is data analyst and as was troubleshooting pipeline failure with de on the it enterprise data engineering team at my company was hoping to see the production data that was used to be copied into sql db from azure blob storage the reason wanted to see it doesn quite matter as my question is on data access he mentioned only the ops lead on their team had access to production data asked why and he said then he have access to payroll and other restricted pii data is this typical figured any de on the enterprise de team would have access to production data contained in the resources they managed isn it in their role responsibilities to manage sensitive data isn there the ability to restrict subdirectories or store payroll data as an example in another directory from source system data that business users like myself use and is not restricted pii just curious as this seems limiting and unnecessary,0,question a de do you have access to all production in storage container,howdy my role is data analyst and a wa troubleshooting pipeline failure with de on the it enterprise data engineering team at my company wa hoping to see the production data that wa used to be copied into sql db from azure blob storage the reason wanted to see it doesn quite matter a my question is on data access he mentioned only the ops lead on their team had access to production data asked why and he said then he have access to payroll and other restricted pii data is this typical figured any de on the enterprise de team would have access to production data contained in the resource they managed isn it in their role responsibility to manage sensitive data isn there the ability to restrict subdirectory or store payroll data a an example in another directory from source system data that business user like myself use and is not restricted pii just curious a this seems limiting and unnecessary
turning petabyte scale raw video into great datasets,imagine cameras running at fps that million frames generated in single day knowing what in that data or finding the of things that are actually interesting is hard genuinely think there way too little attention put forth to how people should use their raw data effectively even though so many people choose to store petabytes of it just in case wrote little article about taking raw video and turning that into an actionable computer vision model would love to have discussion about this so comment away also the op of this other post from few days back but just wanted to have more technical conversation here,0,turning petabyte scale raw video into great datasets,imagine camera running at fps that million frame generated in single day knowing what in that data or finding the of thing that are actually interesting is hard genuinely think there way too little attention put forth to how people should use their raw data effectively even though so many people choose to store petabyte of it just in case wrote little article about taking raw video and turning that into an actionable computer vision model would love to have discussion about this so comment away also the op of this other post from few day back but just wanted to have more technical conversation here
recommended software cloud options for fitting algorithms on tb of data,hello have collected hundreds of csvs of monthly loan and economic data which continues to grow each month the bulk of the data is the loans and tracks individuals loan performance over time such as the payment amount made whether the customer paid off more than they needed to whether they went delinquent refinanced etc it also has borrower characteristics like fico scores and dti ratios what would like to do is build model to predict prepayments delinquencies refinances etc with consideration for macro conditions and borrower characteristics if successful this model could be implemented at my company to replace our vendor model conceptually have idea about how this might work have built many ml models with datasets that were small enough to work on my local machine but the computing requirements of this are beyond that my question may be more of data engineering question but am wondering what the lowest cost method would be to store manipulate and fit models on this set first for the proof of concept and then potentially longer term for running loans through this model on monthly basis right now am thinking of simply storing the data on some low cost cloud service like amazon s3 and using apache spark via databricks to manipulate analyze and fit models on it is this is good idea or is it more or less than would need at least for the proof of concept work for small company that has relatively weak and outdated data support so am leading this alone but could get little bit of money towards it apologies if this is outside of the scope of this subreddit thanks,0,recommended software cloud option for fitting algorithm on tb of data,hello have collected hundred of csvs of monthly loan and economic data which continues to grow each month the bulk of the data is the loan and track individual loan performance over time such a the payment amount made whether the customer paid off more than they needed to whether they went delinquent refinanced etc it also ha borrower characteristic like fico score and dti ratio what would like to do is build model to predict prepayment delinquency refinances etc with consideration for macro condition and borrower characteristic if successful this model could be implemented at my company to replace our vendor model conceptually have idea about how this might work have built many ml model with datasets that were small enough to work on my local machine but the computing requirement of this are beyond that my question may be more of data engineering question but am wondering what the lowest cost method would be to store manipulate and fit model on this set first for the proof of concept and then potentially longer term for running loan through this model on monthly basis right now am thinking of simply storing the data on some low cost cloud service like amazon s3 and using apache spark via databricks to manipulate analyze and fit model on it is this is good idea or is it more or le than would need at least for the proof of concept work for small company that ha relatively weak and outdated data support so am leading this alone but could get little bit of money towards it apology if this is outside of the scope of this subreddit thanks
changing company after month,am currently in new company since months where things are going fine but not great also the job is more focused on backend api development than creating data pipelines in which am more interested on bad day at work responded to some other recruiters and finally got an offer which is better than my current salary on more data pipeline focused job do you think it would be premature to change company after only months feel than didn let enough time to my current company to prove its value and that it could backfire as would be changing companies too often,0,changing company after month,am currently in new company since month where thing are going fine but not great also the job is more focused on backend api development than creating data pipeline in which am more interested on bad day at work responded to some other recruiter and finally got an offer which is better than my current salary on more data pipeline focused job do you think it would be premature to change company after only month feel than didn let enough time to my current company to prove it value and that it could backfire a would be changing company too often
getting started with de in,how to get started with data engineering knows about sql some python have ds exp,0,getting started with de in,how to get started with data engineering know about sql some python have d exp
data pipelining in databricks delta lake,hi everyone trying to do cleaning within delta lake pipeline bronze gt silver the silver pipeline runs on trigger while bronze is writing in stream through appending the question is how can get checkpoint for my silver pipeline to start transforming data from the latest checkpoint that it read on the previous batch,0,data pipelining in databricks delta lake,hi everyone trying to do cleaning within delta lake pipeline bronze gt silver the silver pipeline run on trigger while bronze is writing in stream through appending the question is how can get checkpoint for my silver pipeline to start transforming data from the latest checkpoint that it read on the previous batch
wanna find niche or offer product within analytics,as the title wanna find niche and also offer service product or solution within data analytics but need road help or ideas from you guys today work as bi analyst and data engineering the whole elt process use only three tools to accomplish the elt step and also as last step power bi or tableau but feel that everyone today can do my job or tasks therefore wanna come up with something special that few person have to compete with me can basic code in python also when using for analysis maybe should only focus on databricks don know really feel lost and bit anxious any ideas or tips,0,wanna find niche or offer product within analytics,a the title wanna find niche and also offer service product or solution within data analytics but need road help or idea from you guy today work a bi analyst and data engineering the whole elt process use only three tool to accomplish the elt step and also a last step power bi or tableau but feel that everyone today can do my job or task therefore wanna come up with something special that few person have to compete with me can basic code in python also when using for analysis maybe should only focus on databricks don know really feel lost and bit anxious any idea or tip
airflow development style for personal project,just wondering how everyone deploys airflow for the purpose of personal projects would love to hear different approaches amp x200b do you install and configure everything in vm guess this way you can use vs code and just write your dag directly on the vm or are you using docker if so how do you write your dags would love to have syntax highlight lol,0,airflow development style for personal project,just wondering how everyone deploys airflow for the purpose of personal project would love to hear different approach amp x200b do you install and configure everything in vm guess this way you can use v code and just write your dag directly on the vm or are you using docker if so how do you write your dag would love to have syntax highlight lol
data modelling resources for streaming use cases,in context of data modelling kimbal inmon and data vault all seem to be focussed optimised for batch workloads any specific resources equivalent to kimbal but focussed on streaming use cases,0,data modelling resource for streaming use case,in context of data modelling kimbal inmon and data vault all seem to be focussed optimised for batch workload any specific resource equivalent to kimbal but focussed on streaming use case
scalable cloud saas approaches and tools,helping to build service for my company that allows users to log in give our application permission to retrieve data from an api for which they have an account and then warehouse said data for our users automatically we don plan for many users yet obviously but when more users jump on board we ll need to make sure that we aren overloading single task node the client facing web app will be separated from the task node actually performing the data extraction and loading currently using azure for what we ve built so far any good blueprints out there with reasonable complexity all recommendations welcome,0,scalable cloud saas approach and tool,helping to build service for my company that allows user to log in give our application permission to retrieve data from an api for which they have an account and then warehouse said data for our user automatically we don plan for many user yet obviously but when more user jump on board we ll need to make sure that we aren overloading single task node the client facing web app will be separated from the task node actually performing the data extraction and loading currently using azure for what we ve built so far any good blueprint out there with reasonable complexity all recommendation welcome
is python and sql enough to get my first data engineering job with decent pay,know python and sql but saw lot of jds requiring lot more stuff am not sure if python and sql will be enough for my first job,0,is python and sql enough to get my first data engineering job with decent pay,know python and sql but saw lot of jds requiring lot more stuff am not sure if python and sql will be enough for my first job
what is engineering for data how should we manage data at scale,what is engineering for data how should we manage data at scale,0,what is engineering for data how should we manage data at scale,what is engineering for data how should we manage data at scale
testing candidates on thinking like data person,not my best work in the title but hear me out understand that thinking like data person isn very concrete soft skill to test for hell be hard pressed to define what it means exactly but hoping people in this sub will intuitively know what mean by that it mixture of critical thinking working with ambiguity and incomplete information and having feel for possible issues arising in data model as the market for candidates in the data space is quite small we got few very interesting applications from software engineers looking to move into bi development and data engineering it goes without saying that we re hiring for attitude and planning to invest some time in whomever we hire to get them up to speed but since never really hired this type of profile wanted to ask you about some of the questions you like to ask during the interview process and general interview strategies for these types of candidates looking forward to the discussion arising from this,0,testing candidate on thinking like data person,not my best work in the title but hear me out understand that thinking like data person isn very concrete soft skill to test for hell be hard pressed to define what it mean exactly but hoping people in this sub will intuitively know what mean by that it mixture of critical thinking working with ambiguity and incomplete information and having feel for possible issue arising in data model a the market for candidate in the data space is quite small we got few very interesting application from software engineer looking to move into bi development and data engineering it go without saying that we re hiring for attitude and planning to invest some time in whomever we hire to get them up to speed but since never really hired this type of profile wanted to ask you about some of the question you like to ask during the interview process and general interview strategy for these type of candidate looking forward to the discussion arising from this
unit test for databricks,hello data people anyone here using databricks for their etl pipeline did you write unit test for your code if yes how ve only heard about databricks connect is the best solution for unit testing in databricks,0,unit test for databricks,hello data people anyone here using databricks for their etl pipeline did you write unit test for your code if yes how ve only heard about databricks connect is the best solution for unit testing in databricks
can we copy bak file to sql database table in azure data factory,want to know if can copy bak backup dump file to sql database table in azure data factory without using sql server management studio if yes then how if no what are the possible reasons,0,can we copy bak file to sql database table in azure data factory,want to know if can copy bak backup dump file to sql database table in azure data factory without using sql server management studio if yes then how if no what are the possible reason
data modelling for data lakehouse enterprise wide dimensional model or no model needed at all,the concept of the enterprise data warehouse model got popularity in the good ole days when everything was database based can only process structured data now we have data lakes and more importantly data lakehouse concept by databricks which is flat file based workflow can process structured semi structured unstructured data what is the standard to follow for modeling the data for olap in data lakehouse,0,data modelling for data lakehouse enterprise wide dimensional model or no model needed at all,the concept of the enterprise data warehouse model got popularity in the good ole day when everything wa database based can only process structured data now we have data lake and more importantly data lakehouse concept by databricks which is flat file based workflow can process structured semi structured unstructured data what is the standard to follow for modeling the data for olap in data lakehouse
databricks cluster capacity,hello looking for examples of how to properly configure databricks cluster capacity which machines chose for workers and driver how many workers etc have typical batch job load data from csv files and merge them into delta tables have the info on how many tables will process what is the size of input csv files use databricks api to launch job with fixed number of workers in job run concurrent notebooks and want to precisely choose the right number of needed workers thanks in advance,0,databricks cluster capacity,hello looking for example of how to properly configure databricks cluster capacity which machine chose for worker and driver how many worker etc have typical batch job load data from csv file and merge them into delta table have the info on how many table will process what is the size of input csv file use databricks api to launch job with fixed number of worker in job run concurrent notebook and want to precisely choose the right number of needed worker thanks in advance
updating database when an excel file is changed,hi guys for personal project manually maintaining an excel file where insert delete rows occasionally and it tiring to restart new database each time how would automate this process such that the database gets updated whenever make change to the excel file my database is mongodb cloud,0,updating database when an excel file is changed,hi guy for personal project manually maintaining an excel file where insert delete row occasionally and it tiring to restart new database each time how would automate this process such that the database get updated whenever make change to the excel file my database is mongodb cloud
salesforce sandbox config for de,are there any specific configuration changes or settings that need to be made when setting up full sandbox for integration and warehousing development and testing,0,salesforce sandbox config for de,are there any specific configuration change or setting that need to be made when setting up full sandbox for integration and warehousing development and testing
learning scala,hi there ve been looking through data engineering vacancies on the internet and quite few mention scala as preferred skill know fair bit of python but don have experience with scala or any other jvm languages heard it supposed to be beneficial for performance in some areas of data engineering do you guys know when and why scala would be preferred over python and also how would go about learning scala like to learn while doing project but not sure what kinds of projects are doable for scala beginner and what specific kinds of things it is used for,0,learning scala,hi there ve been looking through data engineering vacancy on the internet and quite few mention scala a preferred skill know fair bit of python but don have experience with scala or any other jvm language heard it supposed to be beneficial for performance in some area of data engineering do you guy know when and why scala would be preferred over python and also how would go about learning scala like to learn while doing project but not sure what kind of project are doable for scala beginner and what specific kind of thing it is used for
column level lineage with dbt in bigquery,ve recently written post about how we are using zetasql to extract column level lineage in bigquery using the compiled models from dbt ll try to write in the upcoming weeks more technical post around zetasql and how to use it to parse sql sentences format png amp auto webp amp d00121404f6d686b8f37923a7e4ea445c6dcba0,0,column level lineage with dbt in bigquery,ve recently written post about how we are using zetasql to extract column level lineage in bigquery using the compiled model from dbt ll try to write in the upcoming week more technical post around zetasql and how to use it to parse sql sentence format png amp auto webp amp d00121404f6d686b8f37923a7e4ea445c6dcba0
career in de with mis,college senior here ll be graduating with degree in business administration management information systems concentration this summer took big data class and really enjoyed wrangling yelp dataset recently found out about de and like to learn some more about the career what can do to increase my chances of landing job in de ve taken class in big data and database management systems here is link to my big data group project if anyone is curious to check it out,0,career in de with mi,college senior here ll be graduating with degree in business administration management information system concentration this summer took big data class and really enjoyed wrangling yelp dataset recently found out about de and like to learn some more about the career what can do to increase my chance of landing job in de ve taken class in big data and database management system here is link to my big data group project if anyone is curious to check it out
true or false,true or false,0,true or false,true or false
please guide me for interview study material am extremely overwhelmed,was software developer worked as pseudo data engineer at my last job did batch streaming python etl scripts but now am moving to make career in data engineering at this moment have searched numerous articles online and am overwhelmed on how to prepare for the interviews so far according to my understanding need to get hands on python sql data modeling data warehousing data pipeline batch and stream distributed system fundamentals system design behavioral any major topic missed it can take months if dive deep in all of the above sections am unemployed and want to get job sooner than later am preparing for and th point so far but how to find sufficient resources on rest of the points each book can take weeks to complete should target watching youtube udemy videos instead please request please someone guide me properly to ace interviews have been unemployed since pandemic started can commit more than hours of studying and want to crack interviews,0,please guide me for interview study material am extremely overwhelmed,wa software developer worked a pseudo data engineer at my last job did batch streaming python etl script but now am moving to make career in data engineering at this moment have searched numerous article online and am overwhelmed on how to prepare for the interview so far according to my understanding need to get hand on python sql data modeling data warehousing data pipeline batch and stream distributed system fundamental system design behavioral any major topic missed it can take month if dive deep in all of the above section am unemployed and want to get job sooner than later am preparing for and th point so far but how to find sufficient resource on rest of the point each book can take week to complete should target watching youtube udemy video instead please request please someone guide me properly to ace interview have been unemployed since pandemic started can commit more than hour of studying and want to crack interview
scheduling airflow job to run on particular day,tried looking this up online but couldn find any concrete answer hence posting it here am trying to run my dag on every wednesday at midnight and am not sure how to schedule that as the options are only once weekly monthly etc with dag dag_id test_dag schedule_interval every_wednesday default_args default_args start_date datetime catchup false,0,scheduling airflow job to run on particular day,tried looking this up online but couldn find any concrete answer hence posting it here am trying to run my dag on every wednesday at midnight and am not sure how to schedule that a the option are only once weekly monthly etc with dag dag_id test_dag schedule_interval every_wednesday default_args default_args start_date datetime catchup false
first data engineer job how should prep,coming from an analyst role quiet competent in sql and able to feel my way through the dark with python what prep should focus on to hit the ground running day one amp x200b really cool with any opinions or suggestions thank you,0,first data engineer job how should prep,coming from an analyst role quiet competent in sql and able to feel my way through the dark with python what prep should focus on to hit the ground running day one amp x200b really cool with any opinion or suggestion thank you
should continue with an interview for job know won take,am currently sales analyst trying to transition to data engineering things are going well in an interview with large company but after glowing first round they revealed the position would be hybrid forcing me to move to hcol city and the salary and all benefits bonus and match would be only modest raise from what make now when factoring in cost of living increases she was resistant to negotiation the recruiter set up one more technical interview considering still going ahead to get practice for data engineering interviews but is this very rude or bad idea,0,should continue with an interview for job know won take,am currently sale analyst trying to transition to data engineering thing are going well in an interview with large company but after glowing first round they revealed the position would be hybrid forcing me to move to hcol city and the salary and all benefit bonus and match would be only modest raise from what make now when factoring in cost of living increase she wa resistant to negotiation the recruiter set up one more technical interview considering still going ahead to get practice for data engineering interview but is this very rude or bad idea
deep dive to our data stack architecture including healthy dose of dogfooding,hey everyone we wrote post as part of series of posts about our data stack it includes some dogfooding for obvious reasons and hopefully not lot of inevitable bias but believe it will be helpful for anyone who interested in designing implementing data stack you can find the post here feedback is more than welcome and happy to answer any questions,0,deep dive to our data stack architecture including healthy dose of dogfooding,hey everyone we wrote post a part of series of post about our data stack it includes some dogfooding for obvious reason and hopefully not lot of inevitable bias but believe it will be helpful for anyone who interested in designing implementing data stack you can find the post here feedback is more than welcome and happy to answer any question
data virtualization denodo,my work primarily involves this platform and the integrations around it data sources business applications authentication data integration etc through my work am able to understand what data engineering is all about and find it interesting so far to give an idea about what my day to day work looks like customer support consulting handling cases troubleshooting calls architectural suggestions possibilities want to understand how should plan my career yoe how is denodo seen in the de circle and how widely is it used will sticking to the same product technology have an impact later as am from non coding background comfortable with anything sql related what are the other skills that should acquire to stay relevant in the de domain in my case the product is already built with all the features functionalities related to de and able to understand what happening but in this group see people talking about doing the extraction integration building the pipelines all by themselves using other tools softwares want to understand what type of specific coding skills will be required to move in the direction any response will be great help and appreciate it,0,data virtualization denodo,my work primarily involves this platform and the integration around it data source business application authentication data integration etc through my work am able to understand what data engineering is all about and find it interesting so far to give an idea about what my day to day work look like customer support consulting handling case troubleshooting call architectural suggestion possibility want to understand how should plan my career yoe how is denodo seen in the de circle and how widely is it used will sticking to the same product technology have an impact later a am from non coding background comfortable with anything sql related what are the other skill that should acquire to stay relevant in the de domain in my case the product is already built with all the feature functionality related to de and able to understand what happening but in this group see people talking about doing the extraction integration building the pipeline all by themselves using other tool software want to understand what type of specific coding skill will be required to move in the direction any response will be great help and appreciate it
de vs de bi,currently doing masters in ds in europe and working as de intern at mid size tech company ish employees and considering move to better brand tech company as de bi top startup with around employees salary and perks are basically identical only diference is the de role while both are in data management team the one at is way more technical than the de bi role ve heard that some fang have this kind of de bi role where you are basically bi engineer since you work daily with dashboards and bi tools in my current role we don do any bi just pure de with scala my goal eventually is to become mle after some years of de experience another thing to consider is that my company ds team is nothing compared to the startup could join the most we do is few recommenders and search optimization the startup have an inhouse research team building crazy stuff with chatbots and rl and other sota applications which one should choose leaning towards the startup but don know if ll get enough technical experience to be able to transition to mle in the same company,0,de v de bi,currently doing master in d in europe and working a de intern at mid size tech company ish employee and considering move to better brand tech company a de bi top startup with around employee salary and perk are basically identical only diference is the de role while both are in data management team the one at is way more technical than the de bi role ve heard that some fang have this kind of de bi role where you are basically bi engineer since you work daily with dashboard and bi tool in my current role we don do any bi just pure de with scala my goal eventually is to become mle after some year of de experience another thing to consider is that my company d team is nothing compared to the startup could join the most we do is few recommenders and search optimization the startup have an inhouse research team building crazy stuff with chatbots and rl and other sota application which one should choose leaning towards the startup but don know if ll get enough technical experience to be able to transition to mle in the same company
best book to learn about data warehouse and data modeling,have an interview in few weeks for data engineering role haven worked with data warehousing yet additionally want to grasp more knowledge on developing data model for random situations like carpooling or bookstore etc they will give me situation and would have to design its data model want to prepare and possibly books are viable solution please recommend books for the same also if you have another recommendation on what to study or prepare please enlighten me so far have practiced python sql designing pipeline techniques thank you,0,best book to learn about data warehouse and data modeling,have an interview in few week for data engineering role haven worked with data warehousing yet additionally want to grasp more knowledge on developing data model for random situation like carpooling or bookstore etc they will give me situation and would have to design it data model want to prepare and possibly book are viable solution please recommend book for the same also if you have another recommendation on what to study or prepare please enlighten me so far have practiced python sql designing pipeline technique thank you
how hard is it to learn aws if you know azure,have been working as data engineer for months now and really like it my company mainly uses azure so lack exposure to aws starting to slowly apply to new jobs and wondering what should focus on learning in aws and how hard it will be taught myself or learned from my boss everything know in azure but it is pretty user friendly so it wasn that hard does anybody have any suggestions on the best way to start learning important aws services for data engineer thanks,0,how hard is it to learn aws if you know azure,have been working a data engineer for month now and really like it my company mainly us azure so lack exposure to aws starting to slowly apply to new job and wondering what should focus on learning in aws and how hard it will be taught myself or learned from my bos everything know in azure but it is pretty user friendly so it wasn that hard doe anybody have any suggestion on the best way to start learning important aws service for data engineer thanks
is the purdue pg course in data engineering from simplilearn any good,is the purdue pg course in data engineering from simplilearn any good,0,is the purdue pg course in data engineering from simplilearn any good,is the purdue pg course in data engineering from simplilearn any good
thoughts on managing independent processes,have system in which discrete event notifications are received for millions of users but relatively little data per user each event message is tagged with the user when we process the events we group them by user and our downstream analyses calculates features for each user for ml models we need to pull from other data sources like key value stores to augment the events with additional data but the data is completely partitioned by user through the whole process am currently considering having the raw event data be ingested by kafka partitioned based on hash of the user id this would allow us to handle the data processing by running single independent process per kafka partition think kafka streaming am curious if anybody knows of any platforms that are good for deploying and managing the state of large number of independent processes as group similar to grid schedulers like sun grid engine from back in the day want to be able to say go execute processes say packaged as docker images on the cluster but these are not just going to run once they will run continuously if process fails want the system to restart it and notify me want to be able to check the status of the processes hesitant to use something like spark because it seems better suited for small number of large data sets we don need the ability to join across partitions and explicitly want to avoid enabling that does anybody have similar use cases any recommendations tia,0,thought on managing independent process,have system in which discrete event notification are received for million of user but relatively little data per user each event message is tagged with the user when we process the event we group them by user and our downstream analysis calculates feature for each user for ml model we need to pull from other data source like key value store to augment the event with additional data but the data is completely partitioned by user through the whole process am currently considering having the raw event data be ingested by kafka partitioned based on hash of the user id this would allow u to handle the data processing by running single independent process per kafka partition think kafka streaming am curious if anybody know of any platform that are good for deploying and managing the state of large number of independent process a group similar to grid scheduler like sun grid engine from back in the day want to be able to say go execute process say packaged a docker image on the cluster but these are not just going to run once they will run continuously if process fails want the system to restart it and notify me want to be able to check the status of the process hesitant to use something like spark because it seems better suited for small number of large data set we don need the ability to join across partition and explicitly want to avoid enabling that doe anybody have similar use case any recommendation tia
switching to de career any advice would be appreciated,hi have years of statistical programming experience in pharma but am thinking about switching to de cuz feel like statistical programmers are being paid waaay less than data engineers for example in my country stat programmers receive usd anually on average while data engineers can get usd anually on average so know python sql pretty well don have industry experience for them though would you advice to stay in the field am currently in or better switch to de my main concern is that although have programming experience and know python sql no one will hire me as de because don have relevant experience with it and will end up with nothing any advice thanks,0,switching to de career any advice would be appreciated,hi have year of statistical programming experience in pharma but am thinking about switching to de cuz feel like statistical programmer are being paid waaay le than data engineer for example in my country stat programmer receive usd anually on average while data engineer can get usd anually on average so know python sql pretty well don have industry experience for them though would you advice to stay in the field am currently in or better switch to de my main concern is that although have programming experience and know python sql no one will hire me a de because don have relevant experience with it and will end up with nothing any advice thanks
is this course good way of getting an entry level de job,hi there ve been looking for ways to build up the required skills knowledge to get an entry level de job and came across this course yesterday my python pretty good sql basic and ve done few ds and data wrangling projects at work and for personal projects but don have any practical experience in things like docker kafka spark mongodb etc so do you think this course would be good for me has anyone done it before any other suggestions alternatives think the main thing is it would give me outlines for number of practical projects that could adapt for my portfolio but not sure if that important for de interviews,0,is this course good way of getting an entry level de job,hi there ve been looking for way to build up the required skill knowledge to get an entry level de job and came across this course yesterday my python pretty good sql basic and ve done few d and data wrangling project at work and for personal project but don have any practical experience in thing like docker kafka spark mongodb etc so do you think this course would be good for me ha anyone done it before any other suggestion alternative think the main thing is it would give me outline for number of practical project that could adapt for my portfolio but not sure if that important for de interview
what is databricks and how can learn it,am currently enrolled in data engineering boot camp we go over various technologies azure pyspark airflow hoodop nosql sql python but not over something like databricks am in contact with lots of recent graduates who landed job almost everyone recommend me to learn databricks but have no idea what it is and how to learn there not much good resources out there on youtube or any other website,0,what is databricks and how can learn it,am currently enrolled in data engineering boot camp we go over various technology azure pyspark airflow hoodop nosql sql python but not over something like databricks am in contact with lot of recent graduate who landed job almost everyone recommend me to learn databricks but have no idea what it is and how to learn there not much good resource out there on youtube or any other website
resource for building data models and data marts,what are the best resources to learn to build data models and data marts,0,resource for building data model and data mart,what are the best resource to learn to build data model and data mart
manager left will be assigned new one how can politely ask for manager who is interested in technical side of things,am very new to de lt yo and previously worked as analyst for year my previous manager left which is unfortunate for me because we got along really well one reason for that was that he was very into technical side of engineering often during our catchups would show him the code that wrote for clients projects and he would give me pointers how to do better job or we would talk about best engineering practices and share articles about technical data engineering topics because he has left will be assigned new manager the current option is to assign the head of data engineering team as my permanent manager my worry is that will not get the technical feedback from him as he seems to be interested in strategy and data engineering vision for the company obviously nothing wrong with that but am worried that will not learn as much in terms of technical skills and engineering side as did before was asked if am happy for the head of de to be my manager told them that don mind but am really ibterested in learning the technical aspect of the job in my first years as engineer and that am worried that will not get the same guidance on my work as did before will have meeting with head of de and would like to know how can diplomatically address this issue with my concerns without offending anyone,0,manager left will be assigned new one how can politely ask for manager who is interested in technical side of thing,am very new to de lt yo and previously worked a analyst for year my previous manager left which is unfortunate for me because we got along really well one reason for that wa that he wa very into technical side of engineering often during our catchups would show him the code that wrote for client project and he would give me pointer how to do better job or we would talk about best engineering practice and share article about technical data engineering topic because he ha left will be assigned new manager the current option is to assign the head of data engineering team a my permanent manager my worry is that will not get the technical feedback from him a he seems to be interested in strategy and data engineering vision for the company obviously nothing wrong with that but am worried that will not learn a much in term of technical skill and engineering side a did before wa asked if am happy for the head of de to be my manager told them that don mind but am really ibterested in learning the technical aspect of the job in my first year a engineer and that am worried that will not get the same guidance on my work a did before will have meeting with head of de and would like to know how can diplomatically address this issue with my concern without offending anyone
having hard time getting de job should aim for different position for my first job,tried making personal projects and stuff but everywhere job experience is requirement even for internship so was thinking should look for data analyst or back end dev job was confused which kinda of profile skills will help when ll later eventually transition in data engineering your help would be appreciated thanks,0,having hard time getting de job should aim for different position for my first job,tried making personal project and stuff but everywhere job experience is requirement even for internship so wa thinking should look for data analyst or back end dev job wa confused which kinda of profile skill will help when ll later eventually transition in data engineering your help would be appreciated thanks
has someone tried this,it is something in lines with any source to any sink there is no proper documentation related to its architecture,0,ha someone tried this,it is something in line with any source to any sink there is no proper documentation related to it architecture
data engineering entry,looking for suggestions getting into the data engineering world have good sql knowledge and intermediate python skills plus pandas what courses and what kind of projects might be best for me break into de field,0,data engineering entry,looking for suggestion getting into the data engineering world have good sql knowledge and intermediate python skill plus panda what course and what kind of project might be best for me break into de field
incrementing tables that use union queries,have summary table that need to refresh on daily basis only problem is that the summary table is collection of unions and it doesn look like merge operation can be performed on union queries any ideas how to proceed with this seems to me like ll have to truncate and perform full table inserts for now which is fine but could get problematic once the volume of data increases,0,incrementing table that use union query,have summary table that need to refresh on daily basis only problem is that the summary table is collection of union and it doesn look like merge operation can be performed on union query any idea how to proceed with this seems to me like ll have to truncate and perform full table insert for now which is fine but could get problematic once the volume of data increase
better ways of iterating through data frames other than loops,work as junior data engineer and much of the data we migrate to our dbs are first stored in data frames in python sometimes we do things like fetch the response from an api to migrating data across different databases or different kinds of databases among other things am wondering if there are better ways for iterating through data frame to do few things writing the data in the dataframe to table in database know that pandas has the to_sql function which appears to do this but am looking for other options here doing operations on individual records in the dataframe am mainly asking these questions because would like to have other tools techniques under my belt other than loops especially when working with larger datasets like said am junior engineer so please forgive me if my terminology is incorrect,0,better way of iterating through data frame other than loop,work a junior data engineer and much of the data we migrate to our db are first stored in data frame in python sometimes we do thing like fetch the response from an api to migrating data across different database or different kind of database among other thing am wondering if there are better way for iterating through data frame to do few thing writing the data in the dataframe to table in database know that panda ha the to_sql function which appears to do this but am looking for other option here doing operation on individual record in the dataframe am mainly asking these question because would like to have other tool technique under my belt other than loop especially when working with larger datasets like said am junior engineer so please forgive me if my terminology is incorrect
data engineering projects to add to your resume seattle data guy,data engineering projects to add to your resume seattle data guy,0,data engineering project to add to your resume seattle data guy,data engineering project to add to your resume seattle data guy
beginner de project,hi all ve been trying to apply for ds da jobs but realized that may be more interested in the de side of things back end of how to structure data together so ds da can draw insights from have experience in python and sql but am kind of overwhelmed mainly am stuck on where to start for beginner de project showcasing that know how to use big data technologies s3 redshift etc see lot of people drawing pipelines plans of their project in addition ve been taking this course to understand what the software technologies use are but any tips on planning starting amp executing de project from scratch from blank slate is the problem since in undergrad was often babied by professors amp tas where the base structure of the code is set up thanks lot appreciate any help,0,beginner de project,hi all ve been trying to apply for d da job but realized that may be more interested in the de side of thing back end of how to structure data together so d da can draw insight from have experience in python and sql but am kind of overwhelmed mainly am stuck on where to start for beginner de project showcasing that know how to use big data technology s3 redshift etc see lot of people drawing pipeline plan of their project in addition ve been taking this course to understand what the software technology use are but any tip on planning starting amp executing de project from scratch from blank slate is the problem since in undergrad wa often babied by professor amp ta where the base structure of the code is set up thanks lot appreciate any help
policies around downloading installing software,question for data engineers in enterprise companies what are your policies procedures or limitations around downloading and installing software to perform your job locally or on corporate server,0,policy around downloading installing software,question for data engineer in enterprise company what are your policy procedure or limitation around downloading and installing software to perform your job locally or on corporate server
help need to give work demonstration on project for my dream job any tips,am on another round of interviews and they would like me to bring in work sample of something have done in the past problem is that everything have done for my current job would be info breach to show them any suggestions on something else could show instead don mind putting in the work and creating something new however not even sure where to begin maybe something in tableau ssis with sql or power bi would also be options thanks any insight is appreciated this is really dream job for me and feel close but don quite know what to try to build to show them or if should take tableau report from work and change text numbers however most are really just plain data dumps with few visuals,0,help need to give work demonstration on project for my dream job any tip,am on another round of interview and they would like me to bring in work sample of something have done in the past problem is that everything have done for my current job would be info breach to show them any suggestion on something else could show instead don mind putting in the work and creating something new however not even sure where to begin maybe something in tableau ssis with sql or power bi would also be option thanks any insight is appreciated this is really dream job for me and feel close but don quite know what to try to build to show them or if should take tableau report from work and change text number however most are really just plain data dump with few visuals
pipeline help sas7bdat gt s3 gt dashboard,am seeking assistance with pipeline am envisioning have numerous sas7bdat files that need to transform using and feed into visualization app rshiny this has already been accomplished manually but would like to scale up the solution using proper data engineering principles before join and transform these sas files am thinking it is advantageous to load them into an s3 bucket these sas files are currently scattered all over the place on our server and would like to store them in one location that can draw from in the future have knowledge of sql from using proc sql in sas but am struggling to wrap my head around the advantage of converting these sas files to proper sql database and tables can anyone please provide me with resource or explanation on why this is important our company is shifting away from sas to python so am in favor of moving away from sas for that reason alone but feel like there is something am missing once all these files are in an s3 bucket would like to join and transform the data into our common data model and store this transformed data single file at this point into another s3 bucket can draw upon in the future what is the best practice or tool to use here unsure of the pros cons of using something like aws lambda over python code that is triggered with airflow once the data is in this second s3 bucket ll easily be able to read it in with our visualization app am really most confused about getting the data into the first s3 bucket proper sql schema if you have any recommendations for tools or python packages to explore greatly appreciate it know there are procedures in sas that can either write to an sql database or think even to an s3 bucket but would like to avoid these solutions as our company is completely moving away from sas and just need to retain these old sas7bdat files thanks,0,pipeline help sas7bdat gt s3 gt dashboard,am seeking assistance with pipeline am envisioning have numerous sas7bdat file that need to transform using and feed into visualization app rshiny this ha already been accomplished manually but would like to scale up the solution using proper data engineering principle before join and transform these sa file am thinking it is advantageous to load them into an s3 bucket these sa file are currently scattered all over the place on our server and would like to store them in one location that can draw from in the future have knowledge of sql from using proc sql in sa but am struggling to wrap my head around the advantage of converting these sa file to proper sql database and table can anyone please provide me with resource or explanation on why this is important our company is shifting away from sa to python so am in favor of moving away from sa for that reason alone but feel like there is something am missing once all these file are in an s3 bucket would like to join and transform the data into our common data model and store this transformed data single file at this point into another s3 bucket can draw upon in the future what is the best practice or tool to use here unsure of the pro con of using something like aws lambda over python code that is triggered with airflow once the data is in this second s3 bucket ll easily be able to read it in with our visualization app am really most confused about getting the data into the first s3 bucket proper sql schema if you have any recommendation for tool or python package to explore greatly appreciate it know there are procedure in sa that can either write to an sql database or think even to an s3 bucket but would like to avoid these solution a our company is completely moving away from sa and just need to retain these old sas7bdat file thanks
dba looking to move into data engineering what skills to prioritise,hi apologies if this question gets asked lot and irks some folks reddit search isn great and google with reddit at the end didn work so well plus wanna hear from people already in the industry am currently production database administrator with experience in ms sql all version from onward mysql oracle postgres most of my exp sits within ms sql and have very strong sql and admin experience would this experience alone land me decent data engineer role suspect not or at least not one that pays well but let me know if wrong what skills do you think should prioritise learning to make myself more attractive to potential employers from scouring the internet of job postings it would suggest this is python so ve already been learning but be keen to know if there is anything else so far ve built couple of random text apps and web scraper this might not be the best place to ask this particular question what are some good python projects that could try to undertake that would directly relate to something that data engineer might encounter in their day to day was thinking potentially rudamentary etl but may be biting off more than can chew,0,dba looking to move into data engineering what skill to prioritise,hi apology if this question get asked lot and irks some folk reddit search isn great and google with reddit at the end didn work so well plus wanna hear from people already in the industry am currently production database administrator with experience in m sql all version from onward mysql oracle postgres most of my exp sits within m sql and have very strong sql and admin experience would this experience alone land me decent data engineer role suspect not or at least not one that pay well but let me know if wrong what skill do you think should prioritise learning to make myself more attractive to potential employer from scouring the internet of job posting it would suggest this is python so ve already been learning but be keen to know if there is anything else so far ve built couple of random text apps and web scraper this might not be the best place to ask this particular question what are some good python project that could try to undertake that would directly relate to something that data engineer might encounter in their day to day wa thinking potentially rudamentary etl but may be biting off more than can chew
handling multiple dynamic dependencies in azure data factory,ve question regarding dependency management in complex set of data pipelines extracting data from say fifteen different source systems some of those source systems have hundereds of tables others only handful each table is saved out to as parquet to datalake an event is fired for each complete table extracted that event is caught and schedules standard process is run to tidy up the data and produce standardised parquet file also in the datalake from that data produce dozen different datasets using jobs in spark the dependencies inputs for those datasets are subset of the tables from the extract phase different datasets may have overlapping requirements for example salesforce user table might be needed in three different output datasets the dataset is created when the various dependencies are satisfied the technique for identifying when the dependencies are met and the spark job can be run is polling pipeline in azure data factory finally to my question are there any standard patterns or tooling orchestration tools etc to support this approach or am going around this completely the wrong way the process as above has been implemented and works fine but it seems overly complex and the polling activity in adf is chargeable which seems little off to me we re paying to wait essentially any ideas anyone,0,handling multiple dynamic dependency in azure data factory,ve question regarding dependency management in complex set of data pipeline extracting data from say fifteen different source system some of those source system have hundereds of table others only handful each table is saved out to a parquet to datalake an event is fired for each complete table extracted that event is caught and schedule standard process is run to tidy up the data and produce standardised parquet file also in the datalake from that data produce dozen different datasets using job in spark the dependency input for those datasets are subset of the table from the extract phase different datasets may have overlapping requirement for example salesforce user table might be needed in three different output datasets the dataset is created when the various dependency are satisfied the technique for identifying when the dependency are met and the spark job can be run is polling pipeline in azure data factory finally to my question are there any standard pattern or tooling orchestration tool etc to support this approach or am going around this completely the wrong way the process a above ha been implemented and work fine but it seems overly complex and the polling activity in adf is chargeable which seems little off to me we re paying to wait essentially any idea anyone
what skills to build for backend software engineering,transitioned into de from data analyst background my short term career goal is to grow and cement my skills in the software engineering side of de like to be able to bounce back and forth between backend swe and de roles further into my career what are some skills that would allow me to grow on the backend swe side of things as current data engineer the extent of my backend experience is developing simple api with django to serve data from postgres database,0,what skill to build for backend software engineering,transitioned into de from data analyst background my short term career goal is to grow and cement my skill in the software engineering side of de like to be able to bounce back and forth between backend swe and de role further into my career what are some skill that would allow me to grow on the backend swe side of thing a current data engineer the extent of my backend experience is developing simple api with django to serve data from postgres database
will it change in,will it change in,0,will it change in,will it change in
the best machine learning courses on udemy,the best machine learning courses on udemy,0,the best machine learning course on udemy,the best machine learning course on udemy
how did you become sql pro,can solve all hackerrank leercode sql problems but feel like don understand sql as whole as such would not be able to handle data cleansing wrangling project are there any practice sql projects available what would you recommend,0,how did you become sql pro,can solve all hackerrank leercode sql problem but feel like don understand sql a whole a such would not be able to handle data cleansing wrangling project are there any practice sql project available what would you recommend
anyone going to data council this year,first one ll be going to pretty excited ready to learn amp be exposed to new ideas,0,anyone going to data council this year,first one ll be going to pretty excited ready to learn amp be exposed to new idea
are there any data engineer discord servers out there,wondering if there is discord community out there to discuss data engineering topics,0,are there any data engineer discord server out there,wondering if there is discord community out there to discus data engineering topic
need help choosing masters,hello all from india with yoe in data engineering thinking of masters in us please help me with best affordable masters programs bigdata inspecific not into datascience in us with fall intake lately seeing these advertisement for accelerated masters program one year do they serve the purpose thanks in advance,0,need help choosing master,hello all from india with yoe in data engineering thinking of master in u please help me with best affordable master program bigdata inspecific not into datascience in u with fall intake lately seeing these advertisement for accelerated master program one year do they serve the purpose thanks in advance
is year months yoe not enough,currently applying live in nyc but open to relocating anywhere in office or remote so don think being too picky knew getting the first job would be grind but now that looking for my second still having hard time getting bites on my resume have yoe as de at an f500 and was pretty much kicked to mid level super early on so my projects are large and high impact to the tune of few hundred million have ton of responsibility owning and implementing pipelines and data products for half dozen teams regularly communicate with stakeholders and present to senior management beyond that we ve hired new grads after joined and ve taken on huge informal leadership role in acting as the de facto tech lead our stack is pyspark for transforms with some proprietary tools similar to airflow tableau and fivetran still have lot of areas to improve upon but thought be pretty competitive candidate this time around now that more proven especially hearing how hot the market is for experienced people de is typically more senior software engineering position and wondering if recruiters see my yoe and just assume have title inflation,0,is year month yoe not enough,currently applying live in nyc but open to relocating anywhere in office or remote so don think being too picky knew getting the first job would be grind but now that looking for my second still having hard time getting bite on my resume have yoe a de at an f500 and wa pretty much kicked to mid level super early on so my project are large and high impact to the tune of few hundred million have ton of responsibility owning and implementing pipeline and data product for half dozen team regularly communicate with stakeholder and present to senior management beyond that we ve hired new grad after joined and ve taken on huge informal leadership role in acting a the de facto tech lead our stack is pyspark for transforms with some proprietary tool similar to airflow tableau and fivetran still have lot of area to improve upon but thought be pretty competitive candidate this time around now that more proven especially hearing how hot the market is for experienced people de is typically more senior software engineering position and wondering if recruiter see my yoe and just assume have title inflation
pretty fair run down reasons why you should not adopt data mesh,pretty fair run down reasons why you should not adopt data mesh,0,pretty fair run down reason why you should not adopt data mesh,pretty fair run down reason why you should not adopt data mesh
lessons from the field in building your mlops strategy with harpreet sahota data scientist at comet jan at pm et,hi dataengineering wanted to share this upcoming enterprise data amp ai webinar with you put the info from the website below along with the link if you re interested interested in hearing about the uber and the realreal case studies featured speaker harpreet sahota data scientist at comet and host of the artists of data science podcast as machine learning expands and larger organizations begin deploying across bigger teams the need to efficiently operationalize becomes critical for enterprises in our discussions with leading organizations utilizing ml like the realreal and uber we have compiled real world case studies and organizational best practices for mlops in the enterprise join us for discussion where we ll explore the benefits of mlops and discuss when and how to deploy mlops in your ml we ll review three real world case studies that will answer key questions when to start implementing in mlops how to start implementing in mlops how to measure the value of your mlops strategy agenda pm featured presentation pm your amp and interaction link to website,0,lesson from the field in building your mlops strategy with harpreet sahota data scientist at comet jan at pm et,hi dataengineering wanted to share this upcoming enterprise data amp ai webinar with you put the info from the website below along with the link if you re interested interested in hearing about the uber and the realreal case study featured speaker harpreet sahota data scientist at comet and host of the artist of data science podcast a machine learning expands and larger organization begin deploying across bigger team the need to efficiently operationalize becomes critical for enterprise in our discussion with leading organization utilizing ml like the realreal and uber we have compiled real world case study and organizational best practice for mlops in the enterprise join u for discussion where we ll explore the benefit of mlops and discus when and how to deploy mlops in your ml we ll review three real world case study that will answer key question when to start implementing in mlops how to start implementing in mlops how to measure the value of your mlops strategy agenda pm featured presentation pm your amp and interaction link to website
open source dw,its bad idea use redash as etl and data warehouse im not data engineer just looking for low mid scale solution for experiment,0,open source dw,it bad idea use redash a etl and data warehouse im not data engineer just looking for low mid scale solution for experiment
dont understand this job post no pyspark but extensive experience of spark jobs with python,dont understand this job post no pyspark but extensive experience of spark jobs with python,0,dont understand this job post no pyspark but extensive experience of spark job with python,dont understand this job post no pyspark but extensive experience of spark job with python
automated regression testing guide,automated regression testing guide,0,automated regression testing guide,automated regression testing guide
help advice on upgrading business,long story short friend of mine parent who has medium size book reselling business recently passed away and he has to handle it all the business has local stores that sell books and some toy products an online store for books and they supply educational books to schools as well they have around skus but the accounts and inventory are just stored in various excel files which is mess for us to understand for example they take part in book fairs and let say someone takes some books for the fair from the inventory warehouse and some sell and some do not but the excel files are only of sold books at the fair so we don know which books were taken we basically need to provide structure and use something to handle the inventory and accounts in the cheapest and fastest possible way please advise us like what options are good and is like upgrading to proper database system even good idea,0,help advice on upgrading business,long story short friend of mine parent who ha medium size book reselling business recently passed away and he ha to handle it all the business ha local store that sell book and some toy product an online store for book and they supply educational book to school a well they have around skus but the account and inventory are just stored in various excel file which is mess for u to understand for example they take part in book fair and let say someone take some book for the fair from the inventory warehouse and some sell and some do not but the excel file are only of sold book at the fair so we don know which book were taken we basically need to provide structure and use something to handle the inventory and account in the cheapest and fastest possible way please advise u like what option are good and is like upgrading to proper database system even good idea
the th edition of data engineering weekly featuring articles from twitter netflix etsy shopify and more,the th edition of data engineering weekly featuring articles from twitter netflix etsy shopify and more,0,the th edition of data engineering weekly featuring article from twitter netflix etsy shopify and more,the th edition of data engineering weekly featuring article from twitter netflix etsy shopify and more
rough estimate for total compensation as data engineering manager,looking for insight into what to expect in this type of role not building the dbs or the pipelines but due to my business knowledge regarding the data and industry knowledge in this specific field have the ability to deliver insights and roadmap items in addition to building the sql processes to transform the data ingested from our inbound pipelines initially managing small team with the expectation to grow fairly rapidly based in major west coast metro non bay area but company is remote first what compensation packages would you expect here,0,rough estimate for total compensation a data engineering manager,looking for insight into what to expect in this type of role not building the db or the pipeline but due to my business knowledge regarding the data and industry knowledge in this specific field have the ability to deliver insight and roadmap item in addition to building the sql process to transform the data ingested from our inbound pipeline initially managing small team with the expectation to grow fairly rapidly based in major west coast metro non bay area but company is remote first what compensation package would you expect here
open tools to replace ssis process including incoming and outgoing data,like to learn some other tools besides ssis ve created process which pulls data from excel and dozen csvs into sql server tables and do transformations using ssis tools as well as stored procedures to replace this would like to use tools like dbt and airflow like to use open tools for now as an amateur engineer working in finance and won really get any monetary resources to explore this realize this will probably be more work for me but what can do what the best way to just import data into the process from excel csv etc ssis makes this simple with its source and destination tools not sure what to use without that tool it not that much data only running at most once day maybe lines with lookup tables have about steps in the process and ssis can do it in about seconds and am an untrained amateur so sure that can be even better though very happy with seconds to process bunch of data sources guess see it as something like use airflow to orchestrate dbt to transform postgres for warehouse some way to move the data in and out from excel csv etc as mentioned can use tools like snowflake which ll need monetary and it resources for thanks for your advice,0,open tool to replace ssis process including incoming and outgoing data,like to learn some other tool besides ssis ve created process which pull data from excel and dozen csvs into sql server table and do transformation using ssis tool a well a stored procedure to replace this would like to use tool like dbt and airflow like to use open tool for now a an amateur engineer working in finance and won really get any monetary resource to explore this realize this will probably be more work for me but what can do what the best way to just import data into the process from excel csv etc ssis make this simple with it source and destination tool not sure what to use without that tool it not that much data only running at most once day maybe line with lookup table have about step in the process and ssis can do it in about second and am an untrained amateur so sure that can be even better though very happy with second to process bunch of data source guess see it a something like use airflow to orchestrate dbt to transform postgres for warehouse some way to move the data in and out from excel csv etc a mentioned can use tool like snowflake which ll need monetary and it resource for thanks for your advice
best resources to practice sql for data engineer interviews,currently work as data engineer long story short severally underpaid and looking to start interviewing soon anyone have any helpful resources for sql and or data engineering concepts in general that will help me prepare for interviews thanks,0,best resource to practice sql for data engineer interview,currently work a data engineer long story short severally underpaid and looking to start interviewing soon anyone have any helpful resource for sql and or data engineering concept in general that will help me prepare for interview thanks
data science appropriate for moving into data ml engineering,hi everyone currently student and looking to move into data engineering ml engineering role am in the process currently of applying to in data science programs and noticed that most of the programs state that their purpose is to develop data scientists would they not be the proper outlet for someone looking to move into an engineering role under data science my goals for example are to develop data science and machine learning applications throughout my career am concerned that if be honest about my intentions to use the degree to move into data engineering that the admissions committees may feel like my goals are not aligned with what they are looking for even though they have the exact curriculum need to meet my goals does anyone have any experience in this area have you pursued data science degree and been upfront about having the goal of moving into an engineering role after graduation should be concerned about being honest about my goals in my personal statements when university claims their program is aimed at creating data scientists do they mean strictly data scientists or would this normally include professionals in broader data science field such as data ml engineers as well thank you all for your time and perspectives they re very much appreciated,0,data science appropriate for moving into data ml engineering,hi everyone currently student and looking to move into data engineering ml engineering role am in the process currently of applying to in data science program and noticed that most of the program state that their purpose is to develop data scientist would they not be the proper outlet for someone looking to move into an engineering role under data science my goal for example are to develop data science and machine learning application throughout my career am concerned that if be honest about my intention to use the degree to move into data engineering that the admission committee may feel like my goal are not aligned with what they are looking for even though they have the exact curriculum need to meet my goal doe anyone have any experience in this area have you pursued data science degree and been upfront about having the goal of moving into an engineering role after graduation should be concerned about being honest about my goal in my personal statement when university claim their program is aimed at creating data scientist do they mean strictly data scientist or would this normally include professional in broader data science field such a data ml engineer a well thank you all for your time and perspective they re very much appreciated
sieve we processed hours of security footage in lt mins now semantically searchable per frame,hey everyone one of the creators of sieve and excited to be sharing it sieve is an api that helps you store process and automatically search your video data instantly and efficiently just think cameras recording footage at fps that would be million frames generated in single day the videos might be searchable by timestamp but finding moments of interest is like searching for needle in haystack we built this visual demo link here little while back which we love to get feedback on it hours of security footage that our api processed in lt mins and has simple querying and export functionality enabled we see applications in better understanding what data you have figuring out which data to send to labeling sampling datasets for training and building multiple test sets for models by scenario to try it on your videos visual dashboard walkthrough uyjp hgzl4,0,sieve we processed hour of security footage in lt min now semantically searchable per frame,hey everyone one of the creator of sieve and excited to be sharing it sieve is an api that help you store process and automatically search your video data instantly and efficiently just think camera recording footage at fps that would be million frame generated in single day the video might be searchable by timestamp but finding moment of interest is like searching for needle in haystack we built this visual demo link here little while back which we love to get feedback on it hour of security footage that our api processed in lt min and ha simple querying and export functionality enabled we see application in better understanding what data you have figuring out which data to send to labeling sampling datasets for training and building multiple test set for model by scenario to try it on your video visual dashboard walkthrough uyjp hgzl4
writing psets using notepad in abinitio etl,hi guys apologies for posting this here couldn find the right subreddit for this new to abinitio and have trouble understanding the pset file structure one of the requirements of the project assigned is to create pset file by hand using text editor checked few pset files and couldn figure out the pattern in parameter initializations also ve been searching on docs abinitio com for the past few days the best came across was pdl but it just has some basic variable declaration not the in depth guide looking for please point me to specific topic in the abintio docs or rd party resource thanks for your help,0,writing psets using notepad in abinitio etl,hi guy apology for posting this here couldn find the right subreddit for this new to abinitio and have trouble understanding the pset file structure one of the requirement of the project assigned is to create pset file by hand using text editor checked few pset file and couldn figure out the pattern in parameter initialization also ve been searching on doc abinitio com for the past few day the best came across wa pdl but it just ha some basic variable declaration not the in depth guide looking for please point me to specific topic in the abintio doc or rd party resource thanks for your help
suggestions to level up my stack mostly etl ish related stuff,currently work as freelance developer contracted to few companies mostly labelled as web scraper although ve grown to dislike that label since feel like do so much more than that my deliverable is basically an etl system that takes data from website many websites keeps all the data consistent up to date etc and then the database create either have that as the deliverable to create sort of api system around it so they can extract the data while am very confident in my pure data extraction abilities and truly see myself as veteran in this regard knowing the structures of every type of website using every known framework to most efficiently collect the data would love to improve on all other aspects of my work my stack atm is python flask with celery mongodb deployed on aws ec2 mainly very proficient in pandas however not really using it much these days recent jobs the transform step has been as simple as editing bit of json in or so lines of code using simple dict operations and never see the need to overcomplicate things can help but feel being very inefficient with this process while it works sure my stack could be better this is mainly so can land better contracts that deal with bigger data any suggestions any input is welcome especially if you deal with similar stuff also criticism is welcome as well am fairly young and fully self taught learning every day since feel like lot of people will say sql do welcome suggestions based on this too am proficient but far from an expert and my recent projects have definitely had use for mongo,0,suggestion to level up my stack mostly etl ish related stuff,currently work a freelance developer contracted to few company mostly labelled a web scraper although ve grown to dislike that label since feel like do so much more than that my deliverable is basically an etl system that take data from website many website keep all the data consistent up to date etc and then the database create either have that a the deliverable to create sort of api system around it so they can extract the data while am very confident in my pure data extraction ability and truly see myself a veteran in this regard knowing the structure of every type of website using every known framework to most efficiently collect the data would love to improve on all other aspect of my work my stack atm is python flask with celery mongodb deployed on aws ec2 mainly very proficient in panda however not really using it much these day recent job the transform step ha been a simple a editing bit of json in or so line of code using simple dict operation and never see the need to overcomplicate thing can help but feel being very inefficient with this process while it work sure my stack could be better this is mainly so can land better contract that deal with bigger data any suggestion any input is welcome especially if you deal with similar stuff also criticism is welcome a well am fairly young and fully self taught learning every day since feel like lot of people will say sql do welcome suggestion based on this too am proficient but far from an expert and my recent project have definitely had use for mongo
how to add tests to your data pipelines,how to add tests to your data pipelines,0,how to add test to your data pipeline,how to add test to your data pipeline
please suggest book for data engineering concepts,think it would be good idea to grasp more knowledge about de concepts terms and data pipelines am interviewing to be de was sde for years and have worked with relational and non relational dbs in the past have knowledge of nlp and ml concepts too can prepare for the interviews through google articles but it does not give me satisfactory wisdom with de in interviews get lost when they ask me to create data model from start to end need to learn more can you please suggest book if not book then some series of articles or anything else,0,please suggest book for data engineering concept,think it would be good idea to grasp more knowledge about de concept term and data pipeline am interviewing to be de wa sde for year and have worked with relational and non relational db in the past have knowledge of nlp and ml concept too can prepare for the interview through google article but it doe not give me satisfactory wisdom with de in interview get lost when they ask me to create data model from start to end need to learn more can you please suggest book if not book then some series of article or anything else
bounty to solve hypervector data embedding,need to know what is the most optimal way to embed natural data in hypervector optimal in the sense of minimal data loss why preserving the properties of the vectors so let say you have an x28x3 image of numbers how to get that flattened vector into binary hypervector optimally likewise if you have floating point vector of size how to transfer to binary hypervector also the binary hypervectors could also be or instead of and think of quantum data embeddings here where data is converted into the bloch sphere except here we want to embed into the hypervector space that may be useful what need to know at the end of the project is what function best converts natural data into the hypervectors with minimal data loss and maximal preservation of the hypervector properties if you feel like you are capable of solving this problem please message meditatinginmicrog for bounty details,0,bounty to solve hypervector data embedding,need to know what is the most optimal way to embed natural data in hypervector optimal in the sense of minimal data loss why preserving the property of the vector so let say you have an x28x3 image of number how to get that flattened vector into binary hypervector optimally likewise if you have floating point vector of size how to transfer to binary hypervector also the binary hypervectors could also be or instead of and think of quantum data embeddings here where data is converted into the bloch sphere except here we want to embed into the hypervector space that may be useful what need to know at the end of the project is what function best convert natural data into the hypervectors with minimal data loss and maximal preservation of the hypervector property if you feel like you are capable of solving this problem please message meditatinginmicrog for bounty detail
salary resource guidelines for data engineering roles within toronto,glassdoor is very inaccurate along with pay scale since inflation has grown covid and general demand was hoping if anyone has any resources for salary and what data engineering roles salaries are within toronto any help would be extremely appreciated,0,salary resource guideline for data engineering role within toronto,glassdoor is very inaccurate along with pay scale since inflation ha grown covid and general demand wa hoping if anyone ha any resource for salary and what data engineering role salary are within toronto any help would be extremely appreciated
project ideas for end to end data engineering project for beginner,hi everyone looking for entry level jobs in ds in europe currently and looking to add some relevant projects to my resume ve done some machine learning data mining projects at university but ve not managed to get out of python and pandas would really like to get started with some good projects that showcase some basic data engineering pipeline knowledge have an interview next month and the technical round is to demonstrate personal project and really want to use this opportunity well looking for an end to end project that can complete in about weeks would also appreciate resources for learning while doing the project thanks in advance,0,project idea for end to end data engineering project for beginner,hi everyone looking for entry level job in d in europe currently and looking to add some relevant project to my resume ve done some machine learning data mining project at university but ve not managed to get out of python and panda would really like to get started with some good project that showcase some basic data engineering pipeline knowledge have an interview next month and the technical round is to demonstrate personal project and really want to use this opportunity well looking for an end to end project that can complete in about week would also appreciate resource for learning while doing the project thanks in advance
what kinds of factors influenced you choosing sas managed solution the open source alternative,hi all in the past the the teams have worked on chose one option or another based on factors like the following wanted to hear some of the reasons you decided to go one way or another here are some examples from my experiences team knowledge base about the underlying laguage tech and interoperability with existing stack we chose to use beam because team has strong python background and larger org has many eng strong go background so we can potentially draw on that as we start to tune performance in the future speed of the timeline in which we had to deliver results opted to use stitch instead of open source singer alternative because it allowed us to move various lines of business rdbms onto snowflake dwh far faster and more reliably than if we custom wrote taps targets for each using singer open source framework its built off of data residency on prem requirements opted against using astronomer in cloud offering and instead decided to use self managed airflow because some of the data we were piping through was not allowed to reside in any of the countries the clouds they offered to deploy through were located in at the time note realize this was discussed in at least one previous post years back was hoping for fresh perspectives,0,what kind of factor influenced you choosing sa managed solution the open source alternative,hi all in the past the the team have worked on chose one option or another based on factor like the following wanted to hear some of the reason you decided to go one way or another here are some example from my experience team knowledge base about the underlying laguage tech and interoperability with existing stack we chose to use beam because team ha strong python background and larger org ha many eng strong go background so we can potentially draw on that a we start to tune performance in the future speed of the timeline in which we had to deliver result opted to use stitch instead of open source singer alternative because it allowed u to move various line of business rdbms onto snowflake dwh far faster and more reliably than if we custom wrote tap target for each using singer open source framework it built off of data residency on prem requirement opted against using astronomer in cloud offering and instead decided to use self managed airflow because some of the data we were piping through wa not allowed to reside in any of the country the cloud they offered to deploy through were located in at the time note realize this wa discussed in at least one previous post year back wa hoping for fresh perspective
unstructured data governance for ml,want to leave this quite open ended want to hear from the community about best practices tools challenges success stories of managing large volumes of unstructured data the context is juxtaposed to relational databases when used well will automatically instill type constraints relational constraints functional dependencies in relations will imply the needed normalization identifying keys will help cdc and versioning acid allows for coherent isolated transactions even with high concurrent requests so the point being while it definitely is not easy dbm systems are mature technology that have over decades fostered good practices and an ecosystem of supporting tools unstructured data very quickly can become data swamps in cloud object storage it presents new set of problems and opportunities and want to hear the communities war stories and accomplishments know delta lake oss and delta lake by databricks is one successful project for managing data lakes but want to hear about what else is happening there seem to be lots of interesting tools and practices like people to add their experience to this thread,0,unstructured data governance for ml,want to leave this quite open ended want to hear from the community about best practice tool challenge success story of managing large volume of unstructured data the context is juxtaposed to relational database when used well will automatically instill type constraint relational constraint functional dependency in relation will imply the needed normalization identifying key will help cdc and versioning acid allows for coherent isolated transaction even with high concurrent request so the point being while it definitely is not easy dbm system are mature technology that have over decade fostered good practice and an ecosystem of supporting tool unstructured data very quickly can become data swamp in cloud object storage it present new set of problem and opportunity and want to hear the community war story and accomplishment know delta lake os and delta lake by databricks is one successful project for managing data lake but want to hear about what else is happening there seem to be lot of interesting tool and practice like people to add their experience to this thread
advice request google cloud large repeated text matching design,hi all ve recently joined team where given transaction research task is being conducted and is in need of optimization consider bulk tables of transactions include ng user dates prices and text description mil rows currently stored in self managed postgresql on google cloud reference tables of regex statements and providers those statements infer analysts semi regularly need to tune these match statements and therefore refresh the summarized result match table currently django admin model supports analyst interaction with the regex table and the ability to refresh the matches currently the computation is handled by postgresql overall the team is sure the current pipeline including many other not mentioned tasks is stale and in need of significant revamp to support scalability and part of my duties is to think about this matching process and the refreshing process we are google cloud organization but have team competent in self management or off the shelf tool usage we would prefer not to pay for any additional services beyond google cloud tools we use django extensively but have zero love for it we have begun teasing apart some other tasks with composer airflow so that is available fyi any thoughts on improving the matching approach and implementing any other cloud services key goals are to maintain analyst access to refine matches and improve seamlessness of refreshing while using more suited tool to support future scaling thanks for reading,0,advice request google cloud large repeated text matching design,hi all ve recently joined team where given transaction research task is being conducted and is in need of optimization consider bulk table of transaction include ng user date price and text description mil row currently stored in self managed postgresql on google cloud reference table of regex statement and provider those statement infer analyst semi regularly need to tune these match statement and therefore refresh the summarized result match table currently django admin model support analyst interaction with the regex table and the ability to refresh the match currently the computation is handled by postgresql overall the team is sure the current pipeline including many other not mentioned task is stale and in need of significant revamp to support scalability and part of my duty is to think about this matching process and the refreshing process we are google cloud organization but have team competent in self management or off the shelf tool usage we would prefer not to pay for any additional service beyond google cloud tool we use django extensively but have zero love for it we have begun teasing apart some other task with composer airflow so that is available fyi any thought on improving the matching approach and implementing any other cloud service key goal are to maintain analyst access to refine match and improve seamlessness of refreshing while using more suited tool to support future scaling thanks for reading
will this idea be useful to develop further,hello community happy new year to everyone prototyped an idea called storytelling whatever you see which is chrome extension that helps users to turn any data they see on the web into sharable story in couple of clicks amp x200b processing video a3mzf1awly881 on youtube minute prototype demo ideally it should support any data like spreadsheet excel csv json in http url or even html table unstructured data could be as creative as word counting etc this prototype covers spreadsheet only my question is this type of extension useful at all if yes in what scenario if not any possible changes could make it useful would you like to give it try reply with an email so that will send you an access link any other thoughts to make it useful thanks for your feedback in advance,0,will this idea be useful to develop further,hello community happy new year to everyone prototyped an idea called storytelling whatever you see which is chrome extension that help user to turn any data they see on the web into sharable story in couple of click amp x200b processing video a3mzf1awly881 on youtube minute prototype demo ideally it should support any data like spreadsheet excel csv json in http url or even html table unstructured data could be a creative a word counting etc this prototype cover spreadsheet only my question is this type of extension useful at all if yes in what scenario if not any possible change could make it useful would you like to give it try reply with an email so that will send you an access link any other thought to make it useful thanks for your feedback in advance
coursera plus subscription off new year offer,coursera plus subscription off new year offer,0,coursera plus subscription off new year offer,coursera plus subscription off new year offer
wanted to set up alerts for my kpis if they cross certain threshold but not sure if it possible any ideas on how to implement this in bigquery also is it possible to mail people about job statuses without using external services like functions,any help would be appreciated,0,wanted to set up alert for my kpis if they cross certain threshold but not sure if it possible any idea on how to implement this in bigquery also is it possible to mail people about job status without using external service like function,any help would be appreciated
how did you get started with data engineering,hello everyone comp sci grad that ended up transitioning from coding to analyst work and decided wanted to get into data engineering instead was wondering how all got started were you software engineer that did more data work just straight out of college etc how difficult did you find it to get into the industry am currently doing the springboard de program yes ve seen mixed reviews about it and wanted to gain some more insight do have some professional background coding in python vba and using sql is there anything specific should focus on and should be aiming for entry level junior roles or higher if all will have is portfolio of personal capstone projects thank you and happy new year,0,how did you get started with data engineering,hello everyone comp sci grad that ended up transitioning from coding to analyst work and decided wanted to get into data engineering instead wa wondering how all got started were you software engineer that did more data work just straight out of college etc how difficult did you find it to get into the industry am currently doing the springboard de program yes ve seen mixed review about it and wanted to gain some more insight do have some professional background coding in python vba and using sql is there anything specific should focus on and should be aiming for entry level junior role or higher if all will have is portfolio of personal capstone project thank you and happy new year
books on lakefs and related technologies,this is bit embarrassing but will probably soon be working on or with this stream lakehouse time travels delta lake s3 of technologies but complete novice on some of the core pieces with exception to s3 want to do my due diligence so can have fighting chance to not stuff it up once get cracking seem to do better through examples and mix of theory to doing something new can anyone help me out with some references and or books to get up to speed,0,book on lakefs and related technology,this is bit embarrassing but will probably soon be working on or with this stream lakehouse time travel delta lake s3 of technology but complete novice on some of the core piece with exception to s3 want to do my due diligence so can have fighting chance to not stuff it up once get cracking seem to do better through example and mix of theory to doing something new can anyone help me out with some reference and or book to get up to speed
big data pipelines,hi can anyone recommend me some materials regarding big data pipelines it has to be like full pipelines from begging to the end examples thanks cheers,0,big data pipeline,hi can anyone recommend me some material regarding big data pipeline it ha to be like full pipeline from begging to the end example thanks cheer
building self service tools for non des,solo de here there some sentiment in my workplace which don really disagree with in principle that we like to build self service tools to surface data for some departments purposes we re an investment firm so most queries would need to support arbitrary sets of securities dates and times and really mean arbitrary when say it so obviously can give people live python or sql script and tell them to edit particular variables and not touch anything else that just bad idea on so many levels really all need is way for someone to interact with it and set like two variables tickers and date safely without being able to break anything suppose could ask for input into the console except input prompts are evil and half my code would probably be fatfinger sanity checks formatting of whatever they typed in amp x200b does anyone have suggestions on how they ve built similar tools or alternatively what existing libraries tools you used and of course anecdotes about how something like this can go wrong are welcome as well as an aside this firm is small but quite open to change they just haven quite realized the extent of changes that are possible with proper access to data,0,building self service tool for non de,solo de here there some sentiment in my workplace which don really disagree with in principle that we like to build self service tool to surface data for some department purpose we re an investment firm so most query would need to support arbitrary set of security date and time and really mean arbitrary when say it so obviously can give people live python or sql script and tell them to edit particular variable and not touch anything else that just bad idea on so many level really all need is way for someone to interact with it and set like two variable ticker and date safely without being able to break anything suppose could ask for input into the console except input prompt are evil and half my code would probably be fatfinger sanity check formatting of whatever they typed in amp x200b doe anyone have suggestion on how they ve built similar tool or alternatively what existing library tool you used and of course anecdote about how something like this can go wrong are welcome a well a an aside this firm is small but quite open to change they just haven quite realized the extent of change that are possible with proper access to data
how much of your time is spent on ad hoc or one time requests,ve been reading recently about data as product basically the advantages of data teams working like product teams opposed to working as consulting teams it got me thinking how much time do data engineers and other members of data teams spend working on one off requests vs working on improving data infrastructure and ongoing processes would love to hear any specific feedback in the comments view poll,0,how much of your time is spent on ad hoc or one time request,ve been reading recently about data a product basically the advantage of data team working like product team opposed to working a consulting team it got me thinking how much time do data engineer and other member of data team spend working on one off request v working on improving data infrastructure and ongoing process would love to hear any specific feedback in the comment view poll
how to validate data in bigquery,hey folks we aim to set up proper data validation pipeline for our bigquery warehouse spanning multiple datasets and many more tables we aimed for validations like provided with the great expectations framework but personally find it nightmare to set up and it seems to be quite buggy with bigquery what you approach here,0,how to validate data in bigquery,hey folk we aim to set up proper data validation pipeline for our bigquery warehouse spanning multiple datasets and many more table we aimed for validation like provided with the great expectation framework but personally find it nightmare to set up and it seems to be quite buggy with bigquery what you approach here
pooling in convolutional neural network,pooling in convolutional neural network,0,pooling in convolutional neural network,pooling in convolutional neural network
the best approach to build apis for billions of data points,let say you have billions of data points on bigquery or comparable and you want to build an api to give users access to the data the closer you can get to real time what would be some feasible solution to achieve the above and where would you cache the data if necessary thanks,0,the best approach to build apis for billion of data point,let say you have billion of data point on bigquery or comparable and you want to build an api to give user access to the data the closer you can get to real time what would be some feasible solution to achieve the above and where would you cache the data if necessary thanks
databases in year in review,databases in year in review,0,database in year in review,database in year in review
best way to sink multiple kafka topics to s3,we currently have handful of kafka topics that we write to s3 in min batch windows using spark steaming with emr for analytic events one of the advantages of the min window is that we don end up with multiple parquet files per day which goes some way to helping redshift performance we use external tables there are several disadvantages though so we want to break the kafka topics down into topic per analytics event so will likely end up with topics what not so sure on is the best way to sink the data to s3 in parquet whilst still maintaining min window thinking about maintaining redshift performance here configure something that will cover all topics are we talking container per topic or is there toolset that can do it quite easily has anyone got any ideas or experience they can share,0,best way to sink multiple kafka topic to s3,we currently have handful of kafka topic that we write to s3 in min batch window using spark steaming with emr for analytic event one of the advantage of the min window is that we don end up with multiple parquet file per day which go some way to helping redshift performance we use external table there are several disadvantage though so we want to break the kafka topic down into topic per analytics event so will likely end up with topic what not so sure on is the best way to sink the data to s3 in parquet whilst still maintaining min window thinking about maintaining redshift performance here configure something that will cover all topic are we talking container per topic or is there toolset that can do it quite easily ha anyone got any idea or experience they can share
airflow framework for validation,working on file level validation file pattern empty file etc module which will populate rdbms table with flags before the actual job runs so that if any flag is negative actual job will be put on hold and will alert process teams have around jobs which needs to be validated on daily basis and schedule vary from every mins to daily in future there could be addition of new jobs coming in would like to create framework using airflow which should use only one dag to validate each and every application using configuration passed as json files for each application whats your thought on this,0,airflow framework for validation,working on file level validation file pattern empty file etc module which will populate rdbms table with flag before the actual job run so that if any flag is negative actual job will be put on hold and will alert process team have around job which need to be validated on daily basis and schedule vary from every min to daily in future there could be addition of new job coming in would like to create framework using airflow which should use only one dag to validate each and every application using configuration passed a json file for each application whats your thought on this
how to use hasty annotation tool part,how to use hasty annotation tool part,0,how to use hasty annotation tool part,how to use hasty annotation tool part
where do learn about best practices for storing usernames amp passwords for etl,that this is naive question been searching the internet off and on about month and it seems like it always workaround or jury rigged solution here few sketchy looking solutions ve seen store them in your environment variables windows store them in clear text on your user profile better looking solution but seen password vault suggested but this seems to always be followed by someone saying it kind of pointless from security perspective like how is this done in the professional world here my scenario have service account with vendor to connect to an oracle database and send ad hoc queries there and download my results wrote up some python with sqlalchemy and cx_oracle to extract and save my data can schedule this with windows task scheduler but don want to save the credentials in some random text file on my desktop am worrying too much it feels so janky what can do with this username amp password to secure it somewhere and run script that can access it without feeling like opening gaping security hole more importantly where do learn this sort of thing if you know the answer how did you get the answer frustrated that can seem to figure this out reddit posts like this are usually my last resort ve searched this subreddit ve searched stack overflow and anything else on google don know where supposed to be looking for these types of best practices just some stupid da hoping to be de one day but stuff like this makes me feel like if you have to ask you ll never know lol don see other people asking this question on here so that why beating myself up over it,0,where do learn about best practice for storing usernames amp password for etl,that this is naive question been searching the internet off and on about month and it seems like it always workaround or jury rigged solution here few sketchy looking solution ve seen store them in your environment variable window store them in clear text on your user profile better looking solution but seen password vault suggested but this seems to always be followed by someone saying it kind of pointless from security perspective like how is this done in the professional world here my scenario have service account with vendor to connect to an oracle database and send ad hoc query there and download my result wrote up some python with sqlalchemy and cx_oracle to extract and save my data can schedule this with window task scheduler but don want to save the credential in some random text file on my desktop am worrying too much it feel so janky what can do with this username amp password to secure it somewhere and run script that can access it without feeling like opening gaping security hole more importantly where do learn this sort of thing if you know the answer how did you get the answer frustrated that can seem to figure this out reddit post like this are usually my last resort ve searched this subreddit ve searched stack overflow and anything else on google don know where supposed to be looking for these type of best practice just some stupid da hoping to be de one day but stuff like this make me feel like if you have to ask you ll never know lol don see other people asking this question on here so that why beating myself up over it
what are your opinions friends will this change or impact the data engineering landscape gartner expects these technology trends to act as force multipliers of digital business and innovation over the next three to five years,what are your opinions friends will this change or impact the data engineering landscape gartner expects these technology trends to act as force multipliers of digital business and innovation over the next three to five years,0,what are your opinion friend will this change or impact the data engineering landscape gartner expects these technology trend to act a force multiplier of digital business and innovation over the next three to five year,what are your opinion friend will this change or impact the data engineering landscape gartner expects these technology trend to act a force multiplier of digital business and innovation over the next three to five year
best ide for databricks,know it broad question but new to the databricks world all of our engineering in the past was done in visual studio with sql server but it just not cutting it anymore our thoughts are to start with pyspark and go from there any pointers would be greatly appreciated,0,best ide for databricks,know it broad question but new to the databricks world all of our engineering in the past wa done in visual studio with sql server but it just not cutting it anymore our thought are to start with pyspark and go from there any pointer would be greatly appreciated
adf trigger pipe on pipe successful completion,want to chain adf pipelines together if pipeline runs successfully want to trigger pipeline please note that pipeline has schedule based trigger and dont want to create another pipeline where combine pipeline and in aws we have something called cloudwatch events rules that takes care of this requirement is there something similar in azure if not what are my options,0,adf trigger pipe on pipe successful completion,want to chain adf pipeline together if pipeline run successfully want to trigger pipeline please note that pipeline ha schedule based trigger and dont want to create another pipeline where combine pipeline and in aws we have something called cloudwatch event rule that take care of this requirement is there something similar in azure if not what are my option
need help with python pandas job scheduling tools,my team is working on previously and they are using rshiny for dashboards and cron jobs for scheduling rscripts to run daily using bash scripts now we have started using python and snowflake for data warehousing so we are now looking for an alternative to cron jobs where we can schedule all kind of scripts or notebook if possible so that it can be managed and tracked easily,0,need help with python panda job scheduling tool,my team is working on previously and they are using rshiny for dashboard and cron job for scheduling rscripts to run daily using bash script now we have started using python and snowflake for data warehousing so we are now looking for an alternative to cron job where we can schedule all kind of script or notebook if possible so that it can be managed and tracked easily
aws glue or emr for data transformations,trying to train myself on aws and de tools through personal project but stuck with whether to use glue or emr to perform some data calculations basically just trying to get some practice using pyspark to perform data transformations managed to get data into s3 and load it into staging tables in redshift now want to perform some calculations on that data technical analysis for stocks crypto via pyspark figured my options are to learn how to set up and us aws glue or aws emr but which one might be best for this situation also is using the provided notebooks in glue emr the best way to practice writing some pyspark code appreciate any insight,0,aws glue or emr for data transformation,trying to train myself on aws and de tool through personal project but stuck with whether to use glue or emr to perform some data calculation basically just trying to get some practice using pyspark to perform data transformation managed to get data into s3 and load it into staging table in redshift now want to perform some calculation on that data technical analysis for stock crypto via pyspark figured my option are to learn how to set up and u aws glue or aws emr but which one might be best for this situation also is using the provided notebook in glue emr the best way to practice writing some pyspark code appreciate any insight
is anyone using apache pulsar in prod,curious about the communities experiences with apache pulsar additionally if anyone is well versed in streaming technologies what are the overlaps distinctions between pulsar kafka flink spark structured streaming if had to pick streaming stack now and lets say we have far more python skills and very limited java skills what would you recommend,0,is anyone using apache pulsar in prod,curious about the community experience with apache pulsar additionally if anyone is well versed in streaming technology what are the overlap distinction between pulsar kafka flink spark structured streaming if had to pick streaming stack now and let say we have far more python skill and very limited java skill what would you recommend
dataframes vs sparksql what to use and why,use dataframes in spark instead of sparksql feel like sparksql is easier but leads to bad habits like too much logic lumped together harder to unit test etc just generally worse coding practices writing functions that use bits of the dataframe api seems to make the code more reusable and extensible as well curious what most other people use when writing spark and why,0,dataframes v sparksql what to use and why,use dataframes in spark instead of sparksql feel like sparksql is easier but lead to bad habit like too much logic lumped together harder to unit test etc just generally worse coding practice writing function that use bit of the dataframe api seems to make the code more reusable and extensible a well curious what most other people use when writing spark and why
implement advance snapping in openlayers,amp x200b openlayers snapping implement advance snapping in openlayers,0,implement advance snapping in openlayers,amp x200b openlayers snapping implement advance snapping in openlayers
starting my job search critique my resume,problem have always had making resume is don have degree or anything that feels able to fill out resume with know can do the work but have hard time selling that on paper any help would be greatly appreciated anon anon gmail com mailto anon gmail com data engineer ambitious data enthusiast seeking opportunities to further my experience in the world of data engineering and analytics am interested in using skills in computer programming and data engineering to improve productivity by automating etl pipelines and providing data driven business insight technical skill set snowflake experience developing and implementing data warehouse solutions and data pipelines for large data sets and business analytics comfortability managing relational data models to provide efficient access to business intelligence snowflake zero to hero certificate python on the job experience using the python programming language to automate elt processes for large datasets short list of library knowledge pandas apache spark numpy scipy selenium pyautogui matplotlib sql robust knowledge of database querying language sql daily use of this language to process data in preparation for visualization ultimate mysql bootcamp certification amp x200b work experience endurance international group phoenix az november current sales data analyst august current provide business intelligence to the stake holders through data analytics achievements revolutionized the daily reporting process as the key man on the team used my knowledge of vba and python programming to automate the process of reporting out daily analytics to the stake holders saving on average to hours of work each morning lead the transition from excel to snowflake took leading role in transitioning my departments data environment from excel sheets to relational data model in the snowflake warehouse enabled the scaling of data etl from agents locally to tracking the sales of agents in multiple calls centers globally detailed analytics of data using my experience in data analytics and data processing it was my responsibility to identify irregularities and opportunities in enterprise data,0,starting my job search critique my resume,problem have always had making resume is don have degree or anything that feel able to fill out resume with know can do the work but have hard time selling that on paper any help would be greatly appreciated anon anon gmail com mailto anon gmail com data engineer ambitious data enthusiast seeking opportunity to further my experience in the world of data engineering and analytics am interested in using skill in computer programming and data engineering to improve productivity by automating etl pipeline and providing data driven business insight technical skill set snowflake experience developing and implementing data warehouse solution and data pipeline for large data set and business analytics comfortability managing relational data model to provide efficient access to business intelligence snowflake zero to hero certificate python on the job experience using the python programming language to automate elt process for large datasets short list of library knowledge panda apache spark numpy scipy selenium pyautogui matplotlib sql robust knowledge of database querying language sql daily use of this language to process data in preparation for visualization ultimate mysql bootcamp certification amp x200b work experience endurance international group phoenix az november current sale data analyst august current provide business intelligence to the stake holder through data analytics achievement revolutionized the daily reporting process a the key man on the team used my knowledge of vba and python programming to automate the process of reporting out daily analytics to the stake holder saving on average to hour of work each morning lead the transition from excel to snowflake took leading role in transitioning my department data environment from excel sheet to relational data model in the snowflake warehouse enabled the scaling of data etl from agent locally to tracking the sale of agent in multiple call center globally detailed analytics of data using my experience in data analytics and data processing it wa my responsibility to identify irregularity and opportunity in enterprise data
hypothesis that the federal reserve can set interest rates based on the movements of the planet mars here are the daily percentage changes in the dow jones going back to,hypothesis that the federal reserve can set interest rates based on the movements of the planet mars here are the daily percentage changes in the dow jones going back to the introduction section of the book which can be read for free on amazon proposes currency that could overpower the value of both cryptocurrency and the dollar during major financial meltdown newbks amp newbks redir amp hl en newbks amp newbks_redir amp hl en,0,hypothesis that the federal reserve can set interest rate based on the movement of the planet mar here are the daily percentage change in the dow jones going back to,hypothesis that the federal reserve can set interest rate based on the movement of the planet mar here are the daily percentage change in the dow jones going back to the introduction section of the book which can be read for free on amazon proposes currency that could overpower the value of both cryptocurrency and the dollar during major financial meltdown newbks amp newbks redir amp hl en newbks amp newbks_redir amp hl en
junior data engineer interview advice,hello all have an interview coming up for junior data engineer at ikea they have data engineer accelerator program see link below and have an interview coming up with their head of data my background is quantitative finance in the process of finishing my masters and they know this but have decent python skills have an interview coming up and they asked me to show project worked on will be showing script wrote for my current internship which automated the requesting via excel and retrieval of data via ftp from large financial data provider what should do to prepare for the technical questions what kind of questions can expect should build something else from scratch to show them if so what your assistance would be much appreciated this is big opportunity for me and want to nail it data engineer accelerator program,0,junior data engineer interview advice,hello all have an interview coming up for junior data engineer at ikea they have data engineer accelerator program see link below and have an interview coming up with their head of data my background is quantitative finance in the process of finishing my master and they know this but have decent python skill have an interview coming up and they asked me to show project worked on will be showing script wrote for my current internship which automated the requesting via excel and retrieval of data via ftp from large financial data provider what should do to prepare for the technical question what kind of question can expect should build something else from scratch to show them if so what your assistance would be much appreciated this is big opportunity for me and want to nail it data engineer accelerator program
leaving faang after only months,apologize for the clickbaity title but wanted to make post that hopefully provides some insight for anyone looking to become de in faang like company know for many people that the dream and for good reason meta was fantastic company to work for it just wasn for me ve attempted to explain why below it just metrics person that really enjoys working with data early in its lifecycle closer to the collection processing and storage phases however des at meta and from what ve heard all faang like companies are involved much later in that lifecycle in the analysis and visualization stages in my opinion des at faang are actually analytics engineers and lot of the work you ll do will involve building dashboards tweaking metrics and maintaining pipelines that have already been built because the company data infra is so mature there not lot of pioneering work to be done so if you re looking to build_ something you might have better luck at smaller company it all tables lot of the data at meta is generated in house by the products that they ve developed this means that any data generated or collected is made available through the logs which are then parsed and stored in tables there are no apis to connect to csvs to ingest or tools that need to be connected so they can share data it just tables the pipelines that parse the logs have for the most part already been built and thus your job as de is to work with the tables that are created every night found this incredibly boring because get more joy satisfaction out of working with really dirty raw data that where feel can add value but data at meta is already pretty clean just due to the nature of how it generated and collected if your joy satisfaction comes from helping data scientists make the most of the data that available then faang is definitely for you but if you get your satisfaction from making unusable data usable then this likely isn what you re looking for it the wrong kind of scale think one of the appeals to working as de in faang is that there is just so much data the idea of working with petabytes of data brings thoughts of how to work at such large scale and it all sounds really exciting that was certainly the case for me the problem though is that this has all pretty much been solved in faang and it being solved by swes not des distributed computing hyper efficient query engines load balancing etc are all implemented by swes and so working at scale means implementing basic common sense in your sql queries so that you re not going over the gb memory limit on any given node much prefer breadth over depth when it comes to scale much rather work with large variety of data types solving large variety of problems faang doesn provide this at least not in my experience can feel the impact lot of the work you do as data engineer is related to metrics and dashboards with the goal of helping the data scientists use the data more effectively for me this resulted in all of my impact being along the lines of put number on dashboard to facilitate tracking of the metric this doesn resonate with me it doesn motivate me can certainly understand how some people would enjoy that and it definitely important work it just not what gets me out of bed in the morning and result was struggling to stay focused or get tasks done in the end meta and imagine all of faang was great company to work at with lot of really important and interesting work being done but for me as data engineer it just wasn my thing wanted to put this all out there for those who might be considering pursuing role in faang so that they can make more informed decision think it also helpful to provide some contrast to all of the hype around and faang and acknowledge that it not for everyone and that okay tl dr thought being de in faang would be the ultimate data experience but it was far too analytical for my taste and wasn able to feel the impact was making so left,0,leaving faang after only month,apologize for the clickbaity title but wanted to make post that hopefully provides some insight for anyone looking to become de in faang like company know for many people that the dream and for good reason meta wa fantastic company to work for it just wasn for me ve attempted to explain why below it just metric person that really enjoys working with data early in it lifecycle closer to the collection processing and storage phase however de at meta and from what ve heard all faang like company are involved much later in that lifecycle in the analysis and visualization stage in my opinion de at faang are actually analytics engineer and lot of the work you ll do will involve building dashboard tweaking metric and maintaining pipeline that have already been built because the company data infra is so mature there not lot of pioneering work to be done so if you re looking to build_ something you might have better luck at smaller company it all table lot of the data at meta is generated in house by the product that they ve developed this mean that any data generated or collected is made available through the log which are then parsed and stored in table there are no apis to connect to csvs to ingest or tool that need to be connected so they can share data it just table the pipeline that parse the log have for the most part already been built and thus your job a de is to work with the table that are created every night found this incredibly boring because get more joy satisfaction out of working with really dirty raw data that where feel can add value but data at meta is already pretty clean just due to the nature of how it generated and collected if your joy satisfaction come from helping data scientist make the most of the data that available then faang is definitely for you but if you get your satisfaction from making unusable data usable then this likely isn what you re looking for it the wrong kind of scale think one of the appeal to working a de in faang is that there is just so much data the idea of working with petabyte of data brings thought of how to work at such large scale and it all sound really exciting that wa certainly the case for me the problem though is that this ha all pretty much been solved in faang and it being solved by swes not de distributed computing hyper efficient query engine load balancing etc are all implemented by swes and so working at scale mean implementing basic common sense in your sql query so that you re not going over the gb memory limit on any given node much prefer breadth over depth when it come to scale much rather work with large variety of data type solving large variety of problem faang doesn provide this at least not in my experience can feel the impact lot of the work you do a data engineer is related to metric and dashboard with the goal of helping the data scientist use the data more effectively for me this resulted in all of my impact being along the line of put number on dashboard to facilitate tracking of the metric this doesn resonate with me it doesn motivate me can certainly understand how some people would enjoy that and it definitely important work it just not what get me out of bed in the morning and result wa struggling to stay focused or get task done in the end meta and imagine all of faang wa great company to work at with lot of really important and interesting work being done but for me a data engineer it just wasn my thing wanted to put this all out there for those who might be considering pursuing role in faang so that they can make more informed decision think it also helpful to provide some contrast to all of the hype around and faang and acknowledge that it not for everyone and that okay tl dr thought being de in faang would be the ultimate data experience but it wa far too analytical for my taste and wasn able to feel the impact wa making so left
if were you system design diablo ressurected database replication,diablo ressurected experienced outage at launch it is good read for data engineering gt the problem with the servers gt gt so to alleviate load and latency on our global database each region na eu and asia has individual databases that also store your character information and progress and your region database will periodically write to the global one most of your in game actions are performed against this regional database because it faster and your character is locked there to maintain the individual character record integrity the global database also has back up in case the main fails gt gt gt gt in staying true to the original game we kept lot of legacy code however one legacy service in particular is struggling to keep up with modern player behavior gt gt gt gt this service with some upgrades from the original handles critical pieces of game functionality namely game creation joining updating reading filtering game lists verifying game server health and reading characters from the database to ensure your character can participate in whatever it is you re filtering for importantly this service is singleton which means we can only run one instance of it in order to ensure all players are seeing the most up to date and correct game list at all times gt gt gt gt additionally overall we were saving too often to the global database there is no need to do this as often as we were we should really be saving you to the regional database and only saving you to the global database when we need to unlock you this is one of the mitigations we have put in place right now we are writing code to change how we do this entirely so we will almost never be saving to the global database my shower thought of treating it like system desgin interview question if design the system of distributed database while leaving legacy code alone my immediate thought is that the replication to global database can be replaced by queue of update request like cassadra kafka sound ideal so you can replay lost progress in case of disaster presumably this is faster than convensional global database my second thought is that to ensure the legecy code have the latest view of character updates it is probably simple to keep cached view of players but stored in lightweight structure that can be sent across in blink no matter how heavily loaded the queue is it can be bloomfilter of list of players that can see your created game or even list of players has updated dirty page that require to fetch from the queue sending bloomfilter across regions in nanoseconds should be relatively lightweight now discuss,0,if were you system design diablo ressurected database replication,diablo ressurected experienced outage at launch it is good read for data engineering gt the problem with the server gt gt so to alleviate load and latency on our global database each region na eu and asia ha individual database that also store your character information and progress and your region database will periodically write to the global one most of your in game action are performed against this regional database because it faster and your character is locked there to maintain the individual character record integrity the global database also ha back up in case the main fails gt gt gt gt in staying true to the original game we kept lot of legacy code however one legacy service in particular is struggling to keep up with modern player behavior gt gt gt gt this service with some upgrade from the original handle critical piece of game functionality namely game creation joining updating reading filtering game list verifying game server health and reading character from the database to ensure your character can participate in whatever it is you re filtering for importantly this service is singleton which mean we can only run one instance of it in order to ensure all player are seeing the most up to date and correct game list at all time gt gt gt gt additionally overall we were saving too often to the global database there is no need to do this a often a we were we should really be saving you to the regional database and only saving you to the global database when we need to unlock you this is one of the mitigation we have put in place right now we are writing code to change how we do this entirely so we will almost never be saving to the global database my shower thought of treating it like system desgin interview question if design the system of distributed database while leaving legacy code alone my immediate thought is that the replication to global database can be replaced by queue of update request like cassadra kafka sound ideal so you can replay lost progress in case of disaster presumably this is faster than convensional global database my second thought is that to ensure the legecy code have the latest view of character update it is probably simple to keep cached view of player but stored in lightweight structure that can be sent across in blink no matter how heavily loaded the queue is it can be bloomfilter of list of player that can see your created game or even list of player ha updated dirty page that require to fetch from the queue sending bloomfilter across region in nanosecond should be relatively lightweight now discus
expected uk data engineer salary ranges,what would you expect the salary ranges to be for the uk london and non london years of experience in software but as data engineer last year was on two of my colleagues told me that my salary was way too low but sites like payscale and glassdoor say that on track for manchester salary months later got promoted at the same company to senior data engineer to left after year for data engineer role at still manchester is what you expect for manchester data engineer salary what would the difference be for senior data engineer also may have to move down to london for my partner what are the salary expectations there if relevant ve got experience in azure aws gcp python sql airflow and cicd,0,expected uk data engineer salary range,what would you expect the salary range to be for the uk london and non london year of experience in software but a data engineer last year wa on two of my colleague told me that my salary wa way too low but site like payscale and glassdoor say that on track for manchester salary month later got promoted at the same company to senior data engineer to left after year for data engineer role at still manchester is what you expect for manchester data engineer salary what would the difference be for senior data engineer also may have to move down to london for my partner what are the salary expectation there if relevant ve got experience in azure aws gcp python sql airflow and cicd
serving layer for delta lake,hi all we are currently building out delta lake and using the bronze silver and gold logic and are working out how to best serve the tables to end users we have mix of skill sets so some will use databricks notebooks while others want traditional sql ui such as ssms we plan to expose the delta tables to serverless sql as external tables and will give access to these tables to users our challenge is where to store view logic we want any view logic to be in one place but if we build views in synapse they are locked to that platform so we could build these as silver gold tables and expose them as external tables unless there is better option thought the view could be stored in the hive metastore but views need spark compute to work so won be picked up by serveless sql,0,serving layer for delta lake,hi all we are currently building out delta lake and using the bronze silver and gold logic and are working out how to best serve the table to end user we have mix of skill set so some will use databricks notebook while others want traditional sql ui such a ssms we plan to expose the delta table to serverless sql a external table and will give access to these table to user our challenge is where to store view logic we want any view logic to be in one place but if we build view in synapse they are locked to that platform so we could build these a silver gold table and expose them a external table unless there is better option thought the view could be stored in the hive metastore but view need spark compute to work so won be picked up by serveless sql
data lake warehouse and permissions,hey everyone work in smaller company and we are starting to take data more seriously have design question that stumped on what the best way to manage permissions with the least amount of overhead we re smaller company so our current setup is pretty brutal for data analysts and we want to bring in some new technologies to help them out currently our setup looks like this prod gt kafka connect gt mysql analytics db currently analysts have full permissions to the analytics database and its in real time thanks to kafka sensitive tables are removed through kafka but this is hands on as new tables are created frequently have clean slate as we kill mysql which is awesome so any input would be appreciated need to get away from managing tables in kafka and just have kafka dump everything somewhere and then worry about permissions and where the data goes next,0,data lake warehouse and permission,hey everyone work in smaller company and we are starting to take data more seriously have design question that stumped on what the best way to manage permission with the least amount of overhead we re smaller company so our current setup is pretty brutal for data analyst and we want to bring in some new technology to help them out currently our setup look like this prod gt kafka connect gt mysql analytics db currently analyst have full permission to the analytics database and it in real time thanks to kafka sensitive table are removed through kafka but this is hand on a new table are created frequently have clean slate a we kill mysql which is awesome so any input would be appreciated need to get away from managing table in kafka and just have kafka dump everything somewhere and then worry about permission and where the data go next
is data science experience valued if you functioned as data engineer,one of my first jobs had the official title of data scientist although functioned as data engineer since then have worked other jobs where held the title data engineer officially would my experience as data scientist count towards my years of experience as data engineer,0,is data science experience valued if you functioned a data engineer,one of my first job had the official title of data scientist although functioned a data engineer since then have worked other job where held the title data engineer officially would my experience a data scientist count towards my year of experience a data engineer
da to de vs swe,da to de or swe hi reddit at bit of crossroads and was hoping for some advice about me ve been working as da at large bank for the past years ve become the technical lead on my team the majority of my work has been designing data product with python used for marketing segmentation and qa and writing running many etl pipelines with python or pyspark databricks relatively strong with python and sql hoping to transition to swe or de at faang or similar company in the next several months enjoy technically challenging work and want to focus on improving my technical skills throughout my career generally enjoy working with tech partners over business but have gotten lots of positive feedback on my ability to work with business enjoy working with business but not extensively ve been told by an ex coworker at fb my responsibilities seem similar to what their de coworkers do find python more interesting than sql and hate building dashboards why considering swe swes appear to solve more diverse problems it seems as though many de jobs involve lots of etl work which is beginning to get stale generally it seems as though swes build products while des run processes this impression might be wrong higher pay it likely start as junior so it probably be lower at first why not it daunting concerned even with extensive interview prep getting job might be stretch and if get job imposter syndrome will be real why considering de it more closely matches my experience think ll be much more prepared than for swe more likely to get job at worst it step in the direction want to go while swe is leap enjoy my work currently but have so much room for improvement feel like ve gotten very good at what do in my environment but know have so much to learn from peers on new more technical team multiple de contacts at target companies why not concerned the work will become stale eventually it appears de has less breadth but could be wrong concerned if want to switch to swe my de experience won be super relevant sure this is team dependent for those who made it this far any advice ve started interview prepping but the material is diverging and need to make decision if get bored might it be realistic to switch to swe after years of de experience thanks all,0,da to de v swe,da to de or swe hi reddit at bit of crossroad and wa hoping for some advice about me ve been working a da at large bank for the past year ve become the technical lead on my team the majority of my work ha been designing data product with python used for marketing segmentation and qa and writing running many etl pipeline with python or pyspark databricks relatively strong with python and sql hoping to transition to swe or de at faang or similar company in the next several month enjoy technically challenging work and want to focus on improving my technical skill throughout my career generally enjoy working with tech partner over business but have gotten lot of positive feedback on my ability to work with business enjoy working with business but not extensively ve been told by an ex coworker at fb my responsibility seem similar to what their de coworkers do find python more interesting than sql and hate building dashboard why considering swe swes appear to solve more diverse problem it seems a though many de job involve lot of etl work which is beginning to get stale generally it seems a though swes build product while de run process this impression might be wrong higher pay it likely start a junior so it probably be lower at first why not it daunting concerned even with extensive interview prep getting job might be stretch and if get job imposter syndrome will be real why considering de it more closely match my experience think ll be much more prepared than for swe more likely to get job at worst it step in the direction want to go while swe is leap enjoy my work currently but have so much room for improvement feel like ve gotten very good at what do in my environment but know have so much to learn from peer on new more technical team multiple de contact at target company why not concerned the work will become stale eventually it appears de ha le breadth but could be wrong concerned if want to switch to swe my de experience won be super relevant sure this is team dependent for those who made it this far any advice ve started interview prepping but the material is diverging and need to make decision if get bored might it be realistic to switch to swe after year of de experience thanks all
can someone paint picture on how my path to de looks like,currently am studying economics in university and learning de fundamentals from dataquest on the side want to know what approach can take to get de as my first job after graduating in years do look for de internship will interviewers even be interested in me or do look for da opportunities and work my way into de additionally how else can prepare myself for the role ie examination ve been told to take many hard math modules in my university because they will interest interviewers is that true any advice is appreciated,0,can someone paint picture on how my path to de look like,currently am studying economics in university and learning de fundamental from dataquest on the side want to know what approach can take to get de a my first job after graduating in year do look for de internship will interviewer even be interested in me or do look for da opportunity and work my way into de additionally how else can prepare myself for the role ie examination ve been told to take many hard math module in my university because they will interest interviewer is that true any advice is appreciated
useful pyspark functions,useful pyspark functions,0,useful pyspark function,useful pyspark function
uploading files to s3,hi have about images on ec2 instance and have file txt where each line is path to image that want to upload out of images there is about hat want to upload to my s3 bucket what is the best way to do it,0,uploading file to s3,hi have about image on ec2 instance and have file txt where each line is path to image that want to upload out of image there is about hat want to upload to my s3 bucket what is the best way to do it
airflow best practice to transfer data between tasks,hi all am relatively new to airflow have already written smaller dags in which in each task data is fetched from an api and written to azure blob would now like to fetch data from mssql database or further csv file or azure blob then transform it with python and finally write the result to the data warehouse mssql am coming from drag drop etl background like pentaho my approach would be to run sql script in the first task to fetch the data mssqloperator in the second task to transform the data with pandas and pythonoperator and in the last step to write the result to the dwh mssqloperator but don know how to transfer the data between the tasks usually it should be possible to pass the result of one step as pandas dataframe to next step right think this is possible somehow with xcom but didn understand it yet and was advised not to work with it what would be your approach to build such data pipelines it important to me that don throw everything into one big script want to store the sql scripts separately and call them from the dag for better version control and uniqueness of scripts amp x200b thanks lot in advance,0,airflow best practice to transfer data between task,hi all am relatively new to airflow have already written smaller dag in which in each task data is fetched from an api and written to azure blob would now like to fetch data from mssql database or further csv file or azure blob then transform it with python and finally write the result to the data warehouse mssql am coming from drag drop etl background like pentaho my approach would be to run sql script in the first task to fetch the data mssqloperator in the second task to transform the data with panda and pythonoperator and in the last step to write the result to the dwh mssqloperator but don know how to transfer the data between the task usually it should be possible to pas the result of one step a panda dataframe to next step right think this is possible somehow with xcom but didn understand it yet and wa advised not to work with it what would be your approach to build such data pipeline it important to me that don throw everything into one big script want to store the sql script separately and call them from the dag for better version control and uniqueness of script amp x200b thanks lot in advance
which nosql database do you use,removed view poll,0,which nosql database do you use,removed view poll
interview preparation,hello everyone applied to position in big retailer company and will be interviewed in the next few days it will be my first interview for this position have some knowledge in cloudera stack on premisses think that know the basics about but want be prepared for the interview for what know they use aws do you have any advice to prepare for the interview thank you,0,interview preparation,hello everyone applied to position in big retailer company and will be interviewed in the next few day it will be my first interview for this position have some knowledge in cloudera stack on premiss think that know the basic about but want be prepared for the interview for what know they use aws do you have any advice to prepare for the interview thank you
system design interview at datadog,hi reddit have an upcoming system design interview at datadog where told ll have to think about scalable product something like twitter or spotify ll have to draft the global architecture then dive in specific component of my choice ll have to show knowledge about things like distributed systems queues consumer producer load balancers sql no sql have decent programming experience python sql ds but not with large distributed systems any advice on how to prepare for the interview ve seen leetcode premium mentioned in similar threads is it worth it thank you for the help,0,system design interview at datadog,hi reddit have an upcoming system design interview at datadog where told ll have to think about scalable product something like twitter or spotify ll have to draft the global architecture then dive in specific component of my choice ll have to show knowledge about thing like distributed system queue consumer producer load balancer sql no sql have decent programming experience python sql d but not with large distributed system any advice on how to prepare for the interview ve seen leetcode premium mentioned in similar thread is it worth it thank you for the help
resources to learn about running spark on emr,am having some trouble with my spark job running out of space want to find some good resources that can help me learn more about spark and tunning it can you guys please help,0,resource to learn about running spark on emr,am having some trouble with my spark job running out of space want to find some good resource that can help me learn more about spark and tunning it can you guy please help
need help to come up with an idea use case for my master thesis project big data ml web3,hey everyone am data engineer who is finishing his master in statistics data analytics and ml and am really struggling to find good idea use case for my graduation thesis project so bear with me on daily basis am working as data engineer with lots of de technologies spark with scala python sql kafka rabbitmq databricks sparkml but started my master in ml because was really interested in machine learning and have solid experience in it with tensorflow sparkml sklearn but recently ve gained lot of interest in web3 too ve started learning lot about web3 technologies like crypto nfts and smart contracts and was able to implement them in react app is there some use case where can do some data analysis on web3 data or where can implement some ml model in order to predict something know that the scope of technologies is very large but if you have any ideas please write them in the comments,0,need help to come up with an idea use case for my master thesis project big data ml web3,hey everyone am data engineer who is finishing his master in statistic data analytics and ml and am really struggling to find good idea use case for my graduation thesis project so bear with me on daily basis am working a data engineer with lot of de technology spark with scala python sql kafka rabbitmq databricks sparkml but started my master in ml because wa really interested in machine learning and have solid experience in it with tensorflow sparkml sklearn but recently ve gained lot of interest in web3 too ve started learning lot about web3 technology like crypto nfts and smart contract and wa able to implement them in react app is there some use case where can do some data analysis on web3 data or where can implement some ml model in order to predict something know that the scope of technology is very large but if you have any idea please write them in the comment
new grad data engineer,an undergrad junior cs major who has done de for projects and internships and really want to work in de role right out of college however ve been hearing most de roles do not hire new grads and instead want candidates with yoe is there chance for me or should go for swe or analyst roles then transition thanks,0,new grad data engineer,an undergrad junior c major who ha done de for project and internship and really want to work in de role right out of college however ve been hearing most de role do not hire new grad and instead want candidate with yoe is there chance for me or should go for swe or analyst role then transition thanks
how to scale out your clickhouse cluster,how to scale out your clickhouse cluster,0,how to scale out your clickhouse cluster,how to scale out your clickhouse cluster
tech stack with snowflake tool,what will be the best and minimum tech stack to learn with the snowflake tool currently know python and sql could you please guide me on this,0,tech stack with snowflake tool,what will be the best and minimum tech stack to learn with the snowflake tool currently know python and sql could you please guide me on this
getting an internship,am enrolled for computer science bachelor junior at the moment and going on linkedin to apply for remote de internships to replace my grocery store job have little experience with python and sql just one class on each should scattershot approach work am just applying for anything with data intern as keywords am hungry for any experience in the field should be looking for more entry level position since ve had no experience at all yet am on the right track obviously not asking you guys for an internship just thoughts also will remote options work with my macbook live quite outside chattanooga and will get car in the next few months hopefully so can get an in person internship in chattanooga,0,getting an internship,am enrolled for computer science bachelor junior at the moment and going on linkedin to apply for remote de internship to replace my grocery store job have little experience with python and sql just one class on each should scattershot approach work am just applying for anything with data intern a keywords am hungry for any experience in the field should be looking for more entry level position since ve had no experience at all yet am on the right track obviously not asking you guy for an internship just thought also will remote option work with my macbook live quite outside chattanooga and will get car in the next few month hopefully so can get an in person internship in chattanooga
how do you retrieve data from data warehouse,how do you retrieve data from data warehouse,0,how do you retrieve data from data warehouse,how do you retrieve data from data warehouse
upload csv to s3 with aws lambda,hey am very new to aws am working on de project in this project am getting data from an online static link that spits out json want to get that data convert it to csv and upload it to the aws s3 bucket also want to run this lambda function every days am trying to do this by following for some reason had no luck here is my code aws blob main lambda py have attached the iam role that has amazons3fullaccess policies arn aaws aiam aaws apolicy famazons3fullaccess awslambdabasicexecutionrole policies arn aaws aiam aaws apolicy fservice role fawslambdabasicexecutionrole policies have attached some screenshots of the output can some help me execution results format png amp auto webp amp ec14dab71f7c5f06dec385e59edac9f88550b867 python function format png amp auto webp amp c0bb8c8c1a3c6a80524e8fc4eca2f4edbb99b0ab policies attached to the role format png amp auto webp amp c0cca775e930442d0031fd9abf0c0d9d8ee9ff62,0,upload csv to s3 with aws lambda,hey am very new to aws am working on de project in this project am getting data from an online static link that spit out json want to get that data convert it to csv and upload it to the aws s3 bucket also want to run this lambda function every day am trying to do this by following for some reason had no luck here is my code aws blob main lambda py have attached the iam role that ha amazons3fullaccess policy arn aaws aiam aaws apolicy famazons3fullaccess awslambdabasicexecutionrole policy arn aaws aiam aaws apolicy fservice role fawslambdabasicexecutionrole policy have attached some screenshots of the output can some help me execution result format png amp auto webp amp ec14dab71f7c5f06dec385e59edac9f88550b867 python function format png amp auto webp amp c0bb8c8c1a3c6a80524e8fc4eca2f4edbb99b0ab policy attached to the role format png amp auto webp amp c0cca775e930442d0031fd9abf0c0d9d8ee9ff62
kafka best practices for de,how is everyone utilizing kafka in de am wondering what the most popular use cases for kafka besides cdc data streaming processing is it common to have reports query kafka with some type of middleware consumer api layer that can query it how are your teams building maintaining kafka infrastructure fully managed with confluent fully managed with aws msk self managed with kubernetes cluster lastly what language are your teams using to build kafka apps scala or java,0,kafka best practice for de,how is everyone utilizing kafka in de am wondering what the most popular use case for kafka besides cdc data streaming processing is it common to have report query kafka with some type of middleware consumer api layer that can query it how are your team building maintaining kafka infrastructure fully managed with confluent fully managed with aws msk self managed with kubernetes cluster lastly what language are your team using to build kafka apps scala or java
how to make career switch from hr to data engineering,hi as the title says started my career little over two years ago in hr however it not the typical hr career you have in mind got my masters degree in industrial and organizational psychology and was hired into an analytical role within hr my job focused on analyzing people data regularly use excel to run analyses but also learned sql and oracle specific syntax to run or optimize data model queries for output reporting which then use for analysis over the two years in my role began to project lead oracle module implementations and became really good with the software began performing etls to load large volume of data into specific business object areas and began specializing in specific functional areas of the software in addition to that also lead the initiative to develop dashboard visualization became tableau desktop verified and developed new pipeline data workflows for data refreshes am now starting new role as senior oracle functional analyst for multinational company as of now am working towards oracle saas cloud certification would like to pivot my career towards data engineering looked into various areas data analyst data scientist software engineering but realized data engineering is most aligned with my interest enjoy the building aspect more than the strategic data analysis and storytelling piece to managers in terms of training was looking into the ibm data engineering certification or the mit program can anyone recommend which program or others that could help me prepare for this career switch thanks,0,how to make career switch from hr to data engineering,hi a the title say started my career little over two year ago in hr however it not the typical hr career you have in mind got my master degree in industrial and organizational psychology and wa hired into an analytical role within hr my job focused on analyzing people data regularly use excel to run analysis but also learned sql and oracle specific syntax to run or optimize data model query for output reporting which then use for analysis over the two year in my role began to project lead oracle module implementation and became really good with the software began performing etls to load large volume of data into specific business object area and began specializing in specific functional area of the software in addition to that also lead the initiative to develop dashboard visualization became tableau desktop verified and developed new pipeline data workflow for data refreshes am now starting new role a senior oracle functional analyst for multinational company a of now am working towards oracle saas cloud certification would like to pivot my career towards data engineering looked into various area data analyst data scientist software engineering but realized data engineering is most aligned with my interest enjoy the building aspect more than the strategic data analysis and storytelling piece to manager in term of training wa looking into the ibm data engineering certification or the mit program can anyone recommend which program or others that could help me prepare for this career switch thanks
anyone know of any job fairs going on near west cost usa virtually,on paper my resume looks like shit no degree few online certificates and only years exp as an analyst been getting lot of auto rejection letters lately however know can sell myself if can just sit down with someone anyone know of any job fairs going on so can meet someone face to face before they auto reject my resume,0,anyone know of any job fair going on near west cost usa virtually,on paper my resume look like shit no degree few online certificate and only year exp a an analyst been getting lot of auto rejection letter lately however know can sell myself if can just sit down with someone anyone know of any job fair going on so can meet someone face to face before they auto reject my resume
apache hive best practice advice,working on project which is using data split into categories all related to socio economic metrics crime health care food availability government expenditure each category health care will have several files access to health care with different metrics in single file access to healthcare by household income of rural population with access to healthcare etc the metrics names are on row by row basis with the measure for every recorded year in the columns all the data is at global level so the rows in each csv will essentially contain all the metrics for every country on average about rows per country in each file most csv files in the same category have the same columnar format if this was relational db be looking to normalize this data and break it all down into several tables but ve been researching hive and apparently normalisation is not the best option in most cases as new to hive much appreciate some advice on the best practices based on what ve been reading online my plan is to reshape the files so there is single year column and column for each metric simply bringing the metrics from rows to columns and vice versa for the years once done concatenate the files within the same category into one orc table so tables in total if there are any good resources on hive best practices that you can recommend please also suggest some slightly overwhelmed with the sheer amount of resources thanks,0,apache hive best practice advice,working on project which is using data split into category all related to socio economic metric crime health care food availability government expenditure each category health care will have several file access to health care with different metric in single file access to healthcare by household income of rural population with access to healthcare etc the metric name are on row by row basis with the measure for every recorded year in the column all the data is at global level so the row in each csv will essentially contain all the metric for every country on average about row per country in each file most csv file in the same category have the same columnar format if this wa relational db be looking to normalize this data and break it all down into several table but ve been researching hive and apparently normalisation is not the best option in most case a new to hive much appreciate some advice on the best practice based on what ve been reading online my plan is to reshape the file so there is single year column and column for each metric simply bringing the metric from row to column and vice versa for the year once done concatenate the file within the same category into one orc table so table in total if there are any good resource on hive best practice that you can recommend please also suggest some slightly overwhelmed with the sheer amount of resource thanks
what are people doing using for data contracts,so have been chatting with few folks in the data mesh and broader data community re data contracts essentially extending the idea of api schema contracts to data where there is also guarantee to not change the semantic meaning of the data prevents the unbroken pipeline but broken data issue of only using schema contracts if you aren familiar with api schema contracts here brief on api contracts think of schema contracts as basically the same consumer agrees to the contract and if things change they get alerted but searching around the web am finding very little on data contracts or even schema contracts will drop the links have found re data contracts in comment but what are you using tools or doing processes to do data contracts is it all roll your own or is anyone even doing data contracts am launching podcast soon so we can dig into specifics of how tos re data mesh and related topics and data contracts is my first deep dive so if there are people that also want to chat or potentially be interviewed doing interviews week as crazy person hit me up at scheduled and potentials that have said maybe so hopefully it good start on solving this,0,what are people doing using for data contract,so have been chatting with few folk in the data mesh and broader data community re data contract essentially extending the idea of api schema contract to data where there is also guarantee to not change the semantic meaning of the data prevents the unbroken pipeline but broken data issue of only using schema contract if you aren familiar with api schema contract here brief on api contract think of schema contract a basically the same consumer agrees to the contract and if thing change they get alerted but searching around the web am finding very little on data contract or even schema contract will drop the link have found re data contract in comment but what are you using tool or doing process to do data contract is it all roll your own or is anyone even doing data contract am launching podcast soon so we can dig into specific of how tos re data mesh and related topic and data contract is my first deep dive so if there are people that also want to chat or potentially be interviewed doing interview week a crazy person hit me up at scheduled and potential that have said maybe so hopefully it good start on solving this
downloading websites using nas,good day is it possible to download web pages automatically with my qnap t419 or any qnap so would like whenever website uploads new article or changes it that my qnap automatically downloads this article website can then evaluate these records,0,downloading website using na,good day is it possible to download web page automatically with my qnap t419 or any qnap so would like whenever website uploads new article or change it that my qnap automatically downloads this article website can then evaluate these record
seeking advice on data collection for multiple organizations into centralized location would utilize data lake data warehouse or something else entirely,am part of project that consists of multiple organizations wanting to share data with each other on an ongoing basis it would consist of the same data elements from each company and it would be refreshed at set interval ideally we would like to join all of the collected data together while providing access to the data for each of the companies involved what would be the best approach for accomplishing this that would also have high level of security,0,seeking advice on data collection for multiple organization into centralized location would utilize data lake data warehouse or something else entirely,am part of project that consists of multiple organization wanting to share data with each other on an ongoing basis it would consist of the same data element from each company and it would be refreshed at set interval ideally we would like to join all of the collected data together while providing access to the data for each of the company involved what would be the best approach for accomplishing this that would also have high level of security
api integration saas with hubspot compatibility,would like to ask if you know of any saas that offers complete hubspot compatibility especially with this api ve tried trials with fivetran segment rudderstack stitch hevo data panoply dataddo but any of them have complete integration with hubspot and is difficult to request new features any help is welcomed thank you in advance,0,api integration saas with hubspot compatibility,would like to ask if you know of any saas that offer complete hubspot compatibility especially with this api ve tried trial with fivetran segment rudderstack stitch hevo data panoply dataddo but any of them have complete integration with hubspot and is difficult to request new feature any help is welcomed thank you in advance
which one of these might correspond to software data engineering on net,which one of these might correspond to software data engineering on net,0,which one of these might correspond to software data engineering on net,which one of these might correspond to software data engineering on net
have you tried starrocks,here is interesting project it seems that starrocks runs blazing fast especially when it comes to multi table join queries also find comparison to clickhouse wangtianyi clickhouse or starrocks here is detailed comparison c743a0b7b95f wangtianyi_86442 clickhouse or starrocks here is detailed comparison c743a0b7b95f can anyone give some insight about this mpp database,0,have you tried starrocks,here is interesting project it seems that starrocks run blazing fast especially when it come to multi table join query also find comparison to clickhouse wangtianyi clickhouse or starrocks here is detailed comparison c743a0b7b95f wangtianyi_86442 clickhouse or starrocks here is detailed comparison c743a0b7b95f can anyone give some insight about this mpp database
google cloud data engineer,hi group have upcoming phone screen for google cloud data engineer there is not much information what is expected like it leetcode hard or medium or mostly sql since it on data side does any body given idea what to expect recruiter just says min screen and no additional information,0,google cloud data engineer,hi group have upcoming phone screen for google cloud data engineer there is not much information what is expected like it leetcode hard or medium or mostly sql since it on data side doe any body given idea what to expect recruiter just say min screen and no additional information
managing data lake clean what type of data should be stored on each layer,hello everyone working on some personal project scraping data from friend api and would like to ask you for advice as not trying to figure out what data should be stored on each layer of my data lake ve come up with idea to build layers of my data lake bronze silver and gold trying to build kimball style delta lake on top of those data at the moment using databricks for it in mid to long term thinking about moving to postgres to simulate data warehouse loading jobs and test writing scripts as python files and executing them via hd insight instead of running it inside databricks environment my main concern is if it worth to store archive data on staging layer or should only maintain only one file for each normalized endpoint for this layer maintaining schema that creates new source directory with each job is little pain in the ass for me personally but not sure if it important yet also little concerned about gold layer schema is it enough to maintain sufficient access control if it was business use or should redesign it if so what layer should be used and when amp x200b my architecture concept looks like described below bronze layer purpose reads application api and stores it in distinct files each get job yields new file directory named of job date files are saved as json schema bronze general application endpoint ts_xx xx xx_xx xx xx ts_xx xx xx_xx xx xx endpoint application endpoint schema schema sensitive application silver layer purpose normalizes data and applies correct data formats to schemas files saved as parquet schema silver general master application endpoint ts_xx xx xx_xx xx xx ts_xx xx xx_xx xx xx endpoint application endpoint schema schema delta application endpoint file parquet endpoint file parquet application endpoint schema file parquet schema file parquet sensitive application gold layer purpose provides analytics grade datasets files saved as csv as reading them later using functions each next job moves previous result file to archive directory so in case of any etl error able to easily revert data to reflect previous cycle data schema bronze general application dataset archive result_file csv dataset archive result_file csv application dataset thanks for taking your time and happy holidays,0,managing data lake clean what type of data should be stored on each layer,hello everyone working on some personal project scraping data from friend api and would like to ask you for advice a not trying to figure out what data should be stored on each layer of my data lake ve come up with idea to build layer of my data lake bronze silver and gold trying to build kimball style delta lake on top of those data at the moment using databricks for it in mid to long term thinking about moving to postgres to simulate data warehouse loading job and test writing script a python file and executing them via hd insight instead of running it inside databricks environment my main concern is if it worth to store archive data on staging layer or should only maintain only one file for each normalized endpoint for this layer maintaining schema that creates new source directory with each job is little pain in the as for me personally but not sure if it important yet also little concerned about gold layer schema is it enough to maintain sufficient access control if it wa business use or should redesign it if so what layer should be used and when amp x200b my architecture concept look like described below bronze layer purpose read application api and store it in distinct file each get job yield new file directory named of job date file are saved a json schema bronze general application endpoint ts_xx xx xx_xx xx xx ts_xx xx xx_xx xx xx endpoint application endpoint schema schema sensitive application silver layer purpose normalizes data and applies correct data format to schema file saved a parquet schema silver general master application endpoint ts_xx xx xx_xx xx xx ts_xx xx xx_xx xx xx endpoint application endpoint schema schema delta application endpoint file parquet endpoint file parquet application endpoint schema file parquet schema file parquet sensitive application gold layer purpose provides analytics grade datasets file saved a csv a reading them later using function each next job move previous result file to archive directory so in case of any etl error able to easily revert data to reflect previous cycle data schema bronze general application dataset archive result_file csv dataset archive result_file csv application dataset thanks for taking your time and happy holiday
data engineers salaries in canada,hello yo and paid net after taxes in paris have about years of full time work experience would love to go to canada toronto or quebec what are the salaries there thanks,0,data engineer salary in canada,hello yo and paid net after tax in paris have about year of full time work experience would love to go to canada toronto or quebec what are the salary there thanks
what to learn to become more of full stack engineer,want to branch out in my free time to learn more front end stuff as de my knowledge is all backend like linux python dbs etc thinking of just learning some html and javascript so can do some basic front end stuff and build webapps this isn necessarily for my career just for own personal development not sure want to be de my whole life and would like to round out my skills to be more full stack than just back end,0,what to learn to become more of full stack engineer,want to branch out in my free time to learn more front end stuff a de my knowledge is all backend like linux python db etc thinking of just learning some html and javascript so can do some basic front end stuff and build webapps this isn necessarily for my career just for own personal development not sure want to be de my whole life and would like to round out my skill to be more full stack than just back end
technology stack with snowflake,hello so currently am learning tools used in de and know snowflake python and sql intermediate want to know what should learn to make my tech stack good as de would you please let me know some more essential tools used more frequently with the above skills thanks,0,technology stack with snowflake,hello so currently am learning tool used in de and know snowflake python and sql intermediate want to know what should learn to make my tech stack good a de would you please let me know some more essential tool used more frequently with the above skill thanks
considerations for system design to solve scalability consistency partition and availability in distributed and non distributed environments,considerations for system design to solve scalability consistency partition and availability in distributed and non distributed environments,0,consideration for system design to solve scalability consistency partition and availability in distributed and non distributed environment,consideration for system design to solve scalability consistency partition and availability in distributed and non distributed environment
any advice for years commitment to data engineering world,just starting out in data engineering world want to know what would it be like after years and should be committed to this field given that interested in it will it be fruitful,0,any advice for year commitment to data engineering world,just starting out in data engineering world want to know what would it be like after year and should be committed to this field given that interested in it will it be fruitful
is data engineering stressful,hi everyone senior data analyst thinking about transitioning into something new and data engineering is of interest currently work in biotech my work is very fast paced and demanding generally how is work life balance given the industry you work in,0,is data engineering stressful,hi everyone senior data analyst thinking about transitioning into something new and data engineering is of interest currently work in biotech my work is very fast paced and demanding generally how is work life balance given the industry you work in
save numpy arrays to csv files,save numpy arrays to csv files,0,save numpy array to csv file,save numpy array to csv file
is being data engineer just specialised software engineer,ive been thinking about how similar both jobs are and what not and how alot of data engineers had backrounds in designing websites so am right or wrong with this analogy,0,is being data engineer just specialised software engineer,ive been thinking about how similar both job are and what not and how alot of data engineer had backrounds in designing website so am right or wrong with this analogy
need help to prepare for faang data engineer interview question,hi all first of all wanna wish everyone merry mas and happy new year may be the most glorious year for you and your loved ones am preparing for technical design interview for one of the faang de role interview and would like to know your experience of those interviews what are the most common sql python design questions they ask and what are the best resources to learn them,0,need help to prepare for faang data engineer interview question,hi all first of all wanna wish everyone merry ma and happy new year may be the most glorious year for you and your loved one am preparing for technical design interview for one of the faang de role interview and would like to know your experience of those interview what are the most common sql python design question they ask and what are the best resource to learn them
anyone using python ray io framework in production,what your experience with ray whats the advantage over spark or is it used in totally different scenarios some article says it faster than spark is this true,0,anyone using python ray io framework in production,what your experience with ray whats the advantage over spark or is it used in totally different scenario some article say it faster than spark is this true
comments on using wsl2 or linux for data engineering pros and cons,so recently restarted my career as de coming from non swe data background so don have that much experience of what is best practice or industry standard when developing working as de ve only ever used windows as an os and never wanted to switch to mac because it seems horribly overpriced so far ve been using vscode on wsl2 ubuntu exclusively because found it easier to combine with airflow like the idea of linux so much that thinking of switching fully but don know if ll be missing something from the combination of windows and linux using wsl2 like applications that are not available for linux oss perhaps one other concern have is that as work as consultant ll probably work with the azure ecosystem lot gaining popularity where located would that be hindrance some way tldr using wsl2 now thinking of going full linux pros cons of doing this in de work merry christmas,0,comment on using wsl2 or linux for data engineering pro and con,so recently restarted my career a de coming from non swe data background so don have that much experience of what is best practice or industry standard when developing working a de ve only ever used window a an o and never wanted to switch to mac because it seems horribly overpriced so far ve been using vscode on wsl2 ubuntu exclusively because found it easier to combine with airflow like the idea of linux so much that thinking of switching fully but don know if ll be missing something from the combination of window and linux using wsl2 like application that are not available for linux os perhaps one other concern have is that a work a consultant ll probably work with the azure ecosystem lot gaining popularity where located would that be hindrance some way tldr using wsl2 now thinking of going full linux pro con of doing this in de work merry christmas
small company looking for data engineer consultant where should we post,featured job adverts in online job portals can be expensive so wondering if anyone has recommendations for the best places to post advice from someone who has hired data engineers would be really helpful and if anyone is curious here the job description,0,small company looking for data engineer consultant where should we post,featured job advert in online job portal can be expensive so wondering if anyone ha recommendation for the best place to post advice from someone who ha hired data engineer would be really helpful and if anyone is curious here the job description
do oracle hire data engineers and data scientist,when gave an interview as fresher at oracle the interviewer from oracle told that they don have lot of projects in data science as told that am really interested in it got rejected in the final round because of it though did very well in all the other rounds now work as data engineer in service based company but really want get my foot into oracle again after two years anyone here work in oracle as data engineer how to apply and give some interview experience,0,do oracle hire data engineer and data scientist,when gave an interview a fresher at oracle the interviewer from oracle told that they don have lot of project in data science a told that am really interested in it got rejected in the final round because of it though did very well in all the other round now work a data engineer in service based company but really want get my foot into oracle again after two year anyone here work in oracle a data engineer how to apply and give some interview experience
as per the college syllabus must do internship in web dev should choose backend over frontend right want to go in de and aws later,am thinking backend would make more sense as ll get exp dealing with server db and stuff,0,a per the college syllabus must do internship in web dev should choose backend over frontend right want to go in de and aws later,am thinking backend would make more sense a ll get exp dealing with server db and stuff
system design interview for data engineers,am planning to take up interviews fir de roles in and was wondering how should prepare for the system design round found that grokking the system design interview from educative io to cater towards swe as whole so was wondering if the same holds good for data engineer of if there was anything else that need to focus on,0,system design interview for data engineer,am planning to take up interview fir de role in and wa wondering how should prepare for the system design round found that grokking the system design interview from educative io to cater towards swe a whole so wa wondering if the same hold good for data engineer of if there wa anything else that need to focus on
resources for breaking into blockchain field as data engineer analyst,does anyone have any good resources or recommendations on getting job in crypto blockchain company as data engineer analyst this is the question have at the moment and wondering if there are any groups telegrams discords that might be focused on data engineers in blockchain as the crypto industry matures is seem like they have strong need for data engineers so interested if anyone has any experience getting into crypto or any tips for data engineering projects to better understand blockchain data thanks all,0,resource for breaking into blockchain field a data engineer analyst,doe anyone have any good resource or recommendation on getting job in crypto blockchain company a data engineer analyst this is the question have at the moment and wondering if there are any group telegram discord that might be focused on data engineer in blockchain a the crypto industry matures is seem like they have strong need for data engineer so interested if anyone ha any experience getting into crypto or any tip for data engineering project to better understand blockchain data thanks all
rest api to s3 bucket using python,tldr could somebody advice send me url refer me to course that goes into step by step detail on how to extract data from api and dump it into s3 using python ve been in traditional etl bi space for almost decade now ssis informatica azure data factory etc and recently transitioned to modern tech stack fivetran dbt airflow etc learning basics of python everyday but to accomplish this project ve started ll need to find out how to use python to get data from rest gt s3,0,rest api to s3 bucket using python,tldr could somebody advice send me url refer me to course that go into step by step detail on how to extract data from api and dump it into s3 using python ve been in traditional etl bi space for almost decade now ssis informatica azure data factory etc and recently transitioned to modern tech stack fivetran dbt airflow etc learning basic of python everyday but to accomplish this project ve started ll need to find out how to use python to get data from rest gt s3
personal project ideas with snowflake,hello could you guys please help me with some of the personal project ideas that can create with snowflake currently have knowledge of python sql snowflake and etl will be helpful if you can provide some guidance on this,0,personal project idea with snowflake,hello could you guy please help me with some of the personal project idea that can create with snowflake currently have knowledge of python sql snowflake and etl will be helpful if you can provide some guidance on this
is it unrealistic to expect higher salary for my skill level,tldr data analyst working to transition to data engineer looking for career advice am data analyst for the sales team at web hosting company and making year do not have degree or much in the way of portfolio to show right now however have been teaching myself everything can about data engineering and am leader in my team in our transition from excel to an oracle database am self taught with python sql vba and snowflake in january am starting free community data engineering course that will give certificate but don really have any other credentials to put on resume recently have lost faith that will receive anything more than cost of living raise from my company have started putting my resume out to see what bites but have only gotten automatic rejection letters so far honestly feeling like don qualify for anything labeled data engineer or anything with higher starting salary than am currently making however with wife and two kids can really accept pay cut right now am fooling myself by thinking am qualified for anything and above is that took much to ask for effectively junior or starting role very hard working and know could be good asset to de team but hard to get to the part where can show that any advice on the job search,0,is it unrealistic to expect higher salary for my skill level,tldr data analyst working to transition to data engineer looking for career advice am data analyst for the sale team at web hosting company and making year do not have degree or much in the way of portfolio to show right now however have been teaching myself everything can about data engineering and am leader in my team in our transition from excel to an oracle database am self taught with python sql vba and snowflake in january am starting free community data engineering course that will give certificate but don really have any other credential to put on resume recently have lost faith that will receive anything more than cost of living raise from my company have started putting my resume out to see what bite but have only gotten automatic rejection letter so far honestly feeling like don qualify for anything labeled data engineer or anything with higher starting salary than am currently making however with wife and two kid can really accept pay cut right now am fooling myself by thinking am qualified for anything and above is that took much to ask for effectively junior or starting role very hard working and know could be good asset to de team but hard to get to the part where can show that any advice on the job search
implementing dwh for startup without any data architect or data engineer,working for medium saas company we are finally building data warehouse chose snowflake and it is time to put data into it set up of snowflake was done by the dev team so far management and cto believe we should just connect our data source and start creating the tables we need believe we should follow certain philosophy kimball etc and that someone with experience should work on it am over complicating stuff or am the only one that has any grasp on reality,0,implementing dwh for startup without any data architect or data engineer,working for medium saas company we are finally building data warehouse chose snowflake and it is time to put data into it set up of snowflake wa done by the dev team so far management and cto believe we should just connect our data source and start creating the table we need believe we should follow certain philosophy kimball etc and that someone with experience should work on it am over complicating stuff or am the only one that ha any grasp on reality
how do you handle schema changes in tables and data files,hello everyone work in mid size company with lot of tables and databases mongodb and postgresql mostly that need to get synced and store their data as parquet format files on hdfs the problem is everytime the backend teams launch new feature one or multiple columns are added to the tables and when we want to sync their data we face an error that schema is changed and some new columns are added our solution for now is to find out what columns are added names and their types and then we read all previous parquet files and manually add these columns to data and again store it on hdfs and now we can sync the tables without an error but this process takes lot of time and effort and as the company is growing lot of features and these kinds of changes are going to happen and it very hard and time consuming to handle this issue manually really wonder to know how bigger companies and professional data engineers can overcome this very common problem how can somehow automate this whole process when syncing data the technologies we are using are spark zookeeper hdfs hive scala python thank you in advance for your help and advices,0,how do you handle schema change in table and data file,hello everyone work in mid size company with lot of table and database mongodb and postgresql mostly that need to get synced and store their data a parquet format file on hdfs the problem is everytime the backend team launch new feature one or multiple column are added to the table and when we want to sync their data we face an error that schema is changed and some new column are added our solution for now is to find out what column are added name and their type and then we read all previous parquet file and manually add these column to data and again store it on hdfs and now we can sync the table without an error but this process take lot of time and effort and a the company is growing lot of feature and these kind of change are going to happen and it very hard and time consuming to handle this issue manually really wonder to know how bigger company and professional data engineer can overcome this very common problem how can somehow automate this whole process when syncing data the technology we are using are spark zookeeper hdfs hive scala python thank you in advance for your help and advice
anyone here ever use the huawei cloud stack,starting working as an ml de eng at startup they have some super weird setup with dvc that just isn working no shade on dvc think they just set it up incorrectly anyway redoing their architecture mostly from scratch and had good s3 storage system with django api servicing access requests thing going when all of sudden our ceo got ton of huawei credits for christmas don know don wana know and now all of sudden on the huawei stack tbh it basically looks like an aws rebrand for the chinese market again no shade the tech world was built on that kind of thing so hoping it won be too bad to switch up the api calls but the sdk looks not good anyone have experience working with huawei ecs object block storage obs definitely not s3 huawei postgres rds again hopin its straightforward but there always gotchas and tricks and my mandarin isn so good ty and merry xmas eve,0,anyone here ever use the huawei cloud stack,starting working a an ml de eng at startup they have some super weird setup with dvc that just isn working no shade on dvc think they just set it up incorrectly anyway redoing their architecture mostly from scratch and had good s3 storage system with django api servicing access request thing going when all of sudden our ceo got ton of huawei credit for christmas don know don wana know and now all of sudden on the huawei stack tbh it basically look like an aws rebrand for the chinese market again no shade the tech world wa built on that kind of thing so hoping it won be too bad to switch up the api call but the sdk look not good anyone have experience working with huawei ec object block storage ob definitely not s3 huawei postgres rds again hopin it straightforward but there always gotchas and trick and my mandarin isn so good ty and merry xmas eve
kimball vs inmon vs vault,this post does good job explaining the nuances of the three methodologies would love to hear your experience with using any of the three,0,kimball v inmon v vault,this post doe good job explaining the nuance of the three methodology would love to hear your experience with using any of the three
can you tell if am data engineer,hi all working for startup in north africa morocco and in the last years my work was about building datalake in degree customer view for our client developing etl using pyspark airflow bash sql hadoop sqoop mongodb implementing lambda architecture using spark structured streaming kafka pyspark airflow bash sql hadoop mongodb implementing cdc proof of concept using docker docker compose extending the datalake for bank nano loan etl computing client kpis developing rest api for the end client the bank amp x200b always keep in mind build data framework not data pipeline and about the above mentioned techs know only what need to know amp x200b opinions are always welcome and constructive criticism and debate are always enriching,0,can you tell if am data engineer,hi all working for startup in north africa morocco and in the last year my work wa about building datalake in degree customer view for our client developing etl using pyspark airflow bash sql hadoop sqoop mongodb implementing lambda architecture using spark structured streaming kafka pyspark airflow bash sql hadoop mongodb implementing cdc proof of concept using docker docker compose extending the datalake for bank nano loan etl computing client kpis developing rest api for the end client the bank amp x200b always keep in mind build data framework not data pipeline and about the above mentioned tech know only what need to know amp x200b opinion are always welcome and constructive criticism and debate are always enriching
you need to ingest new dataset how do you proceed,hi got asked this question in an interview and am curious about how you would reply how do you approach the ingestion of new data in your database dwh,0,you need to ingest new dataset how do you proceed,hi got asked this question in an interview and am curious about how you would reply how do you approach the ingestion of new data in your database dwh
looking for data engineering mentor willing to pay,basically ve found myself in start up that love but permanently out of my depth setting up data systems not sure can maintain never sure if breaking some industry wide best practice that no one ever got round to telling me about take pay cut to remove some of this stress which is maybe equivalent to just paying someone to teach me guide me through some of my problems don know where to go to get this or how to move forwards,0,looking for data engineering mentor willing to pay,basically ve found myself in start up that love but permanently out of my depth setting up data system not sure can maintain never sure if breaking some industry wide best practice that no one ever got round to telling me about take pay cut to remove some of this stress which is maybe equivalent to just paying someone to teach me guide me through some of my problem don know where to go to get this or how to move forward
airbyte dbt splitgraph how we built our modern data stack,airbyte dbt splitgraph how we built our modern data stack,0,airbyte dbt splitgraph how we built our modern data stack,airbyte dbt splitgraph how we built our modern data stack
how do write pyspark jobs,hi everyone very new to data engineering and like to know how to write pyspark jobs ve read some tutorials over net but couldn find any help ps aware of pyspark,0,how do write pyspark job,hi everyone very new to data engineering and like to know how to write pyspark job ve read some tutorial over net but couldn find any help p aware of pyspark
how to find mature data orgs how to get in,what do you look for when searching for good organization to work for what are some red flags for example good sign they have blog that is regularly updated with new articles this shows me that they take pride in their engineering culture and want to show it off bad sign the position listed is part of one to three person data team this may be explainable maybe the company itself is very small but this is red flag to me it signal that they don invest in their data team and that will be swamped with trying to support every business unit data needs,0,how to find mature data orgs how to get in,what do you look for when searching for good organization to work for what are some red flag for example good sign they have blog that is regularly updated with new article this show me that they take pride in their engineering culture and want to show it off bad sign the position listed is part of one to three person data team this may be explainable maybe the company itself is very small but this is red flag to me it signal that they don invest in their data team and that will be swamped with trying to support every business unit data need
am crazy to think being underpaid,currently cloud data engineer with years of professional experience computer science bachelors degree and recently attained google associate cloud engineer certification working in major canadian city for multinational organization that has only just really started developing their cloud infrastructure in the past year right now making cad annually am crazy to think this is below average what salary ranges do people in similar positions find themselves in,0,am crazy to think being underpaid,currently cloud data engineer with year of professional experience computer science bachelor degree and recently attained google associate cloud engineer certification working in major canadian city for multinational organization that ha only just really started developing their cloud infrastructure in the past year right now making cad annually am crazy to think this is below average what salary range do people in similar position find themselves in
good data related questions to ask on product demo before buying software,am currently working for startup internet company as data engineer we have plan to buy customer success platform or csp the next coming months this is also my first time to integrate this kind of software on data pipeline newbie in the data engineering realm what will happen is that we have snowflake dw which will upload the data to the csp then using the data in the csp can create metrics dashboards customer segments trigger another software and create customer success workflows etc in my mind these are the questions that want to ask what is the expected shape of the data to be synched for the csp to understand it do need to clean massage the data where will the csp store the data can we access it via an api rate limits do you have retention period for the data how long have limited background to this kind of integration which is why can generate more critical rich questions hope someone who had the same experience even those other type of platform but the same concept before can give some useful advice,0,good data related question to ask on product demo before buying software,am currently working for startup internet company a data engineer we have plan to buy customer success platform or csp the next coming month this is also my first time to integrate this kind of software on data pipeline newbie in the data engineering realm what will happen is that we have snowflake dw which will upload the data to the csp then using the data in the csp can create metric dashboard customer segment trigger another software and create customer success workflow etc in my mind these are the question that want to ask what is the expected shape of the data to be synched for the csp to understand it do need to clean massage the data where will the csp store the data can we access it via an api rate limit do you have retention period for the data how long have limited background to this kind of integration which is why can generate more critical rich question hope someone who had the same experience even those other type of platform but the same concept before can give some useful advice
data pipelines in aws is this normal,ve recently joined an early stage startup as data engineer where ve been tasked mostly with working on improving the companies etl pipelines our daily pipelines involve extracting data from some external data vendors cleaning amalgamating transforming all of this data in several steps and finally feeding the results into various ml models haven worked in an aws environment before and have only had one data engineering job before this so not sure if this is normal but the following things have struck me as not very good practice we don use databases at all each daily process reads from writes to flat files json csv h5 in s3 buckets most of our data is either structured or semi structured this makes historical data analysis very difficult also means the entire file needs to be read into memory even if just one row is needed absence of monitoring scheduling tools if something breaks in the pipeline there is no way of knowing other than trawling through log files again stored in s3 then each pipeline step has to be rerun manually in my previous role we used ansible to chain tasks together so entire pipelines could be rerun easily on failure trying to decide whether to stick with this company long term ve brought these issues up with the team before but no one seems to think it that big of deal is this kind of thing common in the industry suspect there are much more advanced tools offered by aws to solve these problems that we could use but not sure what those tools are,0,data pipeline in aws is this normal,ve recently joined an early stage startup a data engineer where ve been tasked mostly with working on improving the company etl pipeline our daily pipeline involve extracting data from some external data vendor cleaning amalgamating transforming all of this data in several step and finally feeding the result into various ml model haven worked in an aws environment before and have only had one data engineering job before this so not sure if this is normal but the following thing have struck me a not very good practice we don use database at all each daily process read from writes to flat file json csv h5 in s3 bucket most of our data is either structured or semi structured this make historical data analysis very difficult also mean the entire file need to be read into memory even if just one row is needed absence of monitoring scheduling tool if something break in the pipeline there is no way of knowing other than trawling through log file again stored in s3 then each pipeline step ha to be rerun manually in my previous role we used ansible to chain task together so entire pipeline could be rerun easily on failure trying to decide whether to stick with this company long term ve brought these issue up with the team before but no one seems to think it that big of deal is this kind of thing common in the industry suspect there are much more advanced tool offered by aws to solve these problem that we could use but not sure what those tool are
free docker webinar at am pst,hi all we are hosting intro to docker amp using docker in cloud data engineering today at am pst our webinars are free and we answer data engineering related questions during the webinar thanks,0,free docker webinar at am pst,hi all we are hosting intro to docker amp using docker in cloud data engineering today at am pst our webinars are free and we answer data engineering related question during the webinar thanks
how to integrate dbt and lookml datawarehouse on bigquery,hey there at our company we use bq and are changing the way we model data by doing poc with dbt before that the analysts would create in silos their tables views we schedule the queries with airflow the team also wants to change the viz tool from using datastudio gsheets to looker have you guys implemented both dbt and lookml how do you make so that both complement each other my fear is that there could be some data governance issues as in should we trust that dbt model or the query generated with lookml etc,0,how to integrate dbt and lookml datawarehouse on bigquery,hey there at our company we use bq and are changing the way we model data by doing poc with dbt before that the analyst would create in silo their table view we schedule the query with airflow the team also want to change the viz tool from using datastudio gsheets to looker have you guy implemented both dbt and lookml how do you make so that both complement each other my fear is that there could be some data governance issue a in should we trust that dbt model or the query generated with lookml etc
data warehousing on bigquery tools and processes for data modelling,hey there at our company we use bq and are changing the way we model data by doing poc with dbt before that the analysts would create in silos their tables views we schedule the queries with airflow the team also wants to change the viz tool from using datastudio gsheets to looker have you guys implemented both dbt and lookml how do you make so that both complement each other my fear is that there could be some data governance issues as in should we trust that dbt model or the query generated with lookml etc,0,data warehousing on bigquery tool and process for data modelling,hey there at our company we use bq and are changing the way we model data by doing poc with dbt before that the analyst would create in silo their table view we schedule the query with airflow the team also want to change the viz tool from using datastudio gsheets to looker have you guy implemented both dbt and lookml how do you make so that both complement each other my fear is that there could be some data governance issue a in should we trust that dbt model or the query generated with lookml etc
follow progress of reading from s3,hey if read objecy from s3 bucket obj s3 object bucket key stream obj get body read is there way to track progress of reading with tqdm,0,follow progress of reading from s3,hey if read objecy from s3 bucket obj s3 object bucket key stream obj get body read is there way to track progress of reading with tqdm
options for building scalable database,hi everyone data engineering newbie recently joined company that uses airtable as its database its paired with research tool that gathers the data the data is then uploaded via an api thru zapier automations what would be the best way option to replace this current setup it also seems that my supervisors are more keen to buying product rather than building one so ideally an all in one scalable solution would be great any suggestions,0,option for building scalable database,hi everyone data engineering newbie recently joined company that us airtable a it database it paired with research tool that gather the data the data is then uploaded via an api thru zapier automation what would be the best way option to replace this current setup it also seems that my supervisor are more keen to buying product rather than building one so ideally an all in one scalable solution would be great any suggestion
transition to cloud engineering,hey all happy holidays ve been presented with an interesting series of moves that would essentially mean sidelining about of my work to spend training in gcp to take job as cloud engineer in my current department this and raise are in writing if decline can continue as de and shouldn have issues progressing although that won be in writing ve been with the company while and not too worried about whether the next level comes on jan if decline the cloud opportunity some pros pay obviously fun opportunity to learn some new valuable skills on company time and dollar chance to get in near the ground level of fairly new field ve enjoyed the gcp work ve done so far bur it new addition to my tool belt and brand new to my company some cons fairly fresh in the de field years analytics engineer months de so be specializing earlier than expected to nobody in my current department has strong knowledge in cloud computing or gcp so be the guy in some ways this is pro too have any of you made transition to full time cloud developers any thoughts thanks,0,transition to cloud engineering,hey all happy holiday ve been presented with an interesting series of move that would essentially mean sidelining about of my work to spend training in gcp to take job a cloud engineer in my current department this and raise are in writing if decline can continue a de and shouldn have issue progressing although that won be in writing ve been with the company while and not too worried about whether the next level come on jan if decline the cloud opportunity some pro pay obviously fun opportunity to learn some new valuable skill on company time and dollar chance to get in near the ground level of fairly new field ve enjoyed the gcp work ve done so far bur it new addition to my tool belt and brand new to my company some con fairly fresh in the de field year analytics engineer month de so be specializing earlier than expected to nobody in my current department ha strong knowledge in cloud computing or gcp so be the guy in some way this is pro too have any of you made transition to full time cloud developer any thought thanks
oh that fun awards for data engineering tooling,oh that fun awards for data engineering tooling,0,oh that fun award for data engineering tooling,oh that fun award for data engineering tooling
how to you release pyspark jobs,suppose have pyspark job that looks like this import pyspark import pandas import pytorch import my_helpers some package from corporate pypi do spark stuff do pandas stuff do ml stuff really struggling to understand how can create tar gz that could be passed to spark submit my idea is this create conda env with build tools and build conda package containing our code install this package into new minimal environment use conda pack to create tarball with that environment and upload it to s3 or wherever really struggling with understanding what is proper way to build conda packages specifically build sh and build section in meta yaml,0,how to you release pyspark job,suppose have pyspark job that look like this import pyspark import panda import pytorch import my_helpers some package from corporate pypi do spark stuff do panda stuff do ml stuff really struggling to understand how can create tar gz that could be passed to spark submit my idea is this create conda env with build tool and build conda package containing our code install this package into new minimal environment use conda pack to create tarball with that environment and upload it to s3 or wherever really struggling with understanding what is proper way to build conda package specifically build sh and build section in meta yaml
two way unidirectional sync platform also help,disclaimer john snow know nothing where do start according to the interwebs there are data integration patterns migration broadcast one to many bi directional sync target system can override fields in source correlation bi directional sync on common data objects between two systems aggregation consolidate multiple sources into one two way unidirectional sync target field cannot override fields that belong to source employer has salesforce as main source and wants bi directional integration with targets salesforce is the main source master we have applications like accounting systems csm psa erp systems for example prospect conversion in salesforce from the time lead is created up until customer places an order salesforce owns the prospect record after an order is placed the prospect is converted into customer and the erp takes ownership of the record or salesforce owns info in an object but data from another system needs to be surfaced in salesforce so designated set of fields is updated an regular basis from that other system the erp system owns and pushes updates for billing addresses read only in salesforce but mailing addresses are owned and maintained in salesforce and get pushed to the erp read only in erp this calls for two way unidirectional sync if you ask me bi directional sync will cause chaos how can achieve this my scope is to build warehouse for application integration dq and reporting decided on delta lake infrastructure overwhelmed in terms of the integration would messaging broker be the best solution to orchestrate updates and integration between salesforce and long list of other applications we are talking gb very poorly integrated salesforce data with the scope of rapid expansion over the next months at loss every conversation loops back to kafka but surely that is overkill the data engineering team consist of me entry level and inexperienced sorry for the long post would rather take advice than set myself up for failure,0,two way unidirectional sync platform also help,disclaimer john snow know nothing where do start according to the interwebs there are data integration pattern migration broadcast one to many bi directional sync target system can override field in source correlation bi directional sync on common data object between two system aggregation consolidate multiple source into one two way unidirectional sync target field cannot override field that belong to source employer ha salesforce a main source and want bi directional integration with target salesforce is the main source master we have application like accounting system csm psa erp system for example prospect conversion in salesforce from the time lead is created up until customer place an order salesforce owns the prospect record after an order is placed the prospect is converted into customer and the erp take ownership of the record or salesforce owns info in an object but data from another system need to be surfaced in salesforce so designated set of field is updated an regular basis from that other system the erp system owns and push update for billing address read only in salesforce but mailing address are owned and maintained in salesforce and get pushed to the erp read only in erp this call for two way unidirectional sync if you ask me bi directional sync will cause chaos how can achieve this my scope is to build warehouse for application integration dq and reporting decided on delta lake infrastructure overwhelmed in term of the integration would messaging broker be the best solution to orchestrate update and integration between salesforce and long list of other application we are talking gb very poorly integrated salesforce data with the scope of rapid expansion over the next month at loss every conversation loop back to kafka but surely that is overkill the data engineering team consist of me entry level and inexperienced sorry for the long post would rather take advice than set myself up for failure
did you have to do leetcode during your interviews,if not what was the main focus of the interview,0,did you have to do leetcode during your interview,if not what wa the main focus of the interview
dummy data for testing,generally using production data in test environment is not allowed at clients how do you guys get over this any specific tools you use to create dummy fake data for your etl pipeline testing thank you,0,dummy data for testing,generally using production data in test environment is not allowed at client how do you guy get over this any specific tool you use to create dummy fake data for your etl pipeline testing thank you
which language should learn to work in azure databricks delta lake and kafka,entry level data engineer only know sql need to pick the first language to learn view poll,0,which language should learn to work in azure databricks delta lake and kafka,entry level data engineer only know sql need to pick the first language to learn view poll
just wrote ain introductory post on how to use meltano open source elt to move data from csvs to postgres sql,just wrote ain introductory post on how to use meltano open source elt to move data from csvs to postgres sql,0,just wrote ain introductory post on how to use meltano open source elt to move data from csvs to postgres sql,just wrote ain introductory post on how to use meltano open source elt to move data from csvs to postgres sql
tools for ad hoc analysis,when you think about an ad hoc analysis what tools pop up in your mind which are the best why,0,tool for ad hoc analysis,when you think about an ad hoc analysis what tool pop up in your mind which are the best why
when you want all that juicy ai and ml but you have no time for the boring dwh data cleaning or maths malarkey,amp x200b format png amp auto webp amp d2887f537a04d7a5231f797fc0a3c1bd2cc8f8b8,0,when you want all that juicy ai and ml but you have no time for the boring dwh data cleaning or math malarkey,amp x200b format png amp auto webp amp d2887f537a04d7a5231f797fc0a3c1bd2cc8f8b8
when you want all that juicy ai and ml without all that bring dwh data cleaning malarkey or maths,amp x200b format png amp auto webp amp bcaf0f1ce58f84af782e8c3beab35770b494447,0,when you want all that juicy ai and ml without all that bring dwh data cleaning malarkey or math,amp x200b format png amp auto webp amp bcaf0f1ce58f84af782e8c3beab35770b494447
data engineer or product owner,data engineer on scrum team and being asked to take the product owner role do you think this is good career move,0,data engineer or product owner,data engineer on scrum team and being asked to take the product owner role do you think this is good career move
building an edge api gateway with fauna and securing it with auth0,in this tutorial we ll explore architecting rest apis in fully serverless manner by leveraging fastly compute edge fauna and auth0 read more utm_medium sc amp utm_campaign fauna,0,building an edge api gateway with fauna and securing it with auth0,in this tutorial we ll explore architecting rest apis in fully serverless manner by leveraging fastly compute edge fauna and auth0 read more utm_medium sc amp utm_campaign fauna
spark glue performance help needed,hi all am beginner in the field am trying to run simple etl process using aws glue the process is simple use jdbc connector to read from tables from database and then sink them in s3 everything works fine the only issue is the amount of time it is required to run the job hours the main bottleneck is caused by some very large tables to millions records and by the fact that have to extract number of rows and fields list the glue job uses python spark workers of which driver first read the table df sparksession read format jdbc option url connection_url option dbtable table option driver driver load then convert it to gluedynamicframe as it is easier for me to run operations on it df dynamicframe fromdf df gluecontext df then proceed to calculate number of rows n_rows df count which starts the pain for some tables it takes to minutes to just return this value have researched and think understand the concept of lazy evaluations and computations in spark but it seems to me that this operation should take way less anyway and am surely doing something wrong anyway then proceed to generate field list fields df schema fields name for in range len df schema fields which again to minutes to run eventually sink the dataframe gluecontext write_dynamic_frame from_options frame df connection_type s3 connection_options path s3 bucket_name zone tier source extraction partitionkeys dumpdate format parquet which again it takes long time for these large tables it is worth mentioning that extract from db tables that contain few rows as well mention this as have read to repartition as soon as read the table but it would make zero senso to repartition dataframe of rows the only way of doing it sistematically would be to count rows first and then base on rows repartition but it takes already forever also have read that the number of partitions should be somewhat related to the number of workers have worker so partition seems logical to me my question would be what am doing wrong should just increase number of workers and repartition accordingly at the moment of reading or what other solutions are available thanks lot for any advice,0,spark glue performance help needed,hi all am beginner in the field am trying to run simple etl process using aws glue the process is simple use jdbc connector to read from table from database and then sink them in s3 everything work fine the only issue is the amount of time it is required to run the job hour the main bottleneck is caused by some very large table to million record and by the fact that have to extract number of row and field list the glue job us python spark worker of which driver first read the table df sparksession read format jdbc option url connection_url option dbtable table option driver driver load then convert it to gluedynamicframe a it is easier for me to run operation on it df dynamicframe fromdf df gluecontext df then proceed to calculate number of row n_rows df count which start the pain for some table it take to minute to just return this value have researched and think understand the concept of lazy evaluation and computation in spark but it seems to me that this operation should take way le anyway and am surely doing something wrong anyway then proceed to generate field list field df schema field name for in range len df schema field which again to minute to run eventually sink the dataframe gluecontext write_dynamic_frame from_options frame df connection_type s3 connection_options path s3 bucket_name zone tier source extraction partitionkeys dumpdate format parquet which again it take long time for these large table it is worth mentioning that extract from db table that contain few row a well mention this a have read to repartition a soon a read the table but it would make zero senso to repartition dataframe of row the only way of doing it sistematically would be to count row first and then base on row repartition but it take already forever also have read that the number of partition should be somewhat related to the number of worker have worker so partition seems logical to me my question would be what am doing wrong should just increase number of worker and repartition accordingly at the moment of reading or what other solution are available thanks lot for any advice
how to reach out to recruiters as someone who is transitioning to de,am data analyst transitioning to de and don have any prior experience in data engineer role looking at the de ob descriptions gives me severe stress as every opportunity lists multiple years of de experience as the first criteria at least in india don know how to reach out to recruiters to discuss my application for these as well as other positions as already feel that am ineligible for such positions even though ve got good grasp of sql and python and continuously learning more de specific stuff want to know how to reach out to recruiters to at least discuss my resume skills and not get outright rejected because don have prior de experience also don want to lie and state that do have prior experience just to pass the job posting criteria any tips would be welcome especially from the indian crowd here thanks ps in case anyone wants to check my resume to understand where am coming from here copy,0,how to reach out to recruiter a someone who is transitioning to de,am data analyst transitioning to de and don have any prior experience in data engineer role looking at the de ob description give me severe stress a every opportunity list multiple year of de experience a the first criterion at least in india don know how to reach out to recruiter to discus my application for these a well a other position a already feel that am ineligible for such position even though ve got good grasp of sql and python and continuously learning more de specific stuff want to know how to reach out to recruiter to at least discus my resume skill and not get outright rejected because don have prior de experience also don want to lie and state that do have prior experience just to pas the job posting criterion any tip would be welcome especially from the indian crowd here thanks p in case anyone want to check my resume to understand where am coming from here copy
optimizing rds gt s3 ingest help understanding pg_stat table sizes container memory constraints and resultant csv disk size,working on an rds gt s3 process using aws mwaa using mwaa medium containers that have gb ram to ingest around this constraint using psycopg2 and server side cursors roughly cursor itersize cursor execute sql rows cursor fetchmany write_rows_to_s3 rows how can optimize the size of is number of rows which is variable between tables can calculate avg row size in postgresql with pg_stat tables as table_size reltuples then feel like should be able to calculate mem_limit gb avg_tuple_size max itersize but in practice my job starts to fail from memory constraints at roughly half of what would expect and the resultant csvs are in the range of mb not close to the gb max would expect how much of this is from the postgresql disk size being significantly different from the in memory psycopg2 representation of the data and then the on disk csv representation of the data is the psycopg2 cursor fetchmany result set significantly larger in memory than the csv postgresql native storage of that same info any ideas on how you would optimize for an optimal chunksize itersize value across tables,0,optimizing rds gt s3 ingest help understanding pg_stat table size container memory constraint and resultant csv disk size,working on an rds gt s3 process using aws mwaa using mwaa medium container that have gb ram to ingest around this constraint using psycopg2 and server side cursor roughly cursor itersize cursor execute sql row cursor fetchmany write_rows_to_s3 row how can optimize the size of is number of row which is variable between table can calculate avg row size in postgresql with pg_stat table a table_size reltuples then feel like should be able to calculate mem_limit gb avg_tuple_size max itersize but in practice my job start to fail from memory constraint at roughly half of what would expect and the resultant csvs are in the range of mb not close to the gb max would expect how much of this is from the postgresql disk size being significantly different from the in memory psycopg2 representation of the data and then the on disk csv representation of the data is the psycopg2 cursor fetchmany result set significantly larger in memory than the csv postgresql native storage of that same info any idea on how you would optimize for an optimal chunksize itersize value across table
bootcamp,does anyone have experience with either weclouddata or pipeline academy they re bootcamps specifically for data engineering hoping to do one of the two know lot of ppl recommend learning on your own via projects sure can do that but feel like ll have lot of gaps in my knowledge and won have good grasp of the fundamentals little about myself coming from sql analyst ish background in the finance and software consulting sector,0,bootcamp,doe anyone have experience with either weclouddata or pipeline academy they re bootcamps specifically for data engineering hoping to do one of the two know lot of ppl recommend learning on your own via project sure can do that but feel like ll have lot of gap in my knowledge and won have good grasp of the fundamental little about myself coming from sql analyst ish background in the finance and software consulting sector
how to prepare for data bricks apache,hi as the title says would you guys share some tips ve tried googling but didn learn much thanks,0,how to prepare for data brick apache,hi a the title say would you guy share some tip ve tried googling but didn learn much thanks
anyone ever work for sports franchise,found some job openings for data engineers in the mlb but none really for nfl or nba do understand the mlb though was the first pro franchise to take up making data driven decision making,0,anyone ever work for sport franchise,found some job opening for data engineer in the mlb but none really for nfl or nba do understand the mlb though wa the first pro franchise to take up making data driven decision making
what is etl really,sorry for the stupid question is this just fancy term for extracting data from sources doing some processing and then loading the data to the target system in my last project wrote sql queries that got me of the way there but had to do some processing in powershell because of the difference in datamodel of the target system the processing included remapping of data based on another source is this etl the reason asking is because there client who wants me to help them with an etl pipeline not sure can with good confidence say yes even though they are willing to let me learn on the job,0,what is etl really,sorry for the stupid question is this just fancy term for extracting data from source doing some processing and then loading the data to the target system in my last project wrote sql query that got me of the way there but had to do some processing in powershell because of the difference in datamodel of the target system the processing included remapping of data based on another source is this etl the reason asking is because there client who want me to help them with an etl pipeline not sure can with good confidence say yes even though they are willing to let me learn on the job
senior data engineer interview,currently interviewing for bank in the uk for senior python data engineering role got through the first round which was mostly around previous projects and other spark python related topics the next stage will be the technical stage and the heads up got from the recruiter was it will involve working on incomplete data science problems publicly available on github on google colab honestly wasn expecting this but immediately thought of polishing on pandas and numpy since dont use the data science libraries very often in my day to day but was curious if anyone here has had similar interview experience in the past and can provide any other insight would need to prepare for this thanks,0,senior data engineer interview,currently interviewing for bank in the uk for senior python data engineering role got through the first round which wa mostly around previous project and other spark python related topic the next stage will be the technical stage and the head up got from the recruiter wa it will involve working on incomplete data science problem publicly available on github on google colab honestly wasn expecting this but immediately thought of polishing on panda and numpy since dont use the data science library very often in my day to day but wa curious if anyone here ha had similar interview experience in the past and can provide any other insight would need to prepare for this thanks
amazon sde more superior than de,have been getting interview calls for amazon sde role very frequently and none to data engineer de positions why is there any one her who got an offer for de roles from amazon recently please share your interview experience and how you got call from recruiter,0,amazon sde more superior than de,have been getting interview call for amazon sde role very frequently and none to data engineer de position why is there any one her who got an offer for de role from amazon recently please share your interview experience and how you got call from recruiter
anybody want to lend me hand with options data analysis for hours pay is,dm for details,0,anybody want to lend me hand with option data analysis for hour pay is,dm for detail
have any of you actually run the tpcds v3 benchmark,finding it such pia to do this the tool from the official website is shitshow doesn build correctly the documentation is mixed across versions and refers to executables that don exist some things build with certain version of visual studio other things build under different version of visual studio trying to get it to work is just oof want to try recreate databricks benchmark but the two repositories they point me to tpcds kit and spark sql perf are both out of date and refer to v2 they also don seem to work out of the box ended up cloning from this repo because it was the closest thing to usable queries could find then read them in as string one by one and then run spark sql query which only creates the plan to actually time it did df spark sql query df collect but just the first query has been running for close to an hour on node cluster which doesn seem right how would you even time query in this situation anyone actually run this benchmark successfully or have anything reproducible,0,have any of you actually run the tpcds v3 benchmark,finding it such pia to do this the tool from the official website is shitshow doesn build correctly the documentation is mixed across version and refers to executables that don exist some thing build with certain version of visual studio other thing build under different version of visual studio trying to get it to work is just oof want to try recreate databricks benchmark but the two repository they point me to tpcds kit and spark sql perf are both out of date and refer to v2 they also don seem to work out of the box ended up cloning from this repo because it wa the closest thing to usable query could find then read them in a string one by one and then run spark sql query which only creates the plan to actually time it did df spark sql query df collect but just the first query ha been running for close to an hour on node cluster which doesn seem right how would you even time query in this situation anyone actually run this benchmark successfully or have anything reproducible
need help with my resume for data engineer open position,hi everyone this my first official reddit post and really looking to tap into the power of this wonderful reddit community that has helped me great deal in my de journey thus far ll get right to the point am applying for de position at company that interests me connected to an alumni of my current company who introduced me to the director of de at company and he wants to review my resume think getting anxious and having episodes of imposter syndrome as it is my first de resume review made by hiring manager am new in the de space and would appreciate your help in reviewing my resume against the job description your suggestions will be greatly appreciated thank you job description de resume ouid amp rtpof true amp sd true,0,need help with my resume for data engineer open position,hi everyone this my first official reddit post and really looking to tap into the power of this wonderful reddit community that ha helped me great deal in my de journey thus far ll get right to the point am applying for de position at company that interest me connected to an alumnus of my current company who introduced me to the director of de at company and he want to review my resume think getting anxious and having episode of imposter syndrome a it is my first de resume review made by hiring manager am new in the de space and would appreciate your help in reviewing my resume against the job description your suggestion will be greatly appreciated thank you job description de resume ouid amp rtpof true amp sd true
productionizing warehouse data,hey everyone just looking for little guidance on some issues that are coming up with how my org works with data ll preface all of this by saying full stack engineer with minimal data engineering experience so apologize if this is noob question we have variety of data coming in through different sources airflow jobs airbyte fivetran and it ends up in our snowflake warehouse finding that more often wanting to pull data out of the warehouse to reintegrate into existing api service databases is this smell should use the warehouse directly instead ve come across the concept of reverse etl but that seems more like services that sync data into third party services,0,productionizing warehouse data,hey everyone just looking for little guidance on some issue that are coming up with how my org work with data ll preface all of this by saying full stack engineer with minimal data engineering experience so apologize if this is noob question we have variety of data coming in through different source airflow job airbyte fivetran and it end up in our snowflake warehouse finding that more often wanting to pull data out of the warehouse to reintegrate into existing api service database is this smell should use the warehouse directly instead ve come across the concept of reverse etl but that seems more like service that sync data into third party service
learning how to schedule jobs in azkaban,hey everyone currently working as an analyst but have been asked to step up and help with process that formerly belonged to one of our data scientists this mostly involves maintaining existing scheduled processes in azkaban which know how to do however this person left the company without teaching me how to create new scheduled process as was ooo ve looked around for tutorials but am not yet as familiar with this side of things ve only had one course in big data where we learned about hadoop the rest of my masters was largely focused on statistical analytics and how to use python is there good example of how to create job file and how to schedule query appreciate any advice thanks,0,learning how to schedule job in azkaban,hey everyone currently working a an analyst but have been asked to step up and help with process that formerly belonged to one of our data scientist this mostly involves maintaining existing scheduled process in azkaban which know how to do however this person left the company without teaching me how to create new scheduled process a wa ooo ve looked around for tutorial but am not yet a familiar with this side of thing ve only had one course in big data where we learned about hadoop the rest of my master wa largely focused on statistical analytics and how to use python is there good example of how to create job file and how to schedule query appreciate any advice thanks
tyny dev mock data creation using rapid prototyping to speed up development time blog,tyny dev mock data creation using rapid prototyping to speed up development time blog,0,tyny dev mock data creation using rapid prototyping to speed up development time blog,tyny dev mock data creation using rapid prototyping to speed up development time blog
elt database in aws leaning towards aurora serverless postgresql anything to beware,sorry for the long post just trying to share all the relevant info for our issue at hand overview we are using python to ingest music data from across the web this includes handful of web scrapers as well as data ingestion from different data apis our data pipelines are batch and we are orchestrating things using self managed apache airflow run with docker on an ec2 instance the data ingested will be displayed in the front end of our mern stack app some additional info relative to how big data gets our data is not too big we anticipate the entire database to be gb in size over the next few years our data is relational in music data we have tables for artists songs albums etc songs belong to albums and artists and albums belong to artists we are following elt framework rather than etl and so there may be some transformations to the data after loading into the database all in all we do not have too many transformations and the ones we do have are fairly basic linear combinations of the data we are working within aws ecosystem and need to determine which production database to use here is where my mind is currently we need relational database so not any of the non relational databases so no to dynamodb no to elasticache we need to prioritize fast reads by our node api not fast analytical transformations it seems we need an application database not an analytical database so no to redshift it seems our remaining options are rds postgresql rds mysql rds aurora postgresql or mysql rds aurora serverless postgres or rds the decision between postgresql vs mysql seems like matter of preference and am fairly neutral on the two of these options assuming we go with postgresql am then stuck between rds postgresql rds aurora postgresql and rds aurora serverless postgresql the main full stack dev on our team is encouraging us to use rds aurora serverless postgresql my main question then are is there anything to beware with aurora serverless postgresql for elt is it challenging or doable to build data pipelines with python amp airflow that insert data into aurora serverless postgresql can we easily load data from an s3 bucket into aurora serverless postgresql how challenging is it to incorporate aurora serverless postgresql into our apache airflow project generally it seems like aurora serverless postgresql will be cheaper than the alternatives and am simply trying to figure out if the lower cost comes with additional complexities for building our data pipelines let me know if can share any additional info on this,0,elt database in aws leaning towards aurora serverless postgresql anything to beware,sorry for the long post just trying to share all the relevant info for our issue at hand overview we are using python to ingest music data from across the web this includes handful of web scraper a well a data ingestion from different data apis our data pipeline are batch and we are orchestrating thing using self managed apache airflow run with docker on an ec2 instance the data ingested will be displayed in the front end of our mern stack app some additional info relative to how big data get our data is not too big we anticipate the entire database to be gb in size over the next few year our data is relational in music data we have table for artist song album etc song belong to album and artist and album belong to artist we are following elt framework rather than etl and so there may be some transformation to the data after loading into the database all in all we do not have too many transformation and the one we do have are fairly basic linear combination of the data we are working within aws ecosystem and need to determine which production database to use here is where my mind is currently we need relational database so not any of the non relational database so no to dynamodb no to elasticache we need to prioritize fast read by our node api not fast analytical transformation it seems we need an application database not an analytical database so no to redshift it seems our remaining option are rds postgresql rds mysql rds aurora postgresql or mysql rds aurora serverless postgres or rds the decision between postgresql v mysql seems like matter of preference and am fairly neutral on the two of these option assuming we go with postgresql am then stuck between rds postgresql rds aurora postgresql and rds aurora serverless postgresql the main full stack dev on our team is encouraging u to use rds aurora serverless postgresql my main question then are is there anything to beware with aurora serverless postgresql for elt is it challenging or doable to build data pipeline with python amp airflow that insert data into aurora serverless postgresql can we easily load data from an s3 bucket into aurora serverless postgresql how challenging is it to incorporate aurora serverless postgresql into our apache airflow project generally it seems like aurora serverless postgresql will be cheaper than the alternative and am simply trying to figure out if the lower cost come with additional complexity for building our data pipeline let me know if can share any additional info on this
has anyone compiled all the data in the quarterly salary discussions,would be interesting to see trends in the data and also be able to filter on locations,0,ha anyone compiled all the data in the quarterly salary discussion,would be interesting to see trend in the data and also be able to filter on location
spark execution planning,hello junior de and ve been working with spark for some time but there one topic can get too information on and it is spark execution planning so trying to better understand the spark execution planning and find the articles on the internet quite imprecise think we re all familiar with this diagram amp x200b spark logical planning diagram format png amp auto webp amp c1b4df312c600a3cc0cf408c6edc2303ac702cca was wondering which components are involved in each of those phase parsing what component is responsible of parsing the dataframe dataset transformations into trees is it spark catalystparser analyzing to resolve the logical plan think the component involved here is the analyzer and it uses catalyst rules along with catalog to resolve the logical plan but want to know how is the catalog used here is it possible to interfere with the catalog and the analyzer mean inject some code to do further verification in order to resolve the logical plan optimization guess that here we only uses catalyst along with rule executors and sets to predefined rules like projection pruning etc to optimize the logical plan wrong is it possible to exclude some rules from the logical planning optimizations or write new sets of rules physical planning amp cost model the information found on some databricks blogs says that here we use catalyst rules and strategies to convert the logical plan into one or more physical plan how is that actually done why does the logical plan translated into multiple physical plan for example if the logical plan tree has join node will that node be translated into physical plan with sort merge join another one with broadcast join and so on and then the cost model will chose the most economic way to actually do that join code generation ve read somewhere that tungsten is used here to compile parts of queries to java bytecode but can really find how this is done can you explain me this part too would really appreciate your answers as this topic is kinda getting on my nerves haha every article found on the internet isn precise enough regards,0,spark execution planning,hello junior de and ve been working with spark for some time but there one topic can get too information on and it is spark execution planning so trying to better understand the spark execution planning and find the article on the internet quite imprecise think we re all familiar with this diagram amp x200b spark logical planning diagram format png amp auto webp amp c1b4df312c600a3cc0cf408c6edc2303ac702cca wa wondering which component are involved in each of those phase parsing what component is responsible of parsing the dataframe dataset transformation into tree is it spark catalystparser analyzing to resolve the logical plan think the component involved here is the analyzer and it us catalyst rule along with catalog to resolve the logical plan but want to know how is the catalog used here is it possible to interfere with the catalog and the analyzer mean inject some code to do further verification in order to resolve the logical plan optimization guess that here we only us catalyst along with rule executor and set to predefined rule like projection pruning etc to optimize the logical plan wrong is it possible to exclude some rule from the logical planning optimization or write new set of rule physical planning amp cost model the information found on some databricks blog say that here we use catalyst rule and strategy to convert the logical plan into one or more physical plan how is that actually done why doe the logical plan translated into multiple physical plan for example if the logical plan tree ha join node will that node be translated into physical plan with sort merge join another one with broadcast join and so on and then the cost model will chose the most economic way to actually do that join code generation ve read somewhere that tungsten is used here to compile part of query to java bytecode but can really find how this is done can you explain me this part too would really appreciate your answer a this topic is kinda getting on my nerve haha every article found on the internet isn precise enough regard
is data warehouse the correct place for combining storing private company financial statement data,consult for number of companies all owned at least in part by the same person the data income statement balance sheet etc is stored in excel files and various accounting softwares for each recently the discussion came up that it would be great to bring together all this information in single place even if just the key line items are available at first at first thought sqlite database with few dimensions like this would do the trick but can help but wonder if using the right tool for the job thinking about this project the right way this would be my first experience with schema design creation has anyone here had experience with project like this,0,is data warehouse the correct place for combining storing private company financial statement data,consult for number of company all owned at least in part by the same person the data income statement balance sheet etc is stored in excel file and various accounting software for each recently the discussion came up that it would be great to bring together all this information in single place even if just the key line item are available at first at first thought sqlite database with few dimension like this would do the trick but can help but wonder if using the right tool for the job thinking about this project the right way this would be my first experience with schema design creation ha anyone here had experience with project like this
facebook reputation is so bad the company must pay even more now to hire and retain talent some are calling it brand tax as tech workers fear black mark on their careers,facebook reputation is so bad the company must pay even more now to hire and retain talent some are calling it brand tax as tech workers fear black mark on their careers,0,facebook reputation is so bad the company must pay even more now to hire and retain talent some are calling it brand tax a tech worker fear black mark on their career,facebook reputation is so bad the company must pay even more now to hire and retain talent some are calling it brand tax a tech worker fear black mark on their career
database client for delta lake,are there any database clients that would work with delta lake like to just be able to view databases and tables don necessarily need the sql editor but could be nice know databaricks has data tab and has the new sql interface but have couple of different databases and wanted to just use one database client for high level viewing stuff table names etc wanted to use something that would behave like db visualizer squirrel db or jetbrain datagrip haven tried yet has anyone done this or have any suggestions,0,database client for delta lake,are there any database client that would work with delta lake like to just be able to view database and table don necessarily need the sql editor but could be nice know databaricks ha data tab and ha the new sql interface but have couple of different database and wanted to just use one database client for high level viewing stuff table name etc wanted to use something that would behave like db visualizer squirrel db or jetbrain datagrip haven tried yet ha anyone done this or have any suggestion
bigquery add column from column using python and spacy,hey have bigquery data warehouse containing all the data from mongodb database those data are sync once day would like to add column to one of my table that column is cleaned lemmatized version of another column the type is string can do that with dbt because need to use the python library spacy how could run such transformation on my table without having to get all the data locally and sending update on bigquery is there some gcp tools to run python function against bigquery like dataflow or something like that thanks for your help,0,bigquery add column from column using python and spacy,hey have bigquery data warehouse containing all the data from mongodb database those data are sync once day would like to add column to one of my table that column is cleaned lemmatized version of another column the type is string can do that with dbt because need to use the python library spacy how could run such transformation on my table without having to get all the data locally and sending update on bigquery is there some gcp tool to run python function against bigquery like dataflow or something like that thanks for your help
how to connect central fact table to datasets,hi question about database design am creating database that stores many datasets and have central fact table that just has the unique names of the datasets the problem is cannot find way to connect the actual datasets to the fact table for example have dataset unistudents that has the name imaginary unistudents but the actual unistudents dataset has columns student_id student_name and program is there logical way to connect them the central fact table and unistudents table could create fictional column in the unistudents dataset so it would be table unistudent with columns student_id student_name program and dataset_name with value imaginary unistudents but that would create rather redundant column for example dataset name with only one value imaginary unistudents everywhere is there better more logical way to do it,0,how to connect central fact table to datasets,hi question about database design am creating database that store many datasets and have central fact table that just ha the unique name of the datasets the problem is cannot find way to connect the actual datasets to the fact table for example have dataset unistudents that ha the name imaginary unistudents but the actual unistudents dataset ha column student_id student_name and program is there logical way to connect them the central fact table and unistudents table could create fictional column in the unistudents dataset so it would be table unistudent with column student_id student_name program and dataset_name with value imaginary unistudents but that would create rather redundant column for example dataset name with only one value imaginary unistudents everywhere is there better more logical way to do it
actuarial analyst interested in switching to de,hi an actuarial analyst looking to switch out of the actuarial field currently interested in data analyst positions but have this suspicion that data engineering what ultimately interested in and would to have the door to that career path open my skills include cleaning data and setting up simple ml models in and python can write advanced sql queries and have bit of experience with tableau what kind of skills responsibilities team dynamic should be looking for in data analyst position that will help me learn more and possibly transition into de role in the future are there any supplemental skills can pick up right now to help me land such data analyst position any tips would be appreciated amp x200b thanks,0,actuarial analyst interested in switching to de,hi an actuarial analyst looking to switch out of the actuarial field currently interested in data analyst position but have this suspicion that data engineering what ultimately interested in and would to have the door to that career path open my skill include cleaning data and setting up simple ml model in and python can write advanced sql query and have bit of experience with tableau what kind of skill responsibility team dynamic should be looking for in data analyst position that will help me learn more and possibly transition into de role in the future are there any supplemental skill can pick up right now to help me land such data analyst position any tip would be appreciated amp x200b thanks
best practices for nested json with pyspark specifically dynamic ways to create relational tables from nested arrays,like the title says doing super common task of pulling log data from an api in json format and looking to transform it into parquet files essentially one complex object in relational objects out ve got great process that takes any json you throw at it identifies arrays and flattens the data like to enhance it with an option to flatten or explode explode would take each child array and put them in their own dataframe but going round and round on the best way to implement keys surely this is an old problem anyone know any blogposts or boilerplate that deals with this model all the search results are heavily skewed towards recursive flattening so thought ask here,0,best practice for nested json with pyspark specifically dynamic way to create relational table from nested array,like the title say doing super common task of pulling log data from an api in json format and looking to transform it into parquet file essentially one complex object in relational object out ve got great process that take any json you throw at it identifies array and flattens the data like to enhance it with an option to flatten or explode explode would take each child array and put them in their own dataframe but going round and round on the best way to implement key surely this is an old problem anyone know any blogposts or boilerplate that deal with this model all the search result are heavily skewed towards recursive flattening so thought ask here
modern data stack is live on product hunt,mds is live on product hunt it big day for us we love to get your support and hear your feedback in the comments modern data stack platform for everything you need to know about the modern data stack companies amp categories shaping the modern data stack data stacks of the world top companies resources to get updates on the latest in this space jobs in data engineering amp more say hello to mds on product hunt,0,modern data stack is live on product hunt,md is live on product hunt it big day for u we love to get your support and hear your feedback in the comment modern data stack platform for everything you need to know about the modern data stack company amp category shaping the modern data stack data stack of the world top company resource to get update on the latest in this space job in data engineering amp more say hello to md on product hunt
need help with eer diagram if this isn allowed ill delete,this database is used to be able to persist train stations train station is uniquely identified by name ypres in addition for each station also unique coordinate pair is stored consisting of the latitude and longitude distinction is made between internal national and foreign stations in the sense that for foreign stations the national code of the country in which the station is located is stored fr for france and not for domestic stations for domestic stations the unique name is always displayed in dutch but may be several translations of this name this does not apply to foreign stations translation of station name logically consists of the translation as well as from the country code of the language to which the translation belongs ypres is the fr english translation of ypres for each country code maximum of times translation the nmbs also has different types of trains available with unique generic name ic bus each of which refers to exactly one category of connection hear high speed regional these train types can be placed on several routes each trajectory has unique name and is linked to all train types that travel this route for example the trajectory with name blankenberge gent sint pieters linked to the train types with name ic and it trajectory is carried out one or more times day the implementation of route in which train of certain type stops at fixed sequence stations stops at fixed sequence of times is called trip every first of all trip has unique code maximum capacity of travelers that can take place on this trip and collection of dates on which these trip is performed in addition it is necessary to save before every trip at what time regardless of the dates train arrives and departs specific stop with the exception of the first stop which has no arrival information matie and the last stop that has no departure information of course you can trains only depart from stop when they have arrived there first additionally in this case it is necessary to indicate whether this is an arrival and departure time falls on the day of the original departure of the train before midnight or on the day after the original departure of the train after midnight like train arrives at stop after midnight it also departs after midnight conversely train will certainly arrive at stop before midnight if it is there too leaves before midnight because the departure and arrival times are not necessarily unique for trip but the order of the stations must be known every stop where train is stops unique stop number per trip with the first stop being numbered the order of the stop numbers per trip should of course be consistent with the arrival and departure times of the same trip this means that the arrival time of train at stop must be same or later than the departure time at stops with lower stop number and the reverse should also apply for ie other stop on trip for which departure information is known all stops except the latter an expected passenger occupancy should also be stored of course this expected occupation may never exceed the maximum capacity of the trip exceed amp x200b my third iteration of my eer diagram format png amp auto webp amp dce5faf0a9492924c8398483928dd3ff605fa2,0,need help with eer diagram if this isn allowed ill delete,this database is used to be able to persist train station train station is uniquely identified by name ypres in addition for each station also unique coordinate pair is stored consisting of the latitude and longitude distinction is made between internal national and foreign station in the sense that for foreign station the national code of the country in which the station is located is stored fr for france and not for domestic station for domestic station the unique name is always displayed in dutch but may be several translation of this name this doe not apply to foreign station translation of station name logically consists of the translation a well a from the country code of the language to which the translation belongs ypres is the fr english translation of ypres for each country code maximum of time translation the nmbs also ha different type of train available with unique generic name ic bus each of which refers to exactly one category of connection hear high speed regional these train type can be placed on several route each trajectory ha unique name and is linked to all train type that travel this route for example the trajectory with name blankenberge gent sint pieters linked to the train type with name ic and it trajectory is carried out one or more time day the implementation of route in which train of certain type stop at fixed sequence station stop at fixed sequence of time is called trip every first of all trip ha unique code maximum capacity of traveler that can take place on this trip and collection of date on which these trip is performed in addition it is necessary to save before every trip at what time regardless of the date train arrives and departs specific stop with the exception of the first stop which ha no arrival information matie and the last stop that ha no departure information of course you can train only depart from stop when they have arrived there first additionally in this case it is necessary to indicate whether this is an arrival and departure time fall on the day of the original departure of the train before midnight or on the day after the original departure of the train after midnight like train arrives at stop after midnight it also departs after midnight conversely train will certainly arrive at stop before midnight if it is there too leaf before midnight because the departure and arrival time are not necessarily unique for trip but the order of the station must be known every stop where train is stop unique stop number per trip with the first stop being numbered the order of the stop number per trip should of course be consistent with the arrival and departure time of the same trip this mean that the arrival time of train at stop must be same or later than the departure time at stop with lower stop number and the reverse should also apply for ie other stop on trip for which departure information is known all stop except the latter an expected passenger occupancy should also be stored of course this expected occupation may never exceed the maximum capacity of the trip exceed amp x200b my third iteration of my eer diagram format png amp auto webp amp dce5faf0a9492924c8398483928dd3ff605fa2
way to practice sql to be aligned with real work,cheer all will start an analyst job next month the job is expected to do lots of sql would love to practice advanced sql found lots of sql resources but afraid it like the exercies homework which is not really close to real work scenario was thinking also to pay leetcode premium to practice sql not sure it really worthy anyone knows any resources to practce real work level of sql thanks,0,way to practice sql to be aligned with real work,cheer all will start an analyst job next month the job is expected to do lot of sql would love to practice advanced sql found lot of sql resource but afraid it like the exercies homework which is not really close to real work scenario wa thinking also to pay leetcode premium to practice sql not sure it really worthy anyone know any resource to practce real work level of sql thanks
any data engineers here who work in government public sector,know there are lots of software engineers and data analysts in government but what about data engineers if you are data engineer in government or public sector what has your experience been like,0,any data engineer here who work in government public sector,know there are lot of software engineer and data analyst in government but what about data engineer if you are data engineer in government or public sector what ha your experience been like
we ve all done it at some point,we ve all done it at some point,0,we ve all done it at some point,we ve all done it at some point
de course,any one tried andreas kretz data engineering courses from any reviews on that,0,de course,any one tried andreas kretz data engineering course from any review on that
download schemas and tables from teradata using python,hi guys facing challenge and would be great if you can share your input for this want to export schema or tables ddl dml sql files from teradata to local system using python what should be the best approach for this process is there any scripter available for teradata unlike mssql scripter if so can you please share the link for the same also is there any website course blog anything for learning teradata using python if so please share the link it would be of great help python highlightedupdateurns urn ali aactivity a6878192997729734656 python3 highlightedupdateurns urn ali aactivity a6878192997729734656 teradata highlightedupdateurns urn ali aactivity a6878192997729734656 datamigration highlightedupdateurns urn ali aactivity a6878192997729734656,0,download schema and table from teradata using python,hi guy facing challenge and would be great if you can share your input for this want to export schema or table ddl dml sql file from teradata to local system using python what should be the best approach for this process is there any scripter available for teradata unlike mssql scripter if so can you please share the link for the same also is there any website course blog anything for learning teradata using python if so please share the link it would be of great help python highlightedupdateurns urn ali aactivity a6878192997729734656 python3 highlightedupdateurns urn ali aactivity a6878192997729734656 teradata highlightedupdateurns urn ali aactivity a6878192997729734656 datamigration highlightedupdateurns urn ali aactivity a6878192997729734656
operational data in lake for data governance purpose how common,new data in data lake are commonly incorporated after analyst request like to ask opinion here what are the implications if we turn the tables and set that all operational data must land copy to lake at limited frequency year for data governance purpose extension to that is all business glossary must be updated here onwards,0,operational data in lake for data governance purpose how common,new data in data lake are commonly incorporated after analyst request like to ask opinion here what are the implication if we turn the table and set that all operational data must land copy to lake at limited frequency year for data governance purpose extension to that is all business glossary must be updated here onwards
ex boston consulting group amp google manager reveals hiring processes required skills amp experiences,ex boston consulting group amp google manager reveals hiring processes required skills amp experiences,0,ex boston consulting group amp google manager reveals hiring process required skill amp experience,ex boston consulting group amp google manager reveals hiring process required skill amp experience
schemaless data store within your sql database,schemaless data store within your sql database,0,schemaless data store within your sql database,schemaless data store within your sql database
have you ever had predecessor who impressed you or at least did decent job in your eyes,asking because it seems common to hear the story where guy takes solo gig and has to fix mess fire wondering if anyone here has taken over role and actually been impressed by system the previous person built if so what did your predecessor do that you appreciated,0,have you ever had predecessor who impressed you or at least did decent job in your eye,asking because it seems common to hear the story where guy take solo gig and ha to fix mess fire wondering if anyone here ha taken over role and actually been impressed by system the previous person built if so what did your predecessor do that you appreciated
looking for career advice data engineer to finance insight analyst,currently data engineer at bank in australia got yrs of experience and primarily work on mssql stack with the end goal of producing automated reports systems performance data set not related to the business years exp includes years of working as data analyst and as data engineer which comprised of quite bit of managing migration project try to learn and implement different things out of my own interest for eg implement python scripts instead of ssis for etl tableau for reporting etc recently realised that there is not much opportunity for growth technical or otherwise in my current role have been eyeing an financial insight analyst role and wondering if that is good career move from the level of data engineering at or should be looking at other data engineering roles which utilise spark python etc,0,looking for career advice data engineer to finance insight analyst,currently data engineer at bank in australia got yr of experience and primarily work on mssql stack with the end goal of producing automated report system performance data set not related to the business year exp includes year of working a data analyst and a data engineer which comprised of quite bit of managing migration project try to learn and implement different thing out of my own interest for eg implement python script instead of ssis for etl tableau for reporting etc recently realised that there is not much opportunity for growth technical or otherwise in my current role have been eyeing an financial insight analyst role and wondering if that is good career move from the level of data engineering at or should be looking at other data engineering role which utilise spark python etc
secret society,what if there is secret society for data engineers which can allow its members to share how their companies are tackling their data problems which can help other members to use those approaches in their careers will you join and trust other members with confidential information,0,secret society,what if there is secret society for data engineer which can allow it member to share how their company are tackling their data problem which can help other member to use those approach in their career will you join and trust other member with confidential information
thoughts on talend,any talend users that can comment on the capabilities around data governance dictionary etl etc does it work well with large datasets does it support cdc how much does it cost,0,thought on talend,any talend user that can comment on the capability around data governance dictionary etl etc doe it work well with large datasets doe it support cdc how much doe it cost
staying fresh with freshness tables,staying fresh with freshness tables,0,staying fresh with freshness table,staying fresh with freshness table
joining two snapshot tables,hi hoping for some guidance as this has me puzzled we have two or more tables that need to be joined there is primary key easy enough however both or snapshots dbt snapshot if it helps with valid_from and valid_to timestamps they update at different frequencies how do join these into one wider table making unified snapshot table taking into account the valid dates from each of the tables hope that makes sense dbt guides say to snapshot first rather than downstream at the dimension table level happy to go with that but they fail to mention how to actually do it,0,joining two snapshot table,hi hoping for some guidance a this ha me puzzled we have two or more table that need to be joined there is primary key easy enough however both or snapshot dbt snapshot if it help with valid_from and valid_to timestamps they update at different frequency how do join these into one wider table making unified snapshot table taking into account the valid date from each of the table hope that make sense dbt guide say to snapshot first rather than downstream at the dimension table level happy to go with that but they fail to mention how to actually do it
tyny dev tracking ui why tyny dev has the best command of ui on the market blog,tyny dev tracking ui why tyny dev has the best command of ui on the market blog,0,tyny dev tracking ui why tyny dev ha the best command of ui on the market blog,tyny dev tracking ui why tyny dev ha the best command of ui on the market blog
what is the best way to practice transformations,hi am looking for some practice material practice projects and the likes to practice transformations etl with spark sql and pandas am looking for something to practice the data engineering analytics side of things how did you practice transformations,0,what is the best way to practice transformation,hi am looking for some practice material practice project and the like to practice transformation etl with spark sql and panda am looking for something to practice the data engineering analytics side of thing how did you practice transformation
what did you guys wish you knew before implementing everything for your company,so in charge of the dev dataengineering devops for our startup and it daunting how many things need to be done what did you wish you knew before building data pipelines and choosing datawarehouses data lakes delta lakes etc thanks guys,0,what did you guy wish you knew before implementing everything for your company,so in charge of the dev dataengineering devops for our startup and it daunting how many thing need to be done what did you wish you knew before building data pipeline and choosing datawarehouses data lake delta lake etc thanks guy
airflow orchestration only or transformation as well,hey peeps reading lot about how cool airflow is to manage your pipeline dependencies and scheduling can someone please clarify if airflow can be used as full fledged transformation tool as well the in elt if my flow is postgres s3 snowflake how best can use airflow,0,airflow orchestration only or transformation a well,hey peep reading lot about how cool airflow is to manage your pipeline dependency and scheduling can someone please clarify if airflow can be used a full fledged transformation tool a well the in elt if my flow is postgres s3 snowflake how best can use airflow
epidemiologist seeking advice on where improvements can be made in workflow,as disclaimer am in hybrid role where am an epidemiologist programmer and work on several projects that would describe as data engineering am reaching out because think there is lot of room for potential process improvement or technologies our team should be taking advantage of but would like your advice on which to prioritize our company runs clinical trials and the current workflow requires the generation of dozens of sas7bdat datasets for each study that are required to be sent to the fda so let say we have ongoing trials we may notice that there are handful of common lab parameters of interest between these studies and we want to build one application that can visualize these lab parameters for these studies then scale up so future studies and teams can also take advantage of this application and reduce the duplication of efforts our company primarily uses sas and but we have the freedom to use python too for the project as described above we will work with study teams to develop common data model that can be read in our visualization app typically developed in rshiny we develop program which can read in all the sas datasets for particular study and transform the input data into the common data model we agreed upon this updated dataset can automatically be read by the visualization app however one major bottle neck is the process from sas dataset to common data model is not automated we have an program that can transform the data quickly but the study teams email us when their source data is refreshed and we manually go and run the program when we have time can you please recommend something to explore here or workflow we should consider due to the current state of the business for the foreseeable future we will always have sas7bdat datasets as the source data our job will be to transform this for various purposes develop common data models and overall develop pipelines from source data from different sources to either dataset or app provided an easy example we have much more complex work which involves pulling data from electronic health records of insurance claims data that is raw messy we will then either send this transformed data to data sciences teams for analysis or we will develop dashboard app to visualize the information ourselves think our team is behind the curve and our processes are overly time consuming and crude if you have any suggestions whatsoever please let me know thank you,0,epidemiologist seeking advice on where improvement can be made in workflow,a disclaimer am in hybrid role where am an epidemiologist programmer and work on several project that would describe a data engineering am reaching out because think there is lot of room for potential process improvement or technology our team should be taking advantage of but would like your advice on which to prioritize our company run clinical trial and the current workflow requires the generation of dozen of sas7bdat datasets for each study that are required to be sent to the fda so let say we have ongoing trial we may notice that there are handful of common lab parameter of interest between these study and we want to build one application that can visualize these lab parameter for these study then scale up so future study and team can also take advantage of this application and reduce the duplication of effort our company primarily us sa and but we have the freedom to use python too for the project a described above we will work with study team to develop common data model that can be read in our visualization app typically developed in rshiny we develop program which can read in all the sa datasets for particular study and transform the input data into the common data model we agreed upon this updated dataset can automatically be read by the visualization app however one major bottle neck is the process from sa dataset to common data model is not automated we have an program that can transform the data quickly but the study team email u when their source data is refreshed and we manually go and run the program when we have time can you please recommend something to explore here or workflow we should consider due to the current state of the business for the foreseeable future we will always have sas7bdat datasets a the source data our job will be to transform this for various purpose develop common data model and overall develop pipeline from source data from different source to either dataset or app provided an easy example we have much more complex work which involves pulling data from electronic health record of insurance claim data that is raw messy we will then either send this transformed data to data science team for analysis or we will develop dashboard app to visualize the information ourselves think our team is behind the curve and our process are overly time consuming and crude if you have any suggestion whatsoever please let me know thank you
is anyone willing to have conversation or help me get started on path to become data engineer,work as an it support tech and want to make transition have the basics of sql and python down would greatly appreciate it,0,is anyone willing to have conversation or help me get started on path to become data engineer,work a an it support tech and want to make transition have the basic of sql and python down would greatly appreciate it
seven reading suggestions for the holidays on distributed systems databases,seven reading suggestions for the holidays on distributed systems databases,0,seven reading suggestion for the holiday on distributed system database,seven reading suggestion for the holiday on distributed system database
the guide to data versioning,the guide to data versioning,0,the guide to data versioning,the guide to data versioning
homomorphic encryption is cryptographic method that returns an encrypted result to the data owner essentially this enables third parties to process encrypted data while having no knowledge about the data or the results,homomorphic encryption is cryptographic method that returns an encrypted result to the data owner essentially this enables third parties to process encrypted data while having no knowledge about the data or the results,0,homomorphic encryption is cryptographic method that return an encrypted result to the data owner essentially this enables third party to process encrypted data while having no knowledge about the data or the result,homomorphic encryption is cryptographic method that return an encrypted result to the data owner essentially this enables third party to process encrypted data while having no knowledge about the data or the result
branching into data centre engineering,this might be silly post but looking into trying for career in data centre engineering preferably through an apprenticeship or on the job training as learn best hands on does anybody have any company or course recommendations coming from background of no formal degree and no formal experience in tech beyond tech support call centre job held for few months in while was at technical school didn end up following through with the cert so pretty useless lol if not any advice on getting into the field helps thanks in advance,0,branching into data centre engineering,this might be silly post but looking into trying for career in data centre engineering preferably through an apprenticeship or on the job training a learn best hand on doe anybody have any company or course recommendation coming from background of no formal degree and no formal experience in tech beyond tech support call centre job held for few month in while wa at technical school didn end up following through with the cert so pretty useless lol if not any advice on getting into the field help thanks in advance
quantum computers in data engineering,hi fellow data engineers do you see opportunities to learn some new quantum computing technology language that would be relevant for data engineering this technology could be in it infancy my vision would be slowly build up portfolio on the intersection of quantum tech data engineering and be prepared for potential boom that could happen in maybe years,0,quantum computer in data engineering,hi fellow data engineer do you see opportunity to learn some new quantum computing technology language that would be relevant for data engineering this technology could be in it infancy my vision would be slowly build up portfolio on the intersection of quantum tech data engineering and be prepared for potential boom that could happen in maybe year
data warehousing course recommendation needed,does anyone know of an online course that teaches all the steps of building data warehouse using real examples ve taken one on udemy that gives high level overview of the concepts but would like to find one that actually makes the students build one on their own,0,data warehousing course recommendation needed,doe anyone know of an online course that teach all the step of building data warehouse using real example ve taken one on udemy that give high level overview of the concept but would like to find one that actually make the student build one on their own
poll de vs mle careers,being data engineer in big team comprising of data engineers machine learning engineers amp data scientists can help but wonder which profile will actually prevail in years since all these profiles require similar if not same skillset so if you were to choose one of these which one do you think will stay in demand data engineer with complimentary mle skills or machine learning engineer with data engineering complimentary skills,0,poll de v mle career,being data engineer in big team comprising of data engineer machine learning engineer amp data scientist can help but wonder which profile will actually prevail in year since all these profile require similar if not same skillset so if you were to choose one of these which one do you think will stay in demand data engineer with complimentary mle skill or machine learning engineer with data engineering complimentary skill
just got onto data and analytics graduate scheme any tips,so ve just secured grad job at jlr however come from background in chemistry so don have much knowledge in data engineering which is key part off the scheme obviously not expected to have lots of prior knowledge and ll learn along the way but love to get bit ahead before starting if anyone has any good resources or tips on where to get started that would be great there so much to learn so don know where to start,0,just got onto data and analytics graduate scheme any tip,so ve just secured grad job at jlr however come from background in chemistry so don have much knowledge in data engineering which is key part off the scheme obviously not expected to have lot of prior knowledge and ll learn along the way but love to get bit ahead before starting if anyone ha any good resource or tip on where to get started that would be great there so much to learn so don know where to start
contradicting information on the spark catalyst optimizer,hey all this is question on how the spark catalyst optimizer works and which info to believe the databricks blog and the definitive guide say one optimized logical plan is created and then multiple physical plans are created and then they are chosen based on the cost attached to it learning spark says there is one selected logical plan which is chosen based on the costs attached by the cost based optimizer and then single physical plan is created out of it now am not sure which one to consider the correct thanks in advance,0,contradicting information on the spark catalyst optimizer,hey all this is question on how the spark catalyst optimizer work and which info to believe the databricks blog and the definitive guide say one optimized logical plan is created and then multiple physical plan are created and then they are chosen based on the cost attached to it learning spark say there is one selected logical plan which is chosen based on the cost attached by the cost based optimizer and then single physical plan is created out of it now am not sure which one to consider the correct thanks in advance
how should good data processing application for data warehouse based on python and pyspark be made,how should be made,0,how should good data processing application for data warehouse based on python and pyspark be made,how should be made
will learning informatica hurt me greatly,currently in my first job out of college in de swe role at large private university in my state in my department team there are developers and data architect who work with we mainly do in house work for other teams in the department helping build pipelines for data warehouses from whatever data sources they have we do everything etl related in informatica sometimes when we work with apis we use python mainly as scripting language to format the api responses but that it ve been here about months and really enjoyed it just hear lot about airflow and writing pipelines in python feel like might be stunting my growth the good news is that the entire data engineering architecture team is me and other guy who has been here about years wondering if it might be feasible to switch or something down the line not sure any advice is great,0,will learning informatica hurt me greatly,currently in my first job out of college in de swe role at large private university in my state in my department team there are developer and data architect who work with we mainly do in house work for other team in the department helping build pipeline for data warehouse from whatever data source they have we do everything etl related in informatica sometimes when we work with apis we use python mainly a scripting language to format the api response but that it ve been here about month and really enjoyed it just hear lot about airflow and writing pipeline in python feel like might be stunting my growth the good news is that the entire data engineering architecture team is me and other guy who ha been here about year wondering if it might be feasible to switch or something down the line not sure any advice is great
performance testing postgres inserts with python,performance testing postgres inserts with python,0,performance testing postgres insert with python,performance testing postgres insert with python
yes another airflow managed vs prefect cloud post,used small airflow deployment to orchestrate pipelines for small subset of data science projects that had relatively simple dag structures no data needed to be passed from one task to another building out new pipelines now at different company torn because airflow is by far the most popular and has way more native integrations and way easier to troubleshoot by virtue of their just being larger community that has already had similar issues my choice is between managed airflow like astronomer or prefect cloud prefect at least from skimming the docs seems like its less intrusive airflow does sometimes feel like writing airflow code whereas prefect seems like simply adding some decorators and largely it stays out of the way anyone have deep experience with both managed airflow and prefect cloud or anyone moving from airflow to prefect or vice versa prefect sounds great and has of the integrations need but don want to pick the new and shiny thing and then see it not mature into the ecosystem well airflow feels like the default and is easier to hire for but people on here in previous threads have said even at teams of people it was nightmare to manage amp x200b if you have used both or are migrating from one to the other please share your experiences or dm me can potentially pay sr data engineer for consulting on this matter,0,yes another airflow managed v prefect cloud post,used small airflow deployment to orchestrate pipeline for small subset of data science project that had relatively simple dag structure no data needed to be passed from one task to another building out new pipeline now at different company torn because airflow is by far the most popular and ha way more native integration and way easier to troubleshoot by virtue of their just being larger community that ha already had similar issue my choice is between managed airflow like astronomer or prefect cloud prefect at least from skimming the doc seems like it le intrusive airflow doe sometimes feel like writing airflow code whereas prefect seems like simply adding some decorator and largely it stay out of the way anyone have deep experience with both managed airflow and prefect cloud or anyone moving from airflow to prefect or vice versa prefect sound great and ha of the integration need but don want to pick the new and shiny thing and then see it not mature into the ecosystem well airflow feel like the default and is easier to hire for but people on here in previous thread have said even at team of people it wa nightmare to manage amp x200b if you have used both or are migrating from one to the other please share your experience or dm me can potentially pay sr data engineer for consulting on this matter
tools or frameworks to simply trigger python functions and view data via ui,hi engineers am looking for way to trigger different python functions with simple ui the background is that have implemented an engine for the commission calculation for the employees in our company with python there are different functions like show the employee revenue or kpi achievement calculate payout for single employee or for group of employees to hand over the execution part to hr am looking for simple way to trigger these functions from ui webui and show the results there can you help ne finding out which tools or frameworks could use for that currently the calculation takes place in secured vm to have web ui on the localhost on the vm would be fine thank you,0,tool or framework to simply trigger python function and view data via ui,hi engineer am looking for way to trigger different python function with simple ui the background is that have implemented an engine for the commission calculation for the employee in our company with python there are different function like show the employee revenue or kpi achievement calculate payout for single employee or for group of employee to hand over the execution part to hr am looking for simple way to trigger these function from ui webui and show the result there can you help ne finding out which tool or framework could use for that currently the calculation take place in secured vm to have web ui on the localhost on the vm would be fine thank you
why use segment instead of just doing it manually,hello am pretty junior in data engineering and would like to use twilio segment within my company early stage startup to start collecting usage data one of the po told me the devs from the web team already created few functions to log some of the users actions and insert those info into the product db unique table with timestamp user id and action for me it not good thing to mix usage data and data used to make the product work on the same db but now am wondering what are the reasons to use solution like segment instead of just creating some backend functions to log what the users are doing thanks for your help,0,why use segment instead of just doing it manually,hello am pretty junior in data engineering and would like to use twilio segment within my company early stage startup to start collecting usage data one of the po told me the devs from the web team already created few function to log some of the user action and insert those info into the product db unique table with timestamp user id and action for me it not good thing to mix usage data and data used to make the product work on the same db but now am wondering what are the reason to use solution like segment instead of just creating some backend function to log what the user are doing thanks for your help
data warehouse normalized vs denormalized fact table measure type dimension,designing data warehouse to hold survey data it will hold various types of surveys each with different questions and answers trying to implement conventional star schema but have some doubts regarding the fact table the two options are to go for what kimball calls measure type dimension essentially the normalized version with column containing the questions and another the answers or fully denormalize the table and have column for each question populated with respective answers if go for the latter approach might eventually end up with up to fact tables was thinking one per survey type would be easier to manage in this case each having columns vs single fact for the first the way see it the first approach has the main advantage that it easier to maintain and has fixed schema even if new questions are added while the second would be better for validating the data connecting to olap and visualisation tools and general and would make it lot easier to aggregate since each measure would have its own column lastly the first with be tall and dense while the second approach would be very wide and sparse measure or value per row at the limit if you have any experience or have been burned by this in the past please share your thoughts which one would you go for and why,0,data warehouse normalized v denormalized fact table measure type dimension,designing data warehouse to hold survey data it will hold various type of survey each with different question and answer trying to implement conventional star schema but have some doubt regarding the fact table the two option are to go for what kimball call measure type dimension essentially the normalized version with column containing the question and another the answer or fully denormalize the table and have column for each question populated with respective answer if go for the latter approach might eventually end up with up to fact table wa thinking one per survey type would be easier to manage in this case each having column v single fact for the first the way see it the first approach ha the main advantage that it easier to maintain and ha fixed schema even if new question are added while the second would be better for validating the data connecting to olap and visualisation tool and general and would make it lot easier to aggregate since each measure would have it own column lastly the first with be tall and dense while the second approach would be very wide and sparse measure or value per row at the limit if you have any experience or have been burned by this in the past please share your thought which one would you go for and why
python etl tools for big data,from what can infer you guys all like python simple flexible and gentle learning curve so what tools would you recommend to someone to use for job as data engineer,0,python etl tool for big data,from what can infer you guy all like python simple flexible and gentle learning curve so what tool would you recommend to someone to use for job a data engineer
what is kubernetes used for in data engineering,im curious as do why people use kubernetes in this field,0,what is kubernetes used for in data engineering,im curious a do why people use kubernetes in this field
data architect with snowflake,how essential is data architect in snowflake environment with complex relationships does snowflake have data model deployment and management options,0,data architect with snowflake,how essential is data architect in snowflake environment with complex relationship doe snowflake have data model deployment and management option
working as an etl developer using informatica and teradata want to move to big data can anyone help which courses should take or any roadmap to follow,am working as data engineer data integrations for the last year am already familiar with data warehousing etl batch processing etc with fair bit of experience with sql and python,0,working a an etl developer using informatica and teradata want to move to big data can anyone help which course should take or any roadmap to follow,am working a data engineer data integration for the last year am already familiar with data warehousing etl batch processing etc with fair bit of experience with sql and python
mis student junior interested in data engineering career,what are the main skills will need to land position after graduation,0,mi student junior interested in data engineering career,what are the main skill will need to land position after graduation
resources for first job,starting my first salaried job in the new year and like some guidance on what resources can use to help speed along the learning process for background ll be the sole data person on finance team at mid size company right now the team is excel only except for small python script to automate some calculations two excel files arrive everyday in the morning and from there they use those files for their financial trading they want to build more scalable system for their financial analysts their idea now is to build database or database system in sql server to store those two excel files and allow the financial analysts to query those pretty well versed with python setting up small databases and database theory undergrad in cs masters in data science but struggling to find good resource that explains how the aforementioned system should be setup know lot of requirements will come when start talking to the team and the it department but want some background knowledge before diving in any resources you think could be helpful for this project would be much appreciated,0,resource for first job,starting my first salaried job in the new year and like some guidance on what resource can use to help speed along the learning process for background ll be the sole data person on finance team at mid size company right now the team is excel only except for small python script to automate some calculation two excel file arrive everyday in the morning and from there they use those file for their financial trading they want to build more scalable system for their financial analyst their idea now is to build database or database system in sql server to store those two excel file and allow the financial analyst to query those pretty well versed with python setting up small database and database theory undergrad in c master in data science but struggling to find good resource that explains how the aforementioned system should be setup know lot of requirement will come when start talking to the team and the it department but want some background knowledge before diving in any resource you think could be helpful for this project would be much appreciated
am data engineer,hello in my job code spark job in scala extract data transform data load data code back end for our application in java spring app to let user query extract data easily set up tool wich is like data ware house juste modify conf file or sql creation table scripts when have to add dimension fact am data engineer or software engineer,0,am data engineer,hello in my job code spark job in scala extract data transform data load data code back end for our application in java spring app to let user query extract data easily set up tool wich is like data ware house juste modify conf file or sql creation table script when have to add dimension fact am data engineer or software engineer
recommendations on process extract data from csv file on url,hi all am learning data engineering and have the first interview with technical challenge amp x200b have to extract the data from csv file from url have to skip the duplicate rows and add the updated date and automate this process so that it runs every week amp x200b the only restriction have regarding programming languages or software to use is to not use sql agent have basic idea outlined in mind was planning to use python for the whole process and linux for automation amp x200b obviously am not looking for solutions to the problem but was open to listening to recommendations advice where to look examples of similar problems in terms of languages libraries frameworks methods and other types of considerations so that it is neat efficient and professional starting from what you tell me or mention start investigating and research feel bit lost in the generation of the logs of the process amp x200b any helpful comments or advice are welcome,0,recommendation on process extract data from csv file on url,hi all am learning data engineering and have the first interview with technical challenge amp x200b have to extract the data from csv file from url have to skip the duplicate row and add the updated date and automate this process so that it run every week amp x200b the only restriction have regarding programming language or software to use is to not use sql agent have basic idea outlined in mind wa planning to use python for the whole process and linux for automation amp x200b obviously am not looking for solution to the problem but wa open to listening to recommendation advice where to look example of similar problem in term of language library framework method and other type of consideration so that it is neat efficient and professional starting from what you tell me or mention start investigating and research feel bit lost in the generation of the log of the process amp x200b any helpful comment or advice are welcome
create an alert on table change for postgres in cloud sql,looking for recommendations on ways could set up some kind of trigger function that will send me an alert when table schema has changed in my database fairly limited in the extra tooling can use since the database in question is hosted on cloud sql so as far as know creating function in pl python is out of the question my ideal solution would be trigger function that pings pubsub topic which can then create alerts from but really it could be as as simple as sending an email,0,create an alert on table change for postgres in cloud sql,looking for recommendation on way could set up some kind of trigger function that will send me an alert when table schema ha changed in my database fairly limited in the extra tooling can use since the database in question is hosted on cloud sql so a far a know creating function in pl python is out of the question my ideal solution would be trigger function that ping pubsub topic which can then create alert from but really it could be a a simple a sending an email
best resources to learn dbt using open tools like postgres and airflow,work for company in division that is not it or data management but do use ssis and am pretty familiar with it like to learn dbt but most training resources rely on dbt cloud snowflake etc won really have access funds or it approval to those things until can prove that dbt is somehow useful to my division have some of you used tools like postgres airflow etc how did you learn to do this have to stick to cli for background have zero experience in dbt snowflake airflow etc but use sql server ssis and excel everyday at my job in one way or another thanks,0,best resource to learn dbt using open tool like postgres and airflow,work for company in division that is not it or data management but do use ssis and am pretty familiar with it like to learn dbt but most training resource rely on dbt cloud snowflake etc won really have access fund or it approval to those thing until can prove that dbt is somehow useful to my division have some of you used tool like postgres airflow etc how did you learn to do this have to stick to cli for background have zero experience in dbt snowflake airflow etc but use sql server ssis and excel everyday at my job in one way or another thanks
what do you guys think about etl platforms vs roll your own,just trying to gauge general opinion on the pros and cons of using dedicated etl tool like fivetran airbyte etc,0,what do you guy think about etl platform v roll your own,just trying to gauge general opinion on the pro and con of using dedicated etl tool like fivetran airbyte etc
do you unit test your etl pipelines,as title suggests do you write unit tests to sense check if units of your etl jobs perform just as expected view poll,0,do you unit test your etl pipeline,a title suggests do you write unit test to sense check if unit of your etl job perform just a expected view poll
analyst to engineer project and skills feedback,currently working as da with with in another semi related data job at nasa but realizing really enjoy the building and programming side of data as opposed to the analytical and business side my current position consists of building dashboards automating reporting building very small scale and crude as there aren really quality standards on our bi team data extraction and manipulation processes all of this is done via and sql server with rcron for scheduling additional skills basic python but pretty confident can pick it up pretty quickly with my and java experience semester just finished of java oop also in grad school part time for data science ms looking to create small scale personal project to learn some basic de skills and hopefully present it on resume with there being so many de tools being thrown around like to start simple thinking docker airflow for scheduling with python some sort of local sql instance for processing and storing data would this be feasible and or enough to supplement my current resume,0,analyst to engineer project and skill feedback,currently working a da with with in another semi related data job at nasa but realizing really enjoy the building and programming side of data a opposed to the analytical and business side my current position consists of building dashboard automating reporting building very small scale and crude a there aren really quality standard on our bi team data extraction and manipulation process all of this is done via and sql server with rcron for scheduling additional skill basic python but pretty confident can pick it up pretty quickly with my and java experience semester just finished of java oop also in grad school part time for data science m looking to create small scale personal project to learn some basic de skill and hopefully present it on resume with there being so many de tool being thrown around like to start simple thinking docker airflow for scheduling with python some sort of local sql instance for processing and storing data would this be feasible and or enough to supplement my current resume
what does landing some data or process mean,does that just mean it completed successfully never heard this jargon before and now its everywhere at work reminds me of how rock climbers are always saying sent it,0,what doe landing some data or process mean,doe that just mean it completed successfully never heard this jargon before and now it everywhere at work reminds me of how rock climber are always saying sent it
best practices for header level measures in line item fact table,while my case is bit different consider the following fact table amp x200b fact invoice lineitem invoice_key product_key customer_key quantity_purchased invoice_status sale_date invoice_payment_date tax_amt amp x200b now with said fact table it at its lowest granularity but what is the best practice for answering invoice level questions such as how many invoices has customer generated how many invoices does customer have that are in status of pending something in table report format such as cust id month invoice count pending invoices we have such dilemma albeit not with invoices but our stack we re attempting to implement powerbi redshift doesn seem jive very well together unless powerbi just ends up as presentation layer still haven figured out how to get query folding to work if reshift supports it and the count distincts and other filters required when building in powerbi at the invoice header level just tank the performance to an unbelievibly bad level guess have questions from modeling perspective what is the best approach here read kimball and he all about not having header fact table and detail fact table but to have everything from header in detail yet implimentations and performance of said header level counts measures just can figure out yet how it would be performant unless that known trade off from performance perspective is there way to get powerbi to better perform when doing these types of queries know we can push the logic down to redshift in views but then we lose some of the flexibility and we have to manage the measures aggregations in redshift rather than powerbi two different teams at play here de team my team has traditionally served data to redshift dw data marts and bi team builds the reports in powerbi our bi team isn very sql savvy so it would require some decent learning and oversight so it doesn turn into plate of spaghetti in there,0,best practice for header level measure in line item fact table,while my case is bit different consider the following fact table amp x200b fact invoice lineitem invoice_key product_key customer_key quantity_purchased invoice_status sale_date invoice_payment_date tax_amt amp x200b now with said fact table it at it lowest granularity but what is the best practice for answering invoice level question such a how many invoice ha customer generated how many invoice doe customer have that are in status of pending something in table report format such a cust id month invoice count pending invoice we have such dilemma albeit not with invoice but our stack we re attempting to implement powerbi redshift doesn seem jive very well together unless powerbi just end up a presentation layer still haven figured out how to get query folding to work if reshift support it and the count distincts and other filter required when building in powerbi at the invoice header level just tank the performance to an unbelievibly bad level guess have question from modeling perspective what is the best approach here read kimball and he all about not having header fact table and detail fact table but to have everything from header in detail yet implimentations and performance of said header level count measure just can figure out yet how it would be performant unless that known trade off from performance perspective is there way to get powerbi to better perform when doing these type of query know we can push the logic down to redshift in view but then we lose some of the flexibility and we have to manage the measure aggregation in redshift rather than powerbi two different team at play here de team my team ha traditionally served data to redshift dw data mart and bi team build the report in powerbi our bi team isn very sql savvy so it would require some decent learning and oversight so it doesn turn into plate of spaghetti in there
snowplow web dbt package released new releases discourse,snowplow web dbt package released new releases discourse,0,snowplow web dbt package released new release discourse,snowplow web dbt package released new release discourse
data ingestion api responses to hive hdfs,so trying to ingest fuck ton of data api responses in our on premisses hive instance this need to be done in daily manner every day will be ingesting yesterday data the data is originally in nested json format but my client need to use it in tables format is it easier to parse the jsons to columns before loading to hive or in post ingestion processing and by parsing into columns it should be denormalized or normalized which works best,0,data ingestion api response to hive hdfs,so trying to ingest fuck ton of data api response in our on premiss hive instance this need to be done in daily manner every day will be ingesting yesterday data the data is originally in nested json format but my client need to use it in table format is it easier to parse the jsons to column before loading to hive or in post ingestion processing and by parsing into column it should be denormalized or normalized which work best
this isn new subject but what do you think should dimensional model be replaced with wide tables,this isn new subject but what do you think should dimensional model be replaced with wide tables,0,this isn new subject but what do you think should dimensional model be replaced with wide table,this isn new subject but what do you think should dimensional model be replaced with wide table
mdm tools and architecture,considering building new data ecosystem and trying to figure out where can add mdm right now thinking about moving data from postgres gt kafka gt snowflake gt tableau if need to merge two sources together and create master records should do it in snowflake using code or is there tool that is good for this some master records may need business logic to pick the right value,0,mdm tool and architecture,considering building new data ecosystem and trying to figure out where can add mdm right now thinking about moving data from postgres gt kafka gt snowflake gt tableau if need to merge two source together and create master record should do it in snowflake using code or is there tool that is good for this some master record may need business logic to pick the right value
data warehouse driven website,have relatively big dataset of public contracts and want to build website that allows search by contract name maintenance of equipment company small search engine basically visualize metrics like costs of all contracts per year bar chart axis year axis cost number of contracts number of unique purchasers api calls this webpage is good reference for what want to build tenders guru could you recommend me good way to achieve this was thinking about building data warehouse with mysql where contracts are facts and the rest of is contained in dimensions then build and api on top of it for the webserver to make searches but don know how to handle pre calculated metrics for visualizations maybe pre calculate them in separate table after new contracts are inserted and make the webserver retrieve the data from it is good idea to have data warehouse driven website or should consider another approach thanks in advance,0,data warehouse driven website,have relatively big dataset of public contract and want to build website that allows search by contract name maintenance of equipment company small search engine basically visualize metric like cost of all contract per year bar chart axis year axis cost number of contract number of unique purchaser api call this webpage is good reference for what want to build tender guru could you recommend me good way to achieve this wa thinking about building data warehouse with mysql where contract are fact and the rest of is contained in dimension then build and api on top of it for the webserver to make search but don know how to handle pre calculated metric for visualization maybe pre calculate them in separate table after new contract are inserted and make the webserver retrieve the data from it is good idea to have data warehouse driven website or should consider another approach thanks in advance
kaggle python api for kaggle stats,hi looking for python api for extracting some data for learning assignment data that would like to extract are following number of users for every week or month since country wise data same for gms competition and dataset tags and find top tags tag vs dataset association for those tags over last few years etc saw kaggle post but can find the dataset hence am thinking if can use kaggle api for extracting such data,0,kaggle python api for kaggle stats,hi looking for python api for extracting some data for learning assignment data that would like to extract are following number of user for every week or month since country wise data same for gm competition and dataset tag and find top tag tag v dataset association for those tag over last few year etc saw kaggle post but can find the dataset hence am thinking if can use kaggle api for extracting such data
challenges with big data and how aws can help handle them,enterprises constantly grapple with the challenges of big data analytics that range from volume and variety of data on prem costs and security to the lack of purpose built tools let take look at big challenges with big data and how aws can help overcome these with wide variety of tools and services,0,challenge with big data and how aws can help handle them,enterprise constantly grapple with the challenge of big data analytics that range from volume and variety of data on prem cost and security to the lack of purpose built tool let take look at big challenge with big data and how aws can help overcome these with wide variety of tool and service
data mesh,just read this wonderful article about data meshs hope this is interesting for someone else,0,data mesh,just read this wonderful article about data mesh hope this is interesting for someone else
etl extract transform load best practices etl process and lifehacks,etl extract transform load best practices etl process and lifehacks,0,etl extract transform load best practice etl process and lifehacks,etl extract transform load best practice etl process and lifehacks
etl extract transform load best practices etl process and lifehacks katarina harbuzava medium,etl extract transform load best practices etl process and lifehacks katarina harbuzava medium,0,etl extract transform load best practice etl process and lifehacks katarina harbuzava medium,etl extract transform load best practice etl process and lifehacks katarina harbuzava medium
do you use unit testing in your data engineering job,doing survey view poll,0,do you use unit testing in your data engineering job,doing survey view poll
can someone explain the big deal with dbt,not sure get what the craze is about it just executes sql using the already existing sql engine in the database get it more observable than filling database with bunch of stored procedure scripts but if already orchestrating my sql transforms with something like airflow or prefect anyways what do need dbt for,0,can someone explain the big deal with dbt,not sure get what the craze is about it just executes sql using the already existing sql engine in the database get it more observable than filling database with bunch of stored procedure script but if already orchestrating my sql transforms with something like airflow or prefect anyways what do need dbt for
testing etl pipelines,hello just started as junior data engineer in my company and this is actually the first time in touch with data engineering work we are using airflow to schedule our pipelines and running the etl scripts in python containers exploring all the code right now and ve never worked with docker before want to test and debug some of our scripts to actually see what each step in the code does to the data we are working on but cant find an easy way to debug the code we have here already setup up docker and tried to run the container from my machine and its working fine but everytime change something in the etl code have to build and run the container again and its just not very practical commented all the code that saves the final data to azure as just testing things and don want to mess up witht the production data looking up on the web found that can mount my local directory in docker so won need to keep building the container everytime change something in the code or can develop some tests to be ran on docker so what are the best practice to test changes and modifications on etl code,0,testing etl pipeline,hello just started a junior data engineer in my company and this is actually the first time in touch with data engineering work we are using airflow to schedule our pipeline and running the etl script in python container exploring all the code right now and ve never worked with docker before want to test and debug some of our script to actually see what each step in the code doe to the data we are working on but cant find an easy way to debug the code we have here already setup up docker and tried to run the container from my machine and it working fine but everytime change something in the etl code have to build and run the container again and it just not very practical commented all the code that save the final data to azure a just testing thing and don want to mess up witht the production data looking up on the web found that can mount my local directory in docker so won need to keep building the container everytime change something in the code or can develop some test to be ran on docker so what are the best practice to test change and modification on etl code
relatively new to my job and having serious issues in data engineering solution,so joined the data services team at my company they kind of went about backasswards and got bi platform domo database redshift and working on getting data lake set up am working on getting the data collection piece set up and having hell of time finding platform that fits nicely with our group of pieces keep hoping azure data factory will work in this instance but it feels lot like azure plays best with its own tools and not when piecemeal like this and am unsure which aws platform would do the things am looking to do amp x200b anyone have experience in this realm any thing should check out my background has predominantly been in the db management and analytics side and less the structure side so this is all very new to me,0,relatively new to my job and having serious issue in data engineering solution,so joined the data service team at my company they kind of went about backasswards and got bi platform domo database redshift and working on getting data lake set up am working on getting the data collection piece set up and having hell of time finding platform that fit nicely with our group of piece keep hoping azure data factory will work in this instance but it feel lot like azure play best with it own tool and not when piecemeal like this and am unsure which aws platform would do the thing am looking to do amp x200b anyone have experience in this realm any thing should check out my background ha predominantly been in the db management and analytics side and le the structure side so this is all very new to me
elt pipelines with prefect airbyte and dbt,hello we just released airbyte integration with prefect now it easy to connect prefect with airbyte and dbt to orchestrate elt pipelines we wrote tutorial to build an elt pipeline to discover github users that have contributed to the prefect airbyte and dbt github repositories curious to know if you use one or multiple tools for your etl elt pipelines,0,elt pipeline with prefect airbyte and dbt,hello we just released airbyte integration with prefect now it easy to connect prefect with airbyte and dbt to orchestrate elt pipeline we wrote tutorial to build an elt pipeline to discover github user that have contributed to the prefect airbyte and dbt github repository curious to know if you use one or multiple tool for your etl elt pipeline
spark configs for datasets eli5,ve been working with spark for over year and still don understand how to optimize spark jobs would anyone mind sharing how optimizations work for different datasets amp x200b gb data set with cols rows gb dataset with cols rows gb dataset with cols rows left joining with left joining with left join with based on multiple primary keys left join with based on single primary keys may return gt rows creating new columns for with sql based transformations some other questions how do optimize driver memory executor memory executor cores how do know how much overhead memory fraction do need how do allocate dynamicallocation minexecutors maxexecutors when to use broadcast join for datasets above how to optimize sql shuffle partitions amp x200b feel free to comment on similar questions,0,spark configs for datasets eli5,ve been working with spark for over year and still don understand how to optimize spark job would anyone mind sharing how optimization work for different datasets amp x200b gb data set with col row gb dataset with col row gb dataset with col row left joining with left joining with left join with based on multiple primary key left join with based on single primary key may return gt row creating new column for with sql based transformation some other question how do optimize driver memory executor memory executor core how do know how much overhead memory fraction do need how do allocate dynamicallocation minexecutors maxexecutors when to use broadcast join for datasets above how to optimize sql shuffle partition amp x200b feel free to comment on similar question
took ms course on data integration and it was underwhelming where can learn more,hey guys in the first year of my masters degree in computer science ai and software engineering majors and last trimester went off course and enrolled in course on data analysis and integration because ve always been interested in the area however thought it was pretty underwhelming stuff we messed with databases mysql implemented some etl pipelines pentaho data integration created datawarehouse we went over star snowflake starflake architectures etc checked data statistics with datacleaner defined some olap cubes with schema workbench and played around with them with saiko analytics ah we also fooled around with some sort of automatic report generating software stuff still ended up with nice grade highest of the class but felt kinda disappointed amp x200b and that was it visit this sub sometimes and feel like didn even scratch the surface of this field we didn even mention the cloud in the course or data integration for that matter also don see anyone using the software that used in the course amp x200b where can strengthen my knowledge of data engineering know it very broad question but struggling to understand where exactly should start the best ve found was this amp x200b thank you and sorry for my grammar,0,took m course on data integration and it wa underwhelming where can learn more,hey guy in the first year of my master degree in computer science ai and software engineering major and last trimester went off course and enrolled in course on data analysis and integration because ve always been interested in the area however thought it wa pretty underwhelming stuff we messed with database mysql implemented some etl pipeline pentaho data integration created datawarehouse we went over star snowflake starflake architecture etc checked data statistic with datacleaner defined some olap cube with schema workbench and played around with them with saiko analytics ah we also fooled around with some sort of automatic report generating software stuff still ended up with nice grade highest of the class but felt kinda disappointed amp x200b and that wa it visit this sub sometimes and feel like didn even scratch the surface of this field we didn even mention the cloud in the course or data integration for that matter also don see anyone using the software that used in the course amp x200b where can strengthen my knowledge of data engineering know it very broad question but struggling to understand where exactly should start the best ve found wa this amp x200b thank you and sorry for my grammar
front end for data lakes,has anyone ever had any experience with front end user application for data lakes thinking of the equivalent of microsoft access connected to sql server we have users who like creating their owns access vba apps as we move completely into data lakes and retire some of our sql dbs trying to see if their is tool like that for them to adopt fwiw our stack is in azure,0,front end for data lake,ha anyone ever had any experience with front end user application for data lake thinking of the equivalent of microsoft access connected to sql server we have user who like creating their owns access vba apps a we move completely into data lake and retire some of our sql db trying to see if their is tool like that for them to adopt fwiw our stack is in azure
building my first pipeline with lakehouse architecture what is your advice,am about to build my first data pipeline ever have read about lakehouses as most recent architecture for data pipelines so why not to start there made some research on tools and approaches and would like to leverage structured streaming starting streaming queries engine delta format and dbt what plan to achieve want ingest json documents classic event tracking from web uis stream from kafka want to do some transformations to silver and gold level via dbt want to implement ci cd for the transformations so can automate my workflow expose the gold to any bi jdbc so far my project is kinda purposeless want just to get in touch with it however if am able to build it want to build on top of it later my ultimate goal is to create an automated semantic layer which will analyze schema of the json documents and generate transformations for the purpose of analytics have very little experience with devops and cloud solutions where to run it is it possible run develop it on my vps and migrate to cloud later or do need to use aws gcp etc straight away so far created dataframe from csv file and created simple streamed sql transformation using pyspark and delta locally what is my next step thanks for all advice,0,building my first pipeline with lakehouse architecture what is your advice,am about to build my first data pipeline ever have read about lakehouses a most recent architecture for data pipeline so why not to start there made some research on tool and approach and would like to leverage structured streaming starting streaming query engine delta format and dbt what plan to achieve want ingest json document classic event tracking from web uis stream from kafka want to do some transformation to silver and gold level via dbt want to implement ci cd for the transformation so can automate my workflow expose the gold to any bi jdbc so far my project is kinda purposeless want just to get in touch with it however if am able to build it want to build on top of it later my ultimate goal is to create an automated semantic layer which will analyze schema of the json document and generate transformation for the purpose of analytics have very little experience with devops and cloud solution where to run it is it possible run develop it on my vps and migrate to cloud later or do need to use aws gcp etc straight away so far created dataframe from csv file and created simple streamed sql transformation using pyspark and delta locally what is my next step thanks for all advice
advice on encrypting sensitive ids that result in values that are secure but still user friendly best practice,we need to leverage account ids in our reporting layer but those ids are sensitive so they need to be encrypted tokenized our footprint is small less than million values token lookups seem like lot of overhead and prefer to not have lookup table with the keys to the kingdom in it encryption seems like the best option especially since we can use different salts for different contexts ex if we need to share data with multiple vendors the ids wouldn map back and we have existing security infrastructure to manage that but the guids we get back from secure encryption are long and not human friendly and humans will be using these is there best practice for masking ids programmatically and globally my initial thought is to double dip the ids use strong encryption initially and then use weak encryption to resolve that to something more readable would be fine including both values in our tables to account for data loss in that process surely this is thing people have dealt with before is there tried amp true solution,0,advice on encrypting sensitive id that result in value that are secure but still user friendly best practice,we need to leverage account id in our reporting layer but those id are sensitive so they need to be encrypted tokenized our footprint is small le than million value token lookup seem like lot of overhead and prefer to not have lookup table with the key to the kingdom in it encryption seems like the best option especially since we can use different salt for different context ex if we need to share data with multiple vendor the id wouldn map back and we have existing security infrastructure to manage that but the guids we get back from secure encryption are long and not human friendly and human will be using these is there best practice for masking id programmatically and globally my initial thought is to double dip the id use strong encryption initially and then use weak encryption to resolve that to something more readable would be fine including both value in our table to account for data loss in that process surely this is thing people have dealt with before is there tried amp true solution
what do need to learn about hive,so am learning hive right now don know much about the industry trend at the moment and whether hive is being used widely these days don even know if people are using something else instead of that so if you could advise me on how much need to learn about hive and what are the topics that need to focus on or maybe which other tool should be learning instead because of the industry trend that would be very very helpful,0,what do need to learn about hive,so am learning hive right now don know much about the industry trend at the moment and whether hive is being used widely these day don even know if people are using something else instead of that so if you could advise me on how much need to learn about hive and what are the topic that need to focus on or maybe which other tool should be learning instead because of the industry trend that would be very very helpful
data engineering fun,come from an engineering background and ended up working as business consultant for big after college decided wanted to change career path to data science and went to get cheap but good masters in data science in top europe university because wanted some experience and money ended up getting the only internship could as data engineer had no idea about data engineering before starting just the basics needed to start sql and python after months in this internship and about months in my masters realized what do at work is more fun than what do at school starting to get bored with all of my classes specially the basics ml and linear algebra there is just too much math and stats that think is useless in real life like do people actually use svm and kernels in real life it is just too boring what don know if what it is like to be data scientist in real life are you doing stats and optimization and using kernels irl are they basically researchers or depends on the title more of doer so that why think starting to like more data engineering can someone who has made the change from ds to de or viceversa give me some insight on jobs as ds,0,data engineering fun,come from an engineering background and ended up working a business consultant for big after college decided wanted to change career path to data science and went to get cheap but good master in data science in top europe university because wanted some experience and money ended up getting the only internship could a data engineer had no idea about data engineering before starting just the basic needed to start sql and python after month in this internship and about month in my master realized what do at work is more fun than what do at school starting to get bored with all of my class specially the basic ml and linear algebra there is just too much math and stats that think is useless in real life like do people actually use svm and kernel in real life it is just too boring what don know if what it is like to be data scientist in real life are you doing stats and optimization and using kernel irl are they basically researcher or depends on the title more of doer so that why think starting to like more data engineering can someone who ha made the change from d to de or viceversa give me some insight on job a d
good data engineering conferences,can anyone recommend conferences that are especially well suited for data engineers preferably in europe but please also mention every other conference that could fit there are so many conferences that it hard to keep track which ones will be helpful,0,good data engineering conference,can anyone recommend conference that are especially well suited for data engineer preferably in europe but please also mention every other conference that could fit there are so many conference that it hard to keep track which one will be helpful
architecture suggestion for alerting system in retail data,hello all am currently working for retail analytics company as data engineer data scientist as of now have built simple alerting system it checks for alert subscriber and alert type during change calculation in etl for example price change and sends email if match is found its done via python scripts with somewhat reconfigurable alert sink sqs email etc and alert data count of product with change or all products list however its time to make it more scalable upto alerts per minute and separate it out from main etl pipeline always prefer simple solution any suggestion on any aspect of the process is welcomed thank you,0,architecture suggestion for alerting system in retail data,hello all am currently working for retail analytics company a data engineer data scientist a of now have built simple alerting system it check for alert subscriber and alert type during change calculation in etl for example price change and sends email if match is found it done via python script with somewhat reconfigurable alert sink sqs email etc and alert data count of product with change or all product list however it time to make it more scalable upto alert per minute and separate it out from main etl pipeline always prefer simple solution any suggestion on any aspect of the process is welcomed thank you
how standardized tooling and metadata saved our data organization,wrote about how we build some standard interfaces and tooling for etl at our company curious if other people have found they needed to do similar at their companies as well or just in general do lot of people have issues standardizing their metadata lineage or dealing with too many data pipelines artifacts also welcome any feedback on the blog how standardized tooling and metadata saved our data organization,0,how standardized tooling and metadata saved our data organization,wrote about how we build some standard interface and tooling for etl at our company curious if other people have found they needed to do similar at their company a well or just in general do lot of people have issue standardizing their metadata lineage or dealing with too many data pipeline artifact also welcome any feedback on the blog how standardized tooling and metadata saved our data organization
advice on creating data warehouse in an old school business that doesn have much of data culture yet,hi everyone be interested to see what those more experienced than me would do in my situation currently one of three analysts in pretty old school business logistics they ve only just created an analytics department over the past years so there are no data engineers and doubt management would believe that that even real job title ve taught myself sql and python and am pretty confident with both more so python than sql spend most of my time creating reports in powerbi this is where ve started noticing issues namely amp x200b we build the reports based on data straight from our oltp databases on prem sql server know this isn best practise and could cause performance issues down the track already sometimes crash the servers if my queries are too heavy there data inconsistency between reports depending on who made it or how it built for example ve built out our amp with whole lot of tiny transformations excluding this gl including this department but not if it from this site etc which sits in dataset for that particular report if ever want to reference back to say revenue for particular period in another report then have to completely replicate the logic ve built into the amp report dataset otherwise those figures will never match this is causing those who rely on those numbers to lose confidence in the data know that if anyone else ever tried to maintain the reports ve built there that many small and subtle transformations that take place for every single dataset that it be close to impossible for them to understand unless they knew the data inside and out let alone make changes to without breaking the report eg if you forget to filter for customer field3 in this particular table you re actually included deleted transactions which is incorrect and therefore won match any of the other reports we definitely have use case for external data already scraping some data from the web that is really useful but have no where to put it because don have write access to our sql server dbs also understand that chucking it into an oltp db probably isn the best move anyway and that it don want people poking around the dbs we use for our applications so am currently just keeping the data on postgres db ve set up on my local machine ve come to the conclusion that considering these issues data warehouse would probably be step in the right direction this will consolidate all of the data from all of the different dbs and there would be single source of truth as bonus really keen on moving my career towards data engineering if can so thought this would be good if not very ambitious project to take on assuming can sell management on it ve done some reading and initially thought okay looks like the tools everyone using is some combination of an orchestration tool first thought is airflow data warehouse snowflake and staging storage area for the data to initially land s3 ve had play around with this exact stack and while it awesome to see how it all works have feeling it way overkill for what we need and that most of what reading is from this sub and most of you guys are probably working with lot more data than we would be for context ve looked at our main dbs and the entire contents of them add up to lt gb also battling with the fact that personally would probably prefer to use airflow cloud products simply to gain experience in these tools to open up further employment opportunities in the future in the data engineering space would really appreciate some advice or guidance on striking balance between which tools should be using that are appropriate for my situation but also tools that will provide me with experience that will help to push my career towards the data engineering space,0,advice on creating data warehouse in an old school business that doesn have much of data culture yet,hi everyone be interested to see what those more experienced than me would do in my situation currently one of three analyst in pretty old school business logistics they ve only just created an analytics department over the past year so there are no data engineer and doubt management would believe that that even real job title ve taught myself sql and python and am pretty confident with both more so python than sql spend most of my time creating report in powerbi this is where ve started noticing issue namely amp x200b we build the report based on data straight from our oltp database on prem sql server know this isn best practise and could cause performance issue down the track already sometimes crash the server if my query are too heavy there data inconsistency between report depending on who made it or how it built for example ve built out our amp with whole lot of tiny transformation excluding this gl including this department but not if it from this site etc which sits in dataset for that particular report if ever want to reference back to say revenue for particular period in another report then have to completely replicate the logic ve built into the amp report dataset otherwise those figure will never match this is causing those who rely on those number to lose confidence in the data know that if anyone else ever tried to maintain the report ve built there that many small and subtle transformation that take place for every single dataset that it be close to impossible for them to understand unless they knew the data inside and out let alone make change to without breaking the report eg if you forget to filter for customer field3 in this particular table you re actually included deleted transaction which is incorrect and therefore won match any of the other report we definitely have use case for external data already scraping some data from the web that is really useful but have no where to put it because don have write access to our sql server db also understand that chucking it into an oltp db probably isn the best move anyway and that it don want people poking around the db we use for our application so am currently just keeping the data on postgres db ve set up on my local machine ve come to the conclusion that considering these issue data warehouse would probably be step in the right direction this will consolidate all of the data from all of the different db and there would be single source of truth a bonus really keen on moving my career towards data engineering if can so thought this would be good if not very ambitious project to take on assuming can sell management on it ve done some reading and initially thought okay look like the tool everyone using is some combination of an orchestration tool first thought is airflow data warehouse snowflake and staging storage area for the data to initially land s3 ve had play around with this exact stack and while it awesome to see how it all work have feeling it way overkill for what we need and that most of what reading is from this sub and most of you guy are probably working with lot more data than we would be for context ve looked at our main db and the entire content of them add up to lt gb also battling with the fact that personally would probably prefer to use airflow cloud product simply to gain experience in these tool to open up further employment opportunity in the future in the data engineering space would really appreciate some advice or guidance on striking balance between which tool should be using that are appropriate for my situation but also tool that will provide me with experience that will help to push my career towards the data engineering space
scrapping reddit,let me run an idea by you data engineers let say wanted to pull data daily from reddit specifically from subreddit for this example let use cats want to pull data on the latest cat videos specifically how to train cats how to take care of cats etc would use the data for website that tells people how to become better cat owners basically blog site with new cat tech everything day is the data scrapping feasible by one person could include twitter too just started ms in data science so pretty new to apis and data mining,0,scrapping reddit,let me run an idea by you data engineer let say wanted to pull data daily from reddit specifically from subreddit for this example let use cat want to pull data on the latest cat video specifically how to train cat how to take care of cat etc would use the data for website that tell people how to become better cat owner basically blog site with new cat tech everything day is the data scrapping feasible by one person could include twitter too just started m in data science so pretty new to apis and data mining
springboard data engineering bootcamp learn online on your own time how is this course,springboard data engineering bootcamp learn online on your own time how is this course,0,springboard data engineering bootcamp learn online on your own time how is this course,springboard data engineering bootcamp learn online on your own time how is this course
tips for dealing with data transfers via dlls,working on extracting data out of legacy system backend rdbs this is very simple time series data two tables the only way to get to this data is via set of dll functions ve done bit of research and these more or less seem to function like api calls it seems like the best way to do this will be to have my it team set up vm on server where can access the data via the dll could then write program in that calls the dll functions need extracts the time series data and sends batches of data over to my mongodb existing on the same network here that data could be used for analysis bi etc and accessed remotely any thoughts am not de am ds working at an old company without des this seems like it would work to me but don know what am missing if there is better way how to orchestrate this etc the one thing know is that the only way to get this data is via these dll functions,0,tip for dealing with data transfer via dlls,working on extracting data out of legacy system backend rdbs this is very simple time series data two table the only way to get to this data is via set of dll function ve done bit of research and these more or le seem to function like api call it seems like the best way to do this will be to have my it team set up vm on server where can access the data via the dll could then write program in that call the dll function need extract the time series data and sends batch of data over to my mongodb existing on the same network here that data could be used for analysis bi etc and accessed remotely any thought am not de am d working at an old company without de this seems like it would work to me but don know what am missing if there is better way how to orchestrate this etc the one thing know is that the only way to get this data is via these dll function
the best data quality framework for senior platform engineers,hello friends does your data team have data quality framework data quality framework is tool that an organization can use to define relevant data quality attributes and provide guidance for data quality management process of continuously ensuring data quality meets consumers expectations slas you can learn more about this from our blog post on building the best data quality framework for senior platform engineers source forum amp utm medium amp utm group de,0,the best data quality framework for senior platform engineer,hello friend doe your data team have data quality framework data quality framework is tool that an organization can use to define relevant data quality attribute and provide guidance for data quality management process of continuously ensuring data quality meet consumer expectation slas you can learn more about this from our blog post on building the best data quality framework for senior platform engineer source forum amp utm medium amp utm group de
how to retrieve words from sentence using sql,ex love data engineering expected result love data use case have column which have whole sentence want to create new column with only first words kind of short description,0,how to retrieve word from sentence using sql,ex love data engineering expected result love data use case have column which have whole sentence want to create new column with only first word kind of short description
dbt for biquery how are you hacking the materialized view thing,so after readling post after post and comment after comment here finally jumped on the dbt bandwagon did first tutorial setup my profile and tested dbt run for the first pipeline my_first_dbt_model as table was smoothly crested in bigquery my_second_dbt_model as view was nice too and then checked the docs for materialized view since it is first step after data ingestion that am creating materialized view on top of my raw layer to convert json into actual columns and viola it was an open issue on their github repo they have shared experimental version and it says to put the macros scripts in the macros folder of your dbt project my question is whats next how to use it also the next step after materialized view is something equivalent to sql procedures in ms sql server orchestrated by ssms we were hoping bigquery routines cloud composer might do it now am not sure if dbt can version the routines part we have of procedures and of lines of sql transformations in each of them that we have to replicate in bigquery am so confused and doubtful now,0,dbt for biquery how are you hacking the materialized view thing,so after readling post after post and comment after comment here finally jumped on the dbt bandwagon did first tutorial setup my profile and tested dbt run for the first pipeline my_first_dbt_model a table wa smoothly crested in bigquery my_second_dbt_model a view wa nice too and then checked the doc for materialized view since it is first step after data ingestion that am creating materialized view on top of my raw layer to convert json into actual column and viola it wa an open issue on their github repo they have shared experimental version and it say to put the macro script in the macro folder of your dbt project my question is whats next how to use it also the next step after materialized view is something equivalent to sql procedure in m sql server orchestrated by ssms we were hoping bigquery routine cloud composer might do it now am not sure if dbt can version the routine part we have of procedure and of line of sql transformation in each of them that we have to replicate in bigquery am so confused and doubtful now
data engineering jargon part final,hi here are the final and some bonus ones as requested by the community is here utm_medium web2x amp context is here is here is here is below sandbox usually refers to an environment where extensive testing can be carried out without compromising the sanctity of the live platform sandbox to prove concept of keyboard metric before getting this accepted in live environment subject area way of defining data model by grouping the enterprise data according to known business directorates customer subject area containing all customer information that can be utilised across the business raw data this is the data as it has been collected in its rawest format before it is processed cleansed and loaded raw data of all the customer orders from the day of trading transactional data this is data that describes an actual event order placed delivery arranged or delivery accepted reference data this is data that allows the classification of other data country code gb representing great britain master data this is data that is the best representation of particular entity in the business this gives you view of that data entity by generally consolidating multiple data sources best customer data representation from multiple sources of information structured data data that is nicely organised in table using rows and columns allowing the user to easily interpret the data finance data in database table easily queryable using sql unstructured data data that cannot be nicely organised in tabular format like images pdf files etc an image stored on data lake cannot be retrieved using common data query languages data quality discipline of measuring the quality of the data to improve and cleanse it checking customer data for completeness accuracy and validity data management discipline encompassing the end to end management of data lifecycle including acquiring transferring securing and querying data combination of improving the quality of data governing the data enriching and cleansing the data bonus terms metadata this is information about the data itself in de this is likely to be table names table size data types and sizes column names even constraints like foreign and primary keys in table with country information uk or us will be the data itself whereas the column name country will be the metadata data marketplace concept of creating marketplace where buyers and sellers come together to trade data this has become even more popular due to iot data that has rapidly increased over the past decade snowflake has data marketplace where you can buy anonymised third party data to help with your use cases royalmail in the uk licenses paf postcode address file to businesses another way of buying data expect with blockchain for this kind of use case to become more consumer focused consumer maybe able to earn crypto tokens for willingly sharing their data so instead of the ad revenue going to google etc it could come to you as an end consumer data product solving business problem using mainly data is defined as data product and using data could mean anything from simple dashboards to ml models helping with recommendations on products to buy on amazon search function on amazon is an example of data product without well catalogued data this function would be useless scalability scalability is generally defined as the ability of the application to scale in light of changing increasing demands in de this could mean the datawarehouse or datalake platforms could also mean creating scalable data pipelines there are many ways of scaling database datawarehouse for example indexing table would allow fast retrieval of information as your query would not need to search every row of data this would be simple change for significant benefits another scalability example is number database sharding partitioning performance improvement technique of breaking up large data sets into small subsets sharding is when this break up of data set happens across multiple machines partitioning is when this break up of data set happens on the singular database the point is if the data is divided into smaller subsets and into different machines you are not bottlenecking the same machine with your queries each time parting thoughts this is the final post in the series hope you all enjoyed reading this as much as enjoyed writing it also hope you at least learnt few new terms it certainly helped me clarify my thinking thanks also to everyone that took part in the comments and helped me improve some of these definitions various people have posted the original medium link of this article where usually write so feel free to check that out let me know what else you like to learn and could write up future series about it thank you is here utm_medium web2x amp context is here is here is here,0,data engineering jargon part final,hi here are the final and some bonus one a requested by the community is here utm_medium web2x amp context is here is here is here is below sandbox usually refers to an environment where extensive testing can be carried out without compromising the sanctity of the live platform sandbox to prove concept of keyboard metric before getting this accepted in live environment subject area way of defining data model by grouping the enterprise data according to known business directorate customer subject area containing all customer information that can be utilised across the business raw data this is the data a it ha been collected in it rawest format before it is processed cleansed and loaded raw data of all the customer order from the day of trading transactional data this is data that describes an actual event order placed delivery arranged or delivery accepted reference data this is data that allows the classification of other data country code gb representing great britain master data this is data that is the best representation of particular entity in the business this give you view of that data entity by generally consolidating multiple data source best customer data representation from multiple source of information structured data data that is nicely organised in table using row and column allowing the user to easily interpret the data finance data in database table easily queryable using sql unstructured data data that cannot be nicely organised in tabular format like image pdf file etc an image stored on data lake cannot be retrieved using common data query language data quality discipline of measuring the quality of the data to improve and cleanse it checking customer data for completeness accuracy and validity data management discipline encompassing the end to end management of data lifecycle including acquiring transferring securing and querying data combination of improving the quality of data governing the data enriching and cleansing the data bonus term metadata this is information about the data itself in de this is likely to be table name table size data type and size column name even constraint like foreign and primary key in table with country information uk or u will be the data itself whereas the column name country will be the metadata data marketplace concept of creating marketplace where buyer and seller come together to trade data this ha become even more popular due to iot data that ha rapidly increased over the past decade snowflake ha data marketplace where you can buy anonymised third party data to help with your use case royalmail in the uk license paf postcode address file to business another way of buying data expect with blockchain for this kind of use case to become more consumer focused consumer maybe able to earn crypto token for willingly sharing their data so instead of the ad revenue going to google etc it could come to you a an end consumer data product solving business problem using mainly data is defined a data product and using data could mean anything from simple dashboard to ml model helping with recommendation on product to buy on amazon search function on amazon is an example of data product without well catalogued data this function would be useless scalability scalability is generally defined a the ability of the application to scale in light of changing increasing demand in de this could mean the datawarehouse or datalake platform could also mean creating scalable data pipeline there are many way of scaling database datawarehouse for example indexing table would allow fast retrieval of information a your query would not need to search every row of data this would be simple change for significant benefit another scalability example is number database sharding partitioning performance improvement technique of breaking up large data set into small subset sharding is when this break up of data set happens across multiple machine partitioning is when this break up of data set happens on the singular database the point is if the data is divided into smaller subset and into different machine you are not bottlenecking the same machine with your query each time parting thought this is the final post in the series hope you all enjoyed reading this a much a enjoyed writing it also hope you at least learnt few new term it certainly helped me clarify my thinking thanks also to everyone that took part in the comment and helped me improve some of these definition various people have posted the original medium link of this article where usually write so feel free to check that out let me know what else you like to learn and could write up future series about it thank you is here utm_medium web2x amp context is here is here is here
handling dynamic structure in during data ingestion py,hello ve run into some interesting scenario where we have data coming from certain source and have finished ingesting billion records out of after billion records it was observed one of the columns which didn have any data during sampling came up with list data type from the source as result the data ingested has to be exploded after the job is finished was thinking if anyone here has implemented any type of intelligence within their code so that it when it sees list type or nested data json it explodes based on the incoming data amp x200b thanks,0,handling dynamic structure in during data ingestion py,hello ve run into some interesting scenario where we have data coming from certain source and have finished ingesting billion record out of after billion record it wa observed one of the column which didn have any data during sampling came up with list data type from the source a result the data ingested ha to be exploded after the job is finished wa thinking if anyone here ha implemented any type of intelligence within their code so that it when it see list type or nested data json it explodes based on the incoming data amp x200b thanks
am doing de work,picked up data analyst internship at tiny startup about months ago currently we are trying to create reporting sql database streaming data from production mongodb so far have designed and implemented majority of the reporting database and is currently writing python script to clean up and insert data into the postgresql database am super new to this type of work and have been primarily learning about data analytics and dba things in my degree what type of roles am working as is this de work,0,am doing de work,picked up data analyst internship at tiny startup about month ago currently we are trying to create reporting sql database streaming data from production mongodb so far have designed and implemented majority of the reporting database and is currently writing python script to clean up and insert data into the postgresql database am super new to this type of work and have been primarily learning about data analytics and dba thing in my degree what type of role am working a is this de work
the pipeline academy awards the pipies,the pipeline academy awards the pipies are brought to you by pipeline data engineering academy want to get data on your premises use airbyte quix and prefect want to rule data already on your premises use dbt clickhouse and great expectations want to run data products use pulumi fly and streamlit honorable mentions go to saturn cloud github actions and datasette praise the teams with meaningful solutions for real problems,0,the pipeline academy award the pipies,the pipeline academy award the pipies are brought to you by pipeline data engineering academy want to get data on your premise use airbyte quix and prefect want to rule data already on your premise use dbt clickhouse and great expectation want to run data product use pulumi fly and streamlit honorable mention go to saturn cloud github action and datasette praise the team with meaningful solution for real problem
what is data streaming,is it just webhooks from your api what the big deal,0,what is data streaming,is it just webhooks from your api what the big deal
dbt tests create staging environments for flawless data ci cd,dbt tests create staging environments for flawless data ci cd,0,dbt test create staging environment for flawless data ci cd,dbt test create staging environment for flawless data ci cd
dbt tests create staging environments for flawless data ci cd,dbt tests create staging environments for flawless data ci cd,0,dbt test create staging environment for flawless data ci cd,dbt test create staging environment for flawless data ci cd
best way to automate files upload to s3,we have our financial budget in excel files that we want to push to s3 these excel are first cleaned using power bi and then manually uploaded to s3 from s3 we have already built an automated ingest process into snowflake is there way to automate the push to s3,0,best way to automate file upload to s3,we have our financial budget in excel file that we want to push to s3 these excel are first cleaned using power bi and then manually uploaded to s3 from s3 we have already built an automated ingest process into snowflake is there way to automate the push to s3
how do you develop and test your etl aws glue pipelines,long story short am very new to de and am working on project as consultant with other consultant engineers we are very new team and we definitely don have set standards agreement on the development process we are developing pipelines in glue studio using interactive ui where we are transforming external redshift table so s3 source to s3 sink the production code will sit in repo where ci cd pipeline will push the code the prod environment we are thinking of setting up local aws environment so we don have to use glue ui because this is expensive for the client as each code iteration is run manually takes ages to execute each glue job doesn feel right to use interractive ui tool which lacks many features what your typical ide would have gt leads to inefficient development unable to run unittes overall is crap dev experience want to set up local environment and already found docker image that lets you imitate aws glue service with aws specific spark functions my question is how would you test your etl code in my mind want to mock s3 and other aws service functionality so can run the code completely locally but there seems to be disagreement how we should test our etl pipeline and people don seem to like my idea of unit testing and mocking services some guys want to manually test as you go where you actually read some chunk of data from s3 and then execute aws glue job which runs locally on your machine using docker image their reasoning is that this way the code will remain exactly the same which really dont understand if you follow tdd then unittests and mocking services dont modify your actual code they sit in seperate file so no refactoring is needed for your code when you push it to production as long as all your tests pass is it normal to develop etl code where you test and debug it by running the code manually where you actually call real services want to understand if am being too anal by pushing everyone to write unit tests where we mock the aws services authentification s3 spark instead of manually doing this,0,how do you develop and test your etl aws glue pipeline,long story short am very new to de and am working on project a consultant with other consultant engineer we are very new team and we definitely don have set standard agreement on the development process we are developing pipeline in glue studio using interactive ui where we are transforming external redshift table so s3 source to s3 sink the production code will sit in repo where ci cd pipeline will push the code the prod environment we are thinking of setting up local aws environment so we don have to use glue ui because this is expensive for the client a each code iteration is run manually take age to execute each glue job doesn feel right to use interractive ui tool which lack many feature what your typical ide would have gt lead to inefficient development unable to run unittes overall is crap dev experience want to set up local environment and already found docker image that let you imitate aws glue service with aws specific spark function my question is how would you test your etl code in my mind want to mock s3 and other aws service functionality so can run the code completely locally but there seems to be disagreement how we should test our etl pipeline and people don seem to like my idea of unit testing and mocking service some guy want to manually test a you go where you actually read some chunk of data from s3 and then execute aws glue job which run locally on your machine using docker image their reasoning is that this way the code will remain exactly the same which really dont understand if you follow tdd then unittests and mocking service dont modify your actual code they sit in seperate file so no refactoring is needed for your code when you push it to production a long a all your test pas is it normal to develop etl code where you test and debug it by running the code manually where you actually call real service want to understand if am being too anal by pushing everyone to write unit test where we mock the aws service authentification s3 spark instead of manually doing this
need reality check will focusing on data operations hurt my career progression as de,background started my first data engineer job in the fintech space six months ago this is my first de job but worked in analytics before our stack is python sql airflow snowflake aws git my responsibilities so far have been largely on data ops my core tasks have involved process documentation and ensuring our weekly airflow pipelines run successfully ll watch their progress at specified times of the week dive into any bugs look at logs sometimes do manual work and implement fix both in our codebase and any configs needed this takes nearly all my time per week there also some dev work to automate things have an upcoming pipeline that involves automating our alerting system by auto creating tickets on jira when pipeline fails at specific step this is fine and learning lot as new de but my manager wants me to largely focus on this side of the business he fully intends this to be my main focus moving forward as he splits us into subteams there are newer des being aligned to other sub teams solely working on building out our data lake warehouse and implementing our transformation processes don touch any of that nor the actual data directly just pipeline management acting on bugs and trying to automate peripheral processes our team is agile but it looks like this separation of focus will be defacto moving forward know there can be gap between expectations vs reality of being de so is my concern of being pigeon holed by doing operations justified is this in line with your de work and just super new de overreacting really like to strike balance between de and swe but worried this ops focus might affect my career job search in the future,0,need reality check will focusing on data operation hurt my career progression a de,background started my first data engineer job in the fintech space six month ago this is my first de job but worked in analytics before our stack is python sql airflow snowflake aws git my responsibility so far have been largely on data ops my core task have involved process documentation and ensuring our weekly airflow pipeline run successfully ll watch their progress at specified time of the week dive into any bug look at log sometimes do manual work and implement fix both in our codebase and any configs needed this take nearly all my time per week there also some dev work to automate thing have an upcoming pipeline that involves automating our alerting system by auto creating ticket on jira when pipeline fails at specific step this is fine and learning lot a new de but my manager want me to largely focus on this side of the business he fully intends this to be my main focus moving forward a he split u into subteams there are newer de being aligned to other sub team solely working on building out our data lake warehouse and implementing our transformation process don touch any of that nor the actual data directly just pipeline management acting on bug and trying to automate peripheral process our team is agile but it look like this separation of focus will be defacto moving forward know there can be gap between expectation v reality of being de so is my concern of being pigeon holed by doing operation justified is this in line with your de work and just super new de overreacting really like to strike balance between de and swe but worried this ops focus might affect my career job search in the future
question about snowpipe architecture,architecture snowflake azure hey we are hoping to get some data streaming setup at my company for some business requirements have decent understanding of snowpipe and how it can automatically load in files from stage s3 blob storage my main confusion is around how to get the data we need into files in blob storage the data we need is in microsoft sql database that is the transactional system would use kafka to capture the changes and new data in the source system and push them into flat file in blob storage any information on how you use data streaming snowpipe in your organization would be helpful,0,question about snowpipe architecture,architecture snowflake azure hey we are hoping to get some data streaming setup at my company for some business requirement have decent understanding of snowpipe and how it can automatically load in file from stage s3 blob storage my main confusion is around how to get the data we need into file in blob storage the data we need is in microsoft sql database that is the transactional system would use kafka to capture the change and new data in the source system and push them into flat file in blob storage any information on how you use data streaming snowpipe in your organization would be helpful
looking for snowflake databricks crash course will pay,recently got hired as data engineer would like to get familiar with using databricks and snowflake together as primarily came from just sql background as an analyst would like to know what cluster is conceptually and maybe quick crash course on how sample workflow between the two and sample data would look like will pay for an hour of this just enough to understand the workflow thanks,0,looking for snowflake databricks crash course will pay,recently got hired a data engineer would like to get familiar with using databricks and snowflake together a primarily came from just sql background a an analyst would like to know what cluster is conceptually and maybe quick crash course on how sample workflow between the two and sample data would look like will pay for an hour of this just enough to understand the workflow thanks
freelancing,hi guys am looking for freelancing please let me know if you guys are looking for de work skills pyspark azure data factory databricks azure data lake sql thank you,0,freelancing,hi guy am looking for freelancing please let me know if you guy are looking for de work skill pyspark azure data factory databricks azure data lake sql thank you
what equipment to get for,hello looking for your help today cross posted this to few other subreddits too am going to join new company as data engineer ll be programming in python bit of ts and will be working with docker lot due to the field my company is working in my disk will need to be encrypted my company will give me budget to equip myself and they will provide me with ultra wide curved screen as well thus for am looking for laptop mouse and keyboard combo and whatever other equipment that you may deem essential to office work or programming productivity most of the company is currently rocking macbook pro with intel processors and gb of ram ve been daily driving linux for the past years and would like to keep it like that if possible can install linux after getting the laptop no need for it to come pre installed know that not all laptops are compatible with linux have heard good things about dell xps and lenovo laptops think ll need at least gb of ram gb ssd and want portable laptop so max appreciate any help you can give me,0,what equipment to get for,hello looking for your help today cross posted this to few other subreddits too am going to join new company a data engineer ll be programming in python bit of t and will be working with docker lot due to the field my company is working in my disk will need to be encrypted my company will give me budget to equip myself and they will provide me with ultra wide curved screen a well thus for am looking for laptop mouse and keyboard combo and whatever other equipment that you may deem essential to office work or programming productivity most of the company is currently rocking macbook pro with intel processor and gb of ram ve been daily driving linux for the past year and would like to keep it like that if possible can install linux after getting the laptop no need for it to come pre installed know that not all laptop are compatible with linux have heard good thing about dell xps and lenovo laptop think ll need at least gb of ram gb ssd and want portable laptop so max appreciate any help you can give me
have you taken courses from andreas kretz learn data engineering how was it do you recommend it to software engineers,link,0,have you taken course from andreas kretz learn data engineering how wa it do you recommend it to software engineer,link
hiring for data engineers in india,looking for lt years of exp folks for data engineer role in groupon bangalore india skills sql spark python airflow let me know if you re interested,0,hiring for data engineer in india,looking for lt year of exp folk for data engineer role in groupon bangalore india skill sql spark python airflow let me know if you re interested
dozens of python scripts to extract data and load to bigquery,old setup was on premise sql server with scripts residing in drive run on schedule via ssms new setup anything on gcp that is faster scaleable and modular should be an improvement from old setup we were thinking google cloud functions and then do scheduling via google cloud composer airflow but while looking at composer realized cloud composer can also have pythin scripts in its directory or this is what understood so my question is what is the best place to put dozens python scripts that extract data and push json to bigquery about these scripts they have to fetch api keys from somewhere we were using db tables in old setup they would be running every hour at the minimum all extractions are independant of each other all extractions have number of things in common bigquery insertion function bigquery insertion format logging format extraction function all the raw data will then be pushed from raw layer to history layer via bigquery routines can sql procedures be written and run airflow too or bigquery routines run from airflow too so we can complete move to airflow thanks in advance for the help just trying to make the right decisions for atleast next years in terms of code modularity scalability and business deliverables,0,dozen of python script to extract data and load to bigquery,old setup wa on premise sql server with script residing in drive run on schedule via ssms new setup anything on gcp that is faster scaleable and modular should be an improvement from old setup we were thinking google cloud function and then do scheduling via google cloud composer airflow but while looking at composer realized cloud composer can also have pythin script in it directory or this is what understood so my question is what is the best place to put dozen python script that extract data and push json to bigquery about these script they have to fetch api key from somewhere we were using db table in old setup they would be running every hour at the minimum all extraction are independant of each other all extraction have number of thing in common bigquery insertion function bigquery insertion format logging format extraction function all the raw data will then be pushed from raw layer to history layer via bigquery routine can sql procedure be written and run airflow too or bigquery routine run from airflow too so we can complete move to airflow thanks in advance for the help just trying to make the right decision for atleast next year in term of code modularity scalability and business deliverable
can someone explain like why do need both mongodb and data lake,mongodb nosql data base data lake take delta lake for example place to through all your data then you have query processor on top of data lake to bring your data to your bi applications etc amp x200b thought data lake was just reselling compute power from aws gcp azure and helping folks organize and process data what does mongodb do and how does it fit in between data lake and aws gcp azure,0,can someone explain like why do need both mongodb and data lake,mongodb nosql data base data lake take delta lake for example place to through all your data then you have query processor on top of data lake to bring your data to your bi application etc amp x200b thought data lake wa just reselling compute power from aws gcp azure and helping folk organize and process data what doe mongodb do and how doe it fit in between data lake and aws gcp azure
question about airflow data pipelines,hi all am currently setting up airflow on an azure linux vm the first want to do is to execute some python scripts which extract data from rest api by now the script is only running locally in jupyter notebook the result is by now stored in pandas dataframe and saved as csv amp x200b how would you recommend me to store the result by using airflow is it possible recommended to store it on the vm in the preinstalled postgresdb,0,question about airflow data pipeline,hi all am currently setting up airflow on an azure linux vm the first want to do is to execute some python script which extract data from rest api by now the script is only running locally in jupyter notebook the result is by now stored in panda dataframe and saved a csv amp x200b how would you recommend me to store the result by using airflow is it possible recommended to store it on the vm in the preinstalled postgresdb
carto raises to lead the way in cloud native spatial analytics,carto raises to lead the way in cloud native spatial analytics,0,carto raise to lead the way in cloud native spatial analytics,carto raise to lead the way in cloud native spatial analytics
event driven data validation with google cloud functions and great expectations,driven data validation with google cloud functions and great expectations using google cloud functions and great expectations to validate data landing in data lake on gcp one of my first times using great expectations and if honest didn find it the easiest library to work with but quite like this solution for automating testing of data landing in the data lake using serverless architecture would appreciate any comments feedback,0,event driven data validation with google cloud function and great expectation,driven data validation with google cloud function and great expectation using google cloud function and great expectation to validate data landing in data lake on gcp one of my first time using great expectation and if honest didn find it the easiest library to work with but quite like this solution for automating testing of data landing in the data lake using serverless architecture would appreciate any comment feedback
data engineering jargon part,hi next is here utm_medium web2x amp context is here is here is below data replication there are multiple ways to do this but mainly it is practice of copying data to multiple servers to protect an organisation against data loss replicating the customer information across two databases to ensure their core details are not lost big data term coined for large amounts of data that cannot be processed using traditional databases refer to data lake hadoop data lake stores all the information received from sensors in smart fridge hive apache hive is data warehouse open source project that allows querying large amounts of data like sql it uses an easy to understand language called hive ql select from tbl returns all rows and columns from data store like hdfs hdfs hadoop distributed file system is data storage system used by hadoop it provides flexibility to manage structured or unstructured data storing large amounts of financial transactional data in an hdfs to query using hive ql nifi it is an open source extract transform and load tool refer to etl this allows filtering integrating and joining data moving postcode data from csv file to hdfs using nifi kafka it is more complex to work with than nifi as it doesn have user interface ui mainly used for real time streaming data it is messaging system first created by linkedin engineers streaming real time weather events using kafka flat file flat files are commonly used to transfer data due to their basic nature flat files are single table storing data in plain text format all customer order numbers stored in comma separated value csv file latency the time it takes for database or web application to respond to query or click takes seconds to query database with million records caching this is when limited data is stored on the ram to allow for quick retrieval of information in memory caching of data in database returns results to query times faster staging the name of storage area that is temporary in nature to allow for processing of etl jobs refer to etl staging area in an etl routine allows data to be cleaned before loading into the final tables is here utm_medium web2x amp context is here is here,0,data engineering jargon part,hi next is here utm_medium web2x amp context is here is here is below data replication there are multiple way to do this but mainly it is practice of copying data to multiple server to protect an organisation against data loss replicating the customer information across two database to ensure their core detail are not lost big data term coined for large amount of data that cannot be processed using traditional database refer to data lake hadoop data lake store all the information received from sensor in smart fridge hive apache hive is data warehouse open source project that allows querying large amount of data like sql it us an easy to understand language called hive ql select from tbl return all row and column from data store like hdfs hdfs hadoop distributed file system is data storage system used by hadoop it provides flexibility to manage structured or unstructured data storing large amount of financial transactional data in an hdfs to query using hive ql nifi it is an open source extract transform and load tool refer to etl this allows filtering integrating and joining data moving postcode data from csv file to hdfs using nifi kafka it is more complex to work with than nifi a it doesn have user interface ui mainly used for real time streaming data it is messaging system first created by linkedin engineer streaming real time weather event using kafka flat file flat file are commonly used to transfer data due to their basic nature flat file are single table storing data in plain text format all customer order number stored in comma separated value csv file latency the time it take for database or web application to respond to query or click take second to query database with million record caching this is when limited data is stored on the ram to allow for quick retrieval of information in memory caching of data in database return result to query time faster staging the name of storage area that is temporary in nature to allow for processing of etl job refer to etl staging area in an etl routine allows data to be cleaned before loading into the final table is here utm_medium web2x amp context is here is here
just got fired,entry level offshore remote bucks an hour contract signed an nda and am not mentioning any name of the company or confidential or any specific information so know am good if the client comes across this and is bitter and wants to have fight about this he has my email so joined this remote startup they wanted to hire fullstack developer but begged and convinced my way into python data engineering role they had me on probation for week period day during the onboarding interview was given simple task where needed to generate two analytics table each time they made request to saturate their database post analysis was told that all needed to run main py and everything is ready to go they have some helper documentation on the readme day so get started and things start to go south first they didn assign me with access permissions to the cloud waste some time figuring that out then the docker file shows that need to setup registry stuff so can setup the machine learning models that took some while to figure that out also needed to setup config dot files for that that doesn work at this moment have spent day but logged around half hours the senior dev was helpful but he was super busy the client comes back and gets mad as all needed to run the main py file and that was supposed to be it said we were trying to set up docker for the python version the machine learning libraries and python packages he says run the requirement file the machine learning libraries calls the database processes and sends it back needed to create an analytics log in the middle of that day weekend if you use multiple python versions in project the requirements txt installation using pip installing isn exactly straightforward after hours of trying to figure this out discover that they were using two versions of python and for the requirements file they used one package that wasn supported by the python version they recommended me change the version of that package and finally installed the packages this might sound like an easy fix but urge to try this out day run the file it doesn work of course because this entire repo heavily depends on the docker file and docker file sets up aliases for certain api calls the file will never work straight try my best to discover my way through the jungle that is that codebase and even still get permission issues get one permission issue which was sorted and the entire file doesn run anyway because wasn granted full permission of something say am not getting stuff done and the client gets mad he says that my skills aren that strong as have indicated in my interview how the heck am supposed to react to that said please re read our conversation how am supposed to solve this issue without permission and configuration issues ironed out in the begging of the project they have provided the schema for the analytics they needed so it should be an easy task said can provide code but will be working be working blind they said have all the necessary things for me to get the job done based on schema alone comment out the sections that doesn work and write code that outputs csv at this moment let the client know let me know when should leave they indicate it is going to happen and they said what do you mean by re reading our conversation day today intentionally didn reply to what meant by re reading thread yeah bad thing to say but what am supposed to say when the client thinks am not getting things done while simultaneously locking me out from the very place need to get things done and moreover questioning my skill just felt bad to me comment out sections of code to find what is going on they are simultaneously fetching and merging data to aggregate data upon layers upon layers with multiple modules then discover they have another cloud service that they are using that didn knew even needed access for said that have found the problem for the issue and need access to the service get the message pack your things and go overall logged about hours for this but set their on idle about double that time and obviously didn get paid of the two task did the first one took me minutes to do but the setup and configuration and code discovery took up the entire logged time the first day joined slack came across few profile and all of them were deactivated scrolling up saw guy quitting under hours don plan to work for another company that supposedly doesn provide some level of mentorship don want to work with someone who isn technically competent and is not willing to guide me understand am supposed to be an expert because my rates are wayyyyyyy too high as an offshore dev but not again am working without pair programming routine video chats or supporting seniors really despise freelancing,0,just got fired,entry level offshore remote buck an hour contract signed an nda and am not mentioning any name of the company or confidential or any specific information so know am good if the client come across this and is bitter and want to have fight about this he ha my email so joined this remote startup they wanted to hire fullstack developer but begged and convinced my way into python data engineering role they had me on probation for week period day during the onboarding interview wa given simple task where needed to generate two analytics table each time they made request to saturate their database post analysis wa told that all needed to run main py and everything is ready to go they have some helper documentation on the readme day so get started and thing start to go south first they didn assign me with access permission to the cloud waste some time figuring that out then the docker file show that need to setup registry stuff so can setup the machine learning model that took some while to figure that out also needed to setup config dot file for that that doesn work at this moment have spent day but logged around half hour the senior dev wa helpful but he wa super busy the client come back and get mad a all needed to run the main py file and that wa supposed to be it said we were trying to set up docker for the python version the machine learning library and python package he say run the requirement file the machine learning library call the database process and sends it back needed to create an analytics log in the middle of that day weekend if you use multiple python version in project the requirement txt installation using pip installing isn exactly straightforward after hour of trying to figure this out discover that they were using two version of python and for the requirement file they used one package that wasn supported by the python version they recommended me change the version of that package and finally installed the package this might sound like an easy fix but urge to try this out day run the file it doesn work of course because this entire repo heavily depends on the docker file and docker file set up alias for certain api call the file will never work straight try my best to discover my way through the jungle that is that codebase and even still get permission issue get one permission issue which wa sorted and the entire file doesn run anyway because wasn granted full permission of something say am not getting stuff done and the client get mad he say that my skill aren that strong a have indicated in my interview how the heck am supposed to react to that said please re read our conversation how am supposed to solve this issue without permission and configuration issue ironed out in the begging of the project they have provided the schema for the analytics they needed so it should be an easy task said can provide code but will be working be working blind they said have all the necessary thing for me to get the job done based on schema alone comment out the section that doesn work and write code that output csv at this moment let the client know let me know when should leave they indicate it is going to happen and they said what do you mean by re reading our conversation day today intentionally didn reply to what meant by re reading thread yeah bad thing to say but what am supposed to say when the client think am not getting thing done while simultaneously locking me out from the very place need to get thing done and moreover questioning my skill just felt bad to me comment out section of code to find what is going on they are simultaneously fetching and merging data to aggregate data upon layer upon layer with multiple module then discover they have another cloud service that they are using that didn knew even needed access for said that have found the problem for the issue and need access to the service get the message pack your thing and go overall logged about hour for this but set their on idle about double that time and obviously didn get paid of the two task did the first one took me minute to do but the setup and configuration and code discovery took up the entire logged time the first day joined slack came across few profile and all of them were deactivated scrolling up saw guy quitting under hour don plan to work for another company that supposedly doesn provide some level of mentorship don want to work with someone who isn technically competent and is not willing to guide me understand am supposed to be an expert because my rate are wayyyyyyy too high a an offshore dev but not again am working without pair programming routine video chat or supporting senior really despise freelancing
hazelcast announces new unified platform with version,hazelcast announces new unified platform with version,0,hazelcast announces new unified platform with version,hazelcast announces new unified platform with version
using types of data why tyny dev is the most suitable api for employing unstructured data,using types of data why tyny dev is the most suitable api for employing unstructured data,0,using type of data why tyny dev is the most suitable api for employing unstructured data,using type of data why tyny dev is the most suitable api for employing unstructured data
s3 to s3 transformation,we are trying to build scalable and generic framework for s3 s3 transformation currently we have aws lambda and databricks up for the competition thoughts on this,0,s3 to s3 transformation,we are trying to build scalable and generic framework for s3 s3 transformation currently we have aws lambda and databricks up for the competition thought on this
how to build commission calculator amp how to automate pdf sheet creation,hello all need to automate commission payments in my company for this need to analyze the sales data and apply various kpis to it on the other hand this must be historized in traceable way and individual recalculations must be possible currently work lot with sql and have the result for certain period calculated per procedure for each department for this there is lot of input data that have to check for completeness beforehand in addition the calculation model changes often and there are many exceptions last but not least have to generate pdf sheet with summary for each employee have you ever had experience with something like this do you have any tips and tricks on how to build such framework what would be your approach do you know solutions how to automatically print pdf sheet per employee if yes which ones,0,how to build commission calculator amp how to automate pdf sheet creation,hello all need to automate commission payment in my company for this need to analyze the sale data and apply various kpis to it on the other hand this must be historized in traceable way and individual recalculation must be possible currently work lot with sql and have the result for certain period calculated per procedure for each department for this there is lot of input data that have to check for completeness beforehand in addition the calculation model change often and there are many exception last but not least have to generate pdf sheet with summary for each employee have you ever had experience with something like this do you have any tip and trick on how to build such framework what would be your approach do you know solution how to automatically print pdf sheet per employee if yes which one
my favorite books for data engineers from streaming to software engineering,my favorite books for data engineers from streaming to software engineering,0,my favorite book for data engineer from streaming to software engineering,my favorite book for data engineer from streaming to software engineering
getting started with data engineering,want to get started with data engineering when looked on the internet for roadmaps and stuff have narrowed in on the following technologies that need to be learnt apart from sql nosql and linux scripting hadoop apache spark spark streaming kafka hive have the following three questions to ask have skipped any of the technologies that need to learn what is the best resource to learn these technologies so that can start applying them to projects do projects that process analyze large gb look ok on the resume even though streaming data isn used also know that aws or gcp must be learnt but feel that it better to go step at time,0,getting started with data engineering,want to get started with data engineering when looked on the internet for roadmaps and stuff have narrowed in on the following technology that need to be learnt apart from sql nosql and linux scripting hadoop apache spark spark streaming kafka hive have the following three question to ask have skipped any of the technology that need to learn what is the best resource to learn these technology so that can start applying them to project do project that process analyze large gb look ok on the resume even though streaming data isn used also know that aws or gcp must be learnt but feel that it better to go step at time
why openmetadata is the right choice for you,if you are interested in metadata management and data discovery quality and collaboration we wrote blog post on why is the right choice please take look and if you are interested in learning more about the project or any feature requests etc please join our community,0,why openmetadata is the right choice for you,if you are interested in metadata management and data discovery quality and collaboration we wrote blog post on why is the right choice please take look and if you are interested in learning more about the project or any feature request etc please join our community
need some advice for year end salary negotiations,some context about me and my position was hired as data analyst making yearly usd about months in started helping out our data engineering team they have given me an incredible opportunity to pivot to become data engineer ve mainly been using python to build data pipelines in aws mainly using lambda also been helping with different bug fixes and error handling in addition to building pipelines and automating tasks don have comp sci background self taught in programming as of now also more senior qa engineer has been mentoring me in all things de my new boss on the data engineering team has said they will hire and backfill my data analyst responsibilities relatively soon ve been doing half and half analyst engineering stuff recently ve asked them if should change my title to better reflect my de abilities since going more that direction now they said can change it to jr data engineer my question for the community is what is fair salary for jr data engineer and how can position myself to negotiate for it my goal for next year is to eventually get the jr dropped don want to leave this company in search for self promotion love the company and culture so far thanks,0,need some advice for year end salary negotiation,some context about me and my position wa hired a data analyst making yearly usd about month in started helping out our data engineering team they have given me an incredible opportunity to pivot to become data engineer ve mainly been using python to build data pipeline in aws mainly using lambda also been helping with different bug fix and error handling in addition to building pipeline and automating task don have comp sci background self taught in programming a of now also more senior qa engineer ha been mentoring me in all thing de my new bos on the data engineering team ha said they will hire and backfill my data analyst responsibility relatively soon ve been doing half and half analyst engineering stuff recently ve asked them if should change my title to better reflect my de ability since going more that direction now they said can change it to jr data engineer my question for the community is what is fair salary for jr data engineer and how can position myself to negotiate for it my goal for next year is to eventually get the jr dropped don want to leave this company in search for self promotion love the company and culture so far thanks
data engineer intern interview tips adobe,hello everyone have an upcoming interview for data engineer intern with adobe can anyone give some tips or suggestions for preparation thanks in advance,0,data engineer intern interview tip adobe,hello everyone have an upcoming interview for data engineer intern with adobe can anyone give some tip or suggestion for preparation thanks in advance
show your love for sql,me and my team at census have build free feature called sync swag in nutshell you can write sql query that will send free shirt right to your doorstep no gimmicks this is totally legitimate please give it try this link provides step by step video tutorials if you prefer documentation you can complete sync swag here,0,show your love for sql,me and my team at census have build free feature called sync swag in nutshell you can write sql query that will send free shirt right to your doorstep no gimmick this is totally legitimate please give it try this link provides step by step video tutorial if you prefer documentation you can complete sync swag here
using python for data engineering,have always stuck to basic etl tools and sql for data loading read articles about people doing massive data loads with python what libraries do people use for massive data loads with python around million to million records day keep researching and everyone is saying use pandas which is huge memory hog on large parallel loads,0,using python for data engineering,have always stuck to basic etl tool and sql for data loading read article about people doing massive data load with python what library do people use for massive data load with python around million to million record day keep researching and everyone is saying use panda which is huge memory hog on large parallel load
fact tables in databricks using delta,hi all working with delta lake and databricks and we re currently in process of building dimension and fact tables gold layer new to this world so like to know what do you recommend for creating fact tables is it recommended to truncate them first and then fill with new data if we want to have only last information have experience in traditional dwh where we just truncate fact but speaking with more experienced colleagues it seems to me that truncate is not best practice in delta world we re currently using merge for updating and inserting facts but in some cases it seems that truncate would do the work in most convenient way any suggestions on this topic are welcome thanks,0,fact table in databricks using delta,hi all working with delta lake and databricks and we re currently in process of building dimension and fact table gold layer new to this world so like to know what do you recommend for creating fact table is it recommended to truncate them first and then fill with new data if we want to have only last information have experience in traditional dwh where we just truncate fact but speaking with more experienced colleague it seems to me that truncate is not best practice in delta world we re currently using merge for updating and inserting fact but in some case it seems that truncate would do the work in most convenient way any suggestion on this topic are welcome thanks
which databricks certification is more valuable for data engineer career,databricks certified associate developer for apache spark or databricks certified professional data engineer,0,which databricks certification is more valuable for data engineer career,databricks certified associate developer for apache spark or databricks certified professional data engineer
ve built service for our community,hi ve built service that may be helpful for new and seasoned de it free but want to make it exclusive for our community how can check if user belongs to dataengineering maybe someone had prior experience with similar idea,0,ve built service for our community,hi ve built service that may be helpful for new and seasoned de it free but want to make it exclusive for our community how can check if user belongs to dataengineering maybe someone had prior experience with similar idea
data engineering webinars mentorship,hi all first appreciate this community so much it not the intention to promote but we have webinars that would be beneficial to those looking to get into data engineering or even career switches truly believe this is needed in the industry coming from seasoned data engineer that been in the field for years you re welcomed to join our upcoming free webinars gt engineering career path gt google cloud data engineering services explained gt engineering career path gt intro to docker amp using docker in cloud data engineering also let us know what you re looking to learn love to speak with anyone that has de questions or needs mentorship we can answer questions here too thanks all parham,0,data engineering webinars mentorship,hi all first appreciate this community so much it not the intention to promote but we have webinars that would be beneficial to those looking to get into data engineering or even career switch truly believe this is needed in the industry coming from seasoned data engineer that been in the field for year you re welcomed to join our upcoming free webinars gt engineering career path gt google cloud data engineering service explained gt engineering career path gt intro to docker amp using docker in cloud data engineering also let u know what you re looking to learn love to speak with anyone that ha de question or need mentorship we can answer question here too thanks all parham
data engineering jargon part,hi this is the next is here utm_medium web2x amp context is here batch processing an automated way of processing millions of data transactions at the same time this is generally carried out overnight with the help of batch jobs loading all the customer data that bought particular item on the day sql sql is structured query language or simply put language used to manage databases sql is transact sql which is proprietary microsoft extension of the sql language sql can be used ms sql server or azure sql database to write statement as follows select customer name from tbl customer information where customer city london this provides the result of all the customer names where customers are based in london nosql although sql has been around for decades nosql not only sql is concept designed for non relational databases particularly to store unstructured data like documents storing an outlook email file in xml with key value pair on mongodb document database bteq batch teradata query like sql is simply utility and query tool for teradata which is relational database system creating bteq script to load data from flat file cloud delivery of computing services such as servers networking analytics etc over the internet instead of using dedicated data centre for an organisation storing data on microsoft azure cloud service instead of on an on premise solution data architecture the discipline of managing the people processes and technologies relating to data includes data strategy data capture processes and technical patterns to derive insight from the data data architect creates framework for an enterprise to manage its data flow end to end data visualisation practice for visualising large amounts of data to derive key insights to drive business decisions an executive dashboard that clearly outlines the sales performance of certain team data centres dedicated space nowadays millions of sqft of space which houses servers and systems for the organisation critical applications microsoft data centre to host all the company critical applications data integration usually the hardest part of the project where multiple sources of data are integrated into singular application data warehouse integrating finance and customer relationship systems integrating into an ms sql server database data migration the practice of migrating the data from source to destination migrating data from ms sql server database to an amazon relational database service,0,data engineering jargon part,hi this is the next is here utm_medium web2x amp context is here batch processing an automated way of processing million of data transaction at the same time this is generally carried out overnight with the help of batch job loading all the customer data that bought particular item on the day sql sql is structured query language or simply put language used to manage database sql is transact sql which is proprietary microsoft extension of the sql language sql can be used m sql server or azure sql database to write statement a follows select customer name from tbl customer information where customer city london this provides the result of all the customer name where customer are based in london nosql although sql ha been around for decade nosql not only sql is concept designed for non relational database particularly to store unstructured data like document storing an outlook email file in xml with key value pair on mongodb document database bteq batch teradata query like sql is simply utility and query tool for teradata which is relational database system creating bteq script to load data from flat file cloud delivery of computing service such a server networking analytics etc over the internet instead of using dedicated data centre for an organisation storing data on microsoft azure cloud service instead of on an on premise solution data architecture the discipline of managing the people process and technology relating to data includes data strategy data capture process and technical pattern to derive insight from the data data architect creates framework for an enterprise to manage it data flow end to end data visualisation practice for visualising large amount of data to derive key insight to drive business decision an executive dashboard that clearly outline the sale performance of certain team data centre dedicated space nowadays million of sqft of space which house server and system for the organisation critical application microsoft data centre to host all the company critical application data integration usually the hardest part of the project where multiple source of data are integrated into singular application data warehouse integrating finance and customer relationship system integrating into an m sql server database data migration the practice of migrating the data from source to destination migrating data from m sql server database to an amazon relational database service
optimizing write to s3,greetings we are generating an orc outputfile to customer but the export job takes hour to complete the export job is only fetching the latest dataset parquet and storing it to s3 as orc the issue is that the customer wants to have only one file which means that we have to use repartition during the write operation if have understood correctly repartition collects the data to single node and therefore all other nodes stand idle and is not optimal is there way to optimize this thank you,0,optimizing write to s3,greeting we are generating an orc outputfile to customer but the export job take hour to complete the export job is only fetching the latest dataset parquet and storing it to s3 a orc the issue is that the customer want to have only one file which mean that we have to use repartition during the write operation if have understood correctly repartition collect the data to single node and therefore all other node stand idle and is not optimal is there way to optimize this thank you
data engineer imposter syndrome,hello everyone currently having years of experience in it and into my second company moved out after years with the first one so far believed that was data engineer creating data pipelines to ingest transform and curate data but off late having an imposter syndrome at both my companies am coming to realise that never created any data pipelines from scratch nor wrote any code for this they used to have their respective framework jobs to ingest and curate the data all did was to add custom json config with the details of the source ingesting into the repo and the framework used to take care of the rest by picking up data specified in the config and transforming it according to the parameters specified in the config have no clue on the logic spark code doing all the heavy lifting in the background basically all my infrastructure is already in place and just adding few lines in separate config for my source is such profile common in the data engineering domain or is my imposter syndrome theory actually correct and would negatively impact my career over the long run any advice and discussion greatly appreciated tldr both companies have worked in have infrastructure already in place for ingestion transformation and curation as just adding custom json configs with details around my source believe having imposter syndrome since not writing any code to develop data pipelines from scratch,0,data engineer imposter syndrome,hello everyone currently having year of experience in it and into my second company moved out after year with the first one so far believed that wa data engineer creating data pipeline to ingest transform and curate data but off late having an imposter syndrome at both my company am coming to realise that never created any data pipeline from scratch nor wrote any code for this they used to have their respective framework job to ingest and curate the data all did wa to add custom json config with the detail of the source ingesting into the repo and the framework used to take care of the rest by picking up data specified in the config and transforming it according to the parameter specified in the config have no clue on the logic spark code doing all the heavy lifting in the background basically all my infrastructure is already in place and just adding few line in separate config for my source is such profile common in the data engineering domain or is my imposter syndrome theory actually correct and would negatively impact my career over the long run any advice and discussion greatly appreciated tldr both company have worked in have infrastructure already in place for ingestion transformation and curation a just adding custom json configs with detail around my source believe having imposter syndrome since not writing any code to develop data pipeline from scratch
software engineer become data engineer,have work as software engineer for about years years as backend engineer year as fullstack and want to change my career to be data engineer amp x200b what should learn and how convince the recruiters that serious to change my career,0,software engineer become data engineer,have work a software engineer for about year year a backend engineer year a fullstack and want to change my career to be data engineer amp x200b what should learn and how convince the recruiter that serious to change my career
how hard would it be to get data scientist job if work as data engineer,let say can choose between data engineer and and data scientist position if accept the data engineer position and work for years and decide it not for me how hard would it be to get data scientist job,0,how hard would it be to get data scientist job if work a data engineer,let say can choose between data engineer and and data scientist position if accept the data engineer position and work for year and decide it not for me how hard would it be to get data scientist job
skilling up in data engineering,interested in skilling up for data engineering for an ai alignment lab and think the stack amp skills gained will translate to other ai labs can buy the book designing data intensive applications as suggested elsewhere in the sub given the list of job requirements below would you suggest mooc certificate or focus on project high performance large scale compute systems kubernetes python os internals networking large scale etl spark pytorch deep learning mooc certificate it not clear this would be helpful but maybe ve missed course cert that covers the above project they re running models on massive clusters so don know if my small project would translate background data scientist for years solid python skills with some deep learning in my spare time timing in this for the long haul and don think ready for the above role today more about building the skills today for the next opportunity_,0,skilling up in data engineering,interested in skilling up for data engineering for an ai alignment lab and think the stack amp skill gained will translate to other ai lab can buy the book designing data intensive application a suggested elsewhere in the sub given the list of job requirement below would you suggest mooc certificate or focus on project high performance large scale compute system kubernetes python o internals networking large scale etl spark pytorch deep learning mooc certificate it not clear this would be helpful but maybe ve missed course cert that cover the above project they re running model on massive cluster so don know if my small project would translate background data scientist for year solid python skill with some deep learning in my spare time timing in this for the long haul and don think ready for the above role today more about building the skill today for the next opportunity_
is there problem if signed an intern contract for months but can finish,currently on intern as backend developer but other company offers me job as data engineer but need to join with them before th january but my intern finish on st february amp x200b my principal professional objective is be data engineer so consider lot leave my intern and bet all for this position as data engineer jr,0,is there problem if signed an intern contract for month but can finish,currently on intern a backend developer but other company offer me job a data engineer but need to join with them before th january but my intern finish on st february amp x200b my principal professional objective is be data engineer so consider lot leave my intern and bet all for this position a data engineer jr
interview question dilemma,what comes to your mind the first time you listen the instructions what is the third largest number of this sequence you say is or what the number you would get after applying max and removing it three times,0,interview question dilemma,what come to your mind the first time you listen the instruction what is the third largest number of this sequence you say is or what the number you would get after applying max and removing it three time
choosing columnar db,have single table with about integer columns with billions of rows goal is to support fast queries that select all columns but can filter on any combination of columns what would be good columnar db for this the data isn live gets full refresh once month so not sure dbs like clickhouse pinot druid are the right choice since they seem to place lot of emphasis on real time ingestion with kafka but that might be wrong impression,0,choosing columnar db,have single table with about integer column with billion of row goal is to support fast query that select all column but can filter on any combination of column what would be good columnar db for this the data isn live get full refresh once month so not sure db like clickhouse pinot druid are the right choice since they seem to place lot of emphasis on real time ingestion with kafka but that might be wrong impression
how well do data engineering skills transfer to software engineering,spent my whole career in data so far year in data science and years as data engineer definitely find the work being done as data engineer more interesting than data science considering that lot of our skills and engineering processes already overlap with software engineering is there lot of overlap between both careers,0,how well do data engineering skill transfer to software engineering,spent my whole career in data so far year in data science and year a data engineer definitely find the work being done a data engineer more interesting than data science considering that lot of our skill and engineering process already overlap with software engineering is there lot of overlap between both career
da to be interviewed by de,have an upcoming da interview with de will be working with am comfortable with sql and proficient in and python what kind of questions should expect am not sure how technical the interview might get have also not done much advanced stats in my previous positions is there anything should brush up on the job description is mostly etl sql and tableau so not even sure if stats questions will be asked but just want to be sure any help is greatly appreciated thanks,0,da to be interviewed by de,have an upcoming da interview with de will be working with am comfortable with sql and proficient in and python what kind of question should expect am not sure how technical the interview might get have also not done much advanced stats in my previous position is there anything should brush up on the job description is mostly etl sql and tableau so not even sure if stats question will be asked but just want to be sure any help is greatly appreciated thanks
how data engineers do software design invitation to participate in research study,hi dataengineering part of group of researchers from carnegie mellon university university of washington and george mason university studying software design your expertise on how solving data engineering problems impacts the design decisions you make would be very useful in our survey link your input in this field will help us better understand software design which we hope to share with other researchers and technology companies to inspire new innovations in developer tools and static analysis tools for software architecture,0,how data engineer do software design invitation to participate in research study,hi dataengineering part of group of researcher from carnegie mellon university university of washington and george mason university studying software design your expertise on how solving data engineering problem impact the design decision you make would be very useful in our survey link your input in this field will help u better understand software design which we hope to share with other researcher and technology company to inspire new innovation in developer tool and static analysis tool for software architecture
which certification would you recommend me to take,hi guys did microsoft ignite cloud skills challenge and got free certification exam but as dp203 is not available on eligible exams list would like to ask you for your suggestions which one would be the most useful one for data engineer az microsoft azure administrator az developing solutions for microsoft azure dp administering relational databases on microsoft azure az administering windows server hybrid core infrastructure az configuring windows server hybrid advanced services pl microsoft power platform developer know that these certificats aren directly connected to my job as beginner data engineer but still it would be good to expend my knowledge about azure because have contact with it on daily basic,0,which certification would you recommend me to take,hi guy did microsoft ignite cloud skill challenge and got free certification exam but a dp203 is not available on eligible exam list would like to ask you for your suggestion which one would be the most useful one for data engineer az microsoft azure administrator az developing solution for microsoft azure dp administering relational database on microsoft azure az administering window server hybrid core infrastructure az configuring window server hybrid advanced service pl microsoft power platform developer know that these certificats aren directly connected to my job a beginner data engineer but still it would be good to expend my knowledge about azure because have contact with it on daily basic
spark jobs unit functional test and amp,hello was wondering how do you guys test your spark batch jobs im junior de and work at huge bank insurance company they have legacy data platform running on mapr distribution of hadoop spark they have clusters one for amp and another one for production they problem is that the data in the amp environment isn enough to cover all use cases so they ask data engineers to write data inside it in order to test new features and submit those test to the business analysts for qualification find this process bit annoying like everytime develop my new features in like hours and then spend day or two to generate proper dataset for testing as it generally involves like tables or so like to know how to guys test your spark jobs and how do you convince the business analysts that what you developped actually does what was specified regards,0,spark job unit functional test and amp,hello wa wondering how do you guy test your spark batch job im junior de and work at huge bank insurance company they have legacy data platform running on mapr distribution of hadoop spark they have cluster one for amp and another one for production they problem is that the data in the amp environment isn enough to cover all use case so they ask data engineer to write data inside it in order to test new feature and submit those test to the business analyst for qualification find this process bit annoying like everytime develop my new feature in like hour and then spend day or two to generate proper dataset for testing a it generally involves like table or so like to know how to guy test your spark job and how do you convince the business analyst that what you developped actually doe what wa specified regard
data engineering jargon part,hi this is the next first ten is here utm_medium web2x amp context ingestion generally the first step in data pipeline where data is ingested into the platform pipeline where customer address data is ingested from source extract transform load etl step process of extracting data and transforming it by applying some kind of logic like aggregation and loading the new information into the destination it could be used as elt where the destination tables transform the data instead an extract of customer address data is taken from the customer relationship management tool and is then aggregated according to their cities and this new information is loaded into destination data models way of organising the data in way that it can be understood in real world scenario taking huge amount of data and logically grouping it into customer product and location data normalisation method of organising the data in granular enough format that it can be utilised for different purposes over time usually this is done by normalising the data into different forms such as nf normal form or nf rd normal form which is the most common taking customer order data and creating granular information model order in one table item ordered in another table customer contact in another table payment of the order in another table this allows for the data to be re used for different purposes over time star schema the simplest way to model data into different quantitative and qualitative data is called facts and dimensions usually the fact table is interpreted with the help of dimensions table resembling star star schema of sales data with dimensions such as customer product amp time facts data warehousing term for quantitative information the number of orders placed by customer dimensions data warehousing term for qualitative information name of the customer or their country of residence schemas term for collection of database objects these are generally used to logically separate data within the database and apply access controls storing hr data in hr schema allows logical segregation from other data in the organisation scd type method to deal with changes in the data over time in data warehouse type is when history is overwritten whereas type most common is when history is maintained each time change occurs when customer changes their address scd type would overwrite the old address with the new one whereas type would store both addresses to maintain history business intelligence slightly out of date term for combination of practices to derive business insights from data by predominantly using data warehousing analytics and dashboarding creating management dashboard to show customer demographics across the country,0,data engineering jargon part,hi this is the next first ten is here utm_medium web2x amp context ingestion generally the first step in data pipeline where data is ingested into the platform pipeline where customer address data is ingested from source extract transform load etl step process of extracting data and transforming it by applying some kind of logic like aggregation and loading the new information into the destination it could be used a elt where the destination table transform the data instead an extract of customer address data is taken from the customer relationship management tool and is then aggregated according to their city and this new information is loaded into destination data model way of organising the data in way that it can be understood in real world scenario taking huge amount of data and logically grouping it into customer product and location data normalisation method of organising the data in granular enough format that it can be utilised for different purpose over time usually this is done by normalising the data into different form such a nf normal form or nf rd normal form which is the most common taking customer order data and creating granular information model order in one table item ordered in another table customer contact in another table payment of the order in another table this allows for the data to be re used for different purpose over time star schema the simplest way to model data into different quantitative and qualitative data is called fact and dimension usually the fact table is interpreted with the help of dimension table resembling star star schema of sale data with dimension such a customer product amp time fact data warehousing term for quantitative information the number of order placed by customer dimension data warehousing term for qualitative information name of the customer or their country of residence schema term for collection of database object these are generally used to logically separate data within the database and apply access control storing hr data in hr schema allows logical segregation from other data in the organisation scd type method to deal with change in the data over time in data warehouse type is when history is overwritten whereas type most common is when history is maintained each time change occurs when customer change their address scd type would overwrite the old address with the new one whereas type would store both address to maintain history business intelligence slightly out of date term for combination of practice to derive business insight from data by predominantly using data warehousing analytics and dashboarding creating management dashboard to show customer demographic across the country
guide to read the data warehouse toolkit to save you from reading cover cover and outdated topics,,0,guide to read the data warehouse toolkit to save you from reading cover cover and outdated topic,
career progression to de,don see lot of junior de positions in my area most of them requires around yrs of it experience what is the easiest way to get into de what are the jobs that should take as stepping stone for becoming de would back end web development be good stepping stone or should take data analysis jobs instead how about being an sql dev career shifter currently studying front end web development hope you can help me thank you,0,career progression to de,don see lot of junior de position in my area most of them requires around yr of it experience what is the easiest way to get into de what are the job that should take a stepping stone for becoming de would back end web development be good stepping stone or should take data analysis job instead how about being an sql dev career shifter currently studying front end web development hope you can help me thank you
looking for video course on effective sql queries and database administration in,hi guys see lot of these posts looking for learning pathways for new data engineers however lot of them point to articles or sql exercise websites enjoy courses because that helps structure my learning ensuring am introduced to concepts in appropriate order etc am after recommendations for paid free course that offers video explanations and exercises that covers effective sql queries sql statements unions sub queries group by having what are cte udf how to use them database administration optimisation data modelling normalisation indexing amp x200b also am interesting in data pipeline infrastructure anything that covers setting up and maintaining pipelines within the azure environment and utilising docker airflow would be great,0,looking for video course on effective sql query and database administration in,hi guy see lot of these post looking for learning pathway for new data engineer however lot of them point to article or sql exercise website enjoy course because that help structure my learning ensuring am introduced to concept in appropriate order etc am after recommendation for paid free course that offer video explanation and exercise that cover effective sql query sql statement union sub query group by having what are cte udf how to use them database administration optimisation data modelling normalisation indexing amp x200b also am interesting in data pipeline infrastructure anything that cover setting up and maintaining pipeline within the azure environment and utilising docker airflow would be great
how to best learn data engineering,have no technical background business major but would like to develop deeper understanding of data engineering in the finance investing field and think that having deeper understanding of the technicalities would give me an edge ve been reading whitepapers research reports high level tutorials etc but feel as if still don understand it quite as well as needed as you can only get so far with reading other people research think the roadblock is that not daily user of data engineering tools snowflake sql etc don know how to code develop programs apps etc do you think that having true understanding of different de tools concepts frameworks overall industry and ability to judge different tools trends would require me to actually partake in some technical learning learning how to code etc for ex even the technical parts of whitepapers don understand thanks for your advice,0,how to best learn data engineering,have no technical background business major but would like to develop deeper understanding of data engineering in the finance investing field and think that having deeper understanding of the technicality would give me an edge ve been reading whitepapers research report high level tutorial etc but feel a if still don understand it quite a well a needed a you can only get so far with reading other people research think the roadblock is that not daily user of data engineering tool snowflake sql etc don know how to code develop program apps etc do you think that having true understanding of different de tool concept framework overall industry and ability to judge different tool trend would require me to actually partake in some technical learning learning how to code etc for ex even the technical part of whitepapers don understand thanks for your advice
does anyone have experience documenting airflow projects with sphinx,tend to use sphinx to document my projects and so far haven found any useful extensions to get sphinx to recognize the doc_mds in the dags and tasks has anyone else found helpful tools for this,0,doe anyone have experience documenting airflow project with sphinx,tend to use sphinx to document my project and so far haven found any useful extension to get sphinx to recognize the doc_mds in the dag and task ha anyone else found helpful tool for this
how does apache airflow create pipelines,so apache airflow is workflow management framework correct so how does it aid in construction of etl pipelines,0,how doe apache airflow create pipeline,so apache airflow is workflow management framework correct so how doe it aid in construction of etl pipeline
in your opinion what skills experience do you consider minimum qualification for data engineering,in my current position not data engineering do what would consider novice data engineering but then read posts here and find much more complex set of skills like to transition into the field but wanted to get an opinion on what the absolute minimum would be versus desired skills listing you see in job postings what basic skills and experiences did you have that made you think of yourself as legit data engineer,0,in your opinion what skill experience do you consider minimum qualification for data engineering,in my current position not data engineering do what would consider novice data engineering but then read post here and find much more complex set of skill like to transition into the field but wanted to get an opinion on what the absolute minimum would be versus desired skill listing you see in job posting what basic skill and experience did you have that made you think of yourself a legit data engineer
moving to consulting role without much knowledge of modern tech stack,hey guys have aboht yoe and am looking to start applying to analytics engineering or data engineering consultant positions don have much experience in the modem tech stack dbt cloud tools fivetran airflow etc my current company uses proprietary system however much of it is very similar to these other tools listed on job postings my current expertise is in spark python etl dwh beyond my technical skills ve been on many very high impact projects and am used to interacting with very senior management because our org is very underdeveloped when it comes to tech professionals so ve gotten lot of experiences that many new grads haven especially with stakeholder client management ve been using free trials to explore other tools become familiar with them but certainly no expert in them is this enough to transition to consulting what other steps should take to get this next role,0,moving to consulting role without much knowledge of modern tech stack,hey guy have aboht yoe and am looking to start applying to analytics engineering or data engineering consultant position don have much experience in the modem tech stack dbt cloud tool fivetran airflow etc my current company us proprietary system however much of it is very similar to these other tool listed on job posting my current expertise is in spark python etl dwh beyond my technical skill ve been on many very high impact project and am used to interacting with very senior management because our org is very underdeveloped when it come to tech professional so ve gotten lot of experience that many new grad haven especially with stakeholder client management ve been using free trial to explore other tool become familiar with them but certainly no expert in them is this enough to transition to consulting what other step should take to get this next role
uk covid dashboard built using postgres and citus for distributed scale cross post from sql,uk covid dashboard built using postgres and citus for distributed scale cross post from sql,0,uk covid dashboard built using postgres and citus for distributed scale cross post from sql,uk covid dashboard built using postgres and citus for distributed scale cross post from sql
what certifications are worth the investment,data engineer with months of experience and really like the kind of work do my day to day tech stack is azure amp databricks work lot with developing pipelines for moving data from sqldb amp sqldw to cosmos along with optimizing spark code for ml models and managing some adf pipelines this month now until mid jan my boss is having my team start no new projects and to focus on standardizing our processes and training am trying to figure out what certifications should study for and if they re worth it my current plan is below azure dp azure data fundamentals azure dp azure data engineering databricks certified associate developer for apache spark it overwhelming trying to study for so many exams at once but when ve looked over the corse material for each already know how to do of it because of my daily work are getting these certifications worth it or are they something future jobs won care about as much as like my current job plan on leaving it in the summer for better pay,0,what certification are worth the investment,data engineer with month of experience and really like the kind of work do my day to day tech stack is azure amp databricks work lot with developing pipeline for moving data from sqldb amp sqldw to cosmos along with optimizing spark code for ml model and managing some adf pipeline this month now until mid jan my bos is having my team start no new project and to focus on standardizing our process and training am trying to figure out what certification should study for and if they re worth it my current plan is below azure dp azure data fundamental azure dp azure data engineering databricks certified associate developer for apache spark it overwhelming trying to study for so many exam at once but when ve looked over the corse material for each already know how to do of it because of my daily work are getting these certification worth it or are they something future job won care about a much a like my current job plan on leaving it in the summer for better pay
opinions on snowflake,what your guy thoughts on the cloud based data warehousing service snowflake good bad do you use it even,0,opinion on snowflake,what your guy thought on the cloud based data warehousing service snowflake good bad do you use it even
how much is data engineer expected to know about ci cd,understand what it is and where it can be applied but don really know anything about implementation do you guys work with data engineers that build pipelines that already include ci cd or do they just write the testing logic and hand it off don know how to webhook,0,how much is data engineer expected to know about ci cd,understand what it is and where it can be applied but don really know anything about implementation do you guy work with data engineer that build pipeline that already include ci cd or do they just write the testing logic and hand it off don know how to webhook
consulting vs saas role,hello work currently as bi developer with an analytics consulting firm and primarily work on dwh projects as data engineer with years exp tech stack is around azure with adf synapse and sql db have be offered data engineer role with year old saas startup role looks to be focused on data ingestion pipelining and preliminary processing heavy lifting on data processing is done by sdes in java tech stack is adf python and sql new role offers decent hike am still unclear on the pros and cons on both sides consulting vs saas whether saas role will restrict my tech stack as may get snowflake amp dbt work in consulting role if req arises need advise on deciding on the new role,0,consulting v saas role,hello work currently a bi developer with an analytics consulting firm and primarily work on dwh project a data engineer with year exp tech stack is around azure with adf synapse and sql db have be offered data engineer role with year old saas startup role look to be focused on data ingestion pipelining and preliminary processing heavy lifting on data processing is done by sdes in java tech stack is adf python and sql new role offer decent hike am still unclear on the pro and con on both side consulting v saas whether saas role will restrict my tech stack a may get snowflake amp dbt work in consulting role if req arises need advise on deciding on the new role
dimensional model for dropbox interview question,hey this sub is great have learnt lots from it so far and hopefully someone can point me in the right direction for my question am prepping for de interview and read tips online to practice building out dimensional models for the likes of uber and dropbox find the uber one straightforward grain being trip but am bit lost with dropbox was thinking the grain could be file upload or else an action file upload download move delete would you treat dim location as ragged hierarchy how would you model dropbox amp x200b tia,0,dimensional model for dropbox interview question,hey this sub is great have learnt lot from it so far and hopefully someone can point me in the right direction for my question am prepping for de interview and read tip online to practice building out dimensional model for the like of uber and dropbox find the uber one straightforward grain being trip but am bit lost with dropbox wa thinking the grain could be file upload or else an action file upload download move delete would you treat dim location a ragged hierarchy how would you model dropbox amp x200b tia
data engineering medium paywall is it worth it,ve been software developer for almost decade and have been increasingly doing data engineering year after year last year made the switch and am now full time data engineer however since switching ve noticed that the topics research for my job are increasingly behind medium paywall unlike when was doing back end or front end development built my career off of google searches stackoverflow twitter and other free sources so it unfamiliar to me that tech blogs would cost money to read does anyone in this space subscribe to medium for data engineering have they found it helpful for example seeing many blogs by towardsdatascience behind this medium paywall,0,data engineering medium paywall is it worth it,ve been software developer for almost decade and have been increasingly doing data engineering year after year last year made the switch and am now full time data engineer however since switching ve noticed that the topic research for my job are increasingly behind medium paywall unlike when wa doing back end or front end development built my career off of google search stackoverflow twitter and other free source so it unfamiliar to me that tech blog would cost money to read doe anyone in this space subscribe to medium for data engineering have they found it helpful for example seeing many blog by towardsdatascience behind this medium paywall
data engineering jargon,wrote list of ll share ten at time data dump file or table containing significant amount of data to be analysed or transferred table containing the data dump of all customer addresses data pipelines data processing method akin to pipeline which starts with data ingestion then processing then completion pipeline where customer address data is ingested from source and then aggregated according to their cities and this new information is loaded into destination dba database administrator is an admin role that understands the particular database technology and how to get the best out of it this includes improving performance backups and recovery performance tuning the database to respond better to particular complex data queries data warehouse method of organising data to make it easy to analyse and report to make business decisions oracle data warehouse organising customer data in data warehouse to be able to report the number of newly acquired customers data mart subset of data warehouse created for very specific business use case finance data mart storing all the relevant financial information required by the accounting team to process their month end cycles ods operational data store generally stores limited and current information to help simple queries unable to handle historical or complex data queries an ods for daily stock fluctuations in warehouse help the warehouse manager decide what to prioritise in the next order delivery edw the same as data warehouse except it includes all the data within an organisation this means that the entire enterprise can rely on this warehouse for their business decisions organising sales customer marketing and finance data in an enterprise data warehouse to be able to create several key management reports rdbms relational database management system all of the above examples are rdbms meaning they store data in structured format using rows and columns microsoft sql server database in memory db traditional databases have been used for complex calculations and queries they store information on the actual disk in the computer in memory db stores all the information on their memory ram this allows for rapid calculations without read and write function to normal disk drill down functionality of live dashboard data lake repository for all kinds of structured and unstructured data mainly based on hadoop storage technology called lake as it is flexible enough to store anything from raw data to unstructured email files hadoop data lake storing logs of all customers called into the inbound call centre including call duration,0,data engineering jargon,wrote list of ll share ten at time data dump file or table containing significant amount of data to be analysed or transferred table containing the data dump of all customer address data pipeline data processing method akin to pipeline which start with data ingestion then processing then completion pipeline where customer address data is ingested from source and then aggregated according to their city and this new information is loaded into destination dba database administrator is an admin role that understands the particular database technology and how to get the best out of it this includes improving performance backup and recovery performance tuning the database to respond better to particular complex data query data warehouse method of organising data to make it easy to analyse and report to make business decision oracle data warehouse organising customer data in data warehouse to be able to report the number of newly acquired customer data mart subset of data warehouse created for very specific business use case finance data mart storing all the relevant financial information required by the accounting team to process their month end cycle od operational data store generally store limited and current information to help simple query unable to handle historical or complex data query an od for daily stock fluctuation in warehouse help the warehouse manager decide what to prioritise in the next order delivery edw the same a data warehouse except it includes all the data within an organisation this mean that the entire enterprise can rely on this warehouse for their business decision organising sale customer marketing and finance data in an enterprise data warehouse to be able to create several key management report rdbms relational database management system all of the above example are rdbms meaning they store data in structured format using row and column microsoft sql server database in memory db traditional database have been used for complex calculation and query they store information on the actual disk in the computer in memory db store all the information on their memory ram this allows for rapid calculation without read and write function to normal disk drill down functionality of live dashboard data lake repository for all kind of structured and unstructured data mainly based on hadoop storage technology called lake a it is flexible enough to store anything from raw data to unstructured email file hadoop data lake storing log of all customer called into the inbound call centre including call duration
data cleansing tools used by large enterprises,what are the most popular data cleansing validation tools used by mid large companies especially around their etl pipelines and data warehouses,0,data cleansing tool used by large enterprise,what are the most popular data cleansing validation tool used by mid large company especially around their etl pipeline and data warehouse
what do you think about automating data assets documentation it is big problem how does data documentation process look like in your company,hi my name is igor am thinking of startup ideas in the data governance space what are the problems do you see in the space what do you think about data catalogs data documentation how does data documentation process look like in your company would really appreciate if you can check my idea here,0,what do you think about automating data asset documentation it is big problem how doe data documentation process look like in your company,hi my name is igor am thinking of startup idea in the data governance space what are the problem do you see in the space what do you think about data catalog data documentation how doe data documentation process look like in your company would really appreciate if you can check my idea here
data engineering course help,hello everyone have it background my last job was net developer but that was years ago cannot find job since then have decided to get into data engineering and getting my gcp does anyone have any suggestions on botocamps that guarantee job or any courses that can be helpful for me thanks,0,data engineering course help,hello everyone have it background my last job wa net developer but that wa year ago cannot find job since then have decided to get into data engineering and getting my gcp doe anyone have any suggestion on botocamps that guarantee job or any course that can be helpful for me thanks
data engineer at big gaming company vs an analytical firm,have two job offers one at big gaming company think ubisoft ea the other at boutique analytical firm that does analytics consulting in my head think an analytical firm is better to learn things but always liked gaming and think analysing game data would be lot more fun but the tech stack does seem bit outdated hadoop spark cassandra etc and its on prem anyone has any experience or suggestions regarding this,0,data engineer at big gaming company v an analytical firm,have two job offer one at big gaming company think ubisoft ea the other at boutique analytical firm that doe analytics consulting in my head think an analytical firm is better to learn thing but always liked gaming and think analysing game data would be lot more fun but the tech stack doe seem bit outdated hadoop spark cassandra etc and it on prem anyone ha any experience or suggestion regarding this
data engineering how to or starter guide,so work for business unit who gets dbms view dumps on montly basis not too large maybe databes and only in the millions of rows per month so far ive been to just used excels power query to load the data then store in data model and create reports recently ppl have been very happy with reporting and are requesting more reports recently we got redshift avialable for use on premise and im trying to figure out how to set this up have couple of years worth of data however its not consistent like the dbms views have changed maybe once or was thinking pushing all the dbms views into into staging area for simpler access and to be able to create pipeline to transform data push it into datamarts then from datamarts create reports so ideas for this project does it sound right bunch of independent datamarts with single staging area then use pipeline to populate datamarts and create reports any ideas for redshift architecture courses on how to set up pods any help is welcomed,0,data engineering how to or starter guide,so work for business unit who get dbms view dump on montly basis not too large maybe databes and only in the million of row per month so far ive been to just used excels power query to load the data then store in data model and create report recently ppl have been very happy with reporting and are requesting more report recently we got redshift avialable for use on premise and im trying to figure out how to set this up have couple of year worth of data however it not consistent like the dbms view have changed maybe once or wa thinking pushing all the dbms view into into staging area for simpler access and to be able to create pipeline to transform data push it into datamarts then from datamarts create report so idea for this project doe it sound right bunch of independent datamarts with single staging area then use pipeline to populate datamarts and create report any idea for redshift architecture course on how to set up pod any help is welcomed
airflow scalability,hello am working on an etl process where created more than dags on airflow and unfortunately the dags break when they run simultaneously tasks fail but the logs are empty so it probably the scheduler or worker fault but when run few dags it never breaks wonder how much airflow can manage and depending on what did anyone encounter such scalability problems with airflow,0,airflow scalability,hello am working on an etl process where created more than dag on airflow and unfortunately the dag break when they run simultaneously task fail but the log are empty so it probably the scheduler or worker fault but when run few dag it never break wonder how much airflow can manage and depending on what did anyone encounter such scalability problem with airflow
apache airflow custom image on gcp,hi has anyone created custom image for apache airflow on google cloud not cloud composer would be great if you can share your experiences with me,0,apache airflow custom image on gcp,hi ha anyone created custom image for apache airflow on google cloud not cloud composer would be great if you can share your experience with me
on prem only data engineering,see that most discussion in the data engineering database field is always about cloud azure aws etc but as someone who company is on prem only due to data protection requirements is there much happening in the on prem sphere or is the area and my career falling behind the curve it feels like pretty much all new technolgy or company is cloud dependent,0,on prem only data engineering,see that most discussion in the data engineering database field is always about cloud azure aws etc but a someone who company is on prem only due to data protection requirement is there much happening in the on prem sphere or is the area and my career falling behind the curve it feel like pretty much all new technolgy or company is cloud dependent
multiprocessing for data pipeline optimization,hello there amp x200b have very simple pipeline that consists of three steps extract data from mongo db with pagination of documents transform data takes document from the step above and do some pandas transformation save data to excel writes pandas data frame chunk from the step above to excel this pipeline goes with the lazy approach over the whole mongodb storage in chunks so did all kind of optimization in each step mongo query is very optimized have their indexes that makes query very fast for pandas transformation the same it improved from sec per documents to seconds and same for excel use openpyxl with write mode only which makes it also much faster but still excel is their pain in the ass it takes like of the pipeline execution so let assume that have kk records in mongo so extract transform part will take but saving to excel days and one more thing is that my excels can have maximum of records more records cause some memory issues on my cloud amp x200b so am thinking about multiprocessing excels wonder about something like count the number of documents in mongo split storage in equal sizes start pipelines in the multi processes does it make any sense one issue is that will run the whole pipeline at the same time but in the end need to only process excel in pareall but as the pipeline is made as generator of three steps mongo documents gt transform gt write excel gt once more then it hard to start multiprocessing at the excel step probably can store data anywhere tried to load data first to sqlite and part of saving of excels was taking part after everything was extracted from mongo but it took to much storage and needed to leave that solution amp x200b can you help me little bit with some potential solutions for my issue would be great,0,multiprocessing for data pipeline optimization,hello there amp x200b have very simple pipeline that consists of three step extract data from mongo db with pagination of document transform data take document from the step above and do some panda transformation save data to excel writes panda data frame chunk from the step above to excel this pipeline go with the lazy approach over the whole mongodb storage in chunk so did all kind of optimization in each step mongo query is very optimized have their index that make query very fast for panda transformation the same it improved from sec per document to second and same for excel use openpyxl with write mode only which make it also much faster but still excel is their pain in the as it take like of the pipeline execution so let assume that have kk record in mongo so extract transform part will take but saving to excel day and one more thing is that my excels can have maximum of record more record cause some memory issue on my cloud amp x200b so am thinking about multiprocessing excels wonder about something like count the number of document in mongo split storage in equal size start pipeline in the multi process doe it make any sense one issue is that will run the whole pipeline at the same time but in the end need to only process excel in pareall but a the pipeline is made a generator of three step mongo document gt transform gt write excel gt once more then it hard to start multiprocessing at the excel step probably can store data anywhere tried to load data first to sqlite and part of saving of excels wa taking part after everything wa extracted from mongo but it took to much storage and needed to leave that solution amp x200b can you help me little bit with some potential solution for my issue would be great
is faang data engineer not far fetched dream,have seen many people from small companies who know nothing in the early stage get selected as data engineer in faang later in their careers but not many data scientists is it very competitive to grow as data scientist or does it requires more complex skills is it because data engineer has specific set of skills to master whereas data scientist needs lot of combined skills and continuous updation,0,is faang data engineer not far fetched dream,have seen many people from small company who know nothing in the early stage get selected a data engineer in faang later in their career but not many data scientist is it very competitive to grow a data scientist or doe it requires more complex skill is it because data engineer ha specific set of skill to master whereas data scientist need lot of combined skill and continuous updation
data analyst interview with the data engineering team,so ve recently applied for data analyst position and scheduled to proceed with interview with the data engineering team my question would be what kind of qualities and skills do you expect from data analyst in your team any tips is much appreciated thanks in advance,0,data analyst interview with the data engineering team,so ve recently applied for data analyst position and scheduled to proceed with interview with the data engineering team my question would be what kind of quality and skill do you expect from data analyst in your team any tip is much appreciated thanks in advance
how to manage changes in dimension field when using big flat table,let say that have sale fact entry made by user dimension initially would insert this sale user into my big flat table in my data warehouse so far sounds very standard but my doubt is about what should do when dimension entry have change for example the user change his address how should proceed in this scenario should introduce new sale using the same data as before with the user information updated into my big flat table note speaking about dimensions etc but not pretending to use at all an star schema as an intermediate step at least for my learning problem,0,how to manage change in dimension field when using big flat table,let say that have sale fact entry made by user dimension initially would insert this sale user into my big flat table in my data warehouse so far sound very standard but my doubt is about what should do when dimension entry have change for example the user change his address how should proceed in this scenario should introduce new sale using the same data a before with the user information updated into my big flat table note speaking about dimension etc but not pretending to use at all an star schema a an intermediate step at least for my learning problem
data format solution for storing nested json data in handy way ideally to be able to query with sql,hello need to create data pipeline using python that retrieves data from rest api and writes it to database currently have first version running using microsoft sql database the problem is that the json responses of the api is complex nested json in which there are several arrays that change frequently in the number of their elements as result have to constantly adjust the pipeline and database tables to accommodate the new fields and changes amp x200b do you know way how can build pipeline in way that can also save changing arrays as fields so that the query of the target data works without problems and without having to make manual changes assuming sql server just isn designed for this would you recommend me mongodb for this or using parquet files amp x200b translated with www deepl com translator free version,0,data format solution for storing nested json data in handy way ideally to be able to query with sql,hello need to create data pipeline using python that retrieves data from rest api and writes it to database currently have first version running using microsoft sql database the problem is that the json response of the api is complex nested json in which there are several array that change frequently in the number of their element a result have to constantly adjust the pipeline and database table to accommodate the new field and change amp x200b do you know way how can build pipeline in way that can also save changing array a field so that the query of the target data work without problem and without having to make manual change assuming sql server just isn designed for this would you recommend me mongodb for this or using parquet file amp x200b translated with www deepl com translator free version
interested in learning together,hi have been trying to move in to de roles for the last few months it not that easy to switch tech stack understand without having any solid side projects to include in the resume it is hard to get in to de roles or something similar follow the seattledataguy youtube channel and his medium posts he has some good info and posts about how to start building side projects and general direction on how to move into the de space slowly and master tools and technologies he has medium post data engineering roadmap for based on that am planing to build python flask api as first project and then slowly build things on top of it that the plan anyone interested in getting into this we can follow and do the different steps projects mentioned in that post please let me know in comments,0,interested in learning together,hi have been trying to move in to de role for the last few month it not that easy to switch tech stack understand without having any solid side project to include in the resume it is hard to get in to de role or something similar follow the seattledataguy youtube channel and his medium post he ha some good info and post about how to start building side project and general direction on how to move into the de space slowly and master tool and technology he ha medium post data engineering roadmap for based on that am planing to build python flask api a first project and then slowly build thing on top of it that the plan anyone interested in getting into this we can follow and do the different step project mentioned in that post please let me know in comment
presenting one to many record comparisons approve amp deny options,hey all this question might be borderline off topic for this sub but think the follower have here are most likely to have been in this situation ve been working on large data matching exercise where we re trying to identify company from legacy system in modernized system we ve knocked out the simple amp straight forward matches using company identifiers and we re down to fuzzy matching company names addresses etc we re getting good results but my output at the moment is an excel spreadsheet that non data team members are struggling to wrap their head around looking for advice on how to present the results in an intuitive way to allow my team members to approve or deny match quickly amp accurately legacy company has attributes while the company in the modernized system has attributes amp many addresses that would need compared could spend couple days spinning up lightweight web app but keep thinking something is already out there,0,presenting one to many record comparison approve amp deny option,hey all this question might be borderline off topic for this sub but think the follower have here are most likely to have been in this situation ve been working on large data matching exercise where we re trying to identify company from legacy system in modernized system we ve knocked out the simple amp straight forward match using company identifier and we re down to fuzzy matching company name address etc we re getting good result but my output at the moment is an excel spreadsheet that non data team member are struggling to wrap their head around looking for advice on how to present the result in an intuitive way to allow my team member to approve or deny match quickly amp accurately legacy company ha attribute while the company in the modernized system ha attribute amp many address that would need compared could spend couple day spinning up lightweight web app but keep thinking something is already out there
how do you usually manage schema evolution in your star data model,building data model just learning project my project has some schemas that helps me to validate data and extract fields from some files then built my data model now know that if my schema change this needs to be reflected in my star data model for example if new column is added have to set null values for previous values if column is removed have to remove the column things like that however how do you usually approach this do you compare column by column is there something elegant out there or an starndard way of doing this thanks in advance honestly this channel is being very helpful thanks,0,how do you usually manage schema evolution in your star data model,building data model just learning project my project ha some schema that help me to validate data and extract field from some file then built my data model now know that if my schema change this need to be reflected in my star data model for example if new column is added have to set null value for previous value if column is removed have to remove the column thing like that however how do you usually approach this do you compare column by column is there something elegant out there or an starndard way of doing this thanks in advance honestly this channel is being very helpful thanks
carto amp databricks bringing spatial analysis to the lakehouse platform link in comments,carto amp databricks bringing spatial analysis to the lakehouse platform link in comments,0,carto amp databricks bringing spatial analysis to the lakehouse platform link in comment,carto amp databricks bringing spatial analysis to the lakehouse platform link in comment
course recommendations to learn data engineering,understand are lot of courses out there but would like to get course recommendations from data engineers themselves already know python and can work with bit of data but want to go through course that ll teach me de from start to finish thanks,0,course recommendation to learn data engineering,understand are lot of course out there but would like to get course recommendation from data engineer themselves already know python and can work with bit of data but want to go through course that ll teach me de from start to finish thanks
hi everyone need your advise,im not exactly data engineer but very interested in the field currently trying to run highload pet project have python scripts that generates large volume of data about several million rows and sends it to the database using airflow once day now want to send batches of data based on certain event which in my case is just random integer was told that airflow is not good solution for that can you give any comments another thing is want to aggregate data along the way to take down the size of batches but not sure how to address the memory issue will airflow store all the million rows during the aggregation is there some limit on the storage it provides can really find that information anywhere thank you in advance,0,hi everyone need your advise,im not exactly data engineer but very interested in the field currently trying to run highload pet project have python script that generates large volume of data about several million row and sends it to the database using airflow once day now want to send batch of data based on certain event which in my case is just random integer wa told that airflow is not good solution for that can you give any comment another thing is want to aggregate data along the way to take down the size of batch but not sure how to address the memory issue will airflow store all the million row during the aggregation is there some limit on the storage it provides can really find that information anywhere thank you in advance
seems like common syntax error to me,seems like common syntax error to me,0,seems like common syntax error to me,seems like common syntax error to me
here how built clickhouse data visualization with metrics api layer,here how built clickhouse data visualization with metrics api layer,0,here how built clickhouse data visualization with metric api layer,here how built clickhouse data visualization with metric api layer
quick data modelling question,hi all hoping for confirmation or discussion on kimball methodology how do you handle an accumulating snapshot fact linked to scd2 dimension do you add the record to the fact based on the first key and then simply update the accumulating columns always pointing to the first version of the dim or do you rebuild the fact from scratch each update using the key from the current version of the dim or something else,0,quick data modelling question,hi all hoping for confirmation or discussion on kimball methodology how do you handle an accumulating snapshot fact linked to scd2 dimension do you add the record to the fact based on the first key and then simply update the accumulating column always pointing to the first version of the dim or do you rebuild the fact from scratch each update using the key from the current version of the dim or something else
ways of productionising data engineering skills,as yoe freelance data engineer looking for ways to make b2b b2c products using my data engineering skills my girlfriend who is designer creates templates for websites like webflow what are similar ways to productionise data engineering skills any ideas,0,way of productionising data engineering skill,a yoe freelance data engineer looking for way to make b2b b2c product using my data engineering skill my girlfriend who is designer creates template for website like webflow what are similar way to productionise data engineering skill any idea
doing easy data ingestion,simplify the data ingestion cxvqdu amp ab channel versatiledatakit ab_channel versatiledatakit,0,doing easy data ingestion,simplify the data ingestion cxvqdu amp ab channel versatiledatakit ab_channel versatiledatakit
handling unicode dash hyphen minus,we are using various system for our reporting and processing sql server for storing reporting data powerbi for dashboarding parquet hive azure synapse for big data and elt layer for some custom web app often time we receive weird data especially from our non english native speaking country they have hyphen vs dash vs minus open paratheses vs round bracket etc this drive data reporting mad as sql and powerbi by default is not case sensitive nor accent sensitive but hive parquet is user is ok for losing some details harmonize hyphen dash and minus to just normal and we can have full list of odd characters to handle any good suggestion how to sanitize this data any good library or something to clean this,0,handling unicode dash hyphen minus,we are using various system for our reporting and processing sql server for storing reporting data powerbi for dashboarding parquet hive azure synapse for big data and elt layer for some custom web app often time we receive weird data especially from our non english native speaking country they have hyphen v dash v minus open paratheses v round bracket etc this drive data reporting mad a sql and powerbi by default is not case sensitive nor accent sensitive but hive parquet is user is ok for losing some detail harmonize hyphen dash and minus to just normal and we can have full list of odd character to handle any good suggestion how to sanitize this data any good library or something to clean this
is powerapps or the power platform ecosystem worth learning as data analyst engineer,need to have cloud hosted application to read and convert sharepoint files with very simple scripts would normally do this in jupyter notebook but we don currently have anywhere that could host the code so anybody could run it do powerapps in sharepoint provide similar or good functionality for data manipulation and etl down the road would have to learn but don mind if it provides some utility later on in my career sorry if this is dumb question and thanks so much for the help,0,is powerapps or the power platform ecosystem worth learning a data analyst engineer,need to have cloud hosted application to read and convert sharepoint file with very simple script would normally do this in jupyter notebook but we don currently have anywhere that could host the code so anybody could run it do powerapps in sharepoint provide similar or good functionality for data manipulation and etl down the road would have to learn but don mind if it provides some utility later on in my career sorry if this is dumb question and thanks so much for the help
launch experiments on databricks from your local environment,launch experiments on databricks from your local environment,0,launch experiment on databricks from your local environment,launch experiment on databricks from your local environment
delta transaction log for non parquet data,curious how difficult it would be to create delta log for non parquet data specifically for lots of blob data images or audio etc delta log is quite good but just don like that it turns image files that would be in directories like images cat img002 png images dog img356 png into gt parquet part0002 parquet the original structure is of course retrievable but it obscures the ability to be able to just look at what data is there unless you run an operation that writes back out from the parquet data or something just using md5 hashes timestamps and transaction audits would it really be so difficult to come up with system that works as well without needing to use parquet,0,delta transaction log for non parquet data,curious how difficult it would be to create delta log for non parquet data specifically for lot of blob data image or audio etc delta log is quite good but just don like that it turn image file that would be in directory like image cat img002 png image dog img356 png into gt parquet part0002 parquet the original structure is of course retrievable but it obscures the ability to be able to just look at what data is there unless you run an operation that writes back out from the parquet data or something just using md5 hash timestamps and transaction audit would it really be so difficult to come up with system that work a well without needing to use parquet
seven tricks to building fast reliable search data integrations,seven tricks to building fast reliable search data integrations,0,seven trick to building fast reliable search data integration,seven trick to building fast reliable search data integration
free time,do you guys get time off from your jobs regularly like what are the average work schedule look like,0,free time,do you guy get time off from your job regularly like what are the average work schedule look like
data catalog with sql to text will it help business users with data discovery and observability,hi there my name is igor am testing different startup ideas in data governance space sometimes business users have problems with data trust dashboard looks strange and they don know whether it technical problem or business one they want to understand how particular metric is calculated usually these metrics are calculated via sql so think sql to text might be usefull here to automatically generate descriptions and explanations for dashboards reports etc do you think it big and important problem which is worth solving will natural language processing sql to text really help here any thoughts are welcome,0,data catalog with sql to text will it help business user with data discovery and observability,hi there my name is igor am testing different startup idea in data governance space sometimes business user have problem with data trust dashboard look strange and they don know whether it technical problem or business one they want to understand how particular metric is calculated usually these metric are calculated via sql so think sql to text might be usefull here to automatically generate description and explanation for dashboard report etc do you think it big and important problem which is worth solving will natural language processing sql to text really help here any thought are welcome
can be an extrovert and still be data engineer,am thinking about transitioning to full on data engineering career and that is what eventually led me to this subreddit in my current job years am in mixed role analyst scientist engineer with some project management responsibilities as well before was in bi developer role for about years so as work with data find that maybe take more pleasure in the data engineering tasks such as dw design and etl processes as opposed to ds projects do enjoy doing analyst work but comparing salaries this seems to be the least payed role of the three de ds da the plan is to do the udacity nano degree course to fill in the missing tools need for fully pledged de see if really like this kind of work and then try to find full on de job while search intend to ask for more de tasks in my current position this brings me to my question was looking at some videos from the seattle data guy on yt and in one of the videos he mentions that de job might be fit for you if you are naturally introverted personally enjoy working with people lot so the occasional meet and presentation do in my current role is very welcome fear that in the de role that am pursuing will find myself staring at the screen all day and not really interacting with people is there anyone here that has transitioned from data analyst or bi developer into data engineer and can share their experiences from social perspective do you work with people less then in your previous role if yes how does this affect you do you miss human interaction thanks and cheers ps really loving this subreddit you guys are great,0,can be an extrovert and still be data engineer,am thinking about transitioning to full on data engineering career and that is what eventually led me to this subreddit in my current job year am in mixed role analyst scientist engineer with some project management responsibility a well before wa in bi developer role for about year so a work with data find that maybe take more pleasure in the data engineering task such a dw design and etl process a opposed to d project do enjoy doing analyst work but comparing salary this seems to be the least payed role of the three de d da the plan is to do the udacity nano degree course to fill in the missing tool need for fully pledged de see if really like this kind of work and then try to find full on de job while search intend to ask for more de task in my current position this brings me to my question wa looking at some video from the seattle data guy on yt and in one of the video he mention that de job might be fit for you if you are naturally introverted personally enjoy working with people lot so the occasional meet and presentation do in my current role is very welcome fear that in the de role that am pursuing will find myself staring at the screen all day and not really interacting with people is there anyone here that ha transitioned from data analyst or bi developer into data engineer and can share their experience from social perspective do you work with people le then in your previous role if yes how doe this affect you do you miss human interaction thanks and cheer p really loving this subreddit you guy are great
can unethically produced data be used ethically survey please do fill it out,can unethically produced data be used ethically this survey serves as the backbone of my research paper which tries to examine the question of ethics in scientific research and its advancement please take minute to fill it out,0,can unethically produced data be used ethically survey please do fill it out,can unethically produced data be used ethically this survey serf a the backbone of my research paper which try to examine the question of ethic in scientific research and it advancement please take minute to fill it out
is it possible to analyse datasets that contains only one value per column over the years,trying to learn more about data science from the too basic courses took during my past bachelor degree in computer science until now have mainly worked with already strucured datasets that found in various places always cleaned these datasets with pandas and then applied supervised or semi supervised methods just by manipulating variables names the structure of the dataframes has always been the same for example amp x200b id gender country age brazil canada amp x200b so decided to challenge myself by building my own dataset with indicators over the years found from different free databases after first batch of headaches with data processing and data cleaning finally managed to obtain two dataframes with the same structure years as index and names as column headers except for the values which are different one indicator per df here is an example first dataframe with weight name name name name year year year second dataframe with size name name name name year year year amp x200b don have the impression that merge or concat are possible since have almost individuals over about years with the names of the columns as labels of the individuals themselves where block is how to obtain same kind of structure that in the first exemple in order to apply some ml methods as usual am wondering about combining all dataframes for multi indexing but how would do my analysis have turned the problem around in several directions and must have made mistake somewhere but without managing where exactly yet dare to assume that this problem must be common in data science unless am wrong how can you perform an analysis with python in this case,0,is it possible to analyse datasets that contains only one value per column over the year,trying to learn more about data science from the too basic course took during my past bachelor degree in computer science until now have mainly worked with already strucured datasets that found in various place always cleaned these datasets with panda and then applied supervised or semi supervised method just by manipulating variable name the structure of the dataframes ha always been the same for example amp x200b id gender country age brazil canada amp x200b so decided to challenge myself by building my own dataset with indicator over the year found from different free database after first batch of headache with data processing and data cleaning finally managed to obtain two dataframes with the same structure year a index and name a column header except for the value which are different one indicator per df here is an example first dataframe with weight name name name name year year year second dataframe with size name name name name year year year amp x200b don have the impression that merge or concat are possible since have almost individual over about year with the name of the column a label of the individual themselves where block is how to obtain same kind of structure that in the first exemple in order to apply some ml method a usual am wondering about combining all dataframes for multi indexing but how would do my analysis have turned the problem around in several direction and must have made mistake somewhere but without managing where exactly yet dare to assume that this problem must be common in data science unless am wrong how can you perform an analysis with python in this case
mba and de,is having master degree in business beneficial to de career in the future,0,mba and de,is having master degree in business beneficial to de career in the future
shopify monolithic architecture averaged tb min in egress,pretty interesting stats from tweetstorm out of shopify engineering twitter new comer de any idea how they were able to achieve this,0,shopify monolithic architecture averaged tb min in egress,pretty interesting stats from tweetstorm out of shopify engineering twitter new comer de any idea how they were able to achieve this
kafka summit london call for papers closes soon,kafka summit london call for papers closes soon,0,kafka summit london call for paper close soon,kafka summit london call for paper close soon
data governance management template,am working at an organization where we deal with really bad data on daily basis the main reason we found is the way the data is being entered in the source system for this reason feel data governance is the need of the hour and was wondering is there any specific data governance template that can use and start documenting the steps and policies,0,data governance management template,am working at an organization where we deal with really bad data on daily basis the main reason we found is the way the data is being entered in the source system for this reason feel data governance is the need of the hour and wa wondering is there any specific data governance template that can use and start documenting the step and policy
do you want to know how cdk pipelines for data lake deployment bring scalability automation and centralized management find out in this article,do you want to know how cdk pipelines for data lake deployment bring scalability automation and centralized management find out in this article,0,do you want to know how cdk pipeline for data lake deployment bring scalability automation and centralized management find out in this article,do you want to know how cdk pipeline for data lake deployment bring scalability automation and centralized management find out in this article
how do you handle small files in data lake,hi everyone new in de ve been working the past month doing data lake to ingest raw data from apis json and xml formats in daily basis once the data is deposited in the raw zone all the data is processed from it into cleansed zone in parquet format so far so good but what can keep me with peace of mind is that every file has like max kb inside the following partitions cleansed_zone source user_id xxxx year xxxx month xx day xx lt parquet file gt from my perspective it handles very well idempotency because every file belongs to specific day in terms of performance ve read that it leads to slower queries when using presto redshift spectrum etc is there something that am missing out thanks update the number of users is like but it is expected to grow to by the end of the year each user has an average of mb data each year we expect to query data months back from the present for each user at least four times month,0,how do you handle small file in data lake,hi everyone new in de ve been working the past month doing data lake to ingest raw data from apis json and xml format in daily basis once the data is deposited in the raw zone all the data is processed from it into cleansed zone in parquet format so far so good but what can keep me with peace of mind is that every file ha like max kb inside the following partition cleansed_zone source user_id xxxx year xxxx month xx day xx lt parquet file gt from my perspective it handle very well idempotency because every file belongs to specific day in term of performance ve read that it lead to slower query when using presto redshift spectrum etc is there something that am missing out thanks update the number of user is like but it is expected to grow to by the end of the year each user ha an average of mb data each year we expect to query data month back from the present for each user at least four time month
how to handle small files in data lake,how to handle small files in data lake,0,how to handle small file in data lake,how to handle small file in data lake
what your preferred data integration method from saas vendor,trying to get sense of what the preferred method data engineers these days use to extract data from saas products what method do you like to work with the most is it apis jdbc streamed via kinesis or similar something else,0,what your preferred data integration method from saas vendor,trying to get sense of what the preferred method data engineer these day use to extract data from saas product what method do you like to work with the most is it apis jdbc streamed via kinesis or similar something else
was anything interesting announced at aws reinvent for data engineering,didn get chance to go to reinvent this year and didn see too many changes regarding de but really appreciate the new serverless redshift coming on board that should cut down costs in our dw anything else that was cool,0,wa anything interesting announced at aws reinvent for data engineering,didn get chance to go to reinvent this year and didn see too many change regarding de but really appreciate the new serverless redshift coming on board that should cut down cost in our dw anything else that wa cool
aws outage my pipeline is down,just curious how many other data engineers are prepared for aws outages all my stuff mwaa and databricks runs on us east of course it down no data pipeline feel like it small price to pay for the reduced complexity of high availability my business can handle day gone what about all,0,aws outage my pipeline is down,just curious how many other data engineer are prepared for aws outage all my stuff mwaa and databricks run on u east of course it down no data pipeline feel like it small price to pay for the reduced complexity of high availability my business can handle day gone what about all
datavault retrieving data,has anyone figured out good way to extract point in time data in datavault using lead and lag have to include sort operator which is really expensive over large dataset does anyone have any tricks to get the data extracted faster ours is built on sql server but the approach for other systems could be ported to work on sql server,0,datavault retrieving data,ha anyone figured out good way to extract point in time data in datavault using lead and lag have to include sort operator which is really expensive over large dataset doe anyone have any trick to get the data extracted faster ours is built on sql server but the approach for other system could be ported to work on sql server
looking for some feedback on new job board ve created for data engineering new jobs are listed automatically times per day,looking for some feedback on new job board ve created for data engineering new jobs are listed automatically times per day,0,looking for some feedback on new job board ve created for data engineering new job are listed automatically time per day,looking for some feedback on new job board ve created for data engineering new job are listed automatically time per day
airflow noob question,how do define multiple dag owners in airflow,0,airflow noob question,how do define multiple dag owner in airflow
how do you make idempotent etl jobs for relational database where records are updated,have relational oltp database that supports an app records are created and updated and there created timestamp and an updated timestamp that tells us when the record was last changed amp x200b my etl jobs at the moment have just been identifying which records have changed been created since the last etl based on the updated timestamp and then updating the reporting database for those records but this isn idempotent amp x200b how are you writing your jobs for this kind of source,0,how do you make idempotent etl job for relational database where record are updated,have relational oltp database that support an app record are created and updated and there created timestamp and an updated timestamp that tell u when the record wa last changed amp x200b my etl job at the moment have just been identifying which record have changed been created since the last etl based on the updated timestamp and then updating the reporting database for those record but this isn idempotent amp x200b how are you writing your job for this kind of source
version control source tables and downstream tables views,we are about to start using fivetran for bringing data into our snowflake instance from various sources and it seems like it automatically detects schema changes from the source and propagates it down to the target tables curious if those of you that use fivetran still maintain git repository of the schema ddl scripts and if so what that process looks like ve looked into alembic flyway schemachange etc but it seems redundant keeping track of migration files if fivetran is doing that automatically for us at the end of the day our goal is to maintain set of ddl scripts so that we can rebuild the database schemas from scratch with or without fivetran perhaps terraform basic sql scripts maintained in git repository in searching through this subreddit it seems like dbt is beneficial module to use for objects built on top of the source tables data such as views etls additional tables etc and it also acts as schema versioning module for the aforementioned is my understanding of this correct,0,version control source table and downstream table view,we are about to start using fivetran for bringing data into our snowflake instance from various source and it seems like it automatically detects schema change from the source and propagates it down to the target table curious if those of you that use fivetran still maintain git repository of the schema ddl script and if so what that process look like ve looked into alembic flyway schemachange etc but it seems redundant keeping track of migration file if fivetran is doing that automatically for u at the end of the day our goal is to maintain set of ddl script so that we can rebuild the database schema from scratch with or without fivetran perhaps terraform basic sql script maintained in git repository in searching through this subreddit it seems like dbt is beneficial module to use for object built on top of the source table data such a view etls additional table etc and it also act a schema versioning module for the aforementioned is my understanding of this correct
snowflake migrating from oracle and tool stack,hi all currently have the need to migrate dwh based on oracle exadata to cloud the target architecture is either snowflake on aws or redshift we are considering snowflake for cost and performance reasons but am not sure about few points concerning the migration my understanding is that we would need to export data from oracle as csv or other formats to cloud storage s3 and then use snowpipe copy to import is that correct which tool can we use to convert ddls know that for redshift they have sct and dmt that it seems would simplify the migration quite bit also we were thinking about using dbt as it seems the most popular transformation tool for snowflake any easy way to migrate oracle pl sql procedures to dbt models thanks lot,0,snowflake migrating from oracle and tool stack,hi all currently have the need to migrate dwh based on oracle exadata to cloud the target architecture is either snowflake on aws or redshift we are considering snowflake for cost and performance reason but am not sure about few point concerning the migration my understanding is that we would need to export data from oracle a csv or other format to cloud storage s3 and then use snowpipe copy to import is that correct which tool can we use to convert ddls know that for redshift they have sct and dmt that it seems would simplify the migration quite bit also we were thinking about using dbt a it seems the most popular transformation tool for snowflake any easy way to migrate oracle pl sql procedure to dbt model thanks lot
airflow azure key vault,hi has anyone configured azure key vault as backend secrets for airflow am running into trouble keep getting this error create app registration and generated client id and client secret and provided the access to key vault api provided my vault url in airflow cfg file don know what else am doing wrong any help appreciated utc environment py info no environment configuration found utc managed_identity py info managedidentitycredential will use imds utc connection py error unable to retrieve connection from secrets backend azurekeyvaultbackend checking subsequent secrets backend traceback most recent call last file home airflowadmin local lib python3 site packages azure core pipeline transport base py line in format_url base self base_url format kwargs rstrip keyerror vaultbaseurl,0,airflow azure key vault,hi ha anyone configured azure key vault a backend secret for airflow am running into trouble keep getting this error create app registration and generated client id and client secret and provided the access to key vault api provided my vault url in airflow cfg file don know what else am doing wrong any help appreciated utc environment py info no environment configuration found utc managed_identity py info managedidentitycredential will use imds utc connection py error unable to retrieve connection from secret backend azurekeyvaultbackend checking subsequent secret backend traceback most recent call last file home airflowadmin local lib python3 site package azure core pipeline transport base py line in format_url base self base_url format kwargs rstrip keyerror vaultbaseurl
help on azure synapse learning,hi was learning for azure data engineer associate and was trying to do this exercise the first sql script to create fact and dimension tables is throwing an error for me as below what can be the issue format png amp auto webp amp e4d07ce9ac58e00e0bace6514e2538d170314,0,help on azure synapse learning,hi wa learning for azure data engineer associate and wa trying to do this exercise the first sql script to create fact and dimension table is throwing an error for me a below what can be the issue format png amp auto webp amp e4d07ce9ac58e00e0bace6514e2538d170314
advice on current data stack and ways to improve,hi everyone working on trying to improve our data stack and any advices study buddy or mentorship is highly appreciated to give some background the first data engineer for the company coming from front end developer background comfortable with advanced sql stuff and beginner python before me was bi developer who created data models and dashes in sisense on prem which is what use now as well current state of my teams data requirements are coming from our mysql db which is maintained by our software dev team any current api ingestion is done by them and stored in tables we do have an analytical db which has views and queries from our main oltp db the sizes of data here are around rows of orders member transactional data remainder of the data is excel csv files some are static some are updated from time to time by other members of the org we ingest the data into sisense and build data model transformations can be done either in the olap db or the model itself the models are then scheduled to built at certain intervals some take few seconds and the biggest ones around hours which is then accessed by sisense to create the dashboards and email reports we do have data scientist who is doing his work in bigquery though have absolutely have no collaboration with him and no idea what he is doing as well not saying won collaborate with him just that haven reached out yet since this modernization task doing is just research preparing for the final proposal to higher ups lastly in terms of ecosystem our company uses lot of microsoft services as well what envisioning to use and reasons please understand don have real experience on these tools this is more on research and basing my judgement on comparison articles guides some tuts did cloud solution snowflake on azure or azure synapse already in microsoft ecosystem and integration to tools is easier and recommendation to higher ups split on the two because snowflake seems to be easier to implement note the only one who will be doing this on top of other tasks so it balance of ease and delivery of requirements if ever would go for azure synapse will be utilizing tools in their ecosystem bi solution sisense since we re already using it just now it will get data from the dw or if ever can use power bi amp x200b airflow don think would be needing this because you could schedule jobs already in azure data factory fivetran don believe need this for now basing on my current data sources but can anticipate in the future api integration role could be moved from software devs to me in the future but that like maybe year from now dbt will utilize this as well having no documentation version control reusability etc is making my head hurt right now any other tools missing any advice you could give appreciate all the help given thank you,0,advice on current data stack and way to improve,hi everyone working on trying to improve our data stack and any advice study buddy or mentorship is highly appreciated to give some background the first data engineer for the company coming from front end developer background comfortable with advanced sql stuff and beginner python before me wa bi developer who created data model and dash in sisense on prem which is what use now a well current state of my team data requirement are coming from our mysql db which is maintained by our software dev team any current api ingestion is done by them and stored in table we do have an analytical db which ha view and query from our main oltp db the size of data here are around row of order member transactional data remainder of the data is excel csv file some are static some are updated from time to time by other member of the org we ingest the data into sisense and build data model transformation can be done either in the olap db or the model itself the model are then scheduled to built at certain interval some take few second and the biggest one around hour which is then accessed by sisense to create the dashboard and email report we do have data scientist who is doing his work in bigquery though have absolutely have no collaboration with him and no idea what he is doing a well not saying won collaborate with him just that haven reached out yet since this modernization task doing is just research preparing for the final proposal to higher ups lastly in term of ecosystem our company us lot of microsoft service a well what envisioning to use and reason please understand don have real experience on these tool this is more on research and basing my judgement on comparison article guide some tuts did cloud solution snowflake on azure or azure synapse already in microsoft ecosystem and integration to tool is easier and recommendation to higher ups split on the two because snowflake seems to be easier to implement note the only one who will be doing this on top of other task so it balance of ease and delivery of requirement if ever would go for azure synapse will be utilizing tool in their ecosystem bi solution sisense since we re already using it just now it will get data from the dw or if ever can use power bi amp x200b airflow don think would be needing this because you could schedule job already in azure data factory fivetran don believe need this for now basing on my current data source but can anticipate in the future api integration role could be moved from software devs to me in the future but that like maybe year from now dbt will utilize this a well having no documentation version control reusability etc is making my head hurt right now any other tool missing any advice you could give appreciate all the help given thank you
transition from mle to ds role,hi am ml engineer and mainly in charge of the full cycle poc data collection preprocessing model architecture coding or use existing toolboxes tuning and etc but not deploying it recently have blast doing all the dataset management and preprocessing work images videos from the brief overview of the data science positions in my country the skillset is varied from one company to another and the overall tech stack looks not that standardized if anyone had experienced it before how hard is it to transit from mle to ds role usually don use any databases for the work and it definitely something need to learn about but other than that,0,transition from mle to d role,hi am ml engineer and mainly in charge of the full cycle poc data collection preprocessing model architecture coding or use existing toolbox tuning and etc but not deploying it recently have blast doing all the dataset management and preprocessing work image video from the brief overview of the data science position in my country the skillset is varied from one company to another and the overall tech stack look not that standardized if anyone had experienced it before how hard is it to transit from mle to d role usually don use any database for the work and it definitely something need to learn about but other than that
how pii data is handled and secured in azure data platforms,hello community would like to know how do you all handle pii data in solution where you use azure data factory to extract data and copy to azure data lake and from data lake to azure sql database azure synapse data warehouse the requirement is to encrypt pii data before it leaves the organization premise what are the approaches you guys follow to fulfill such requirement since data factory is not having built in feature to encrypt pii data would like to know how such scenarios are handled thank you,0,how pii data is handled and secured in azure data platform,hello community would like to know how do you all handle pii data in solution where you use azure data factory to extract data and copy to azure data lake and from data lake to azure sql database azure synapse data warehouse the requirement is to encrypt pii data before it leaf the organization premise what are the approach you guy follow to fulfill such requirement since data factory is not having built in feature to encrypt pii data would like to know how such scenario are handled thank you
data engineer interview at meta facebook,have technical interview coming up with meta facebook for their data engineer role ve been practicing some advanced sql postgresql and python questions did anyone do similar interview with them recently or before can you give me some sample questions you remember that can practice coding thank you,0,data engineer interview at meta facebook,have technical interview coming up with meta facebook for their data engineer role ve been practicing some advanced sql postgresql and python question did anyone do similar interview with them recently or before can you give me some sample question you remember that can practice coding thank you
what languages do need to learn to get ready for the etl part of datascience,need some advice on the engineering part of ds what are the common tools used for etl data pipelines in the private sector thus far planning on learning pyspark pyspark for sql ssid sql cloud etl solutions from azure aws google bi tools etl solutions would that be enough or do need nosql too have forgot something are most etl jobs nowadays done in the cloud or company sql nosql server year in learning to code and thus far fairly proficient in sql and python thank you for your time,0,what language do need to learn to get ready for the etl part of datascience,need some advice on the engineering part of d what are the common tool used for etl data pipeline in the private sector thus far planning on learning pyspark pyspark for sql ssid sql cloud etl solution from azure aws google bi tool etl solution would that be enough or do need nosql too have forgot something are most etl job nowadays done in the cloud or company sql nosql server year in learning to code and thus far fairly proficient in sql and python thank you for your time
what are your biggest data pipeline etl tool problems,currently work for company that does etl elt am not here to advertise so am not mentioning it and am looking to improve the demo experience for prospects currently the demos are generic and very one sided like to put together some more relevant use cases examples that are more relevant what are some of the bigger challenges pain points do you guys run into when building pipelines or using etl tools if you were looking for new etl or elt tool what would you like to see most,0,what are your biggest data pipeline etl tool problem,currently work for company that doe etl elt am not here to advertise so am not mentioning it and am looking to improve the demo experience for prospect currently the demo are generic and very one sided like to put together some more relevant use case example that are more relevant what are some of the bigger challenge pain point do you guy run into when building pipeline or using etl tool if you were looking for new etl or elt tool what would you like to see most
what is the modern data stack,hey everyone on december th at pm et pm pt we ll be hosting as part of the data stack show live panel where we ll discuss about the modern data stack the panel will have people from dbt fivetran databricks hinge and essence vc we wanted to have all the stakeholders share their views on what the modern data stack is and why it important you can register here for the live event poap will be used as proof of attendance and based on that there will also be some gifts given not sure what yet still working on it but promise it will be worth it in case you can make it or just don want to participate in the live event the recording will be published on the data stack show anyway would also like to ask for your help love to hear from the community what to ask the people from these companies so please leave message with your questions on this thread thanks,0,what is the modern data stack,hey everyone on december th at pm et pm pt we ll be hosting a part of the data stack show live panel where we ll discus about the modern data stack the panel will have people from dbt fivetran databricks hinge and essence vc we wanted to have all the stakeholder share their view on what the modern data stack is and why it important you can register here for the live event poap will be used a proof of attendance and based on that there will also be some gift given not sure what yet still working on it but promise it will be worth it in case you can make it or just don want to participate in the live event the recording will be published on the data stack show anyway would also like to ask for your help love to hear from the community what to ask the people from these company so please leave message with your question on this thread thanks
data lineage use cases,am working on data lineage tools benchmark and deep analysis am trying to add survey on data lineage use cases to an article published on tds the opinion of the data engineering community would help me lot what is the most common data lineage use case answer in comments you can find more modern data stack analysis and benchmark here benchmark for data catalogs benchmark for reverse etl benchmark for data quality benchmark for etl view poll,0,data lineage use case,am working on data lineage tool benchmark and deep analysis am trying to add survey on data lineage use case to an article published on tds the opinion of the data engineering community would help me lot what is the most common data lineage use case answer in comment you can find more modern data stack analysis and benchmark here benchmark for data catalog benchmark for reverse etl benchmark for data quality benchmark for etl view poll
first job teaching me informatica,hi there am in my first job after college in sort of hybrid de swe role and for the etl stuff ll be doing it mostly all in informatica work at large university in my state and the pay benefits are good as well as work life balance but can help but feel like it not the best to be learning informatica when airflow azure etc is so much more popular don have any other experience with any sort of etl technology from school and this is my first real work experience didnt do any internships so can really complain should just try to teach myself to work with airflow or azure outside of work bring up using something like airflow instead of informatica once ve been here longer looking for any advice ty,0,first job teaching me informatica,hi there am in my first job after college in sort of hybrid de swe role and for the etl stuff ll be doing it mostly all in informatica work at large university in my state and the pay benefit are good a well a work life balance but can help but feel like it not the best to be learning informatica when airflow azure etc is so much more popular don have any other experience with any sort of etl technology from school and this is my first real work experience didnt do any internship so can really complain should just try to teach myself to work with airflow or azure outside of work bring up using something like airflow instead of informatica once ve been here longer looking for any advice ty
how would you surface graph data neo4j aws neptune for business users,trying to create an interface that will allow non technical user little to no sql knowledge to search for entities within graph db what would be low cost or easy to develop solution for this current proof of concept has raw data as dump file that is loaded into neo4j desktop data is queried using python and cypher thought of using plotly streamlit but what are some other options,0,how would you surface graph data neo4j aws neptune for business user,trying to create an interface that will allow non technical user little to no sql knowledge to search for entity within graph db what would be low cost or easy to develop solution for this current proof of concept ha raw data a dump file that is loaded into neo4j desktop data is queried using python and cypher thought of using plotly streamlit but what are some other option
what is good pipeline to use alongside databricks notebooks,could business entire etl pipelines be built out through databricks jobs,0,what is good pipeline to use alongside databricks notebook,could business entire etl pipeline be built out through databricks job
new role requires usage of infrastructure as code and building of ci cd pipelines what are these in terms of data engineering,got offered new role which accepted will be working as data engineer my boss said that usage of infrastructure as code and building of ci cd pipelines are part of the tasks don know much about these but hey wanted to try something new we will be using the azure stack synapse workspace databricks sql pool data factory how does code as infrastructure and ci cd fit into this realm can you point me towards some useful resources also new to azure been working with ibm stack for yrs,0,new role requires usage of infrastructure a code and building of ci cd pipeline what are these in term of data engineering,got offered new role which accepted will be working a data engineer my bos said that usage of infrastructure a code and building of ci cd pipeline are part of the task don know much about these but hey wanted to try something new we will be using the azure stack synapse workspace databricks sql pool data factory how doe code a infrastructure and ci cd fit into this realm can you point me towards some useful resource also new to azure been working with ibm stack for yr
hive metastore in databricks what to know,hive metastore in databricks what to know,0,hive metastore in databricks what to know,hive metastore in databricks what to know
personal project for interviews list of things to include,want to start planning new personal project for the show and tell portion of interviews have only ever made personal projects geared towards generic junior level software developer gigs but want to start one geared more towards data engineering am looking to make checklist of attributes amp x200b will not be starting from scratch but will recycle parts of another project and build it out amp x200b my general proposal to make program that reads astronomy api data completed and interprets the data into visual animation completed my older project did not store this data into database so that the obvious addition am thinking of creating snapshot table that the api data gets logged to firsttable daily and feature to run the animation at certain timestamp data will be transformed and piped to secondtable the transformation is used to convert astronomy values to animation values the animation script will query secondtable amp x200b aws and ci cd the new project will run from aws so can show basic competency of the service am doing coursera training on the subject want to be able to show can run basic virtual environments linting automated testing git triggered events and other ci cd practices these are things am learning but don use at work amp x200b documentation the project will be well documented this will include general write up system diagrams readme code comments and source code all this will be visible from the project websites amp x200b what am missing from this general formula amp x200b about me am data analyst recently turned data engineer with cs degree who will target de jobs with lots of coding and other swe practices don know anything about big data tools like hadoop spark and all that jazz,0,personal project for interview list of thing to include,want to start planning new personal project for the show and tell portion of interview have only ever made personal project geared towards generic junior level software developer gig but want to start one geared more towards data engineering am looking to make checklist of attribute amp x200b will not be starting from scratch but will recycle part of another project and build it out amp x200b my general proposal to make program that read astronomy api data completed and interprets the data into visual animation completed my older project did not store this data into database so that the obvious addition am thinking of creating snapshot table that the api data get logged to firsttable daily and feature to run the animation at certain timestamp data will be transformed and piped to secondtable the transformation is used to convert astronomy value to animation value the animation script will query secondtable amp x200b aws and ci cd the new project will run from aws so can show basic competency of the service am doing coursera training on the subject want to be able to show can run basic virtual environment linting automated testing git triggered event and other ci cd practice these are thing am learning but don use at work amp x200b documentation the project will be well documented this will include general write up system diagram readme code comment and source code all this will be visible from the project website amp x200b what am missing from this general formula amp x200b about me am data analyst recently turned data engineer with c degree who will target de job with lot of coding and other swe practice don know anything about big data tool like hadoop spark and all that jazz
is hdf5 good bet for storing model embeddings,need to be able to append to the structure without loading it into memory need to be able to index by key need quick load and writes primary use case is for nearest neighbour queries with torch cdist where want to be able to filter by some metadata whether example is already labelled or not for use in active learning secondary use case is to train small cnns hence the reason why don just want to use faiss annoy,0,is hdf5 good bet for storing model embeddings,need to be able to append to the structure without loading it into memory need to be able to index by key need quick load and writes primary use case is for nearest neighbour query with torch cdist where want to be able to filter by some metadata whether example is already labelled or not for use in active learning secondary use case is to train small cnns hence the reason why don just want to use faiss annoy
should use spark or python script,hi all am building pipeline that needs to batch process around gb of raw data sitting in s3 cleans and transforms the data so that it is ready to be loaded into redshift this needs to be done every hour so could do this by either running python script in ec2 or by running spark with glue or emr what is the better solution here is gb too small to justify using spark in what situations would you absolutely need spark instead of python script thank you,0,should use spark or python script,hi all am building pipeline that need to batch process around gb of raw data sitting in s3 clean and transforms the data so that it is ready to be loaded into redshift this need to be done every hour so could do this by either running python script in ec2 or by running spark with glue or emr what is the better solution here is gb too small to justify using spark in what situation would you absolutely need spark instead of python script thank you
schema evolution in this case how should implement it,have personal project learning project an elt system fetch my data incrementally from few mysql tables transformed usual stuff is already done then have this big question how should proceed if add rename or delete column from any of these mysql tables know this is question related to schema evolution but what is your usual approach,0,schema evolution in this case how should implement it,have personal project learning project an elt system fetch my data incrementally from few mysql table transformed usual stuff is already done then have this big question how should proceed if add rename or delete column from any of these mysql table know this is question related to schema evolution but what is your usual approach
the guide to data versioning,the guide to data versioning,0,the guide to data versioning,the guide to data versioning
properly displaying wiki information,hey guys have started going through the sub wiki but am having hard time having pages and tables displayed properly when click on link of the homepage of the wiki vertical section opens on the right side of the web page with the content of new page this leaves about third of the width of the screen to display the content want to read if this new page contains table basically can only see two columns of the table and have to use the sliders to scroll horizontally the button view larger version doesn do anything at first figured had something breaking the site with my browser or my extensions but have the same behavior on all browsers am doing something wrong maybe there is way to collapse the unneeded sections to free up space on my screen thanks for your help,0,properly displaying wiki information,hey guy have started going through the sub wiki but am having hard time having page and table displayed properly when click on link of the homepage of the wiki vertical section open on the right side of the web page with the content of new page this leaf about third of the width of the screen to display the content want to read if this new page contains table basically can only see two column of the table and have to use the slider to scroll horizontally the button view larger version doesn do anything at first figured had something breaking the site with my browser or my extension but have the same behavior on all browser am doing something wrong maybe there is way to collapse the unneeded section to free up space on my screen thanks for your help
statistical data visualization with seaborn from ust free guided project,statistical data visualization with seaborn from ust free guided project,0,statistical data visualization with seaborn from ust free guided project,statistical data visualization with seaborn from ust free guided project
things to consider when defining your apache flink cluster size,things to consider when defining your apache flink cluster size,0,thing to consider when defining your apache flink cluster size,thing to consider when defining your apache flink cluster size
should quit,my first job as data engineer isn what expected not learning much about popular de tools and more how to use cloud techs like gcp and my jobs consist mainly of upgrading maintaining our current data pipeline since our main project is already done when joined and it been liek this for months now not much related to actual data engineering works like writing etl data transformation think thinking of quitting after only months don want to continue for year to just end up with not much actual experience in this field looking for advice thank you,0,should quit,my first job a data engineer isn what expected not learning much about popular de tool and more how to use cloud tech like gcp and my job consist mainly of upgrading maintaining our current data pipeline since our main project is already done when joined and it been liek this for month now not much related to actual data engineering work like writing etl data transformation think thinking of quitting after only month don want to continue for year to just end up with not much actual experience in this field looking for advice thank you
types of matrices with examples,types of matrices with examples,0,type of matrix with example,type of matrix with example
overwhelmed so much to do so little time,just needing to vent bit im the only de at decent sized tech company too many data sources not enough time to manage it all manager the only dba sysadmin other data person at the company for over years is leaving getting the bulk of their responsibilities on top of my own with half the experience like the company and the work do upper management all says they will do everything they can to support me they are working on hiring an experienced data manager and another sr de soon but feel like it going to take forever to find someone in this environment and then also train them on our environment and data just overwhelmed at the moment and the light at the end of the tunnel is so damn far away,0,overwhelmed so much to do so little time,just needing to vent bit im the only de at decent sized tech company too many data source not enough time to manage it all manager the only dba sysadmin other data person at the company for over year is leaving getting the bulk of their responsibility on top of my own with half the experience like the company and the work do upper management all say they will do everything they can to support me they are working on hiring an experienced data manager and another sr de soon but feel like it going to take forever to find someone in this environment and then also train them on our environment and data just overwhelmed at the moment and the light at the end of the tunnel is so damn far away
macs for data engineering,tomorrow start my first full time data engineering job and they gave me macbook to work with as my daily driver ve never used mac for anything professional any tips for someone coming from programming on windows any limitations or things to keep in mind that are different from windows in the data engineering space know the stack is snowflake oriented,0,mac for data engineering,tomorrow start my first full time data engineering job and they gave me macbook to work with a my daily driver ve never used mac for anything professional any tip for someone coming from programming on window any limitation or thing to keep in mind that are different from window in the data engineering space know the stack is snowflake oriented
how is data from an api processed once inside an oltp db,don if discussion is the best one to pick but that should do lol so taking something like an commerce website for example when customer creates an account then know hope that their data gets sent through an api to the oltp database now does this data just get thrown into like stored procedure where it normalized and where the logic determines whether the data needs to be inserted or updated incremental and full refresh overwrites in the correct tables just curious because can never seem to find any information on it and it as if the people that build these sorts of systems were part of some secret society lol and side question why isn it important for data engineer to know this sort of thing or is it and should just keep looking for those resources,0,how is data from an api processed once inside an oltp db,don if discussion is the best one to pick but that should do lol so taking something like an commerce website for example when customer creates an account then know hope that their data get sent through an api to the oltp database now doe this data just get thrown into like stored procedure where it normalized and where the logic determines whether the data need to be inserted or updated incremental and full refresh overwrites in the correct table just curious because can never seem to find any information on it and it a if the people that build these sort of system were part of some secret society lol and side question why isn it important for data engineer to know this sort of thing or is it and should just keep looking for those resource
how often are you moving data from local to cloud,it feels like doing this an unreasonable amount of the time and tooling and libraries don do while lot because the bottleneck usually ends up being network bandwidth multi threading can help to point but still sitting here ready to use spark and dbt and all this fun stuff but end up having to do an rclone which is just less exciting curious if this is common or just my experience,0,how often are you moving data from local to cloud,it feel like doing this an unreasonable amount of the time and tooling and library don do while lot because the bottleneck usually end up being network bandwidth multi threading can help to point but still sitting here ready to use spark and dbt and all this fun stuff but end up having to do an rclone which is just le exciting curious if this is common or just my experience
do need orchestration for fivetran dbt stack,if my stack consists of fivetran for el and dbt for do need to use an orchestration tool like airflow it seems like github actions would be enough you can set workflows to run on schedule or upon merging to particular branch but maybe missing something about benefits of orchestration,0,do need orchestration for fivetran dbt stack,if my stack consists of fivetran for el and dbt for do need to use an orchestration tool like airflow it seems like github action would be enough you can set workflow to run on schedule or upon merging to particular branch but maybe missing something about benefit of orchestration
difference between data scientist analyst and engineer,amp x200b format png amp auto webp amp e705f0040a4ee5f52d29f3a42299e,0,difference between data scientist analyst and engineer,amp x200b format png amp auto webp amp e705f0040a4ee5f52d29f3a42299e
framework to manage and log tasks,it may sound trivial but can find tool to serve my purpose have long list of urls want to download files from apply simple operation on each file and store the result in another place each download and operation is generally slow takes minutes right now have multiprocessing script python to do things in parallel but want to somehow keep track of what has been downloaded and processed because in case the script is interrupted for whatever reason will be able to restart it and only process the items in the url list that have not been processed for this reason use sqlite table where log completed tasks but it feels little hacky and it another thing to maintain is there tool for this kind of tasks looking for something relatively simple to setup but can find anything,0,framework to manage and log task,it may sound trivial but can find tool to serve my purpose have long list of url want to download file from apply simple operation on each file and store the result in another place each download and operation is generally slow take minute right now have multiprocessing script python to do thing in parallel but want to somehow keep track of what ha been downloaded and processed because in case the script is interrupted for whatever reason will be able to restart it and only process the item in the url list that have not been processed for this reason use sqlite table where log completed task but it feel little hacky and it another thing to maintain is there tool for this kind of task looking for something relatively simple to setup but can find anything
preparing for google cloud certification cloud data engineer,helll community hope you re doing well wanna become data engineer currently in final year of my bachelor degree do you recommend me this course to build background as beginner also don know if ll go for the cloud or technologies like hadoop spark,0,preparing for google cloud certification cloud data engineer,helll community hope you re doing well wanna become data engineer currently in final year of my bachelor degree do you recommend me this course to build background a beginner also don know if ll go for the cloud or technology like hadoop spark
moving tbs of json files from gcs to aws s3,hi does anyone have experience moving large amount of data from gcs to aws s3 any insight here would be helpful thanks,0,moving tb of json file from gc to aws s3,hi doe anyone have experience moving large amount of data from gc to aws s3 any insight here would be helpful thanks
dagster in place of airflow,hi in my work place we have teams which are using dagster running in eks and airflow mwaa respectively overall have more experience with dagster in that team and have fairly positive experience with it have also tried airflow mwaa and after weekends of trying out find that still barely getting anything done with it but it may be due to lack of airflow knowledge on my part in this respect dagster is really intuitive ve actually had an intern create pipeline to automate ml workflow within his stay with us my use case would be etl data bought from various data vendors for consumption by internal apps most jobs are fairly small not big data and can run as eks python job minority of the jobs need spark more for concurrency rather than data size would like to seek the opinions of those who have tried and used both should forge ahead to use dagster for things like triggering aws emr for spark jobs on top of our eks based workloads which we are currently using,0,dagster in place of airflow,hi in my work place we have team which are using dagster running in eks and airflow mwaa respectively overall have more experience with dagster in that team and have fairly positive experience with it have also tried airflow mwaa and after weekend of trying out find that still barely getting anything done with it but it may be due to lack of airflow knowledge on my part in this respect dagster is really intuitive ve actually had an intern create pipeline to automate ml workflow within his stay with u my use case would be etl data bought from various data vendor for consumption by internal apps most job are fairly small not big data and can run a eks python job minority of the job need spark more for concurrency rather than data size would like to seek the opinion of those who have tried and used both should forge ahead to use dagster for thing like triggering aws emr for spark job on top of our eks based workload which we are currently using
data engineering resume template,was scouting around places for relevant data engineering resume template but all find is for sde and backend se would like to know if anyone has any sample resumes or any reference sites to help me build my resume,0,data engineering resume template,wa scouting around place for relevant data engineering resume template but all find is for sde and backend se would like to know if anyone ha any sample resume or any reference site to help me build my resume
airflow pipeline that loads data from bigquery sample dataset into manually created table,have created datatable in gcp from existing bigquery sample table select cast repository_created_at as datetime as dt repository_name count repository_open_issues as cnt_open_issues count repository_watchers as cnt_watchers array_agg struct repository_url repository_open_issues repository_watchers as repo from bigquery public data samples github_timeline group by repository_created_at repository_name now would like to automate the process by creating the airflow pipeline that loads data from the bigquery sample to that created table my pipeline looks like following import airflow from airflow import dag from airflow contrib operators bigquery_operator import bigqueryoperator from datetime import datetime default_args owner testing depends_on_past false start_date datetime test_datetime email test_email email_on_failure true email_on_retry false dag dag test_bigquery_timeline default_args default_args catchup false schedule_interval test_interval template_searchpath tmpl_search_path bq_conn_id test_connection_id bq_project_id test_project_id bq_dataset test_table_schema sample_task bigqueryoperator task_id test_load_data_into_table_schema know that source object is for uploading file from external database but how to make it so that it gets new data from bigquery public data samples github_timeline source_objects bigquery public data samples github_timeline destination_dataset_table bq_project_id bq_dataset table write_disposition write_append bigquery_conn_id bq_conn_id dag dag sample_task,0,airflow pipeline that load data from bigquery sample dataset into manually created table,have created datatable in gcp from existing bigquery sample table select cast repository_created_at a datetime a dt repository_name count repository_open_issues a cnt_open_issues count repository_watchers a cnt_watchers array_agg struct repository_url repository_open_issues repository_watchers a repo from bigquery public data sample github_timeline group by repository_created_at repository_name now would like to automate the process by creating the airflow pipeline that load data from the bigquery sample to that created table my pipeline look like following import airflow from airflow import dag from airflow contrib operator bigquery_operator import bigqueryoperator from datetime import datetime default_args owner testing depends_on_past false start_date datetime test_datetime email test_email email_on_failure true email_on_retry false dag dag test_bigquery_timeline default_args default_args catchup false schedule_interval test_interval template_searchpath tmpl_search_path bq_conn_id test_connection_id bq_project_id test_project_id bq_dataset test_table_schema sample_task bigqueryoperator task_id test_load_data_into_table_schema know that source object is for uploading file from external database but how to make it so that it get new data from bigquery public data sample github_timeline source_objects bigquery public data sample github_timeline destination_dataset_table bq_project_id bq_dataset table write_disposition write_append bigquery_conn_id bq_conn_id dag dag sample_task
ogr2ogr simple and powerful command line tool to transform your geographic data,click on following link to know about ogr2ogr and how you can use it to transform your geographic data,0,ogr2ogr simple and powerful command line tool to transform your geographic data,click on following link to know about ogr2ogr and how you can use it to transform your geographic data
when we think about cost is the main consideration the mb per second cpu can process for pipeline,would be awesome to see like back of the envelope what calculation used to let say pipe data from the backend of product via api into lake then warehouse as well detail on the query cost after,0,when we think about cost is the main consideration the mb per second cpu can process for pipeline,would be awesome to see like back of the envelope what calculation used to let say pipe data from the backend of product via api into lake then warehouse a well detail on the query cost after
ressources for leveling up with spark,hi have year if exprience with spark and read spark the definitive guide how can get to the next level do you have book to recommend me,0,ressources for leveling up with spark,hi have year if exprience with spark and read spark the definitive guide how can get to the next level do you have book to recommend me
avro logical types with parquet hive and spark,avro logical types with parquet hive and spark,0,avro logical type with parquet hive and spark,avro logical type with parquet hive and spark
any recommended course on dbt,after being convinced by the spread and popularity of dbt would like to start learning but do not find any good resources watched playlist on youtube the only one but found the content very limited and snowflake oriented can someone please recommend any useful resources,0,any recommended course on dbt,after being convinced by the spread and popularity of dbt would like to start learning but do not find any good resource watched playlist on youtube the only one but found the content very limited and snowflake oriented can someone please recommend any useful resource
tricks for dealing with comma errors in dbt sql,find myself often struggling with missing too many commas in dbt does anyone have any tips for highlighting such errors ve tried my hand at writing regex string to highlight few of them using the vscode regex highlighter extension but it pretty complex to catch them all and the plugin does not support lot of necessary regex syntax like nested capture groups happy to share the scripts if anyone is interested couple of examples sql select field1 field2 lt missing comma here field3 from my_model sql select field1 field2 field3 lt shouldn be comma here from my_model does anyone have any ideas haven seen any ides that handle highlighting like this but maybe don know what looking for know about the convention of writing the comma first but my brain doesn particularly like starting to use that way write lot of python normally,0,trick for dealing with comma error in dbt sql,find myself often struggling with missing too many comma in dbt doe anyone have any tip for highlighting such error ve tried my hand at writing regex string to highlight few of them using the vscode regex highlighter extension but it pretty complex to catch them all and the plugin doe not support lot of necessary regex syntax like nested capture group happy to share the script if anyone is interested couple of example sql select field1 field2 lt missing comma here field3 from my_model sql select field1 field2 field3 lt shouldn be comma here from my_model doe anyone have any idea haven seen any ides that handle highlighting like this but maybe don know what looking for know about the convention of writing the comma first but my brain doesn particularly like starting to use that way write lot of python normally
what are few good degrees can get in machine learning and cloud in india,looking for some good institutions for pg or diplomas on machine learning and cloud saw about upgrad its reviews are neutral any similar kind of degrees that can get online thanks in advance,0,what are few good degree can get in machine learning and cloud in india,looking for some good institution for pg or diploma on machine learning and cloud saw about upgrad it review are neutral any similar kind of degree that can get online thanks in advance
good resources that help to give the big picture for data engineering,have general interest in data engineering but am having trouble understanding the big picture generally find it easier to learn topic if have better idea of what am working towards are there any resources that help to break down some of the individual aspects of data engineering and what function they serve in the overall process lot of resources tell you the skills you need to learn but not necessarily what they are for or how these different skills are going to be used together,0,good resource that help to give the big picture for data engineering,have general interest in data engineering but am having trouble understanding the big picture generally find it easier to learn topic if have better idea of what am working towards are there any resource that help to break down some of the individual aspect of data engineering and what function they serve in the overall process lot of resource tell you the skill you need to learn but not necessarily what they are for or how these different skill are going to be used together
why ibm ibm solutions etc is not loved by data engineers and architects,was searching for reviews on course that was provided by ibm and uses solutions provided by ibm and was surprised how they are not liked at all and many users recommended to skip avoid anything provided by ibm,0,why ibm ibm solution etc is not loved by data engineer and architect,wa searching for review on course that wa provided by ibm and us solution provided by ibm and wa surprised how they are not liked at all and many user recommended to skip avoid anything provided by ibm
test etl on sample data,hello we have some etl we are doing that we want to verify with sample data and expected output data is this something we can incorporate into dbt and is this testing strategy appropriate commonly used,0,test etl on sample data,hello we have some etl we are doing that we want to verify with sample data and expected output data is this something we can incorporate into dbt and is this testing strategy appropriate commonly used
help with interview prep stakeholder management partnership and data pipeline design,have followup interview for senior role that consists of two hour interviews haven been through one of these before and wanted to ask the community for guidance on how best to prepare and what sort of questions to expect the interviews will be stakeholder management and partnership high level case study about creating self service analytics for stakeholders pms swes ds how to be attentive to and address stakeholder needs prioritizing tasks with time resource constraints tradeoffs between avoiding tech debt product stability and delivering user value quickly anti patterns to avoid amp x200b data pipeline design whiteboard style given set of data and sql queries design pipeline thought process on why you would set it up that way how to explain to stakeholders how they would use the pipeline how analytically useful is it for them how to assess how production quality the pipeline is how to set it up to run where are the bottlenecks how to ensure data quality how would you gather requirements amp x200b plan on going through designing data intensive applications for the nd one starting to scour the internet for info regarding the st one as well as reflecting on my own experience super psyched for this company so thank you in advance for your advice,0,help with interview prep stakeholder management partnership and data pipeline design,have followup interview for senior role that consists of two hour interview haven been through one of these before and wanted to ask the community for guidance on how best to prepare and what sort of question to expect the interview will be stakeholder management and partnership high level case study about creating self service analytics for stakeholder pm swes d how to be attentive to and address stakeholder need prioritizing task with time resource constraint tradeoff between avoiding tech debt product stability and delivering user value quickly anti pattern to avoid amp x200b data pipeline design whiteboard style given set of data and sql query design pipeline thought process on why you would set it up that way how to explain to stakeholder how they would use the pipeline how analytically useful is it for them how to ass how production quality the pipeline is how to set it up to run where are the bottleneck how to ensure data quality how would you gather requirement amp x200b plan on going through designing data intensive application for the nd one starting to scour the internet for info regarding the st one a well a reflecting on my own experience super psyched for this company so thank you in advance for your advice
how to understand the schema below,how to understand the schema below what is the difference between count_likes and data_likes data_name string count_likes integer data record lt this column has nested columns data_likes integer on top of the schema written following info dataset is aggregated by the event,0,how to understand the schema below,how to understand the schema below what is the difference between count_likes and data_likes data_name string count_likes integer data record lt this column ha nested column data_likes integer on top of the schema written following info dataset is aggregated by the event
working full time as data analyst should pursue comp sci degree to help make transition to data engineering,appreciate any career advice currently work as data analyst for sales operations team mostly work building power bi dashboards and local data pipelines using python and jupyter notebooks have master degree in business analytics which involved mix of python sql tableau machine learning and statistics am interested in transitioning to data engineering however feel that lack engineering experience although sometimes hear that you can break into de without comp sci degree it seems that many de jobs require it have been considering applying to upenn online mcit degree which is master designed for people without an undergrad in cs feel that this program can help however it would probably take three years to complete part time while working do you all have any advice on whether pursuing this degree is worth it or would it be overkill for me are there any alternatives should be considering thank you for your perspectives it is nice to have community where can ask these questions,0,working full time a data analyst should pursue comp sci degree to help make transition to data engineering,appreciate any career advice currently work a data analyst for sale operation team mostly work building power bi dashboard and local data pipeline using python and jupyter notebook have master degree in business analytics which involved mix of python sql tableau machine learning and statistic am interested in transitioning to data engineering however feel that lack engineering experience although sometimes hear that you can break into de without comp sci degree it seems that many de job require it have been considering applying to upenn online mcit degree which is master designed for people without an undergrad in c feel that this program can help however it would probably take three year to complete part time while working do you all have any advice on whether pursuing this degree is worth it or would it be overkill for me are there any alternative should be considering thank you for your perspective it is nice to have community where can ask these question
apache spark pandas api koalas alone is enough for spark functionalities,spark now includes the full pandas api called koalas can we solely use this api to code spark computations for etl jobs it is lot easier to get people with python and pandas skill than to find someone with full spark skills so if we can achieve the same results as using spark by utilizing koalas then this will lower the bar to entry into spark etl workflow in other words what are the limitations of koalas compared to core spark,0,apache spark panda api koala alone is enough for spark functionality,spark now includes the full panda api called koala can we solely use this api to code spark computation for etl job it is lot easier to get people with python and panda skill than to find someone with full spark skill so if we can achieve the same result a using spark by utilizing koala then this will lower the bar to entry into spark etl workflow in other word what are the limitation of koala compared to core spark
why is data build tool dbt is so popular what are some other alternatives,hi can some one please explain why dbt is so popular,0,why is data build tool dbt is so popular what are some other alternative,hi can some one please explain why dbt is so popular
lambda vs kappa architecture pros and cons,feel like kappa has alot of upsides lower latency simpler codebase and limited downsides difficulty in implementation are there more to the story in what situation should we choose lambda over kappa and vice versa,0,lambda v kappa architecture pro and con,feel like kappa ha alot of upside lower latency simpler codebase and limited downside difficulty in implementation are there more to the story in what situation should we choose lambda over kappa and vice versa
burnout,ok just going to be transparent here have been working as vp of sales growth hacker and even cmo discovered my passion for coding and data about years ago and did and became data engineer year ago have never ever experienced any symptoms of burnout until now the workload ridiculous at least at my workplace started researching and came across this article it basically saying that the of de is experiencing some kind of burnout do guys relate or am just doing this stuff wrong,0,burnout,ok just going to be transparent here have been working a vp of sale growth hacker and even cmo discovered my passion for coding and data about year ago and did and became data engineer year ago have never ever experienced any symptom of burnout until now the workload ridiculous at least at my workplace started researching and came across this article it basically saying that the of de is experiencing some kind of burnout do guy relate or am just doing this stuff wrong
own business,does anyone run their own business seeing ridiculous money being paid for integrations and reporting doing this from sales design through to coding the solutions seems like no brainer to start doing it for myself,0,own business,doe anyone run their own business seeing ridiculous money being paid for integration and reporting doing this from sale design through to coding the solution seems like no brainer to start doing it for myself
task consultation,guys am just starting out as data engineer have data table that looks like following dt date repository_name string cnt_open_issue integer cnt_watchers integer repo record gt repository_url string repository_open_issues integer repository_watchers integer now need to write airflow pipeline with bigqueryoperator that loads data into the schema above using bigquery public data samples github timeline here is what have tried do not worry about the exact code even pseudocode is fine rather want to understand the overall flow of the process see if am on right direction this is my first pipeline import airflow from airflow import dag from airflow contrib operators bigquery_operator import bigqueryoperator from datetime import datetime default_args owner testing depends_on_past false start_date datetime test_datetime email test_email email_on_failure true email_on_retry false retries retry_delay timedelta minutes dag dag test_bigquery_timeline default_args default_args schedule_interval test_interval bq_conn_id test_connection_id bq_project_id test_project_id bq_dataset test_table_schema sample_task bigqueryoperator task_id test_load_data_into_table_schema source_objects bigquery public data samples github_timeline schema_fields name dt type date mode nullable name repository_name type string mode nullable name cnt_open_issues type integer mode nullable name cnt_watchers type integer mode nullable name repo type record mode repeated fields name repository_url type string mode nullable name repository_open_issues type integer mode nullable name repository_watchers type integer mode nullable destination_dataset_table bq_project_id bq_dataset table allow_large_results true write_disposition write_append bigquery_conn_id bq_conn_id dag dag sample_task is there anything wrong with this pipeline amp x200b also what makes me to have doubt about it is for example bigquery public data samples github timeline does not have cnt_open_issues column how this code would know what exacly to copy from the bigquery public data sample github timeline am sure missing out something here am trying to understand how airflow pipeline works know there are bunch of course learn better by doing looking for advice direction correction from you guys thank you all,0,task consultation,guy am just starting out a data engineer have data table that look like following dt date repository_name string cnt_open_issue integer cnt_watchers integer repo record gt repository_url string repository_open_issues integer repository_watchers integer now need to write airflow pipeline with bigqueryoperator that load data into the schema above using bigquery public data sample github timeline here is what have tried do not worry about the exact code even pseudocode is fine rather want to understand the overall flow of the process see if am on right direction this is my first pipeline import airflow from airflow import dag from airflow contrib operator bigquery_operator import bigqueryoperator from datetime import datetime default_args owner testing depends_on_past false start_date datetime test_datetime email test_email email_on_failure true email_on_retry false retries retry_delay timedelta minute dag dag test_bigquery_timeline default_args default_args schedule_interval test_interval bq_conn_id test_connection_id bq_project_id test_project_id bq_dataset test_table_schema sample_task bigqueryoperator task_id test_load_data_into_table_schema source_objects bigquery public data sample github_timeline schema_fields name dt type date mode nullable name repository_name type string mode nullable name cnt_open_issues type integer mode nullable name cnt_watchers type integer mode nullable name repo type record mode repeated field name repository_url type string mode nullable name repository_open_issues type integer mode nullable name repository_watchers type integer mode nullable destination_dataset_table bq_project_id bq_dataset table allow_large_results true write_disposition write_append bigquery_conn_id bq_conn_id dag dag sample_task is there anything wrong with this pipeline amp x200b also what make me to have doubt about it is for example bigquery public data sample github timeline doe not have cnt_open_issues column how this code would know what exacly to copy from the bigquery public data sample github timeline am sure missing out something here am trying to understand how airflow pipeline work know there are bunch of course learn better by doing looking for advice direction correction from you guy thank you all
how would you prefer sales person reach you about product for data processing,how would you prefer sales person reach you about product for data processing view poll,0,how would you prefer sale person reach you about product for data processing,how would you prefer sale person reach you about product for data processing view poll
have list company urls wanting stock market info on at least for the ones that are public what the best service tool to take this list of company urls bulk upload and ideally then export chosen data query based on the companies from the list that are public,know for example that from crunchbase could get stock symbols from the relevant urls then probably take that to another financial service to do the rest unknown which one new to this then connect the data back to my main list but ideally wanting to do it all in one system amp x200b as mentioned too new to working with market stock related systems and would prefer something free but paid is fine too so suggestions on tools that first can associate company urls to stock info and have some flexibility in what can query more the merrier is what seeking big thank you ahead for any help here too,0,have list company url wanting stock market info on at least for the one that are public what the best service tool to take this list of company url bulk upload and ideally then export chosen data query based on the company from the list that are public,know for example that from crunchbase could get stock symbol from the relevant url then probably take that to another financial service to do the rest unknown which one new to this then connect the data back to my main list but ideally wanting to do it all in one system amp x200b a mentioned too new to working with market stock related system and would prefer something free but paid is fine too so suggestion on tool that first can associate company url to stock info and have some flexibility in what can query more the merrier is what seeking big thank you ahead for any help here too
why is snowflake so popular,hi all am just wondering why so many companies use snowflake as far as know it is cloud datawarehouse which means that the main competitors are platforms such as bigquery redshift synapse what makes snowflake better than the rest do you use it in your work in that case which kind of application do you have and which tools do you integrate it with thanks,0,why is snowflake so popular,hi all am just wondering why so many company use snowflake a far a know it is cloud datawarehouse which mean that the main competitor are platform such a bigquery redshift synapse what make snowflake better than the rest do you use it in your work in that case which kind of application do you have and which tool do you integrate it with thanks
does your organization have person position that makes data easier to find or serve as advocate for analysts trying to identify the right data or as bridge between analyst and data engineers to define what is most needed,this would likely be limited to very large companies with significant number of analysts and complex data but am curious what the role may be called and duties that the person or team would take on my employer fortune analysts dozens of co related business lines with data to share across teams has role as described but no good name for it and lots of opportunities to make it better the person specific to line of business is point of contact when you are looking for something or have questions they have the roadmap for new data additions and prioritize building new tables views products the role helps people get access to systems and listens for the things that are difficult then tries to make the process easier,0,doe your organization have person position that make data easier to find or serve a advocate for analyst trying to identify the right data or a bridge between analyst and data engineer to define what is most needed,this would likely be limited to very large company with significant number of analyst and complex data but am curious what the role may be called and duty that the person or team would take on my employer fortune analyst dozen of co related business line with data to share across team ha role a described but no good name for it and lot of opportunity to make it better the person specific to line of business is point of contact when you are looking for something or have question they have the roadmap for new data addition and prioritize building new table view product the role help people get access to system and listens for the thing that are difficult then try to make the process easier
best practices to track and control costs on snowflake,hey everyone wondering what kind of best practices or mechanisms you are using to track and control costs on snowflake the platform offers couple of different options to do that querying system tables check the ui and quota system but feel that there lot to be done on top of that in production system,0,best practice to track and control cost on snowflake,hey everyone wondering what kind of best practice or mechanism you are using to track and control cost on snowflake the platform offer couple of different option to do that querying system table check the ui and quota system but feel that there lot to be done on top of that in production system
dbt pricing for an analytics stack with around tb data is it worth the cost,this is noob question and apologies if it gives you didnot do enough research vibes so in community see lot of dbt recommendations and did some research if it suits our needs for an analytics stack that we are moving from mssql server on premise to gcp mostly cloud functions bigquery looker our data is currently around tb and we heavily rely on sql procedures that we plan to replace by bigquery routines these sql procedures are poorly versioned copy pasting to production kinda of thing and team of data engineers work on different business requirements in the stack so was testing if dbt will resolve issues of versioning and database schema handling that continuously evolves but struggled with couple of issues data engineer monthly felt expensive to me is it can anyone shed light on enterprise version pricing read its open source and have thriving community is it about dbt cli do data security bothers you understand google is not something to be trusted either but lets say company is okay to put their data in gcp how to explain that dbt will also have access to the data now what dbt is everyone using that is so affordable and has all the features and looks like data engineer dream come true again apologies if it all sounds too stupid,0,dbt pricing for an analytics stack with around tb data is it worth the cost,this is noob question and apology if it give you didnot do enough research vibe so in community see lot of dbt recommendation and did some research if it suit our need for an analytics stack that we are moving from mssql server on premise to gcp mostly cloud function bigquery looker our data is currently around tb and we heavily rely on sql procedure that we plan to replace by bigquery routine these sql procedure are poorly versioned copy pasting to production kinda of thing and team of data engineer work on different business requirement in the stack so wa testing if dbt will resolve issue of versioning and database schema handling that continuously evolves but struggled with couple of issue data engineer monthly felt expensive to me is it can anyone shed light on enterprise version pricing read it open source and have thriving community is it about dbt cli do data security bother you understand google is not something to be trusted either but let say company is okay to put their data in gcp how to explain that dbt will also have access to the data now what dbt is everyone using that is so affordable and ha all the feature and look like data engineer dream come true again apology if it all sound too stupid
real time stream sliding window vs tumbling batch,ok get it batching data into mini chunks makes sense in cases like presenting histogram on dashboard or saving data in summaries to conserve storage but for event driven applications batch processing mmm say you are comparing stats from the last minute last hour and last hours in order to detect something like an opportunity in the stock market or shift in crowd sentiment or possible hacker attack etc tumbling window or batching has major drawbacks it has to wait for the batch to fill or closing time which could cost precious time to react the specific patterns you are hoping to detect often get cut off into different batches at point that makes them completely undetected sliding window queries run continually and guarantee that the pattern gets detected but have massive compute requirement it literally applying formula each time stream message is received the bigger the window the slower they get exponentially which might be why sliding window queries aren so popular too expensive so decided to make new stream analytics engine that can query sliding windows at constant speed nanoseconds regardless of window size so that we can build better event driven applications on small vms without spending fortune and yes this entire post is desperate attempt to find collaborators the app is free and open source and almost ready if you are into this topic let have discussion people right now making cutesy website for the project www riodb org there you can find out more about the app and myself suggest starting with about gt faq gt what makes it fast if you aren programmer no worries looking for thoughts questions use cases experiments benchmarks etc or maybe you know producers and consumers tools or dashboards that would be fun to integrate let get this party started thanks tiger,0,real time stream sliding window v tumbling batch,ok get it batching data into mini chunk make sense in case like presenting histogram on dashboard or saving data in summary to conserve storage but for event driven application batch processing mmm say you are comparing stats from the last minute last hour and last hour in order to detect something like an opportunity in the stock market or shift in crowd sentiment or possible hacker attack etc tumbling window or batching ha major drawback it ha to wait for the batch to fill or closing time which could cost precious time to react the specific pattern you are hoping to detect often get cut off into different batch at point that make them completely undetected sliding window query run continually and guarantee that the pattern get detected but have massive compute requirement it literally applying formula each time stream message is received the bigger the window the slower they get exponentially which might be why sliding window query aren so popular too expensive so decided to make new stream analytics engine that can query sliding window at constant speed nanosecond regardless of window size so that we can build better event driven application on small vms without spending fortune and yes this entire post is desperate attempt to find collaborator the app is free and open source and almost ready if you are into this topic let have discussion people right now making cutesy website for the project www riodb org there you can find out more about the app and myself suggest starting with about gt faq gt what make it fast if you aren programmer no worry looking for thought question use case experiment benchmark etc or maybe you know producer and consumer tool or dashboard that would be fun to integrate let get this party started thanks tiger
interview on tuesday,have my final technical interview tuesday morning for job ll make lot more in terrified of being berated for min because ve never done technical interview before just posting for well wishes and luck ll be cramming coursera course in this weekend,0,interview on tuesday,have my final technical interview tuesday morning for job ll make lot more in terrified of being berated for min because ve never done technical interview before just posting for well wish and luck ll be cramming coursera course in this weekend
help with managing airflow dev git branches,have an interesting problem that some of you more experienced might be able to help with as team we have to constantly merge our feature branches since airflow expects single dag folder our ci cd setup means that if push feature branch my branch becomes the current state of the airflow instance the issue with this is that if someone else makes change in separate branch that doesn have my updates they must merge in my branch before pushing otherwise my changes are overwritten in the dev dag folder needless to say this defeats the purpose of branching since when ready to merge into master risk bringing along all the unfinished branches ve merged into mine during development curious to hear how others have managed this setup in an ideal world we would spin up an airflow instance for each feature branch when it is pushed with new changes but that makes small iterative changes slow since deployment with every update to the branch takes time,0,help with managing airflow dev git branch,have an interesting problem that some of you more experienced might be able to help with a team we have to constantly merge our feature branch since airflow expects single dag folder our ci cd setup mean that if push feature branch my branch becomes the current state of the airflow instance the issue with this is that if someone else make change in separate branch that doesn have my update they must merge in my branch before pushing otherwise my change are overwritten in the dev dag folder needle to say this defeat the purpose of branching since when ready to merge into master risk bringing along all the unfinished branch ve merged into mine during development curious to hear how others have managed this setup in an ideal world we would spin up an airflow instance for each feature branch when it is pushed with new change but that make small iterative change slow since deployment with every update to the branch take time
best way to permanently store snowflake table snapshots,hi all normally just use snowflake for analytics and technically could drop the whole db and no actual data would be lost would just have to run and wait for pipelines to populate it again however client wants to take quarterly snapshots of complex view materialized as table for audit purposes this is in case the underlying data ever changes they will have record of the exact data at the time they did audit reporting my instinct is to export the snapshot to version controlled s3 bucket and read it back in to snowflake as an external table this is so could keep the practice of saying snowflake can be deleted it is not the source of anything another colleague just wants to append it into another snowflake table but that feels like we are one accident away from dropping it and now snowflake is the source for this one audit data set am over cautious or thinking of snowflake wrong advice or links to relevant docs discussions welcome,0,best way to permanently store snowflake table snapshot,hi all normally just use snowflake for analytics and technically could drop the whole db and no actual data would be lost would just have to run and wait for pipeline to populate it again however client want to take quarterly snapshot of complex view materialized a table for audit purpose this is in case the underlying data ever change they will have record of the exact data at the time they did audit reporting my instinct is to export the snapshot to version controlled s3 bucket and read it back in to snowflake a an external table this is so could keep the practice of saying snowflake can be deleted it is not the source of anything another colleague just want to append it into another snowflake table but that feel like we are one accident away from dropping it and now snowflake is the source for this one audit data set am over cautious or thinking of snowflake wrong advice or link to relevant doc discussion welcome
distinguishing critical pipeline tests from metrics how do you decide what to actually test,we should all know at this point data quality and testing your data is important but like the angle that this blog takes on avoiding altering fatigue it great that you set system up but it pretty easy to create bunch of extra noise,0,distinguishing critical pipeline test from metric how do you decide what to actually test,we should all know at this point data quality and testing your data is important but like the angle that this blog take on avoiding altering fatigue it great that you set system up but it pretty easy to create bunch of extra noise
version control data pipelines in bigquery,we are kicking off migration to bigquery from mssql server for our analytics stack what is the best way to version control tables views creation routines creation schema creation and changes over time have checked out terraform but looks like it comes with its own set of limitations like when add column table is destroyed and created something we cannot afford with production data looking at github actions too something like this merge to main branch runs bq command to do some action in gcp any similar setups and experiences what are the best practices etc thanks in advance,0,version control data pipeline in bigquery,we are kicking off migration to bigquery from mssql server for our analytics stack what is the best way to version control table view creation routine creation schema creation and change over time have checked out terraform but look like it come with it own set of limitation like when add column table is destroyed and created something we cannot afford with production data looking at github action too something like this merge to main branch run bq command to do some action in gcp any similar setup and experience what are the best practice etc thanks in advance
creating your own data engineering labs,has anyone got any experience or knowledge around creating your own labs either in azure or aws thoughts resources experience all welcome,0,creating your own data engineering lab,ha anyone got any experience or knowledge around creating your own lab either in azure or aws thought resource experience all welcome
what should be the next step should take to improve my career prospects,at my company that work at now my business title is data engineer but all do is work in informatica powercenter almost never write any code the only code that do write is sql and barely even do that pretty much my entire job is just creating workflows in informatica powercenter am worried that this job is not giving me enough opportunity for growth and enough transferable skills to be able to be competitive in the job market what should do also want to make it clear that it seems like my job role will not be changing anytime soon to include any new technologies or new rules genuinely think if stayed in this job would be doing informatica powercenter forever and never learning anything new my opinion is that either need to get new job at new company so that can learn new skills or need to transfer to completely different team within my company and possibly not be data engineer anymore since it seems like the way my company does data engineering is not very beneficial for career growth in my personal opinion but would like to read opinions from people who are far more experienced than am what do you think should do have about months of experience so far and this is the only job ve had since graduating,0,what should be the next step should take to improve my career prospect,at my company that work at now my business title is data engineer but all do is work in informatica powercenter almost never write any code the only code that do write is sql and barely even do that pretty much my entire job is just creating workflow in informatica powercenter am worried that this job is not giving me enough opportunity for growth and enough transferable skill to be able to be competitive in the job market what should do also want to make it clear that it seems like my job role will not be changing anytime soon to include any new technology or new rule genuinely think if stayed in this job would be doing informatica powercenter forever and never learning anything new my opinion is that either need to get new job at new company so that can learn new skill or need to transfer to completely different team within my company and possibly not be data engineer anymore since it seems like the way my company doe data engineering is not very beneficial for career growth in my personal opinion but would like to read opinion from people who are far more experienced than am what do you think should do have about month of experience so far and this is the only job ve had since graduating
snowflake infer_schema for external tables,anyone knows if it is possible to use infer_schema to create external tables with defined schema by default it creates table with single value column that contains all the data have parquet files in adsl that are pretty wide columns and was able to use infer schema to create normal tables with no issue and can create external tables if define the columns manually but the same syntax to infer_schema does not work for external tables have data factory job that loads the data into adsl with timestamp and pick it up with snowpipe but at the moment there is no need to keep all historical data having the current value is good enough so was thinking could use external table and simply overwrite the file in adsl and read the latest through an external table,0,snowflake infer_schema for external table,anyone know if it is possible to use infer_schema to create external table with defined schema by default it creates table with single value column that contains all the data have parquet file in adsl that are pretty wide column and wa able to use infer schema to create normal table with no issue and can create external table if define the column manually but the same syntax to infer_schema doe not work for external table have data factory job that load the data into adsl with timestamp and pick it up with snowpipe but at the moment there is no need to keep all historical data having the current value is good enough so wa thinking could use external table and simply overwrite the file in adsl and read the latest through an external table
spark ml biased result my confusion about machine learning parallelization,hi guys am new to data engineering domain very challenging am still trying to understand basic high level overview on spark ml machine learning parallelization came across material screenshot attached amp x200b format png amp auto webp amp f6105df0e423e7ca9828035d9e0707ea3bb23c my confusion is that would such way produce bias model because each node only worked on partial data instead of taking the whole data as consideration of learning can understand the spark parallelization can benefit batch processing map reduce jobs counting sorting aggregating imagine gb dataset by parallelizing to gb subsets to workers nodes it boosts the processing productivity however in the above case we want to do linear regression supervised learning or svd unsupervised learning from these gb data points ideally the algorithm digest the gb dataset as whole to produce model catching all the nuances relationships correlation variances etc making relatively accurate model but if we partitioned to gb subsets separately into three different workers each worker products its models based on its assigned gb data the system combines the results with the final result wouldn the final model result be very different from the model we train on the whole gb dataset because each work trained model is only based on its given partial data would you please correct me if am wrong kind of confused my question is will the spark machine learning parallelization produce biased model than the way train learning on whole original dataset amp x200b thanks,0,spark ml biased result my confusion about machine learning parallelization,hi guy am new to data engineering domain very challenging am still trying to understand basic high level overview on spark ml machine learning parallelization came across material screenshot attached amp x200b format png amp auto webp amp f6105df0e423e7ca9828035d9e0707ea3bb23c my confusion is that would such way produce bias model because each node only worked on partial data instead of taking the whole data a consideration of learning can understand the spark parallelization can benefit batch processing map reduce job counting sorting aggregating imagine gb dataset by parallelizing to gb subset to worker node it boost the processing productivity however in the above case we want to do linear regression supervised learning or svd unsupervised learning from these gb data point ideally the algorithm digest the gb dataset a whole to produce model catching all the nuance relationship correlation variance etc making relatively accurate model but if we partitioned to gb subset separately into three different worker each worker product it model based on it assigned gb data the system combine the result with the final result wouldn the final model result be very different from the model we train on the whole gb dataset because each work trained model is only based on it given partial data would you please correct me if am wrong kind of confused my question is will the spark machine learning parallelization produce biased model than the way train learning on whole original dataset amp x200b thanks
using both snowflake and databricks,would it make sense to use both snowflake and databricks in my cloud data architecture would consider snowflake as my main data warehouse and apply sql transformations in it of transformations while databricks will be used for more complex data transformations that require python pyspark of transformations as well as considering it as our machine learning experimentation platform making use of its notebooks automl in this case will not be using delta lack lack house instead of this databricks will be read and write the data from to snowflake does this make sense or have to choose either snowflake or databricks,0,using both snowflake and databricks,would it make sense to use both snowflake and databricks in my cloud data architecture would consider snowflake a my main data warehouse and apply sql transformation in it of transformation while databricks will be used for more complex data transformation that require python pyspark of transformation a well a considering it a our machine learning experimentation platform making use of it notebook automl in this case will not be using delta lack lack house instead of this databricks will be read and write the data from to snowflake doe this make sense or have to choose either snowflake or databricks
what would you do if you had some spare time in day,say you had spare hours day to spend doing anything that could contribute to your growth as data scientist possibly increase your income increase job prospects skills set etc what would that be,0,what would you do if you had some spare time in day,say you had spare hour day to spend doing anything that could contribute to your growth a data scientist possibly increase your income increase job prospect skill set etc what would that be
data engineering roadmap for python etl developer need guidance,hi have been working in python for long time even developed custom etls and in airflow and bit in spark but so far could not pursue the career properly now just want to up from being an etl developer only to full fledged data engineering field need guidance in the following amp x200b what roadmap can follow to learn more about data engineering in the next months should specialize in certain tool like spark or airflow is there any data engineering certification that could help in both learning and jobs any course am sorry if am not making sense but in short need clear roadmap and plan to execute things asap amp x200b thanks,0,data engineering roadmap for python etl developer need guidance,hi have been working in python for long time even developed custom etls and in airflow and bit in spark but so far could not pursue the career properly now just want to up from being an etl developer only to full fledged data engineering field need guidance in the following amp x200b what roadmap can follow to learn more about data engineering in the next month should specialize in certain tool like spark or airflow is there any data engineering certification that could help in both learning and job any course am sorry if am not making sense but in short need clear roadmap and plan to execute thing asap amp x200b thanks
s3 cost spikes with glue crawler please help,recently implemented glue crawler to run every hour before it was once week however noticed that my s3 cost in the cost explorer spike up last month and this month my s3 buckets currently have less than gb so don think the cost is due to the storage am however using the glue meta store in spark job every hour then change the glue crawler to only check new folders am hoping that this will resolve this because the cost has been very high for me,0,s3 cost spike with glue crawler please help,recently implemented glue crawler to run every hour before it wa once week however noticed that my s3 cost in the cost explorer spike up last month and this month my s3 bucket currently have le than gb so don think the cost is due to the storage am however using the glue meta store in spark job every hour then change the glue crawler to only check new folder am hoping that this will resolve this because the cost ha been very high for me
about data engineer,so can any one tell me what skills are important for data engineer what the roles and responsibilities of data engineer,0,about data engineer,so can any one tell me what skill are important for data engineer what the role and responsibility of data engineer
announcing aws data exchange for apis find subscribe to and use third party apis with consistent authentication amazon web services,announcing aws data exchange for apis find subscribe to and use third party apis with consistent authentication amazon web services,0,announcing aws data exchange for apis find subscribe to and use third party apis with consistent authentication amazon web service,announcing aws data exchange for apis find subscribe to and use third party apis with consistent authentication amazon web service
free alternatives to cdata drivers,forgive me if sound like noob because am but is there free alternative to cdata drivers just an admin assistant trying to wow my boss by linking netsuite customer records with an access mail log and am wanting to have it so if you add customer record in netsuite it will add the customer to my access database thanks for any help,0,free alternative to cdata driver,forgive me if sound like noob because am but is there free alternative to cdata driver just an admin assistant trying to wow my bos by linking netsuite customer record with an access mail log and am wanting to have it so if you add customer record in netsuite it will add the customer to my access database thanks for any help
data engineering vs data science,heyy soooo im sure this is very common question here so sorry in avance if it is but would love some clarification from much more experienced people like you guys that might have missed through just googling my question is more focused on the starter and education side of things as in what does an aspiring data engineer do learn compared to data scientist feel data engineering would suit me much better but all my knowledge comes from the data science side of things are both fields very similar to each other when it comes to learning and its only once you get job that you are given the title as engineer or scientist like the roadmap towards becoming one or the other are similar at the moment short version of my roadmap was looking like this take data science bootcamp get starters job in the field while continuing into an engineering degree at my uni what changes should make or add thanks in advance everyone this would be such life saver,0,data engineering v data science,heyy soooo im sure this is very common question here so sorry in avance if it is but would love some clarification from much more experienced people like you guy that might have missed through just googling my question is more focused on the starter and education side of thing a in what doe an aspiring data engineer do learn compared to data scientist feel data engineering would suit me much better but all my knowledge come from the data science side of thing are both field very similar to each other when it come to learning and it only once you get job that you are given the title a engineer or scientist like the roadmap towards becoming one or the other are similar at the moment short version of my roadmap wa looking like this take data science bootcamp get starter job in the field while continuing into an engineering degree at my uni what change should make or add thanks in advance everyone this would be such life saver
advice for syncing postgres database with timescaledb database,work at company where we store the majority of our data in several postgresql databases some of our data is in the form of time series we re in the process of trying to improve query performance and after doing some benchmarking with timescaledb ve found it would be much more efficient than the other methods ve tried with just postgres by itself at previous company we used kafka to sync two databases that stored similar data that needed to be slightly reformatted before being inserted into the destination db considering using kafka in this scenario as well to sync the data from postgres to the timescaledb we re on aws so can simply just install the timescaledb extension to the existing host unfortunately does anyone know of any alternatives to kafka to keep postgres host and timescaledb host in sync ve found documentation on streaming replication for timescaledb but that seems to only be for the source replica architecture,0,advice for syncing postgres database with timescaledb database,work at company where we store the majority of our data in several postgresql database some of our data is in the form of time series we re in the process of trying to improve query performance and after doing some benchmarking with timescaledb ve found it would be much more efficient than the other method ve tried with just postgres by itself at previous company we used kafka to sync two database that stored similar data that needed to be slightly reformatted before being inserted into the destination db considering using kafka in this scenario a well to sync the data from postgres to the timescaledb we re on aws so can simply just install the timescaledb extension to the existing host unfortunately doe anyone know of any alternative to kafka to keep postgres host and timescaledb host in sync ve found documentation on streaming replication for timescaledb but that seems to only be for the source replica architecture
data platform generator data solution blueprints,hi everyone we took inspiration from matt turck data amp ai landscape and scott brinker martech landscape to generate visual data solution blueprints to inspire you in the design process of your data platform and help you to find alternative combinations of data technologies our idea was simple let take some technologies from these landscapes and generate simple visual data solution blueprints to inspire other data aficionados in the design process of data platform also expose them to different technologies that exist on the market which could be good alternatives to already better known technologies create generator to display one picture at time we have millions of combinations can you discover all of them we used for the first time no code applications like bubble and airtable we launched this page today this is concept site we call picture data solution for all the data aficionados out there have fun be inspired and we hope you enjoy it don forget to share it with your friends if you like it we also appreciate your brutal and honest feedback,0,data platform generator data solution blueprint,hi everyone we took inspiration from matt turck data amp ai landscape and scott brinker martech landscape to generate visual data solution blueprint to inspire you in the design process of your data platform and help you to find alternative combination of data technology our idea wa simple let take some technology from these landscape and generate simple visual data solution blueprint to inspire other data aficionado in the design process of data platform also expose them to different technology that exist on the market which could be good alternative to already better known technology create generator to display one picture at time we have million of combination can you discover all of them we used for the first time no code application like bubble and airtable we launched this page today this is concept site we call picture data solution for all the data aficionado out there have fun be inspired and we hope you enjoy it don forget to share it with your friend if you like it we also appreciate your brutal and honest feedback
scaling data ingest for nlp models,we were testing out an nlp pipeline but most of the data test runs were done with data in panda dataframes now as we are happy with the results of the ml model wondered what would be the best way to scale the ingestion as we have more than million text files potentially would it have to be file store of individual files or they could be grouped into something like parquets with the string column my background is primarily in sql and data modelling and only in past few months ve been exploring apache spark and databricks as potential solution for scaling,0,scaling data ingest for nlp model,we were testing out an nlp pipeline but most of the data test run were done with data in panda dataframes now a we are happy with the result of the ml model wondered what would be the best way to scale the ingestion a we have more than million text file potentially would it have to be file store of individual file or they could be grouped into something like parquet with the string column my background is primarily in sql and data modelling and only in past few month ve been exploring apache spark and databricks a potential solution for scaling
which cloud certification for de,hello everyone can you guys please share few suggestions which aws certification will be more suitable and preferable for data engineering profile,0,which cloud certification for de,hello everyone can you guy please share few suggestion which aws certification will be more suitable and preferable for data engineering profile
spark data pipeline design considerations,hi all have extensive experience in building etl elt jobs typical etl job consists of multiple jobs with transformation logic scheduling auditing logging etc iam new to data pipelines with spark spark streaming however have worked on few pocs proof of concepts on databrics spark jobs and found concepts are very similar while designing spark jobs what are the design decisions you consider are they same as designing etl jobs like job dependencies orchestration can airflow be used on data bricks jobs master data loads dimension updates job auditing and job logs where and how the audits are stored how to size the cluster based on valume of data thanks,0,spark data pipeline design consideration,hi all have extensive experience in building etl elt job typical etl job consists of multiple job with transformation logic scheduling auditing logging etc iam new to data pipeline with spark spark streaming however have worked on few pocs proof of concept on databrics spark job and found concept are very similar while designing spark job what are the design decision you consider are they same a designing etl job like job dependency orchestration can airflow be used on data brick job master data load dimension update job auditing and job log where and how the audit are stored how to size the cluster based on valume of data thanks
what percent of your work would you say is done using python sql etc,what is your job title,0,what percent of your work would you say is done using python sql etc,what is your job title
steps to managing and prioritizing data requests free template included,as an early data hire at fast growing company one of the first things that you ll likely encounter is backlog of questions from employees this backlog alongside all the other tasks associated with reporting maintaining data and creating new pipelines can feel extremely overwhelming these long lists of tasks and questions prevent many data teams from being proactive about analysis without proper process in place that allows analysts to enable self service managing data requests proactively can feel like never ending battle against current additionally data teams that constantly need to answer the same question are not creating processes that help them manage the scale and complexity that companies experience as they grow below are some of our tips on how to manage the data requests backlog we believe that great data teams should be proactive they should adopt tools and processes that ensure that the data team never has to answer the same question twice we hope this step by step list is helpful to all data teams that want to improve their efficiency and reduce their workload in the future set expectations about the data requests workflow with reasonable timelines the first step of setting expectations around team is communicating the way that your team is working to the stakeholders in the company we suggest adopting an agile workflow with weekly or bi weekly sprints although your scrum team may be small at first setting expectations about when certain requests will be answered with the sprint methodology can be helpful with scrum product is built in series of iterations called sprints that break down big complex projects into bite sized pieces define your requests workflow data requests are questions that employees have about data that exists in the organization or about new data that isn being collected yet traditionally data teams will take data requests through an intake form or slack channel where employees can ask for the request some of the requests are unique and difficult questions which require the full attention of the data team on the other hand some data requests are repetitive and low priority which doesn require much effort from the data team create requests template we ve created data requests template for teams that are looking for better way to manage inbound questions below is the template feel free to copy the template and use it in your team workflow here our template what is the business question you are trying to answer what is the impact of this question and how will it help the company who will be using this data what time frames are crucial here example monthly weekly daily what is the visualization you are trying to create what interactions drill downs are required ie the type of use revenue amount etc are there any other details we should know about this data request automate repetitive data requests data teams that take the next step with their data requests process can start to think proactively about data requests customer support teams have been deflecting inbound questions for years using tools like intercoms knowledge base and ada automated customer support chatbots smart data teams realize that they can do the same data teams can automate and deflect common questions with tools that allow them to document data requests in the same place teammates are looking for answers measure the data requests workflow lastly you can improve anything you don measure taking the time to measure what your users are asking which tables are used the most and who is the most influential user in your organization is great way to automate more common questions if you found this useful you can find the full article here,0,step to managing and prioritizing data request free template included,a an early data hire at fast growing company one of the first thing that you ll likely encounter is backlog of question from employee this backlog alongside all the other task associated with reporting maintaining data and creating new pipeline can feel extremely overwhelming these long list of task and question prevent many data team from being proactive about analysis without proper process in place that allows analyst to enable self service managing data request proactively can feel like never ending battle against current additionally data team that constantly need to answer the same question are not creating process that help them manage the scale and complexity that company experience a they grow below are some of our tip on how to manage the data request backlog we believe that great data team should be proactive they should adopt tool and process that ensure that the data team never ha to answer the same question twice we hope this step by step list is helpful to all data team that want to improve their efficiency and reduce their workload in the future set expectation about the data request workflow with reasonable timeline the first step of setting expectation around team is communicating the way that your team is working to the stakeholder in the company we suggest adopting an agile workflow with weekly or bi weekly sprint although your scrum team may be small at first setting expectation about when certain request will be answered with the sprint methodology can be helpful with scrum product is built in series of iteration called sprint that break down big complex project into bite sized piece define your request workflow data request are question that employee have about data that exists in the organization or about new data that isn being collected yet traditionally data team will take data request through an intake form or slack channel where employee can ask for the request some of the request are unique and difficult question which require the full attention of the data team on the other hand some data request are repetitive and low priority which doesn require much effort from the data team create request template we ve created data request template for team that are looking for better way to manage inbound question below is the template feel free to copy the template and use it in your team workflow here our template what is the business question you are trying to answer what is the impact of this question and how will it help the company who will be using this data what time frame are crucial here example monthly weekly daily what is the visualization you are trying to create what interaction drill down are required ie the type of use revenue amount etc are there any other detail we should know about this data request automate repetitive data request data team that take the next step with their data request process can start to think proactively about data request customer support team have been deflecting inbound question for year using tool like intercom knowledge base and ada automated customer support chatbots smart data team realize that they can do the same data team can automate and deflect common question with tool that allow them to document data request in the same place teammate are looking for answer measure the data request workflow lastly you can improve anything you don measure taking the time to measure what your user are asking which table are used the most and who is the most influential user in your organization is great way to automate more common question if you found this useful you can find the full article here
is there an open source etl tool to convert database dump from one dbms to another,hello qa engineer trying to migrate our test database dump from oracle to postgresql in order to perform regression sanity smoke tests on postgres database with no success there always certain set of sequences that do not function correctly rendering the system unfunctional and lots of modules of the product failing the tools tried to do the database migration with are ora2pg and dbeaver unsuccessfully so read that you can use talend which is an etl tool that can do just what want it to do but what trying to find is if an open source alternative exists that can do what need it to do thanks,0,is there an open source etl tool to convert database dump from one dbms to another,hello qa engineer trying to migrate our test database dump from oracle to postgresql in order to perform regression sanity smoke test on postgres database with no success there always certain set of sequence that do not function correctly rendering the system unfunctional and lot of module of the product failing the tool tried to do the database migration with are ora2pg and dbeaver unsuccessfully so read that you can use talend which is an etl tool that can do just what want it to do but what trying to find is if an open source alternative exists that can do what need it to do thanks
ci cd in data engineering help noob,trying to recommend some process improvements at my org as we start standing up snowflake dw and expand our azure app footprint our legacy data infrastructure is pretty wild west so not lot of source control or ops thinking anyone here successfully implemented or worked in an environment that had particularly good tooling and process around data pipeline code review deployment and change control what are some must haves in your opinion,0,ci cd in data engineering help noob,trying to recommend some process improvement at my org a we start standing up snowflake dw and expand our azure app footprint our legacy data infrastructure is pretty wild west so not lot of source control or ops thinking anyone here successfully implemented or worked in an environment that had particularly good tooling and process around data pipeline code review deployment and change control what are some must have in your opinion
how to automate python extract etl script,currently have python script that pulls data from an api now need to store that data somewhere either s3 or google drive or mysql etc haven decided yet once decide on the destination and adapt the script to load the data how do automate the running of the script don want to use my local machine looking for both specifics and general knowledge and how to create an environment where your script is run,0,how to automate python extract etl script,currently have python script that pull data from an api now need to store that data somewhere either s3 or google drive or mysql etc haven decided yet once decide on the destination and adapt the script to load the data how do automate the running of the script don want to use my local machine looking for both specific and general knowledge and how to create an environment where your script is run
how to simplify data engineering work,,0,how to simplify data engineering work,
automatically refresh an xlsx on google drive from an mssql database,hi have an excel spreadsheet on my google drive that refresh every day but since it takes mins to complete was wondering if could automate this say at am right after the data is updated in the mssql database currently the excel is connected to the database with connection string containing my username to the database am planning on doing it this way while somehow adding the credentials into the mix is there better approach to this,0,automatically refresh an xlsx on google drive from an mssql database,hi have an excel spreadsheet on my google drive that refresh every day but since it take min to complete wa wondering if could automate this say at am right after the data is updated in the mssql database currently the excel is connected to the database with connection string containing my username to the database am planning on doing it this way while somehow adding the credential into the mix is there better approach to this
cloud data warehouse the full guide,cloud data warehouse the full guide,0,cloud data warehouse the full guide,cloud data warehouse the full guide
py spark or scala spark,which would be much more useful in long term,0,py spark or scala spark,which would be much more useful in long term
convincing for apache stack,hi folks currently building dwh we are gathering iiot data from industrial machines with time series data and other stuff iot and consumption data only have time so have to use cartesian product to match them with their respective jobid in every factory our system will be on prem know that we are not going to use distributed systems but currently researching apache druid to store our time series data and inmon modeled dwh in short onprem machine asks for iot data every second to industrial machines can be up to and gets an array of all iot change data length of about mostly null concurrently jobid and consumption data will be gathered and all data will be matched with jobid every minutes and etl will trigger to load it to dwh our current system design in velocity this seems lot but not in size think apache stack is cool but cannot just develop it bc want it given the fact that number of sensor machine can increase or system expansion in erp etc how can convince my team leader to practice and learn those techs in prod enviroment in data industry there is no exact definition of big data though so can calculate velocity volume variety formula when you look for requirements you usually see example of very big tech companies with over million sec data flow which is not the case for of companies what would be your approach,0,convincing for apache stack,hi folk currently building dwh we are gathering iiot data from industrial machine with time series data and other stuff iot and consumption data only have time so have to use cartesian product to match them with their respective jobid in every factory our system will be on prem know that we are not going to use distributed system but currently researching apache druid to store our time series data and inmon modeled dwh in short onprem machine asks for iot data every second to industrial machine can be up to and get an array of all iot change data length of about mostly null concurrently jobid and consumption data will be gathered and all data will be matched with jobid every minute and etl will trigger to load it to dwh our current system design in velocity this seems lot but not in size think apache stack is cool but cannot just develop it bc want it given the fact that number of sensor machine can increase or system expansion in erp etc how can convince my team leader to practice and learn those tech in prod enviroment in data industry there is no exact definition of big data though so can calculate velocity volume variety formula when you look for requirement you usually see example of very big tech company with over million sec data flow which is not the case for of company what would be your approach
databricks developer essentials capstone,anyone have some experience in getting through the exercises in this capstone there seems to be some nuances and edge cases that cannot be easily googled it also seems more difficult than real exam any tips on how to get proficient enough with pyspark to get through these with ease,0,databricks developer essential capstone,anyone have some experience in getting through the exercise in this capstone there seems to be some nuance and edge case that cannot be easily googled it also seems more difficult than real exam any tip on how to get proficient enough with pyspark to get through these with ease
puppeteer and bot blocking,have script using puppeteer that works locally on my mac however the same script doesn work on my ubuntu vm running the browser in non headless mode and can see that when it reaches specific page it just doesn load at all has anyone ever encountered something like this does it seem logical that the site might have bot that is blocking this on linux but not on mac any other reasons that this might be happening thanks,0,puppeteer and bot blocking,have script using puppeteer that work locally on my mac however the same script doesn work on my ubuntu vm running the browser in non headless mode and can see that when it reach specific page it just doesn load at all ha anyone ever encountered something like this doe it seem logical that the site might have bot that is blocking this on linux but not on mac any other reason that this might be happening thanks
column data lineage in bigquery with zetasql and dbt,column data lineage in bigquery with zetasql and dbt,0,column data lineage in bigquery with zetasql and dbt,column data lineage in bigquery with zetasql and dbt
low code vs sql what transforms data faster,two data engineers are racing to get the same data model done before the end of the day one using low code the other building it in sql guess who makes it to happy hour,0,low code v sql what transforms data faster,two data engineer are racing to get the same data model done before the end of the day one using low code the other building it in sql guess who make it to happy hour
mac or pc,even though mac isnt requirement lot of engineers use it so im wondering how feasible is it in your work place if everyone is not on the same system view poll,0,mac or pc,even though mac isnt requirement lot of engineer use it so im wondering how feasible is it in your work place if everyone is not on the same system view poll
why you should become data engineer and not data scientist picking the right data career,why you should become data engineer and not data scientist picking the right data career,0,why you should become data engineer and not data scientist picking the right data career,why you should become data engineer and not data scientist picking the right data career
how do you load large datasets to redshift,have around mb parquet or orc files in the s3 location around files how can plan to load the history data to redshift for mb file using copy command is taking forever,0,how do you load large datasets to redshift,have around mb parquet or orc file in the s3 location around file how can plan to load the history data to redshift for mb file using copy command is taking forever
late data arrival,get asked this very often in interviews how do you handle late arrival of data eg if batch job has already ran and you have late data arrival for that job how do you handle scenarios like this how do you guys handle this in your products,0,late data arrival,get asked this very often in interview how do you handle late arrival of data eg if batch job ha already ran and you have late data arrival for that job how do you handle scenario like this how do you guy handle this in your product
saw this on the funny which makes us data engineering get good laugh,saw this on the funny which makes us data engineering get good laugh,0,saw this on the funny which make u data engineering get good laugh,saw this on the funny which make u data engineering get good laugh
update got my first interview,so my previous post on here was about how could tailor my resume to get interviews wasn getting any after incorporating all the feedback given in the comments finally got one not that big of deal maybe but is huge break for me it my first interview for big data engineering role current in an swe role the first round is technical round for about an hour can someone let me know what kind of questions can expect or topics need to brush up on this is the job description that was sent hands on job experience in hadoop technologies pig spark understanding of traditional etl tools rdbms sql experience on big data and nosql technologies proficiency in unix linux as well as scripting languages shell perl python building non relational data models on nosql data stores hands on mapreduce coding including java python pig programming hands on experience on cloud technologies aws gcp good to have accreditations good understanding on agile way of working amp x200b thanks in advance,0,update got my first interview,so my previous post on here wa about how could tailor my resume to get interview wasn getting any after incorporating all the feedback given in the comment finally got one not that big of deal maybe but is huge break for me it my first interview for big data engineering role current in an swe role the first round is technical round for about an hour can someone let me know what kind of question can expect or topic need to brush up on this is the job description that wa sent hand on job experience in hadoop technology pig spark understanding of traditional etl tool rdbms sql experience on big data and nosql technology proficiency in unix linux a well a scripting language shell perl python building non relational data model on nosql data store hand on mapreduce coding including java python pig programming hand on experience on cloud technology aws gcp good to have accreditation good understanding on agile way of working amp x200b thanks in advance
quarterly salary discussion,this is recurring thread that happens quarterly and was created to help increase transparency around salary and compensation for data engineering please comment below and include the following current title years of experience yoe location base salary amp currency dollars euro pesos etc bonuses equity optional industry optional,0,quarterly salary discussion,this is recurring thread that happens quarterly and wa created to help increase transparency around salary and compensation for data engineering please comment below and include the following current title year of experience yoe location base salary amp currency dollar euro peso etc bonus equity optional industry optional
vertexai to build an api,would like to hear some opinions from anyone who worked with vertexai on gcp does it help to manage the end to end ml lifecycle for creating restful apis or it just about the model development itself and offers no contribution to the serving phase assuming want to implement the ml into web app and serve prediction with api calls,0,vertexai to build an api,would like to hear some opinion from anyone who worked with vertexai on gcp doe it help to manage the end to end ml lifecycle for creating restful apis or it just about the model development itself and offer no contribution to the serving phase assuming want to implement the ml into web app and serve prediction with api call
how to optimizing filtering processes,how to optimizing filtering processes,0,how to optimizing filtering process,how to optimizing filtering process
utilizing dbt and monte carlo together,interesting read from the autotrader uk team,0,utilizing dbt and monte carlo together,interesting read from the autotrader uk team
how do you built real time pipeline,guys ve been building data platform but really don understand know how do building real time pipelines so basically run jobs which picks data from sources which picks data from last day till current time but how do buit real time data platform can anybody pls comment or shade some light it be great if share some resources really want to build real time streaming data pipelines using kafka etc,0,how do you built real time pipeline,guy ve been building data platform but really don understand know how do building real time pipeline so basically run job which pick data from source which pick data from last day till current time but how do buit real time data platform can anybody pls comment or shade some light it be great if share some resource really want to build real time streaming data pipeline using kafka etc
building data platform in pyspark part python and scala interop,building data platform in pyspark part python and scala interop,0,building data platform in pyspark part python and scala interop,building data platform in pyspark part python and scala interop
lessons learned from merge operations with billions of records on databricks spark,lessons learned from merge operations with billions of records on databricks spark,0,lesson learned from merge operation with billion of record on databricks spark,lesson learned from merge operation with billion of record on databricks spark
de intern interview at facebook,hi everyone just passed my first round of interview in meta it was very basics sql and python questions lc easy and have my on site virtual interview in the next few days wanted to know if someone has been there and know how to prepare the motivation interview min talking about why do want to be there any extra help or tips will be received also if there is some de in big company willing to give me little time to answer few of my questions on video call will be great in exchange of money of course know you are all busy thanks,0,de intern interview at facebook,hi everyone just passed my first round of interview in meta it wa very basic sql and python question lc easy and have my on site virtual interview in the next few day wanted to know if someone ha been there and know how to prepare the motivation interview min talking about why do want to be there any extra help or tip will be received also if there is some de in big company willing to give me little time to answer few of my question on video call will be great in exchange of money of course know you are all busy thanks
how to improve etl tech stack for robust api calls historization orchestration and versioning,hey am relatively new to the data engineering environment in smaller company until now we mainly use pentaho for etl pipelines and sql procedures to load data from source systems sap sql server rest apis into the data warehouse die to personal changes and new hiring positions have the opportunity to take our infrastructure to new level we hardly use proper verisoning and often code is maintained twice and three times we have bad scheduling and monitoring for pipelines was thinking of apache airflow for orchestrating data pipelines and gut repo to manage dag files and python sql files seperdtly am still missing robust solution to access rest api to our crm system several times day and store the raw between and finally load the result into the data warehouse performant historization do you guys also have the same issues what does your tech stack look like to manage robust api calls including pagination and reliably get the results from the json into sql database or parguet file,0,how to improve etl tech stack for robust api call historization orchestration and versioning,hey am relatively new to the data engineering environment in smaller company until now we mainly use pentaho for etl pipeline and sql procedure to load data from source system sap sql server rest apis into the data warehouse die to personal change and new hiring position have the opportunity to take our infrastructure to new level we hardly use proper verisoning and often code is maintained twice and three time we have bad scheduling and monitoring for pipeline wa thinking of apache airflow for orchestrating data pipeline and gut repo to manage dag file and python sql file seperdtly am still missing robust solution to access rest api to our crm system several time day and store the raw between and finally load the result into the data warehouse performant historization do you guy also have the same issue what doe your tech stack look like to manage robust api call including pagination and reliably get the result from the json into sql database or parguet file
anyone here work or apply to capital one de roles,hear that have strong de culture any tips to prepare for their first online technical assessment,0,anyone here work or apply to capital one de role,hear that have strong de culture any tip to prepare for their first online technical assessment
what to prepare for on de interview,really really want to become data engineer and have about yoe as data analyst talked with the hiring manager took and passed the ta sql python and am on my way to speaking with individuals over two hour period so far so good but am wondering about other concepts and ideas should prepare for with this next round of interviews they know don have extensive de experience but am very eager to learn they will be teaching us to help with the workload so far have been google de interview questions and analyzing every detail of the jd to get familiar with the tech stack any other aspects and ways can better prepare for this thanks,0,what to prepare for on de interview,really really want to become data engineer and have about yoe a data analyst talked with the hiring manager took and passed the ta sql python and am on my way to speaking with individual over two hour period so far so good but am wondering about other concept and idea should prepare for with this next round of interview they know don have extensive de experience but am very eager to learn they will be teaching u to help with the workload so far have been google de interview question and analyzing every detail of the jd to get familiar with the tech stack any other aspect and way can better prepare for this thanks
how does your team company manage documentation,we are part of rapidly growing company and our team is growing too in the process we are also building lot of tools to better build pipelines and other things but we re lagging in how to document all the things that are being implemented or the changes that are brought in there was no need earlier as the team just consisted of two or three people but it already people team now and we want to be future proof how do you guys document changes or just document tools that are build in house,0,how doe your team company manage documentation,we are part of rapidly growing company and our team is growing too in the process we are also building lot of tool to better build pipeline and other thing but we re lagging in how to document all the thing that are being implemented or the change that are brought in there wa no need earlier a the team just consisted of two or three people but it already people team now and we want to be future proof how do you guy document change or just document tool that are build in house
to all of you data engineers,few questions for you can you tell me about your day to day how do you use python in your jobs who in the business do you typically work with which technology do you recommend to someone interested in this line of work,0,to all of you data engineer,few question for you can you tell me about your day to day how do you use python in your job who in the business do you typically work with which technology do you recommend to someone interested in this line of work
cs6400 database systems concepts amp design course buddy udacity,looking for someone to do the course database systems concepts and design by georgia tech on udacity together it is totally free we can keep each other accountable and work on the project for the course,0,cs6400 database system concept amp design course buddy udacity,looking for someone to do the course database system concept and design by georgia tech on udacity together it is totally free we can keep each other accountable and work on the project for the course
supplying application back with data lake data,hello everyone ve been long time lurker dealing with day to day data engineering tasks but today out product owner asked for one not sure how to beat we re gathering analytics data from various systems via api and calculating kpis using databricks on top of azure data lake now we would want to feed the results back to newly created application we haven been creating data warehouse ad delta lake has been sufficient for us thus simple etl process is no go unfortunately team of backend devs is experienced in using rest api and this is their protocol of choice wondering how would you approach it especially do you see any possibility to create restful api to serve those data or would you recommend different way to serve those data thanks in advance,0,supplying application back with data lake data,hello everyone ve been long time lurker dealing with day to day data engineering task but today out product owner asked for one not sure how to beat we re gathering analytics data from various system via api and calculating kpis using databricks on top of azure data lake now we would want to feed the result back to newly created application we haven been creating data warehouse ad delta lake ha been sufficient for u thus simple etl process is no go unfortunately team of backend devs is experienced in using rest api and this is their protocol of choice wondering how would you approach it especially do you see any possibility to create restful api to serve those data or would you recommend different way to serve those data thanks in advance
data engineer consulting,is consulting good way to start data engineering career right out of college for example would job at company such as accenture or capgemini set you up for good jobs or years down the line additionally would this experience be enough to break into tech in years time,0,data engineer consulting,is consulting good way to start data engineering career right out of college for example would job at company such a accenture or capgemini set you up for good job or year down the line additionally would this experience be enough to break into tech in year time
data pipeline automation on azure synapse,hello everyone am new to azure cloud am trying to automate pipeline basically want to trigger notebook everytime an excel file is added to my data lake storage the notebook runs fine but can not seem to get the name of the file that triggered the pipeline want to be able to get the file name load it into pandas or spark dataframe process the file data and insert it into an azure sql database have figured out all the other parts just need to get the file name any help is highly appreciated,0,data pipeline automation on azure synapse,hello everyone am new to azure cloud am trying to automate pipeline basically want to trigger notebook everytime an excel file is added to my data lake storage the notebook run fine but can not seem to get the name of the file that triggered the pipeline want to be able to get the file name load it into panda or spark dataframe process the file data and insert it into an azure sql database have figured out all the other part just need to get the file name any help is highly appreciated
building metrics dashboard with apache superset and cube spoiler it will be blazing fast,building metrics dashboard with apache superset and cube spoiler it will be blazing fast,0,building metric dashboard with apache superset and cube spoiler it will be blazing fast,building metric dashboard with apache superset and cube spoiler it will be blazing fast
how do you handle deletes when doing an incremental refresh,hey am currently designing an incremental refresh from source system into our data warehouse using the high watermark technique basically just get the last modified date from the dw table and then refresh the last days of data my one challenge is the source system has deletes that we want to delete out of the dw tables as well what is the best way to handle this in your experience could pull all the pks and delete the ones that are in the dw table but not the source table,0,how do you handle deletes when doing an incremental refresh,hey am currently designing an incremental refresh from source system into our data warehouse using the high watermark technique basically just get the last modified date from the dw table and then refresh the last day of data my one challenge is the source system ha deletes that we want to delete out of the dw table a well what is the best way to handle this in your experience could pull all the pks and delete the one that are in the dw table but not the source table
cte vs subquery,cte vs subquery,0,cte v subquery,cte v subquery
swe transition to de,hi all have an upcoming interview for de role at large uk company am years out of univeristy with an applied computing degree having completed an honours project in clinical ml dermatology have spent the time since working for startup as the sole backend developer on projects during this time took over product vision gathered requirements and pushed for software best practises such as tdd and version control however am bit apprehensive about the interview given my absense from sql querying in my past role predominantly used orm just wanted to gauge those in the know if my experience will come across positively in de interview and if there is anything should spotlight or not mention cheers my sql skills aren horrendous but my syntax can be flakey and more advanced topics not clued up on,0,swe transition to de,hi all have an upcoming interview for de role at large uk company am year out of univeristy with an applied computing degree having completed an honour project in clinical ml dermatology have spent the time since working for startup a the sole backend developer on project during this time took over product vision gathered requirement and pushed for software best practises such a tdd and version control however am bit apprehensive about the interview given my absense from sql querying in my past role predominantly used orm just wanted to gauge those in the know if my experience will come across positively in de interview and if there is anything should spotlight or not mention cheer my sql skill aren horrendous but my syntax can be flakey and more advanced topic not clued up on
apache beam and cloud data flow,ve been trying to develop simple pipeline on cloud data flow with apache beam have the pipeline running locally and it working the problem is when try to run it on data flow am really struggling with templating my pipeline not sure how if can run it without templating it all the info ve found regarding using the data flow runner seem incomplete do any of you guys use apache beam with the data flow runner how do you launch your pipelines do you template them no what resource did you use to launch your pipelines at first all the resources have come across are not that great thank you,0,apache beam and cloud data flow,ve been trying to develop simple pipeline on cloud data flow with apache beam have the pipeline running locally and it working the problem is when try to run it on data flow am really struggling with templating my pipeline not sure how if can run it without templating it all the info ve found regarding using the data flow runner seem incomplete do any of you guy use apache beam with the data flow runner how do you launch your pipeline do you template them no what resource did you use to launch your pipeline at first all the resource have come across are not that great thank you
the data stack show podcast on data engineering,hey everyone one of the hosts of podcast show called the data stack show we are focusing on data engineering related topics but we never say no to guest who has something interesting to say around technology we even had guest where we chatted about communities we ve been running the show for about year now and never shared its existence with any community love to get feedback and suggestions from you what questions you might have and what people you would like to listen to of course any constructive criticism is more than welcome amp x200b you can check the show here the data stack show,0,the data stack show podcast on data engineering,hey everyone one of the host of podcast show called the data stack show we are focusing on data engineering related topic but we never say no to guest who ha something interesting to say around technology we even had guest where we chatted about community we ve been running the show for about year now and never shared it existence with any community love to get feedback and suggestion from you what question you might have and what people you would like to listen to of course any constructive criticism is more than welcome amp x200b you can check the show here the data stack show
how do you deal with constant business logic requirement change,hi friends here is something that is constantly bugging me let say you are working on some transformation for the in elt the result is view table that gives the end users data easy to query there is classification field say source that indicates the source of measure sales views whatever your analysts need this field to slice the tableau views the problem is that sometimes the definition of source changes source may mean something before and something else afterwards sometimes one layer of source is not enough and you have to add one more columns to differentiate same sources source rd party and you need second field to tell which rd party these make the table very messy because you got all business rules implemented so analysts have to remember the dates you made the changes and write big case when then else end how to solve this issue guess once we put so much information into one table it inevitable that it becomes messy maybe should remove this table and just expose the dim fact structure to analysts we already have dim fact dwh implemented and my table is just boon to reduce the difficulty of writing lot of joins,0,how do you deal with constant business logic requirement change,hi friend here is something that is constantly bugging me let say you are working on some transformation for the in elt the result is view table that give the end user data easy to query there is classification field say source that indicates the source of measure sale view whatever your analyst need this field to slice the tableau view the problem is that sometimes the definition of source change source may mean something before and something else afterwards sometimes one layer of source is not enough and you have to add one more column to differentiate same source source rd party and you need second field to tell which rd party these make the table very messy because you got all business rule implemented so analyst have to remember the date you made the change and write big case when then else end how to solve this issue guess once we put so much information into one table it inevitable that it becomes messy maybe should remove this table and just expose the dim fact structure to analyst we already have dim fact dwh implemented and my table is just boon to reduce the difficulty of writing lot of join
usage of composite key,hi am studying on composite keys and its usage and am not entirely sure what the advantages are over or surrogate key or column which is concatenation of the columns that would be your composite key understand that you would have less column using composite key but doesn make that column save you lots of possible foreign table columns,0,usage of composite key,hi am studying on composite key and it usage and am not entirely sure what the advantage are over or surrogate key or column which is concatenation of the column that would be your composite key understand that you would have le column using composite key but doesn make that column save you lot of possible foreign table column
installation guide of apache airflow on windows fast amp super easy,hey ve written this piece of information on the basis of training took last days while becoming data engineer here the easiest guide to install apache airflow on windows installation guide of apache airflow on windows super fast amp easy http,0,installation guide of apache airflow on window fast amp super easy,hey ve written this piece of information on the basis of training took last day while becoming data engineer here the easiest guide to install apache airflow on window installation guide of apache airflow on window super fast amp easy http
airflow importing python module,am trying to use an existing python module praw with dag am at loss of where to start using the apache airflow docker compose yaml so do just need to follow these instructions,0,airflow importing python module,am trying to use an existing python module praw with dag am at loss of where to start using the apache airflow docker compose yaml so do just need to follow these instruction
resources,guys looking for data engineering resources such as masters courses online bootcamps free videos etc etc something with flow thanks in advance for sharing with me,0,resource,guy looking for data engineering resource such a master course online bootcamps free video etc etc something with flow thanks in advance for sharing with me
data modeling tools with real time online collaboration features,am planning series of virtual online workshops to create and expand logical data model together with small group of business analysts and data modelers across the country goal is to first collaboratively refine scope functionality extensibility and required data entities at the semantic layer before creating ddls table structures etc the team is familiar with and has access to visio licenses but am wondering whether there are other better tools for virtual team to use in such scenario and what has worked for you any feedback is welcome,0,data modeling tool with real time online collaboration feature,am planning series of virtual online workshop to create and expand logical data model together with small group of business analyst and data modeler across the country goal is to first collaboratively refine scope functionality extensibility and required data entity at the semantic layer before creating ddls table structure etc the team is familiar with and ha access to visio license but am wondering whether there are other better tool for virtual team to use in such scenario and what ha worked for you any feedback is welcome
ccs ccus technologies capture co2 emissions at source or directly from the air co2 emissions are then transported away and stored deep underground or turned into useful products what are your thoughts on this technology,ccs ccus technologies capture co2 emissions at source or directly from the air co2 emissions are then transported away and stored deep underground or turned into useful products what are your thoughts on this technology,0,cc ccus technology capture co2 emission at source or directly from the air co2 emission are then transported away and stored deep underground or turned into useful product what are your thought on this technology,cc ccus technology capture co2 emission at source or directly from the air co2 emission are then transported away and stored deep underground or turned into useful product what are your thought on this technology
deep dive into delta lake via apache zeppelin,deep dive into delta lake via apache zeppelin,0,deep dive into delta lake via apache zeppelin,deep dive into delta lake via apache zeppelin
what your org gcp project architecture for pipelines,for example we re considering having data go through these projects landing gt dev gt prod working on designing this at the moment and like to hear different ideas any thoughts,0,what your org gcp project architecture for pipeline,for example we re considering having data go through these project landing gt dev gt prod working on designing this at the moment and like to hear different idea any thought
codd dream tribute to the father of relational databases and his quest to bring access to information to the casual user,codd dream tribute to the father of relational databases and his quest to bring access to information to the casual user,0,codd dream tribute to the father of relational database and his quest to bring access to information to the casual user,codd dream tribute to the father of relational database and his quest to bring access to information to the casual user
how verizon uses ai for enhanced business decisions with anil kumar executive director head of ai industrialization at verizon thursday december at am et,hi dataengineering wanted to share this webinar with you how verizon uses ai for enhanced business decisions with anil kumar executive director head of ai industrialization at verizon on thursday december at am et really interested in learning how company as enormous as verizon is using ai ml at scale what about you below is the information from the website featured speaker anil kumar executive director head of ai industrialization at verizon as companies are looking to leverage cognitive technology and deploying machine learning models organizations need to make sure they have the correct people processes and technology in place to succeed in this presentation anil will share what ai industrialization is and how verizon is moving from pockets of ai ml to ai ml being implemented across the organization he will also share some of the unique opportunities and challenges that adopting ai across an organization as large as verizon presents agenda pm featured presentation pm your amp and interaction link to website,0,how verizon us ai for enhanced business decision with anil kumar executive director head of ai industrialization at verizon thursday december at am et,hi dataengineering wanted to share this webinar with you how verizon us ai for enhanced business decision with anil kumar executive director head of ai industrialization at verizon on thursday december at am et really interested in learning how company a enormous a verizon is using ai ml at scale what about you below is the information from the website featured speaker anil kumar executive director head of ai industrialization at verizon a company are looking to leverage cognitive technology and deploying machine learning model organization need to make sure they have the correct people process and technology in place to succeed in this presentation anil will share what ai industrialization is and how verizon is moving from pocket of ai ml to ai ml being implemented across the organization he will also share some of the unique opportunity and challenge that adopting ai across an organization a large a verizon present agenda pm featured presentation pm your amp and interaction link to website
how do you deploy database objects in ci cd pipeline,come from mssql background so familiar with the concept of building dacpac file from source code and deploying that to db server via automation software microsoft and visual studio makes this rather easy but how do you do this with another rdbms like postgres or even snowflake if you have bunch of sql files that define your table structures and you make change to one of the tables how do you auto deploy that without getting the dba involved or manually making change scripts is there piece of software that makes this process easy,0,how do you deploy database object in ci cd pipeline,come from mssql background so familiar with the concept of building dacpac file from source code and deploying that to db server via automation software microsoft and visual studio make this rather easy but how do you do this with another rdbms like postgres or even snowflake if you have bunch of sql file that define your table structure and you make change to one of the table how do you auto deploy that without getting the dba involved or manually making change script is there piece of software that make this process easy
should take this opportunity,hi guys currently working as an entry level data analyst in london background currently working for consulting company that just had weeks of training on excel sql power bi tableau python and each software covered in week yes it was super intense as so much information was given hours day was called by my manager that said she thinks be capable of joining the data engineering pathway where we ll learn scrum hadoop spark and scala over the next weeks once again at hours day my question is that is this good opportunity to go into becoming data engineer that too when haven even mastered the softwares ve used as data analyst how difficult are the softwares ve mentioned to learn compared to say python other was ok with python but was extremely difficult for me even in python concept such as loop is still difficult for me my strengths were in python pandas which consisted of eda after all these weeks of training feel quite burnt out but at the same time don want to miss this data engineering opportunity if it is good in the long run im years old with kid and mortgage don know if this is relevant the training will be paid so ll be able to support my family the only thing is lack of time with them after the weeks of initial training we have project with company called quantexa that are innovating something called contextualised design intelligence and apparently that something that will be huge in the future this project will take weeks of self learning then we ll be placed in deloitte to used what we learnt with quantexa to identify fraud in the finance department any insights would be greatly appreciated because just don know what to do at that age where don want to study anymore but also don want to miss an incredible opportunity will it be difficult and should take it thank you,0,should take this opportunity,hi guy currently working a an entry level data analyst in london background currently working for consulting company that just had week of training on excel sql power bi tableau python and each software covered in week yes it wa super intense a so much information wa given hour day wa called by my manager that said she think be capable of joining the data engineering pathway where we ll learn scrum hadoop spark and scala over the next week once again at hour day my question is that is this good opportunity to go into becoming data engineer that too when haven even mastered the software ve used a data analyst how difficult are the software ve mentioned to learn compared to say python other wa ok with python but wa extremely difficult for me even in python concept such a loop is still difficult for me my strength were in python panda which consisted of eda after all these week of training feel quite burnt out but at the same time don want to miss this data engineering opportunity if it is good in the long run im year old with kid and mortgage don know if this is relevant the training will be paid so ll be able to support my family the only thing is lack of time with them after the week of initial training we have project with company called quantexa that are innovating something called contextualised design intelligence and apparently that something that will be huge in the future this project will take week of self learning then we ll be placed in deloitte to used what we learnt with quantexa to identify fraud in the finance department any insight would be greatly appreciated because just don know what to do at that age where don want to study anymore but also don want to miss an incredible opportunity will it be difficult and should take it thank you
azure databricks best practices and template for incremental handling using delta lakes,hello guys at my company we are building data lake in azure and is going to create snapshots from incremental data coming from various source by leveraging data bricks delta lakes upsert ops can you guys please share your experiences dos and don ts or best practices when it comes to doing this on azure please share any links if you have seen template for doing this some where,0,azure databricks best practice and template for incremental handling using delta lake,hello guy at my company we are building data lake in azure and is going to create snapshot from incremental data coming from various source by leveraging data brick delta lake upsert ops can you guy please share your experience do and don t or best practice when it come to doing this on azure please share any link if you have seen template for doing this some where
how to find contract freelance gigs in data engineering,bit unsure where to find data engineering projects contract freelance is there any website that lists agencies don suggest freelancing websites thanks,0,how to find contract freelance gig in data engineering,bit unsure where to find data engineering project contract freelance is there any website that list agency don suggest freelancing website thanks
azure arc pseudo serverless by kubernetes,currently developing an evolution architecture for my data platform and have landed on azure arc to power it this tool seems extremely powerful in security conscious enterprise wide environment it lets me bring all my kubernetes deployments under one roof regardless of the platform they are on which is nice but more importantly and currently preview feature can also run my azure function apps and azure logic apps directly on aks via arc for reasons won go into locked within one vnet in single region suddenly can get all the power of serverless with all the cost savings of serverless for spiking workload with the scalability of azure kubernetes service for scaling baseline workloads currently estimating can bring down the overall cost of our platform by literally unable to directly deploy serverless normally because it inherantly is multi region but this seems like better alternative has anyone got any experience with arc keen to hear if there are any downsides besides the usual it in preview,0,azure arc pseudo serverless by kubernetes,currently developing an evolution architecture for my data platform and have landed on azure arc to power it this tool seems extremely powerful in security conscious enterprise wide environment it let me bring all my kubernetes deployment under one roof regardless of the platform they are on which is nice but more importantly and currently preview feature can also run my azure function apps and azure logic apps directly on ak via arc for reason won go into locked within one vnet in single region suddenly can get all the power of serverless with all the cost saving of serverless for spiking workload with the scalability of azure kubernetes service for scaling baseline workload currently estimating can bring down the overall cost of our platform by literally unable to directly deploy serverless normally because it inherantly is multi region but this seems like better alternative ha anyone got any experience with arc keen to hear if there are any downside besides the usual it in preview
best way to store aggregations in dynamodb,hi all have stumbled upon an use case that processes raw daily data into weekly data and needs to be stored in dynamodb part of raw data looks like this w01 service1 rainy w01 service1 snowy w01 service1 rainy w01 service1 sunny w01 service1 rainy w01 service1 sunny w01 service1 sunny need to aggregate each week into one row and store it the final data structure would look like this week and wether service are keys week weather_service country region weather occurences w01 service1 uk north rainy sunny snowy w01 service2 uk south sunny rainy snowy raw data will be pulled using glue python and then once processes it will be pulled from dynamodb using python for further processing is there better way to store weather and occurences than this assuming the rest of the data structure has to remain as is,0,best way to store aggregation in dynamodb,hi all have stumbled upon an use case that process raw daily data into weekly data and need to be stored in dynamodb part of raw data look like this w01 service1 rainy w01 service1 snowy w01 service1 rainy w01 service1 sunny w01 service1 rainy w01 service1 sunny w01 service1 sunny need to aggregate each week into one row and store it the final data structure would look like this week and wether service are key week weather_service country region weather occurences w01 service1 uk north rainy sunny snowy w01 service2 uk south sunny rainy snowy raw data will be pulled using glue python and then once process it will be pulled from dynamodb using python for further processing is there better way to store weather and occurences than this assuming the rest of the data structure ha to remain a is
interview for data engineer intern role how can prepare,officially the role is software developer one but it applied to data feel like it close fit to data engineering sophomore in university what can learn before the interview to maximize my chance of landing the role only have slight experience with sql querying and processing data with python should practice sql,0,interview for data engineer intern role how can prepare,officially the role is software developer one but it applied to data feel like it close fit to data engineering sophomore in university what can learn before the interview to maximize my chance of landing the role only have slight experience with sql querying and processing data with python should practice sql
schemaverse space based strategy game to learn postgresql,schemaverse space based strategy game to learn postgresql,0,schemaverse space based strategy game to learn postgresql,schemaverse space based strategy game to learn postgresql
interview questions,hi all have an interview coming up for an entry level think it entry level they never specified data engineer position am brushing up on some conceptual stuff mostly hadoop and nosql this is my first interview for data engineer position and was wondering if should also brush up on some leetcode or if it will likely be conceptual please let me know what you think,0,interview question,hi all have an interview coming up for an entry level think it entry level they never specified data engineer position am brushing up on some conceptual stuff mostly hadoop and nosql this is my first interview for data engineer position and wa wondering if should also brush up on some leetcode or if it will likely be conceptual please let me know what you think
do you guys want quick stats primer here simple guide with the,if you all need some simple stats here are the go to hope it helps this was presented to graduate students in the faculty of forestry at the university of toronto first general introduction to probability assumptions of inference and hypothesis testing the second requirement is to perform the analyses including managing the data using some statistical software at the vanguard of such software is the statistical software redir_token quffluhqbwvhovnhcg95v1zfcwf5sddsdxvfufgwvk9yz3xbq3jtc0ttyuzyvmqzq1e4mkrantfqn2njnxe4txnqmw5uz1buzghmzuk3sjuwq3rft24tlw5izkdpczj6t0t5dwdlu1faofjdvngxvulrywrnovrezvhxahnqaxk0vwn1dhvxwtrrwfbxs1zky0rvntrytjrlwq amp https fwww project org and its integrative environment rstudio redir_token quffluhqazb0njqttlzvrlp4cdzudwm0dzjfotqyaeviqxxbq3jtc0tsy201bg5pbgltswhwovl2yulvqlnqbk9delg2bwvhrlluognqnufqulhqeevbuetwchfnmdrjr2zszgv4yzvac0pzv050tzvgbelpden3mthytgflodfirljmowdeskc5r0nodw53sgzumdatrhbwaw amp https fwww rstudio com few programs constitute the virtues of open sources highly integrated and powerful programs as the statistical software taught to the modern student in most institutions the following video comprises the time stamped functions and analyses intro to data management probability and data presentation min intro to producing graphs with studio min subsetting the data attach function in min summaryse min using the plot function min package sciplot for bar graphs bargraph ci function min analysis of variance including tukey hsd anova min creating line plots and scatter plot min regression analyses via lm function linear mins correlation analysis mins the video finishes with summation of what we ve learned if you like this material then please like and subscribe to this channel,0,do you guy want quick stats primer here simple guide with the,if you all need some simple stats here are the go to hope it help this wa presented to graduate student in the faculty of forestry at the university of toronto first general introduction to probability assumption of inference and hypothesis testing the second requirement is to perform the analysis including managing the data using some statistical software at the vanguard of such software is the statistical software redir_token quffluhqbwvhovnhcg95v1zfcwf5sddsdxvfufgwvk9yz3xbq3jtc0ttyuzyvmqzq1e4mkrantfqn2njnxe4txnqmw5uz1buzghmzuk3sjuwq3rft24tlw5izkdpczj6t0t5dwdlu1faofjdvngxvulrywrnovrezvhxahnqaxk0vwn1dhvxwtrrwfbxs1zky0rvntrytjrlwq amp http fwww project org and it integrative environment rstudio redir_token quffluhqazb0njqttlzvrlp4cdzudwm0dzjfotqyaeviqxxbq3jtc0tsy201bg5pbgltswhwovl2yulvqlnqbk9delg2bwvhrlluognqnufqulhqeevbuetwchfnmdrjr2zszgv4yzvac0pzv050tzvgbelpden3mthytgflodfirljmowdeskc5r0nodw53sgzumdatrhbwaw amp http fwww rstudio com few program constitute the virtue of open source highly integrated and powerful program a the statistical software taught to the modern student in most institution the following video comprises the time stamped function and analysis intro to data management probability and data presentation min intro to producing graph with studio min subsetting the data attach function in min summaryse min using the plot function min package sciplot for bar graph bargraph ci function min analysis of variance including tukey hsd anova min creating line plot and scatter plot min regression analysis via lm function linear min correlation analysis min the video finish with summation of what we ve learned if you like this material then please like and subscribe to this channel
ve always been curious about this one thing regarding database normalization,sorry for the click bait title given columnar storage system like an olap database like snowflake or even columnar storage format like parquet if you had column that had time as text string would it improve database normalization to split that into one field with one with and the last with my thought is that the number of unique values you need to store in the original case is assuming hr format and assuming hr format for the latter case even though you would have more column objects the number of unique values you have to carry is assuming hr format and assuming hr format wouldn that compress better and not degrade the information persistence this is assuming dataset that is written to significantly more than its read so joining the fields back when queried isn that big of deal if so would it be possible to write program that just does this basic level of scanning through database and finds ways to optimize its storage,0,ve always been curious about this one thing regarding database normalization,sorry for the click bait title given columnar storage system like an olap database like snowflake or even columnar storage format like parquet if you had column that had time a text string would it improve database normalization to split that into one field with one with and the last with my thought is that the number of unique value you need to store in the original case is assuming hr format and assuming hr format for the latter case even though you would have more column object the number of unique value you have to carry is assuming hr format and assuming hr format wouldn that compress better and not degrade the information persistence this is assuming dataset that is written to significantly more than it read so joining the field back when queried isn that big of deal if so would it be possible to write program that just doe this basic level of scanning through database and find way to optimize it storage
current data analyst looking to transition into data engineer,hey guys currently data analyst at small company in the southeast region most of my work is done in excel or writing basic select statements to pull data sometimes will dig through existing sql queries and edit the queries to get the data need that being said am looking for higher salary and more interesting work would like to transition to data engineering but don really know where to start have seen some roadmaps but don know how could really fit those in with my current state of knowledge know basic sql sql in sql server and know very basic python do you guys recommend taking udacity course on data engineering or something similar like having set curriculum or just nice and in depth path that will allow me to have things to check off in list know data engineering is pretty broad and there is no set way of learning the fundamentals but sure there is something out there that would fit in with me right,0,current data analyst looking to transition into data engineer,hey guy currently data analyst at small company in the southeast region most of my work is done in excel or writing basic select statement to pull data sometimes will dig through existing sql query and edit the query to get the data need that being said am looking for higher salary and more interesting work would like to transition to data engineering but don really know where to start have seen some roadmaps but don know how could really fit those in with my current state of knowledge know basic sql sql in sql server and know very basic python do you guy recommend taking udacity course on data engineering or something similar like having set curriculum or just nice and in depth path that will allow me to have thing to check off in list know data engineering is pretty broad and there is no set way of learning the fundamental but sure there is something out there that would fit in with me right
junior data engineer salary,recently received junior de offer for for remote work am graduating this year and this would be my first full time job is this fair salary what would be good counter offer to make,0,junior data engineer salary,recently received junior de offer for for remote work am graduating this year and this would be my first full time job is this fair salary what would be good counter offer to make
interview coming up,hey all have an internal interview coming up for data engineer job currently data analyst in the same division etc we re pretty immature data wise have external consultants doing our data warehousing amp modelling etc whilst our own bi devs engineers are learning from them these guys were just the traditional sql gurus who ve been put into the team don use python or apis or anything like that just wondering what tips would you guys have for the interview some things concepts that might help me stand out appreciate any help at all,0,interview coming up,hey all have an internal interview coming up for data engineer job currently data analyst in the same division etc we re pretty immature data wise have external consultant doing our data warehousing amp modelling etc whilst our own bi devs engineer are learning from them these guy were just the traditional sql guru who ve been put into the team don use python or apis or anything like that just wondering what tip would you guy have for the interview some thing concept that might help me stand out appreciate any help at all
how prevalent is oracle in the data engineering space,have lot of experience with on premise oracle databases in mainly legacy enterprise environment and am wondering how much oracle you guys are exposed to in your daily work or has everyone moved to something like snowflake or postgres,0,how prevalent is oracle in the data engineering space,have lot of experience with on premise oracle database in mainly legacy enterprise environment and am wondering how much oracle you guy are exposed to in your daily work or ha everyone moved to something like snowflake or postgres
is macbook air m1 gb enough for data engineering,currently am computer technologies student and want to develop career in data engineering started my journey with lenovo and this week have encountered blue screen errors and am scared of getting things worse wanted to upgrade my laptop to mac but the currency difference between tl and usd hit me hard thus am looking for entry level macbook air for myself but am curious whether gb ram is enough or not should buy gb maybe can try learning photoshop movie and premiere for the freelance job but this is just an idea and this must last longer because it is so hard to buy new computer in my country so want to use the laptop that want to buy thanks for your time have nice day,0,is macbook air m1 gb enough for data engineering,currently am computer technology student and want to develop career in data engineering started my journey with lenovo and this week have encountered blue screen error and am scared of getting thing worse wanted to upgrade my laptop to mac but the currency difference between tl and usd hit me hard thus am looking for entry level macbook air for myself but am curious whether gb ram is enough or not should buy gb maybe can try learning photoshop movie and premiere for the freelance job but this is just an idea and this must last longer because it is so hard to buy new computer in my country so want to use the laptop that want to buy thanks for your time have nice day
how does normalization increase performance,reading up on normalization vs denormalization it keeps coming up that normalizing your data reduces redundancy and increases performance the reduces redundancy understand but the example in my mind is web log and don see how normalizing this data would increase performance you may get location ip address date and time page requested os and browser information from weblog if you normalize this data you might make date table time table browser table and an os table if you made these separate tables and made them foreign keys in the fact table web log table wouldn that significantly slow down the performance of writing the data you would need to parse the log lookup the value for the date time os and browser create them if they don exist and then construct row for the fact table with the updated row containing the foreign keys could someone provide concrete example of when how normalization speeds up performance,0,how doe normalization increase performance,reading up on normalization v denormalization it keep coming up that normalizing your data reduces redundancy and increase performance the reduces redundancy understand but the example in my mind is web log and don see how normalizing this data would increase performance you may get location ip address date and time page requested o and browser information from weblog if you normalize this data you might make date table time table browser table and an o table if you made these separate table and made them foreign key in the fact table web log table wouldn that significantly slow down the performance of writing the data you would need to parse the log lookup the value for the date time o and browser create them if they don exist and then construct row for the fact table with the updated row containing the foreign key could someone provide concrete example of when how normalization speed up performance
combining data from multiple data sources,how often do you need to combine merge heterogenous data fetched from rest api or loaded from files with the data retrieved from databases what do you use for it what are the challenges view poll,0,combining data from multiple data source,how often do you need to combine merge heterogenous data fetched from rest api or loaded from file with the data retrieved from database what do you use for it what are the challenge view poll
am looking for roadmap on getting into data engineering can hope to follow the popular roadmap shared on this sub,know this roadmap utm_source share it is just massive and can hope to do all of that in years have full time job too it looks like multi year experience check list can anyone share shorter curated roadmap for getting into the field,0,am looking for roadmap on getting into data engineering can hope to follow the popular roadmap shared on this sub,know this roadmap utm_source share it is just massive and can hope to do all of that in year have full time job too it look like multi year experience check list can anyone share shorter curated roadmap for getting into the field
smart solution to reduce an impractical number of iterations in time series model,hello folks know this might not be the most appropriate sub for this discussion but no harm in trying right we have timeseries based ml solution that helps with anomaly detection and finding the root cause for this anomaly how it currently operates is if see an anomaly in metric if the actual is far from prediction we try to examine the associated dimensions and associate the anomaly to its root cause accordingly for example considering covid related deaths across the us if our model senses an anomaly steep rise fall we try to break down the available data by state and see which state is responsible for such steep change in this example there is only one dimension which is the state so our model in essence iterates over states hence iterations when we try to scale this up to multivariate that involves for example dimensions to the already existing state say we add gender age group income group health rating the number of iterations is going to explode what was earlier is now going to become states genders income groups health ratings iterations when such model has to detect anomalies in say commerce data where the number of dimensions is going to be larger hope you see how can our model scale at least without parallelization how do you guys think can we solve this any suggestion is greatly appreciated thanks,0,smart solution to reduce an impractical number of iteration in time series model,hello folk know this might not be the most appropriate sub for this discussion but no harm in trying right we have timeseries based ml solution that help with anomaly detection and finding the root cause for this anomaly how it currently operates is if see an anomaly in metric if the actual is far from prediction we try to examine the associated dimension and associate the anomaly to it root cause accordingly for example considering covid related death across the u if our model sens an anomaly steep rise fall we try to break down the available data by state and see which state is responsible for such steep change in this example there is only one dimension which is the state so our model in essence iterates over state hence iteration when we try to scale this up to multivariate that involves for example dimension to the already existing state say we add gender age group income group health rating the number of iteration is going to explode what wa earlier is now going to become state gender income group health rating iteration when such model ha to detect anomaly in say commerce data where the number of dimension is going to be larger hope you see how can our model scale at least without parallelization how do you guy think can we solve this any suggestion is greatly appreciated thanks
data engineering weekly year in review of,data engineering weekly year in review of,0,data engineering weekly year in review of,data engineering weekly year in review of
how does data looks like when stored in key value and graph nosql databases,given the table bonuses id last first bonus doe john smith jane beck sam row oriented database postgresql doe john smith jane beck sam column oriented database mariadb doe smith beck john jane sam column family database cassandra bonuses row1 id last doe first john bonus row2 id last smith first jane bonus row3 id last beck first sam bonus document based database mongodb document1 id last doe first john bonus document2 id last smith first jane bonus document3 id last beck first sam bonus graph database neo4j key value database redis,0,how doe data look like when stored in key value and graph nosql database,given the table bonus id last first bonus doe john smith jane beck sam row oriented database postgresql doe john smith jane beck sam column oriented database mariadb doe smith beck john jane sam column family database cassandra bonus row1 id last doe first john bonus row2 id last smith first jane bonus row3 id last beck first sam bonus document based database mongodb document1 id last doe first john bonus document2 id last smith first jane bonus document3 id last beck first sam bonus graph database neo4j key value database redis
anyone have sqlpad membership,hey greetings everyone any one of you have sqlpad membership so that we could share,0,anyone have sqlpad membership,hey greeting everyone any one of you have sqlpad membership so that we could share
data engineering interview prep,am planning to take interview to switch to better company and wanted to clarify one thing does data structures and algorithms have more weightage in data engineering interview similar to sde role or is it more focused in sql and good programming skills so that can focus more on sql and data warehousing rather than dsa,0,data engineering interview prep,am planning to take interview to switch to better company and wanted to clarify one thing doe data structure and algorithm have more weightage in data engineering interview similar to sde role or is it more focused in sql and good programming skill so that can focus more on sql and data warehousing rather than dsa
any data engineers who work in florida,wanted to know your view on the market in florida if you like what you do and what your pay if you don mind sharing,0,any data engineer who work in florida,wanted to know your view on the market in florida if you like what you do and what your pay if you don mind sharing
snowflake stored procedure vs tasks,wondering what the best practice is for scheduling operations in snowflake should create single stored procedure that performs multiple operations and schedule it with task or should break it up into smaller pieces and chain them together with multiple tasks don have dbt or airflow so need to find snowflake native solution,0,snowflake stored procedure v task,wondering what the best practice is for scheduling operation in snowflake should create single stored procedure that performs multiple operation and schedule it with task or should break it up into smaller piece and chain them together with multiple task don have dbt or airflow so need to find snowflake native solution
tips az and dp,hi everyone planning on doing these certifications have few questions and am looking for advise will studying the ms documentation be enough will these help me increase my chances of getting job as bi developer data analyst data engineer thanks looking for any tips,0,tip az and dp,hi everyone planning on doing these certification have few question and am looking for advise will studying the m documentation be enough will these help me increase my chance of getting job a bi developer data analyst data engineer thanks looking for any tip
post dp203 azure project,what would be good project to engage with post dp203 like to sink my teeth into something that could take few days or even up to week any thoughts here would be fantastic or specific projects courses could buy that would take me through something significant would be fab,0,post dp203 azure project,what would be good project to engage with post dp203 like to sink my teeth into something that could take few day or even up to week any thought here would be fantastic or specific project course could buy that would take me through something significant would be fab
best data engineer courses,hi everyone so am recent master graduate in business analytics where of my academic projects heavily relied on python ml and the other relied on sql over time fell in love with the coding aspect of data science after months of struggling to find data analyst job got myself position at start up firm however the reality hit me while the job description mentioned lots of sql and python still have to do lots of excel tasks and have limited chances to work with python sql therefore recently have been looking into data engineering roles as well as their requirements unfortunately became overwhelmed by the number of tools and concepts that need to learn its not that feel lazy or anything its just that do not know where to start the correct learning path therefore strongly believe that having good courses can get me off to good start have been researching and found some courses such as data engineer nanodegree or preparing for google cloud certification cloud data engineer professional certificate if you have done these courses would you still recommend me do these courses in what about other online courses that you have found helpful for beginners thank you for reading my long ass essay,0,best data engineer course,hi everyone so am recent master graduate in business analytics where of my academic project heavily relied on python ml and the other relied on sql over time fell in love with the coding aspect of data science after month of struggling to find data analyst job got myself position at start up firm however the reality hit me while the job description mentioned lot of sql and python still have to do lot of excel task and have limited chance to work with python sql therefore recently have been looking into data engineering role a well a their requirement unfortunately became overwhelmed by the number of tool and concept that need to learn it not that feel lazy or anything it just that do not know where to start the correct learning path therefore strongly believe that having good course can get me off to good start have been researching and found some course such a data engineer nanodegree or preparing for google cloud certification cloud data engineer professional certificate if you have done these course would you still recommend me do these course in what about other online course that you have found helpful for beginner thank you for reading my long as essay
skills to learn before my data engineering graduate scheme starts in months,hi have just been made an offer for data engineering graduate scheme in the uk and was wondering what some things should focus on get to grips with to get bit of head start cheers,0,skill to learn before my data engineering graduate scheme start in month,hi have just been made an offer for data engineering graduate scheme in the uk and wa wondering what some thing should focus on get to grip with to get bit of head start cheer
rule based categorisation on free text fields in spark,working on scebario where need to categorise free text fields into categories based on the presence of certain key words for each category the key words will belong to za so if free text contains key word from group then label and so on the da in the draft implememtation requirement has implemented this with nested case statement with more nested like string matches apart from being inefficient from performance perspective this also makes the design very inflexible if in future morr key words are added my approach considering spark as the processing engine is first transform the free text field to individual words using regextokenizer and strip away numeric characters and append new column to the dataframe1 further for the category groups create dataframe2 with the key words stored in an array type that will be broadcast joined with the dataframe1 and then use an array_intersect between the two columns to find match each category will correspond to another case statement in the spark sql but the list of category key words will be abstracted to separate reference file this will allow easy modification of the keywords in the refernce files any suggestions or alternate approached for effectively performing pattern matching using spark pyspark preferable,0,rule based categorisation on free text field in spark,working on scebario where need to categorise free text field into category based on the presence of certain key word for each category the key word will belong to za so if free text contains key word from group then label and so on the da in the draft implememtation requirement ha implemented this with nested case statement with more nested like string match apart from being inefficient from performance perspective this also make the design very inflexible if in future morr key word are added my approach considering spark a the processing engine is first transform the free text field to individual word using regextokenizer and strip away numeric character and append new column to the dataframe1 further for the category group create dataframe2 with the key word stored in an array type that will be broadcast joined with the dataframe1 and then use an array_intersect between the two column to find match each category will correspond to another case statement in the spark sql but the list of category key word will be abstracted to separate reference file this will allow easy modification of the keywords in the refernce file any suggestion or alternate approached for effectively performing pattern matching using spark pyspark preferable
clarification on uber data infrastructure description,was reading this article from uber and don understand one part while moving etl and modeling into hadoop made this process more scalable these steps were still bottlenecks since these etl jobs had to recreate the entire modeled table in every run adding to the problem both ingestion of the new data and modeling of the related derived table were based on creating new snapshots of the entire dataset and swapping the old and new tables to provide users with access to fresh data the part don understand is that why did they need to recreate the entire modeled table with every run they have schema why not just append to existing table nd question is related why do they need to swap the old and new table instead of using partitions with respect to time where time will indicate the table that data was loaded into db,0,clarification on uber data infrastructure description,wa reading this article from uber and don understand one part while moving etl and modeling into hadoop made this process more scalable these step were still bottleneck since these etl job had to recreate the entire modeled table in every run adding to the problem both ingestion of the new data and modeling of the related derived table were based on creating new snapshot of the entire dataset and swapping the old and new table to provide user with access to fresh data the part don understand is that why did they need to recreate the entire modeled table with every run they have schema why not just append to existing table nd question is related why do they need to swap the old and new table instead of using partition with respect to time where time will indicate the table that data wa loaded into db
looking for experienced data engineers for minute product interview on de best practices in amazon gift cards,,0,looking for experienced data engineer for minute product interview on de best practice in amazon gift card,
get access to courses on coursera for limited time offer,get access to courses on coursera for limited time offer,0,get access to course on coursera for limited time offer,get access to course on coursera for limited time offer
data engineering course recomendation,have the basics of sql and have worked bit in python would like to learn more into the data engineering am looking for comprehensive course that would teach me the basics as well as more advanced issues as there are black friday promotions now what course do you currently advise,0,data engineering course recomendation,have the basic of sql and have worked bit in python would like to learn more into the data engineering am looking for comprehensive course that would teach me the basic a well a more advanced issue a there are black friday promotion now what course do you currently advise
why doesn nosql and data warehousing doesn go hand in hand,for oltps have few criterias in determining whether to use sql or nosql for structured data acid compliant small to medium data use sql for unstructured semi structured data big data use nosql but for data warehousing don understand why almost all companies are using sql what could be the reason behind it,0,why doesn nosql and data warehousing doesn go hand in hand,for oltps have few criterias in determining whether to use sql or nosql for structured data acid compliant small to medium data use sql for unstructured semi structured data big data use nosql but for data warehousing don understand why almost all company are using sql what could be the reason behind it
serverless dbt on google cloud platform,serverless dbt on google cloud platform,0,serverless dbt on google cloud platform,serverless dbt on google cloud platform
reasonable experience requirement,reasonable experience requirement,0,reasonable experience requirement,reasonable experience requirement
scaling airflow with celery cluster using docker swarm,as the title says want to setup airflow that would run on cluster master nodes using docker swarm current setup right now have airflow setup that uses the celeryexecutor that is running on single ec2 have dockerfile that pulls airflow image and pip install requirements txt from this dockerfile creating local image and this image is used in the docker compose yml that spins up the different services airflow need webserver scheduler redis flower and some worker metadb is postgres that is on separate rds the docker compose is used in docker swarm mode ie docker stack deploy airflow_stack required setup want to scale the current setup to ec2s master nodes that the master would run the webserver schedule redis and flower and the workers would run in the nodes after searching and web and docs there are few things that are still not clear to me that would love to know amp x200b from what understand in order for the nodes to run the workers the local image that building from the dockerfile need to be pushed to some repository if it really needed would use aws ecr for the airflow workers to be able to create the containers from that image is that correct syncing volumes and env files right now mounting the volumes and insert the envs in the docker compose file would these mounts and envs be synced to the nodes and airflow workers containers if not how can make sure that everything is sync as airflow requires that all the components apart from redis would have all the dependencies etc one of the envs that needs to be set when using celeryexecuter is the broker url how can make sure that the nodes recognize the redis broker that is on the master sure that there are few more things that forget but what wrote is good start any help or recommendation would be greatly appreciated thanks dockerfile from apache airflow python3 user root run apt update run apt install build essential user airflow copy requirements txt requirements txt copy requirements airflow txt requirements airflow txt run pip install upgrade pip run pip install upgrade wheel run pip install requirements airflow txt run pip install requirements txt expose docker compose yml version airflow celery amp airflow celery image local_image latest volumes some_volume env_file some_env_file services webserver lt lt airflow celery command airflow webserver restart always ports healthcheck test cmd shell opt airflow airflow webserver pid interval timeout retries scheduler lt lt airflow celery command airflow scheduler restart always deploy replicas redis image redis command redis server include redis conf healthcheck test cmd redis cli ping interval timeout retries ports environment redis_port worker lt lt airflow celery command airflow celery worker deploy replicas flower lt lt airflow celery command airflow celery flower ports,0,scaling airflow with celery cluster using docker swarm,a the title say want to setup airflow that would run on cluster master node using docker swarm current setup right now have airflow setup that us the celeryexecutor that is running on single ec2 have dockerfile that pull airflow image and pip install requirement txt from this dockerfile creating local image and this image is used in the docker compose yml that spin up the different service airflow need webserver scheduler redis flower and some worker metadb is postgres that is on separate rds the docker compose is used in docker swarm mode ie docker stack deploy airflow_stack required setup want to scale the current setup to ec2s master node that the master would run the webserver schedule redis and flower and the worker would run in the node after searching and web and doc there are few thing that are still not clear to me that would love to know amp x200b from what understand in order for the node to run the worker the local image that building from the dockerfile need to be pushed to some repository if it really needed would use aws ecr for the airflow worker to be able to create the container from that image is that correct syncing volume and env file right now mounting the volume and insert the envs in the docker compose file would these mount and envs be synced to the node and airflow worker container if not how can make sure that everything is sync a airflow requires that all the component apart from redis would have all the dependency etc one of the envs that need to be set when using celeryexecuter is the broker url how can make sure that the node recognize the redis broker that is on the master sure that there are few more thing that forget but what wrote is good start any help or recommendation would be greatly appreciated thanks dockerfile from apache airflow python3 user root run apt update run apt install build essential user airflow copy requirement txt requirement txt copy requirement airflow txt requirement airflow txt run pip install upgrade pip run pip install upgrade wheel run pip install requirement airflow txt run pip install requirement txt expose docker compose yml version airflow celery amp airflow celery image local_image latest volume some_volume env_file some_env_file service webserver lt lt airflow celery command airflow webserver restart always port healthcheck test cmd shell opt airflow airflow webserver pid interval timeout retries scheduler lt lt airflow celery command airflow scheduler restart always deploy replica redis image redis command redis server include redis conf healthcheck test cmd redis cli ping interval timeout retries port environment redis_port worker lt lt airflow celery command airflow celery worker deploy replica flower lt lt airflow celery command airflow celery flower port
scaling airflow with celery cluster using docker swarm,as the title says want to setup airflow that would run on cluster master nodes using docker swarm current setup right now have airflow setup that uses the celeryexecutor that is running on single ec2 have dockerfile that pulls airflow image and pip install requirements txt from this dockerfile creating local image and this image is used in the docker compose yml that spins up the different services airflow need webserver scheduler redis flower and some worker metadb is postgres that is on separate rds the docker compose is used in docker swarm mode ie docker stack deploy airflow_stack required setup want to scale the current setup to ec2s master nodes that the master would run the webserver schedule redis and flower and the workers would run in the nodes after searching and web and docs there are few things that are still not clear to me that would love to know from what understand in order for the nodes to run the workers the local image that building from the dockerfile need to be pushed to some repository if it really needed would use aws ecr for the airflow workers to be able to create the containers from that image is that correct syncing volumes and env files right now mounting the volumes and insert the envs in the docker compose file would these mounts and envs be synced to the nodes and airflow workers containers if not how can make sure that everything is sync as airflow requires that all the components apart from redis would have all the dependencies etc one of the envs that needs to be set when using celeryexecuter is the broker_url how can make sure that the nodes recognize the redis broker that is on the master sure that there are few more things that forget but what wrote is good start any help or recommendation would be greatly appreciated thanks dockerfile from apache airflow python3 user root run apt update run apt install build essential user airflow copy requirements txt requirements txt copy requirements airflow txt requirements airflow txt run pip install upgrade pip run pip install upgrade wheel run pip install requirements airflow txt run pip install requirements txt expose docker compose yml version airflow celery amp airflow celery image local_image latest volumes some_volume env_file some_env_file services webserver lt lt airflow celery command airflow webserver restart always ports healthcheck test cmd shell opt airflow airflow webserver pid interval timeout retries scheduler lt lt airflow celery command airflow scheduler restart always deploy replicas redis image redis command redis server include redis conf healthcheck test cmd redis cli ping interval timeout retries ports environment redis_port worker lt lt airflow celery command airflow celery worker deploy replicas flower lt lt airflow celery command airflow celery flower ports,0,scaling airflow with celery cluster using docker swarm,a the title say want to setup airflow that would run on cluster master node using docker swarm current setup right now have airflow setup that us the celeryexecutor that is running on single ec2 have dockerfile that pull airflow image and pip install requirement txt from this dockerfile creating local image and this image is used in the docker compose yml that spin up the different service airflow need webserver scheduler redis flower and some worker metadb is postgres that is on separate rds the docker compose is used in docker swarm mode ie docker stack deploy airflow_stack required setup want to scale the current setup to ec2s master node that the master would run the webserver schedule redis and flower and the worker would run in the node after searching and web and doc there are few thing that are still not clear to me that would love to know from what understand in order for the node to run the worker the local image that building from the dockerfile need to be pushed to some repository if it really needed would use aws ecr for the airflow worker to be able to create the container from that image is that correct syncing volume and env file right now mounting the volume and insert the envs in the docker compose file would these mount and envs be synced to the node and airflow worker container if not how can make sure that everything is sync a airflow requires that all the component apart from redis would have all the dependency etc one of the envs that need to be set when using celeryexecuter is the broker_url how can make sure that the node recognize the redis broker that is on the master sure that there are few more thing that forget but what wrote is good start any help or recommendation would be greatly appreciated thanks dockerfile from apache airflow python3 user root run apt update run apt install build essential user airflow copy requirement txt requirement txt copy requirement airflow txt requirement airflow txt run pip install upgrade pip run pip install upgrade wheel run pip install requirement airflow txt run pip install requirement txt expose docker compose yml version airflow celery amp airflow celery image local_image latest volume some_volume env_file some_env_file service webserver lt lt airflow celery command airflow webserver restart always port healthcheck test cmd shell opt airflow airflow webserver pid interval timeout retries scheduler lt lt airflow celery command airflow scheduler restart always deploy replica redis image redis command redis server include redis conf healthcheck test cmd redis cli ping interval timeout retries port environment redis_port worker lt lt airflow celery command airflow celery worker deploy replica flower lt lt airflow celery command airflow celery flower port
beginner question de with focus on hadoop,hey don know much about de yet and pretty much stumbled into this path have tons of questions but would start with the imo most important one does starting de career focused on linux and hadoop ansible sql provide reasonable foundation thank you so much,0,beginner question de with focus on hadoop,hey don know much about de yet and pretty much stumbled into this path have ton of question but would start with the imo most important one doe starting de career focused on linux and hadoop ansible sql provide reasonable foundation thank you so much
ex mckinsey manager explains the most valuable skills for management consulting,ex mckinsey manager explains the most valuable skills for management consulting,0,ex mckinsey manager explains the most valuable skill for management consulting,ex mckinsey manager explains the most valuable skill for management consulting
why data engineering is so hot right now,why data engineering is so hot right now,0,why data engineering is so hot right now,why data engineering is so hot right now
college question already have bs but want to learn computer science curriculum,to reiterate the crappy title have bachelors of science in it but want the full computer science education here my question would there be any benefit to pursuing an accredited cs post bachelors would that open any additional doors if not thinking to just save the and go through this free curriculum on github ossu computer science for context it for self satisfaction but also to feel more rounded as begin hunting for my first data engineering job currently data analyst with three years experience using python we didn have data warehouse until now so no actual sql experience ve only used sql in coursework at home say intermediate where writing transformations stored procedures this next year will be sql heavy though at work so not worried there bonus question if complete the ossu path is that worth listing on resume on top of my other stuff or would that just prompt some eye rolls don know what the perception is on that kind of thing maybe in conjunction with home project,0,college question already have b but want to learn computer science curriculum,to reiterate the crappy title have bachelor of science in it but want the full computer science education here my question would there be any benefit to pursuing an accredited c post bachelor would that open any additional door if not thinking to just save the and go through this free curriculum on github ossu computer science for context it for self satisfaction but also to feel more rounded a begin hunting for my first data engineering job currently data analyst with three year experience using python we didn have data warehouse until now so no actual sql experience ve only used sql in coursework at home say intermediate where writing transformation stored procedure this next year will be sql heavy though at work so not worried there bonus question if complete the ossu path is that worth listing on resume on top of my other stuff or would that just prompt some eye roll don know what the perception is on that kind of thing maybe in conjunction with home project
data streaming idea,hey guys working in poc of data streaming using iceberg the idea is to be able to execute row level operations every time some action happens at the transactional database it would be great to hear some opinions and improvements to be made so here is summary of the streaming process there is also flowchart the data streaming process begins at postgres where and when operations are made at the transactional database the postgres table simulation at the flowchart is just script that makes inserts updates and deletes at the database randomly to simulate real life database the tables at postgres have catalog at glue and each one of them gains two columns at glue deleted and timestamp those columns are used to make row level operations that is how the merge knows which column was deleted or updated there is python script responsible for building those catalogs with the additional columns when some action occurs at postgres insert update or delete python script that uses wall2json replication and is always listening to the database send the operations to the correct table data delivery at kinesis firehose adding the correct values for timestamp and deleted if that is the case kinesis firehose saves the operations made at the database into s3 buckets specific for each table as parquet files when new files are saved at s3 lambda function is triggered to create if it doesn exist yet queue at sqs for each table and to feed the queue with the reference to the data at s3 that was just saved as an operation made at the database after that python script responsible for processing the parquet files into iceberg tables listens to the sqs queue and when there are new messages reads the message getting the parquet file reference to get the file at s3 and process it using spark and the two fields that were added timestamp and deleted through spark the row level operations are made into the iceberg tables and they now reflect the updates made at the database finally the iceberg tables have glue catalogs and through them it possible to query the tables at athena there is also flowchart that you can check every opinion and improvement will help lot thanks for reading amp x200b amp x200b format png amp auto webp amp c1dce524f67778a301e8eb663e26ef1be138cb20,0,data streaming idea,hey guy working in poc of data streaming using iceberg the idea is to be able to execute row level operation every time some action happens at the transactional database it would be great to hear some opinion and improvement to be made so here is summary of the streaming process there is also flowchart the data streaming process begin at postgres where and when operation are made at the transactional database the postgres table simulation at the flowchart is just script that make insert update and deletes at the database randomly to simulate real life database the table at postgres have catalog at glue and each one of them gain two column at glue deleted and timestamp those column are used to make row level operation that is how the merge know which column wa deleted or updated there is python script responsible for building those catalog with the additional column when some action occurs at postgres insert update or delete python script that us wall2json replication and is always listening to the database send the operation to the correct table data delivery at kinesis firehose adding the correct value for timestamp and deleted if that is the case kinesis firehose save the operation made at the database into s3 bucket specific for each table a parquet file when new file are saved at s3 lambda function is triggered to create if it doesn exist yet queue at sqs for each table and to feed the queue with the reference to the data at s3 that wa just saved a an operation made at the database after that python script responsible for processing the parquet file into iceberg table listens to the sqs queue and when there are new message read the message getting the parquet file reference to get the file at s3 and process it using spark and the two field that were added timestamp and deleted through spark the row level operation are made into the iceberg table and they now reflect the update made at the database finally the iceberg table have glue catalog and through them it possible to query the table at athena there is also flowchart that you can check every opinion and improvement will help lot thanks for reading amp x200b amp x200b format png amp auto webp amp c1dce524f67778a301e8eb663e26ef1be138cb20
plsql developer wanting to become data engineer what steps to take,basically write complex sql and pl sql code for etl purposes at my job have years of experience doing this to be honest don even have to do much of extracting the data it is already provided in flat files or tables and that is what use to transform the data don use cloud any programming language or any modern technologies want to do much more and won get that at my current job have been watching ton of what does de do videos on youtube and know that is what want to do have masters in cs but bachelors in history and am comfortable with programming know that don know much about de so enrolled into datacamp data engineering track yesterday and plan to finish it within couple of months know this will give me high level overview of what de does and needs to know am not sure what to do after this like learning from books and doing exercises projects are there any resources you guys recommend that will help me in my transition towards becoming de want to learn enough to be able to apply for junior or mid level de positions will be applying to these positions as study,0,plsql developer wanting to become data engineer what step to take,basically write complex sql and pl sql code for etl purpose at my job have year of experience doing this to be honest don even have to do much of extracting the data it is already provided in flat file or table and that is what use to transform the data don use cloud any programming language or any modern technology want to do much more and won get that at my current job have been watching ton of what doe de do video on youtube and know that is what want to do have master in c but bachelor in history and am comfortable with programming know that don know much about de so enrolled into datacamp data engineering track yesterday and plan to finish it within couple of month know this will give me high level overview of what de doe and need to know am not sure what to do after this like learning from book and doing exercise project are there any resource you guy recommend that will help me in my transition towards becoming de want to learn enough to be able to apply for junior or mid level de position will be applying to these position a study
should data engineers work on operational kind of flows,am wondering at high level the title question will put more specific case we me and my des colleagues are helping the catalog team of the company my company is marketplace and there is hugely time consuming task for the catalog team which is the following seller uploads product to be on sale but we don have it in our products database product management tool thus an event of no matching is created and sent to microservice database then the mission of the catalog team is to create those products which we didn have but seller wants to sell and here is where we enter into the game we pull the no matching events from the microservice database and create report in our bi tool with some basic info about the events name of the product n of sellers who has tried to sell it etc then the catalog team takes look at that report and creates the products manually for making their job more efficient we have integrated into our database external providers of products info so that our bi report is highly enriched and basically now the catalog team just have to copy the information that we provide in our product management tool from my point of view this is operational and not analytical but don know if the scope of data team should be only analytical or everything related to data is this something that data teams usually do,0,should data engineer work on operational kind of flow,am wondering at high level the title question will put more specific case we me and my de colleague are helping the catalog team of the company my company is marketplace and there is hugely time consuming task for the catalog team which is the following seller uploads product to be on sale but we don have it in our product database product management tool thus an event of no matching is created and sent to microservice database then the mission of the catalog team is to create those product which we didn have but seller want to sell and here is where we enter into the game we pull the no matching event from the microservice database and create report in our bi tool with some basic info about the event name of the product n of seller who ha tried to sell it etc then the catalog team take look at that report and creates the product manually for making their job more efficient we have integrated into our database external provider of product info so that our bi report is highly enriched and basically now the catalog team just have to copy the information that we provide in our product management tool from my point of view this is operational and not analytical but don know if the scope of data team should be only analytical or everything related to data is this something that data team usually do
is there way by which we can get column lineage diagram from sql query any ideas,is there way by which we can get column lineage diagram from sql query any ideas,0,is there way by which we can get column lineage diagram from sql query any idea,is there way by which we can get column lineage diagram from sql query any idea
any de from india,any fellow indians here wanted to know what is the tech stack you are working on and what is the compensation scene here,0,any de from india,any fellow indian here wanted to know what is the tech stack you are working on and what is the compensation scene here
path to dp,hi all am trying to help team go from zero to dp what would be the best microsoft courses to get me there what is the difference between should use dp900 as base anything people can do to help me would be great,0,path to dp,hi all am trying to help team go from zero to dp what would be the best microsoft course to get me there what is the difference between should use dp900 a base anything people can do to help me would be great
what should learn next,work as data engineer and know all the basic stuff am looking for something that is not that domain specific something that makes me improve overall fundamentally guess one can formulate the question what are some areas you think would make you better data engineer developer overall if any random examples statistics system architecture algebra hardware economics,0,what should learn next,work a data engineer and know all the basic stuff am looking for something that is not that domain specific something that make me improve overall fundamentally guess one can formulate the question what are some area you think would make you better data engineer developer overall if any random example statistic system architecture algebra hardware economics
exploratory data analysis with python an easy way to detect data abnormalities numpy accumulate,exploratory data analysis with python an easy way to detect data abnormalities numpy accumulate,0,exploratory data analysis with python an easy way to detect data abnormality numpy accumulate,exploratory data analysis with python an easy way to detect data abnormality numpy accumulate
azure storage architecture management for streaming high throughput,hi all am wanting to get other peoples opinions on the storage architecture for landing streaming data in azure taking in to account storage limitations and lifecycle management have only included data lake delta lake as the final processed storage to simplify the design yes there would be wh too if needed my thought processes the standard architecture see for storing raw streaming for keep long term is to put in to azure datalake blob storage combined with batch processing and then to delta lake for processing it would look like this amp x200b batch and stream to data lake for raw then to delta lake for processing format png amp auto webp amp bc976d17604d23bd70aab81bcb1ab1814c93 however recently came across this comment about storing log analytics from azure monitor don use an existing storage account that has other non monitoring data stored in it to better control access to the data and prevent reaching storage ingress rate limit failures and latency from it the first time have seen this and have been thinking about what it means have always kind of assumed that there should be only one data lake this idea was first crushed when saw the delta lake is on it own at least based on example have seen my take way is that each streaming source should have it own blob storage data lake sink when think about how this might look with multiple streams of data and life cycle management came up with this format png amp auto webp amp ac32cb1423bf6434e7219898c914535893ad480d the blobs could also be data lakes not sure on which is better for this scenario also suspect that the delta lake would also have azure lifecycle management turned on too alternately have seen blobs storage referenced to as temporary storage forget where in which case the design could also be format png amp auto webp amp f6a262e61b1271e034510651e5d8ba6d25b387 this design weakness is that there is more data movement costs but it advantage is having one long term storage for raw data in stead of many your thoughts and ideas would be much appreciated,0,azure storage architecture management for streaming high throughput,hi all am wanting to get other people opinion on the storage architecture for landing streaming data in azure taking in to account storage limitation and lifecycle management have only included data lake delta lake a the final processed storage to simplify the design yes there would be wh too if needed my thought process the standard architecture see for storing raw streaming for keep long term is to put in to azure datalake blob storage combined with batch processing and then to delta lake for processing it would look like this amp x200b batch and stream to data lake for raw then to delta lake for processing format png amp auto webp amp bc976d17604d23bd70aab81bcb1ab1814c93 however recently came across this comment about storing log analytics from azure monitor don use an existing storage account that ha other non monitoring data stored in it to better control access to the data and prevent reaching storage ingres rate limit failure and latency from it the first time have seen this and have been thinking about what it mean have always kind of assumed that there should be only one data lake this idea wa first crushed when saw the delta lake is on it own at least based on example have seen my take way is that each streaming source should have it own blob storage data lake sink when think about how this might look with multiple stream of data and life cycle management came up with this format png amp auto webp amp ac32cb1423bf6434e7219898c914535893ad480d the blob could also be data lake not sure on which is better for this scenario also suspect that the delta lake would also have azure lifecycle management turned on too alternately have seen blob storage referenced to a temporary storage forget where in which case the design could also be format png amp auto webp amp f6a262e61b1271e034510651e5d8ba6d25b387 this design weakness is that there is more data movement cost but it advantage is having one long term storage for raw data in stead of many your thought and idea would be much appreciated
what is your ideal data team composition,you have given privilege to build your own ideal data team how many people do you allocate for these position data engineers data architects data analysts data scientists bi developers data platform engineers tech lead managers head ps size of team can vary maybe or even more,0,what is your ideal data team composition,you have given privilege to build your own ideal data team how many people do you allocate for these position data engineer data architect data analyst data scientist bi developer data platform engineer tech lead manager head p size of team can vary maybe or even more
why is learning data engineering so opaque,am full stack developer trying to learn more about data engineering but so far everything is so damn opaque know front end has its own messes but at least at this point everyone is unified under one programming language couple open source frameworks and couple open source package managers it not hard to find tutorials frontend or backend that have you developing locally in minutes with commonly accepted tools on sound examples meanwhile under azure there are so many services that seem like they should do the same thing synapse and databricks data explorer and analysis services data factory and hdinsight iot hub and stream analytics and learning any one of them is an annoying exercise in setting up an account and learning user interface that will be swapped out in couple years there are no definitive starting places it feels like where should start know python sql well and ve read kimball and designing data intensive applications and want to start applying this stuff but can even begin to know what tech to choose,0,why is learning data engineering so opaque,am full stack developer trying to learn more about data engineering but so far everything is so damn opaque know front end ha it own mess but at least at this point everyone is unified under one programming language couple open source framework and couple open source package manager it not hard to find tutorial frontend or backend that have you developing locally in minute with commonly accepted tool on sound example meanwhile under azure there are so many service that seem like they should do the same thing synapse and databricks data explorer and analysis service data factory and hdinsight iot hub and stream analytics and learning any one of them is an annoying exercise in setting up an account and learning user interface that will be swapped out in couple year there are no definitive starting place it feel like where should start know python sql well and ve read kimball and designing data intensive application and want to start applying this stuff but can even begin to know what tech to choose
this is the best hands on git tutorial,this is the best hands on git tutorial,0,this is the best hand on git tutorial,this is the best hand on git tutorial
snowplow twitch livestream cet,snowplow twitch livestream cet,0,snowplow twitch livestream cet,snowplow twitch livestream cet
the missing semester of your cs education,the missing semester of your cs education,0,the missing semester of your c education,the missing semester of your c education
loading data into fact table,hello am currently learning the concept of dimensional modeling as described in kimball book more or less understand the idea but wondering what the process of loading data into fact table looks like and what the maintenance of the database should look like found the following article that answers my questions but am not sure if this should be good example,0,loading data into fact table,hello am currently learning the concept of dimensional modeling a described in kimball book more or le understand the idea but wondering what the process of loading data into fact table look like and what the maintenance of the database should look like found the following article that answer my question but am not sure if this should be good example
managing data lake s3 layers,have an elt extracting some data from mysql tables then have upload csv files to s3 copy from s3 into snowflake apply transformations in snowflake create data model in snowflake very classic my question are now should upload intermediate files to the s3 bucket and follow the rules of an s3 data lake with layers does it apply this for an elt what about if instead of an elt is etl should upload to the bucket file after each transformation or just one at the very end,0,managing data lake s3 layer,have an elt extracting some data from mysql table then have upload csv file to s3 copy from s3 into snowflake apply transformation in snowflake create data model in snowflake very classic my question are now should upload intermediate file to the s3 bucket and follow the rule of an s3 data lake with layer doe it apply this for an elt what about if instead of an elt is etl should upload to the bucket file after each transformation or just one at the very end
coursera plus subscription at with cyber week sale,coursera plus subscription at with cyber week sale,0,coursera plus subscription at with cyber week sale,coursera plus subscription at with cyber week sale
example of streaming aside from twitter,have browsed many streaming exercises for spark streaming and almost all of them uses twitter do you know of any public free streaming apis returns json that can use aside from twitter,0,example of streaming aside from twitter,have browsed many streaming exercise for spark streaming and almost all of them us twitter do you know of any public free streaming apis return json that can use aside from twitter
did using cloud services made your job easier more convenient,currently working for bigger digital company that was doing everything on premise so far rdbms dwh appliance solution hadoop cluster in the past was often frustated because for the majority of time was busy with infrastructure deployment related issues mainly kubernetes helm maven ansible sdk and jenkins related configuration problems setting up pods for deployment services of new infrastructure etc the dev part then comes on top of that if didn create the infrastructure myself which often wasn perfectly set up neither ofc was often enforced to deliver features on badly operating infrastructure where feel like beta testing somebody elses work since often working on pionier projects so often delayed as have to identify and clarify issues before integrating my solution in perfect world this shouldn happen as such things should be tested before sprint starts but the communication between my department and the operational support is cumbersome since there re too many people having their fingers everywhere involved and feel like our processes and infrastructure changes faster than able to catch up so constantly adjusting and relearning every process my company is now moving on to gcp and replacing both rdbms and the hadoop cluster slowly with big query as dwh solution and this will take several years if everything remains as planned so wonderd as most companies nowadays are cloud only will cloud services seriously help me to focus more on the development part instead of troubleshooting infrastructure and deployment related issues the majority of the time or are the cloud advertisments mere marketing by promising this not sure if using cloud services will make difference here or if should change the company after several years now as feel that the complexity of my current job burns me out since feel that need to deal with too many technologies on too many different environments which are changing faster than can keep track so currently fee bit like the red queen of alice in wonderland my dear here we must run as fast as we can just to stay in place and if you wish to go anywhere you must run twice as fast as that view poll,0,did using cloud service made your job easier more convenient,currently working for bigger digital company that wa doing everything on premise so far rdbms dwh appliance solution hadoop cluster in the past wa often frustated because for the majority of time wa busy with infrastructure deployment related issue mainly kubernetes helm maven ansible sdk and jenkins related configuration problem setting up pod for deployment service of new infrastructure etc the dev part then come on top of that if didn create the infrastructure myself which often wasn perfectly set up neither ofc wa often enforced to deliver feature on badly operating infrastructure where feel like beta testing somebody el work since often working on pionier project so often delayed a have to identify and clarify issue before integrating my solution in perfect world this shouldn happen a such thing should be tested before sprint start but the communication between my department and the operational support is cumbersome since there re too many people having their finger everywhere involved and feel like our process and infrastructure change faster than able to catch up so constantly adjusting and relearning every process my company is now moving on to gcp and replacing both rdbms and the hadoop cluster slowly with big query a dwh solution and this will take several year if everything remains a planned so wonderd a most company nowadays are cloud only will cloud service seriously help me to focus more on the development part instead of troubleshooting infrastructure and deployment related issue the majority of the time or are the cloud advertisments mere marketing by promising this not sure if using cloud service will make difference here or if should change the company after several year now a feel that the complexity of my current job burn me out since feel that need to deal with too many technology on too many different environment which are changing faster than can keep track so currently fee bit like the red queen of alice in wonderland my dear here we must run a fast a we can just to stay in place and if you wish to go anywhere you must run twice a fast a that view poll
successfully pivoted from etl bi developer to data engineer in seattle wanted to share my experience,ve been posting in here occasionally about my journey to becoming real data engineer would get lot of messages about what did in my journey how currently doing so thought would share with everyone background worked for fortune company in texas had the data engineer title in my previous role but was really an etl bi developer realized had an inflated job title was getting paid lot for what actually did but knew this wasn good for my career in the long term had no industry programming experience only knew gui drag and drop etl tools and some reporting tools had my sights set on seattle and fully expected to restart my career to get my foot in the door the grind started the grind in august learned python syntax through codecademy did easy and medium level questions on leetcode strings and arrays only did easy medium and hard sql questions on leetcode took aws data analytics course on udemy took hadoop ecosystem course on udemy read through the first two chapters of kimball data warehouse toolkit watched youtube videos on system design learned big on basic level learned basic data structures algorithms interviewing submitted as many applications as could about per day starting in october my response rate was actually pretty decent around went into interviews being honest about my experience told interviewers that wanted to pivot into something more technical made it obvious that learning and new challenges are what enjoy it true what everyone says only got asked easy level python questions in interviews even amazon asked easy level questions the only exception was uber for sql leetcode hard is not enough from my experience sql interview questions are on whole other level expect triple or quadruple nested query using multiple ctes and less common functions like mod it sounds hard but if you know sql pretty well it should be easy for you to piece together solution was never asked about aws or hadoop however was asked about data modeling and system design so knowing about cloud systems and big data frameworks at high level ended up being beneficial expect to also be asked about devops troubleshooting and creating fault tolerant pipelines wasn asked about big notation specifically but the interviewer will at least expect you to know about where some efficiency improvements can be made in your code example using two pointer sliding window approach instead of using double nested for loop guess this point can be tied to ds too result didn get any offers from top tier companies but for the offer accepted ended up getting offered senior level position since have almost years industry experience it not faang but something at least satisfied with since fully expected to restart my career was kind of surprised but going to trust in the process here lol for the people who have been messaging me good luck on your journey know it seems like an impossible task but just take it one day at time the offer will come eventually,0,successfully pivoted from etl bi developer to data engineer in seattle wanted to share my experience,ve been posting in here occasionally about my journey to becoming real data engineer would get lot of message about what did in my journey how currently doing so thought would share with everyone background worked for fortune company in texas had the data engineer title in my previous role but wa really an etl bi developer realized had an inflated job title wa getting paid lot for what actually did but knew this wasn good for my career in the long term had no industry programming experience only knew gui drag and drop etl tool and some reporting tool had my sight set on seattle and fully expected to restart my career to get my foot in the door the grind started the grind in august learned python syntax through codecademy did easy and medium level question on leetcode string and array only did easy medium and hard sql question on leetcode took aws data analytics course on udemy took hadoop ecosystem course on udemy read through the first two chapter of kimball data warehouse toolkit watched youtube video on system design learned big on basic level learned basic data structure algorithm interviewing submitted a many application a could about per day starting in october my response rate wa actually pretty decent around went into interview being honest about my experience told interviewer that wanted to pivot into something more technical made it obvious that learning and new challenge are what enjoy it true what everyone say only got asked easy level python question in interview even amazon asked easy level question the only exception wa uber for sql leetcode hard is not enough from my experience sql interview question are on whole other level expect triple or quadruple nested query using multiple ctes and le common function like mod it sound hard but if you know sql pretty well it should be easy for you to piece together solution wa never asked about aws or hadoop however wa asked about data modeling and system design so knowing about cloud system and big data framework at high level ended up being beneficial expect to also be asked about devops troubleshooting and creating fault tolerant pipeline wasn asked about big notation specifically but the interviewer will at least expect you to know about where some efficiency improvement can be made in your code example using two pointer sliding window approach instead of using double nested for loop guess this point can be tied to d too result didn get any offer from top tier company but for the offer accepted ended up getting offered senior level position since have almost year industry experience it not faang but something at least satisfied with since fully expected to restart my career wa kind of surprised but going to trust in the process here lol for the people who have been messaging me good luck on your journey know it seems like an impossible task but just take it one day at time the offer will come eventually
dimensional modeling question concerning multivalued dimension tables,good afternoon looking to create this dimensional model for personal project of mine and the issue sort of having is dealing correctly with multivalued dimensional tables in this instance have business in my fact table which can be placed into multiple categories so to prevent the many to many relationship made bridge table coincidentally also have chain dimension restaurant chain business chain etc now having looked at the data warehouse toolkit this is exactly the table structure it recommends with the chain dimension serving as the group table although if correct the chain dimension could just be removed considering there is no to relationship between the business fact table and the bridge table that sort of question in passing now my real question is concerning the acceptedpaymentsbridge dimension this columns in here would have low cardinality and in the same book it mentions that in such case one could use positional tables that is table which the different values serving as column names now would that be viable option here or is it just fine to implement the bridge table the tables in question are circled in red as well if it helps to see that,0,dimensional modeling question concerning multivalued dimension table,good afternoon looking to create this dimensional model for personal project of mine and the issue sort of having is dealing correctly with multivalued dimensional table in this instance have business in my fact table which can be placed into multiple category so to prevent the many to many relationship made bridge table coincidentally also have chain dimension restaurant chain business chain etc now having looked at the data warehouse toolkit this is exactly the table structure it recommends with the chain dimension serving a the group table although if correct the chain dimension could just be removed considering there is no to relationship between the business fact table and the bridge table that sort of question in passing now my real question is concerning the acceptedpaymentsbridge dimension this column in here would have low cardinality and in the same book it mention that in such case one could use positional table that is table which the different value serving a column name now would that be viable option here or is it just fine to implement the bridge table the table in question are circled in red a well if it help to see that
smaller scale data catalog tools for data lake,we are building an internal data platform for collecting data from our various teams into one place we re taking hybrid data lake data warehouse approach by ingesting raw data but still having some structure around it one key component is the data catalog tagging library layer as is implied by those slashes we re not quite sure what exactly we need to build but fundamentally it is something that tags our data so that we can make sense of what we have ve talked with data catalog vendors and that seems like the thing we need but most of those are way more than we need at our stage we only have gigabytes of relatively slowly changing data weekly updates are probably sufficient most of the data is tabular from niche saas solutions for our relatively specialized industry think csv exports of data with the occasional json payload from webhook push or api pull will also have some mix of structured and unstructured data pdfs with standard but non tabular structure as well as just narrative docs it been suggested to me that elasticsearch or solr may be good tool to use to tag but in looking at those it seems that both are more for humans searching data and not necessarily for extracting metadata that can be used in queries the use case being not merely finding the docs but doing some data warehouse type queries against data sourced from both the tabular data and the unstructured text docs rightly or wrongly what kind of envisioning think is the idea of dimension tables in data warehouse star schema ie extracting attributes from the various data types but instead of only having dimension tables we also have dimension blobs not sure if that makes sense but any other suggestions between elasticsearch and heavy duty data catalog like data world or collibra woudl be welcome,0,smaller scale data catalog tool for data lake,we are building an internal data platform for collecting data from our various team into one place we re taking hybrid data lake data warehouse approach by ingesting raw data but still having some structure around it one key component is the data catalog tagging library layer a is implied by those slash we re not quite sure what exactly we need to build but fundamentally it is something that tag our data so that we can make sense of what we have ve talked with data catalog vendor and that seems like the thing we need but most of those are way more than we need at our stage we only have gigabyte of relatively slowly changing data weekly update are probably sufficient most of the data is tabular from niche saas solution for our relatively specialized industry think csv export of data with the occasional json payload from webhook push or api pull will also have some mix of structured and unstructured data pdfs with standard but non tabular structure a well a just narrative doc it been suggested to me that elasticsearch or solr may be good tool to use to tag but in looking at those it seems that both are more for human searching data and not necessarily for extracting metadata that can be used in query the use case being not merely finding the doc but doing some data warehouse type query against data sourced from both the tabular data and the unstructured text doc rightly or wrongly what kind of envisioning think is the idea of dimension table in data warehouse star schema ie extracting attribute from the various data type but instead of only having dimension table we also have dimension blob not sure if that make sense but any other suggestion between elasticsearch and heavy duty data catalog like data world or collibra woudl be welcome
step by step guide to creating data catalog,simple data cataloging starts with great organization data catalog is collection of metadata and documentation that helps make sense of the data sprawl that exists in most growing companies getting together and starting to use data catalog is simple process but starting to get adoption and having the dictionary exist as part of your workflow is little bit more difficult even though it may seem like an easy task getting different stakeholders to change their routines and start using new tool can be very challenging an example of the data catalog problems shared by one of the delivery companies we spoke with at this company it was difficult to get aligned on which tables were commonly used joined how they were used together and what columns meant similarly it difficult to monitor the number of data assets that exist across different departments especially when the number of resources grows at faster rate than people why is this the case data is becoming more decentralized through concepts like the data mesh as more teams outside of the data function start to use data in their day to day different tables dashboards and definitions are being created at an almost exponential rate data catalogs are important because they help you organize your data whether you are working with structured or unstructured data they help you identify what kind of data you have how it is related to each other and what the best means to store it is so that you can quickly find it when needed below are the steps that teams need to take when creating data catalog gather sources from across the organization the first step data teams need to take is to collect the different resources that are scattered across different tools in the origination this may require multiple meetings and stakeholders to come together and figure out which resources need to be in the catalog today this collection could be done in spreadsheet with an ongoing list of all resources and how they connect give each resource an owner after data teams have identified all the resources from across the company that they would like to include in their data catalog we recommend assigning ownership to each resource teams that we ve worked within the past have assigned ownership based on the source schema or even domain teams that start assigning ownership should look for people who are familiar with the data knowledge they are responsible for managing and are willing to help others who want to learn how to use it get support and sign off once these meetings conclude and owners are on the same page have the owners sign off on their responsibilities the owners should be in alignment with the documentation and feel like the data team worked collaboratively with them to come to this ownership structure one effective strategy is to involve the leadership team in the exercise early to make sure that their team leads are signing off on the owners of data this way leadership can see how widespread the understanding of data is across the company if the team leadership team sees the value of data catalog this can move at much faster pace integrate the catalog base into your workflow after data teams have received support for their data documentation process they should look for ways to integrate this tool into their workflow this step is critical for maintenance and upkeep without tool that allows teammates to receive notifications on slack it will likely be forgotten by creating process around the data catalog teams can ensure that it is not left behind as the team grows upkeep the data catalog although the documentation should be stable it may need to change over time one instance that might require documentation to change is when new revenue stream is introduced or when the pricing of an existing revenue line changes these changes traditionally come from the business team and might require the data team to implement the changes into the data catalog teams that invest the time to get alignment using data catalog can see major benefits in the long term as they make faster decisions as team creating data catalog is not small undertaking tou can read the full step by step guide here if you found this post useful,0,step by step guide to creating data catalog,simple data cataloging start with great organization data catalog is collection of metadata and documentation that help make sense of the data sprawl that exists in most growing company getting together and starting to use data catalog is simple process but starting to get adoption and having the dictionary exist a part of your workflow is little bit more difficult even though it may seem like an easy task getting different stakeholder to change their routine and start using new tool can be very challenging an example of the data catalog problem shared by one of the delivery company we spoke with at this company it wa difficult to get aligned on which table were commonly used joined how they were used together and what column meant similarly it difficult to monitor the number of data asset that exist across different department especially when the number of resource grows at faster rate than people why is this the case data is becoming more decentralized through concept like the data mesh a more team outside of the data function start to use data in their day to day different table dashboard and definition are being created at an almost exponential rate data catalog are important because they help you organize your data whether you are working with structured or unstructured data they help you identify what kind of data you have how it is related to each other and what the best mean to store it is so that you can quickly find it when needed below are the step that team need to take when creating data catalog gather source from across the organization the first step data team need to take is to collect the different resource that are scattered across different tool in the origination this may require multiple meeting and stakeholder to come together and figure out which resource need to be in the catalog today this collection could be done in spreadsheet with an ongoing list of all resource and how they connect give each resource an owner after data team have identified all the resource from across the company that they would like to include in their data catalog we recommend assigning ownership to each resource team that we ve worked within the past have assigned ownership based on the source schema or even domain team that start assigning ownership should look for people who are familiar with the data knowledge they are responsible for managing and are willing to help others who want to learn how to use it get support and sign off once these meeting conclude and owner are on the same page have the owner sign off on their responsibility the owner should be in alignment with the documentation and feel like the data team worked collaboratively with them to come to this ownership structure one effective strategy is to involve the leadership team in the exercise early to make sure that their team lead are signing off on the owner of data this way leadership can see how widespread the understanding of data is across the company if the team leadership team see the value of data catalog this can move at much faster pace integrate the catalog base into your workflow after data team have received support for their data documentation process they should look for way to integrate this tool into their workflow this step is critical for maintenance and upkeep without tool that allows teammate to receive notification on slack it will likely be forgotten by creating process around the data catalog team can ensure that it is not left behind a the team grows upkeep the data catalog although the documentation should be stable it may need to change over time one instance that might require documentation to change is when new revenue stream is introduced or when the pricing of an existing revenue line change these change traditionally come from the business team and might require the data team to implement the change into the data catalog team that invest the time to get alignment using data catalog can see major benefit in the long term a they make faster decision a team creating data catalog is not small undertaking tou can read the full step by step guide here if you found this post useful
resume critique data analyst transitioning to de,hey guys joined this sub about months back when was almost clueless about what data engineering is since then ve asked lot of questions here received lot of answers and learned lot and as my biggest step since then at least according to me passed the microsoft azure data engineer certification this week big thanks to everyone on this sub for the regular informative content and pretty encouraging environment all around would recommend so now feel that can start applying for jobs with the knowledge ve gained and the certification as well added to my resume ve created draft version of my resume it would be great if you guys could critique it and provide some feedback on it should specify that ve taken inspiration very heavily from this post please critique my resume data analyst got some really good pointers from there tl dr was clueless about de months ago now feel ready to start applying to de jobs please critique my resume thanks everyone format png amp auto webp amp a33a3268ec3d8af4ad2ab4e6017cec89e1c19dd7,0,resume critique data analyst transitioning to de,hey guy joined this sub about month back when wa almost clueless about what data engineering is since then ve asked lot of question here received lot of answer and learned lot and a my biggest step since then at least according to me passed the microsoft azure data engineer certification this week big thanks to everyone on this sub for the regular informative content and pretty encouraging environment all around would recommend so now feel that can start applying for job with the knowledge ve gained and the certification a well added to my resume ve created draft version of my resume it would be great if you guy could critique it and provide some feedback on it should specify that ve taken inspiration very heavily from this post please critique my resume data analyst got some really good pointer from there tl dr wa clueless about de month ago now feel ready to start applying to de job please critique my resume thanks everyone format png amp auto webp amp a33a3268ec3d8af4ad2ab4e6017cec89e1c19dd7
what should choose to join,got an offer from an analytics company for de role now my current firm is tending to retain me help me decide what should do pros if stay with current company same company but matching the offered compensation providing resources and mentorship to learn de skills do not work as de as of now asked me to learn about data warehousing fundamentals metallion aws fundamentals apache spark databricks can leave again if don like the learning smooth transition to de profile can test the waters services company probability to see the variety in work cons if stay with current company opportunity cost may not get good job offer again pros if join the analytics company part of founding member of data engineering team can create impact early they will train on the required skill sets by people from us smaller company employees globally so greater visibility product based organization they make analytical products shaped exposure diversity exposure people from more than countries cons if join analytics company might have to work on limited technologies which they use right now what know is they work with tech like mssqlserver hadoop snowflake and aws am not sure if should join place as starting data engineer where have may have minimal peer guidance my decision stay if they pay well amp opportunities to learn and work as de otherwise join the analytics company can reveal company names on request,0,what should choose to join,got an offer from an analytics company for de role now my current firm is tending to retain me help me decide what should do pro if stay with current company same company but matching the offered compensation providing resource and mentorship to learn de skill do not work a de a of now asked me to learn about data warehousing fundamental metallion aws fundamental apache spark databricks can leave again if don like the learning smooth transition to de profile can test the water service company probability to see the variety in work con if stay with current company opportunity cost may not get good job offer again pro if join the analytics company part of founding member of data engineering team can create impact early they will train on the required skill set by people from u smaller company employee globally so greater visibility product based organization they make analytical product shaped exposure diversity exposure people from more than country con if join analytics company might have to work on limited technology which they use right now what know is they work with tech like mssqlserver hadoop snowflake and aws am not sure if should join place a starting data engineer where have may have minimal peer guidance my decision stay if they pay well amp opportunity to learn and work a de otherwise join the analytics company can reveal company name on request
tracking an application pipeline,hi all hoping someone out there can get their heads around this we have business process with non standard flow simplified example would be that there are step an application object can go gt gt gt gt or gt gt gt gt gt it can hit the same step multiple times there are about steps we ve built two kimball model fact tables an accumulating snapshot at the application grain which tracks the first and last time for each status and standard transactional fact with columns for application status date time and metric for days from account creation the second of these allows us to take any application at any two status date time points take one days from account creation away from the other and get the days between the two what the business wants is velocity chart they want to select start and end process step and based on start month see the percentage of applications that got to the end process step in day days days gt days the aim is to see that over period of months we have increased the average speed between the two given steps having hard time wrapping my head around the sql primarily based on the challenge that application can go through the same step multiple times not looking for someone to work this out send me the sql but am hoping for some advice pointers to head me in the right direction,0,tracking an application pipeline,hi all hoping someone out there can get their head around this we have business process with non standard flow simplified example would be that there are step an application object can go gt gt gt gt or gt gt gt gt gt it can hit the same step multiple time there are about step we ve built two kimball model fact table an accumulating snapshot at the application grain which track the first and last time for each status and standard transactional fact with column for application status date time and metric for day from account creation the second of these allows u to take any application at any two status date time point take one day from account creation away from the other and get the day between the two what the business want is velocity chart they want to select start and end process step and based on start month see the percentage of application that got to the end process step in day day day gt day the aim is to see that over period of month we have increased the average speed between the two given step having hard time wrapping my head around the sql primarily based on the challenge that application can go through the same step multiple time not looking for someone to work this out send me the sql but am hoping for some advice pointer to head me in the right direction
how valuable are certifications,and if they are which ones are best to get,0,how valuable are certification,and if they are which one are best to get
development environment for emr,have previously developed in apache spark using databricks the notebook experience there makes developing and debugging functions very straightforward have recently joined new organization that uses aws emr the devs here are developing and submitting their entire script to the cluster each time they test script this is clearly not ideal because it time consuming and puts stress on data sources which are being hit many times for testing purposes for example most of these scripts are hitting redshift cluster or hive tables in s3 emr developers out there what kind of development environment would you recommend should develop locally in pyspark and replace environment variables when push code to emr use emr studio something else,0,development environment for emr,have previously developed in apache spark using databricks the notebook experience there make developing and debugging function very straightforward have recently joined new organization that us aws emr the devs here are developing and submitting their entire script to the cluster each time they test script this is clearly not ideal because it time consuming and put stress on data source which are being hit many time for testing purpose for example most of these script are hitting redshift cluster or hive table in s3 emr developer out there what kind of development environment would you recommend should develop locally in pyspark and replace environment variable when push code to emr use emr studio something else
spark collect method,in my current project have spark integrated with livy in an interactive mode query is our input when we fire query from livy it executes on presto and the presto results needs to be sent back to livy we are creating data frame from the presto results how can get the results back to livy without using the df collect method at present we are using collect for development but worry about its impact when we have huge data can anyone suggest me good approach thanks in advance,0,spark collect method,in my current project have spark integrated with livy in an interactive mode query is our input when we fire query from livy it executes on presto and the presto result need to be sent back to livy we are creating data frame from the presto result how can get the result back to livy without using the df collect method at present we are using collect for development but worry about it impact when we have huge data can anyone suggest me good approach thanks in advance
has anyone actually used palantir products,job post looked at mentioned it can find lot of information about it apart from some youtube demos has anyone here used it and what are your thoughts,0,ha anyone actually used palantir product,job post looked at mentioned it can find lot of information about it apart from some youtube demo ha anyone here used it and what are your thought
tutorial run your sparklyr workloads at scale with spark on kubernetes data mechanics blog,tutorial run your sparklyr workloads at scale with spark on kubernetes data mechanics blog,0,tutorial run your sparklyr workload at scale with spark on kubernetes data mechanic blog,tutorial run your sparklyr workload at scale with spark on kubernetes data mechanic blog
speeding up power bi etl using python,have power bi dashboard that takes too long to run for business needs how do speed up the process think the data is too large and the sql queries take too long to run how can use python to improve the etl process wouldn python still have to read the sql data to transform it can the process of transformations and loading take place simultaneously then go to power bi,0,speeding up power bi etl using python,have power bi dashboard that take too long to run for business need how do speed up the process think the data is too large and the sql query take too long to run how can use python to improve the etl process wouldn python still have to read the sql data to transform it can the process of transformation and loading take place simultaneously then go to power bi
oss vs proprietary,there are so many tooling options how does everyone decide between oss vs prop do you have criteria or factors you evaluate on,0,os v proprietary,there are so many tooling option how doe everyone decide between os v prop do you have criterion or factor you evaluate on
which tech skills are the most marketable in the data engineering job market,heard scala spark is really marketable and pays lot and has high demand with low supply ot talent is that true what other tech skills are in high demand and pay well,0,which tech skill are the most marketable in the data engineering job market,heard scala spark is really marketable and pay lot and ha high demand with low supply ot talent is that true what other tech skill are in high demand and pay well
landing data on s3 the good the bad and the ugly,landing data on s3 the good the bad and the ugly,0,landing data on s3 the good the bad and the ugly,landing data on s3 the good the bad and the ugly
part time data engineering bootcamp for the us,out of stealth are you located in the united states join our new part time course launching january book call with us,0,part time data engineering bootcamp for the u,out of stealth are you located in the united state join our new part time course launching january book call with u
ideas on how to use my data skills for good cause,hey all ve been trying to think of ways to do something positive with my job skills python sql automation ml any of that ideally like to build project that helps people in need not thinking too much about mentoring or tutoring as already do bit and would like to do something else any ideas or experience is appreciated,0,idea on how to use my data skill for good cause,hey all ve been trying to think of way to do something positive with my job skill python sql automation ml any of that ideally like to build project that help people in need not thinking too much about mentoring or tutoring a already do bit and would like to do something else any idea or experience is appreciated
is it just me or is this job description asking for lot,,0,is it just me or is this job description asking for lot,
lakefs and the problem of data lake management,ve been looking through lakefs documentation and am quite sold on the data as code idea branching merging commits and references seems like it could get messy the way git does at times but if you know what you re doing could be quite powerful don see whole lot out there about lakefs though it new ish product guess but was curious what other people in this community are doing today for data lake versioning and what they wish they were doing lol specifically apart from roll backs and atomic transactions the part that escapes me most is the use case of needing to tie different versions of ml models differentiated by git commit ids to specific snapshots of data the data can both have been appended to as well as have been updated gets messy for reproducibility but is an important factor as the ai ethics regulations are starting to loom,0,lakefs and the problem of data lake management,ve been looking through lakefs documentation and am quite sold on the data a code idea branching merging commits and reference seems like it could get messy the way git doe at time but if you know what you re doing could be quite powerful don see whole lot out there about lakefs though it new ish product guess but wa curious what other people in this community are doing today for data lake versioning and what they wish they were doing lol specifically apart from roll back and atomic transaction the part that escape me most is the use case of needing to tie different version of ml model differentiated by git commit id to specific snapshot of data the data can both have been appended to a well a have been updated get messy for reproducibility but is an important factor a the ai ethic regulation are starting to loom
dealing with schema changes with an existing pipeline,let say my pipeline is very simple batch pipeline scheduled by airflow stage data to s3 from rest apis using python program and then process the data my business logic here and finally store the data in an un managed hive table for querying which acts like clean table for analysis it is stored in parquet format in s3 suppose if the data changes at source like few columns added and want to have these columns added in my table as well how would handle this make changes in the airflow day to create another branch called v2 and create another table and store the results or migrate the existing hive table ideally want to have this with minimal code changes please let me know your thoughts thanks,0,dealing with schema change with an existing pipeline,let say my pipeline is very simple batch pipeline scheduled by airflow stage data to s3 from rest apis using python program and then process the data my business logic here and finally store the data in an un managed hive table for querying which act like clean table for analysis it is stored in parquet format in s3 suppose if the data change at source like few column added and want to have these column added in my table a well how would handle this make change in the airflow day to create another branch called v2 and create another table and store the result or migrate the existing hive table ideally want to have this with minimal code change please let me know your thought thanks
the text mining handbook advanced approaches in analyzing unstructured data ronen feldman james sanger,the text mining handbook advanced approaches in analyzing unstructured data ronen feldman james sanger,0,the text mining handbook advanced approach in analyzing unstructured data ronen feldman james sanger,the text mining handbook advanced approach in analyzing unstructured data ronen feldman james sanger
software engineering interview course by pramp,course link promo_code blackfriday amp utm_source drip amp utm_medium email amp utm_campaign just launched software engineering interview course just received this promotion via email do you feel this course is beneficial for preparing for de jobs in nyc area fintech and healthcare specifically wonder about the content under the system design questions heading where it discusses building netflix and twitter is this material relevant to what am doing my job targets location nyc area industry finance primary healthcare secondary title de will learn big data stack,0,software engineering interview course by pramp,course link promo_code blackfriday amp utm_source drip amp utm_medium email amp utm_campaign just launched software engineering interview course just received this promotion via email do you feel this course is beneficial for preparing for de job in nyc area fintech and healthcare specifically wonder about the content under the system design question heading where it discus building netflix and twitter is this material relevant to what am doing my job target location nyc area industry finance primary healthcare secondary title de will learn big data stack
data mesh old idea new label,cant completely understand how the data mesh is different from the idea of dimensional modelling and data marts kimball called the concept independent data marts amp x200b suggested read with inspiration all credit to the author andriy zabavskyy amp x200b my current understanding of the data mesh the idea is that you govern and develop data products based on domain the exact same idea with data marts specific on deparment some dimension kimball terminology are crossing domains shared dimensions however instead of having data engineering team working on central data layer datawarehouse data lake data lakehouse they are split into the specific domain hr finance and being responsible for all the data logic and glossary of the domain therefore the data engineer will be closer to the business and understand their data needs better than any central team will ever be able to however the new decentral nature of domain specific data engineers requres them to work with strict rules to ensure that you keep one source of truth and maintain the same tooling and follow best practice etl rules work into single data layer architecture meaning all data from various sources are fetched into single place be that data lake or database based on the nature of the data so that all the integrations from systems are done centrally before cleasing manipulation and enrichment work are carried out in relation to the data domains amp x200b am on the right track or is there something im missing all your ideas and thoughts on the subject will be greatly appreaciated,0,data mesh old idea new label,cant completely understand how the data mesh is different from the idea of dimensional modelling and data mart kimball called the concept independent data mart amp x200b suggested read with inspiration all credit to the author andriy zabavskyy amp x200b my current understanding of the data mesh the idea is that you govern and develop data product based on domain the exact same idea with data mart specific on deparment some dimension kimball terminology are crossing domain shared dimension however instead of having data engineering team working on central data layer datawarehouse data lake data lakehouse they are split into the specific domain hr finance and being responsible for all the data logic and glossary of the domain therefore the data engineer will be closer to the business and understand their data need better than any central team will ever be able to however the new decentral nature of domain specific data engineer requres them to work with strict rule to ensure that you keep one source of truth and maintain the same tooling and follow best practice etl rule work into single data layer architecture meaning all data from various source are fetched into single place be that data lake or database based on the nature of the data so that all the integration from system are done centrally before cleasing manipulation and enrichment work are carried out in relation to the data domain amp x200b am on the right track or is there something im missing all your idea and thought on the subject will be greatly appreaciated
how to replicate production mysql data for devs to use in local docker,we have large gb aws rds mysql production database that runs our web app previously when dev needed to create new features or alter tables they would run mysql dump from our production database and load the data into their own local instance of mysql then begin development but the mysql dump process anonymizes scrubs the data and takes about hours each time instead of doing the dump which takes lot of time we created docker image that fills mysql database with prepared seeding dataset but this seeding dataset will need to be manually up kept to mimic the production database when column is added to production that column will need to be manually added to the seeding dataset so the seeding dataset will need to be maintained by someone to ensure it is robust enough for dev work in the future our data is customer oriented so we thought about taking subset of customers and using that subset to seed the rest of our database but were unsure how we would enforce all the foreign key constraints associated for each customer and the related tables within the subset what methods do you all use to replicate production servers so that devs can have their own independent development database,0,how to replicate production mysql data for devs to use in local docker,we have large gb aws rds mysql production database that run our web app previously when dev needed to create new feature or alter table they would run mysql dump from our production database and load the data into their own local instance of mysql then begin development but the mysql dump process anonymizes scrub the data and take about hour each time instead of doing the dump which take lot of time we created docker image that fill mysql database with prepared seeding dataset but this seeding dataset will need to be manually up kept to mimic the production database when column is added to production that column will need to be manually added to the seeding dataset so the seeding dataset will need to be maintained by someone to ensure it is robust enough for dev work in the future our data is customer oriented so we thought about taking subset of customer and using that subset to seed the rest of our database but were unsure how we would enforce all the foreign key constraint associated for each customer and the related table within the subset what method do you all use to replicate production server so that devs can have their own independent development database
how data lakehouse helps with amp integration,how data lakehouse helps with amp integration,0,how data lakehouse help with amp integration,how data lakehouse help with amp integration
guidance on whether to store data using temporal tables or just using columns for representing time in mssql server,good afternoon was just looking for some advice on how to store my data as know have multiple alternatives and not really sure which one is the best some background first have project where extracting data from yelp rest api which looking to store in regular sql server database and then from there ingest the data into dwh and after integrate an api so people can query the data this api might even implement ml algo have to see now the json data has this structure approximately see the image below and there also other endpoints from which plan on ingesting data but this here is the focus of my question with that in mind know making decisions about design always should have the end goal in mind so the sql server database would serve as way for users to do time series analysis on businesses rating and review count and see how that changes over time so because the json shows the current review count of business decided to treat the data as one would treat stock data and just add new records at set time each day at least initially as would like to change this later on so technically this would involve incremental data loads however with this in mind came across this feature of temporal tables and in the microsoft documentation specifically in the section titled oltp with auto generated data history they have an usage scenario regarding inventory management which is technically the same as my usage scenario for example they are maintaining the inventory count of product and in my case doing the same thing but with business review count so now not sure whether should go ahead with the temporal tables because not sure if there could be unforeseen negative consequences in the future or if just setting up datetime columns apart from temporal table would be better maybe they re accomplishing the same thing and temporal tables are more simple to use maybe coming at this from completely erroneous perspective would like to add as well that plan on using temporal tales regardless on other tables in my db as would like to implement scd in my dwh for the potential changes in the dimensions and also if it helps ve thought about changing the way represent changes in businesses review count by just programming way for difference in the review count between the businesses current state let say today and previous state let say it last place in the table and like so creating sort of transaction based table that just adds reviews into the table so kind of like purchases but not quite immutable data so hope explained myself and would be willing to provide lot more information if you made it this far already grateful haha any suggestions help is welcome thanks in advance format png amp auto webp amp cb7c080f99b0c6dd154d0c3bf24b5bc248def600,0,guidance on whether to store data using temporal table or just using column for representing time in mssql server,good afternoon wa just looking for some advice on how to store my data a know have multiple alternative and not really sure which one is the best some background first have project where extracting data from yelp rest api which looking to store in regular sql server database and then from there ingest the data into dwh and after integrate an api so people can query the data this api might even implement ml algo have to see now the json data ha this structure approximately see the image below and there also other endpoint from which plan on ingesting data but this here is the focus of my question with that in mind know making decision about design always should have the end goal in mind so the sql server database would serve a way for user to do time series analysis on business rating and review count and see how that change over time so because the json show the current review count of business decided to treat the data a one would treat stock data and just add new record at set time each day at least initially a would like to change this later on so technically this would involve incremental data load however with this in mind came across this feature of temporal table and in the microsoft documentation specifically in the section titled oltp with auto generated data history they have an usage scenario regarding inventory management which is technically the same a my usage scenario for example they are maintaining the inventory count of product and in my case doing the same thing but with business review count so now not sure whether should go ahead with the temporal table because not sure if there could be unforeseen negative consequence in the future or if just setting up datetime column apart from temporal table would be better maybe they re accomplishing the same thing and temporal table are more simple to use maybe coming at this from completely erroneous perspective would like to add a well that plan on using temporal tale regardless on other table in my db a would like to implement scd in my dwh for the potential change in the dimension and also if it help ve thought about changing the way represent change in business review count by just programming way for difference in the review count between the business current state let say today and previous state let say it last place in the table and like so creating sort of transaction based table that just add review into the table so kind of like purchase but not quite immutable data so hope explained myself and would be willing to provide lot more information if you made it this far already grateful haha any suggestion help is welcome thanks in advance format png amp auto webp amp cb7c080f99b0c6dd154d0c3bf24b5bc248def600
eu folks frankfurt or zurich,have an offer from both frankfurt and zurich based org same role and responsibilities frankurt euro zurich chf zurich seems to have high cost of living which option you think would be better have years of experience with years in data engineering any good input is appreciated,0,eu folk frankfurt or zurich,have an offer from both frankfurt and zurich based org same role and responsibility frankurt euro zurich chf zurich seems to have high cost of living which option you think would be better have year of experience with year in data engineering any good input is appreciated
so data engineering jobs are starting to ask for statistics and ml knowledge too,have job notification alert set on linkedin for data engineering and have noticed over the past several days that there an increase in statistics ml knowledge combined with the etl elt dwh experience is this something that we should expect to rise in the future,0,so data engineering job are starting to ask for statistic and ml knowledge too,have job notification alert set on linkedin for data engineering and have noticed over the past several day that there an increase in statistic ml knowledge combined with the etl elt dwh experience is this something that we should expect to rise in the future
how to implement aws cdk to manage data engineering stack,how to implement aws cdk to manage data engineering stack,0,how to implement aws cdk to manage data engineering stack,how to implement aws cdk to manage data engineering stack
gcp dataflow to bigquery batch loading in java,hey everyone am absolute beginner in data engineering trying to work out some basic flows on managed services like dataflow beam taking files from gcs bucket transformation cleaning amp quality checks putting to bigquery for analysing have done this thing in python but now came to know that java is actually well loaded compared to python beam so am trying to accomplish this am stuck at how to go about this searched on google and found many ready projects but all including maven for building and maintening the project tried to compile the sample projects project no in maven archetype selection only it gave me lot of tough time to solve some dependency errors in pom xml amp yet not compiled am using intellij for it is it possible to create standalone without maven java program to do this can do it without maven or java maven is the only way are there any working maven projects around this specifically for beam java combination any suggestions help would be great,0,gcp dataflow to bigquery batch loading in java,hey everyone am absolute beginner in data engineering trying to work out some basic flow on managed service like dataflow beam taking file from gc bucket transformation cleaning amp quality check putting to bigquery for analysing have done this thing in python but now came to know that java is actually well loaded compared to python beam so am trying to accomplish this am stuck at how to go about this searched on google and found many ready project but all including maven for building and maintening the project tried to compile the sample project project no in maven archetype selection only it gave me lot of tough time to solve some dependency error in pom xml amp yet not compiled am using intellij for it is it possible to create standalone without maven java program to do this can do it without maven or java maven is the only way are there any working maven project around this specifically for beam java combination any suggestion help would be great
hi dataengineering the th issue of my unofficial bigquery newsletter is out this is special one as it includes news about snowflake redshift and even this sub,hi dataengineering the th issue of my unofficial bigquery newsletter is out this is special one as it includes news about snowflake redshift and even this sub,0,hi dataengineering the th issue of my unofficial bigquery newsletter is out this is special one a it includes news about snowflake redshift and even this sub,hi dataengineering the th issue of my unofficial bigquery newsletter is out this is special one a it includes news about snowflake redshift and even this sub
how many days of paid training to you get per year,my current company offers fully paid days per year which we can take for extra training eg online courses not completely off topic but within broader range of technical interests what about you view poll,0,how many day of paid training to you get per year,my current company offer fully paid day per year which we can take for extra training eg online course not completely off topic but within broader range of technical interest what about you view poll
gcp bq vs snowflake vs databricks deltalake,deleted earlier post cuz it wasn worded right what are the main advantages disadvantages that factor into choosing one of these options over the other,0,gcp bq v snowflake v databricks deltalake,deleted earlier post cuz it wasn worded right what are the main advantage disadvantage that factor into choosing one of these option over the other
gcp vs snowflake vs databricks deltalake,what are the main advantages disadvantages that factor into choosing one of these options over the other,0,gcp v snowflake v databricks deltalake,what are the main advantage disadvantage that factor into choosing one of these option over the other
am in any sort of position to ask for pay rise,long story short in my first job in the field as an etl developer data analyst another junior and myself were hired to supplement the two etl devs been there years each for few months while they focus on getting on top of their work then we were going be placed on the data insights team doing analysis months in both etl devs resigned meaning that my colleague and have become responsible their workload which is extremely critical to the business function we are expected to maintain this workload until the processes are automated which we are involved in too this is estimated to take months just to be clear am aware that may come off as entitled and ungrateful but truly am not and am extremely grateful for the challenges and opportunities to prove myself however can help but feel bit under appreciated especially while knowing we are being paid and considered as juniors even though we have taken on work that mid senior developers were probably paid twice as much to do if was to ask for compensation would it be too early considering ve only been there for months,0,am in any sort of position to ask for pay rise,long story short in my first job in the field a an etl developer data analyst another junior and myself were hired to supplement the two etl devs been there year each for few month while they focus on getting on top of their work then we were going be placed on the data insight team doing analysis month in both etl devs resigned meaning that my colleague and have become responsible their workload which is extremely critical to the business function we are expected to maintain this workload until the process are automated which we are involved in too this is estimated to take month just to be clear am aware that may come off a entitled and ungrateful but truly am not and am extremely grateful for the challenge and opportunity to prove myself however can help but feel bit under appreciated especially while knowing we are being paid and considered a junior even though we have taken on work that mid senior developer were probably paid twice a much to do if wa to ask for compensation would it be too early considering ve only been there for month
managing schema changes when using drag and drop dashboarding tool,about to set up db for superset analytics consider it as running on top of data warehouse the only function of that db is to be the storage for visualization used to operating in situations where you have db gt flask api with sqlalchemy gt react front end this gives you enough opportunities for abstraction to smoothly manage schema changes how do you manage schema change when your viz tool is linked directly to the db,0,managing schema change when using drag and drop dashboarding tool,about to set up db for superset analytics consider it a running on top of data warehouse the only function of that db is to be the storage for visualization used to operating in situation where you have db gt flask api with sqlalchemy gt react front end this give you enough opportunity for abstraction to smoothly manage schema change how do you manage schema change when your viz tool is linked directly to the db
please review my resume data engineer wanna be with yoe in analytics,hi guys recently have been trying to transition into data engineering from data analytics have yoe working as data scientist and data analyst and in my current data analyst role building etl pipelines data warehousing and bi reporting mainly with python ms sql and power bi ve learned some other data engineering techs in my spare time and also listed them on my resume appreciate your time in reviewing my resume based in australia btw,0,please review my resume data engineer wanna be with yoe in analytics,hi guy recently have been trying to transition into data engineering from data analytics have yoe working a data scientist and data analyst and in my current data analyst role building etl pipeline data warehousing and bi reporting mainly with python m sql and power bi ve learned some other data engineering tech in my spare time and also listed them on my resume appreciate your time in reviewing my resume based in australia btw
aws dms for cdc from source likely postgres rds,hello has anyone implemented cdc using dms in production scale systems designed pipeline using dms but now realize there is too much dependency on the source db setup changing internal params additional load on source db for wal logs etc am also realizing too many limitations on what the tool can replicate to targets source postgresql html going by all of this am second guessing if dms can be scalable choice considering might want to use the similar approach for multiple objects tables in the future wondering if anyone can share their experience in real time thank you,0,aws dm for cdc from source likely postgres rds,hello ha anyone implemented cdc using dm in production scale system designed pipeline using dm but now realize there is too much dependency on the source db setup changing internal params additional load on source db for wal log etc am also realizing too many limitation on what the tool can replicate to target source postgresql html going by all of this am second guessing if dm can be scalable choice considering might want to use the similar approach for multiple object table in the future wondering if anyone can share their experience in real time thank you
post your data engineering black friday resources here,this is just thread where you can drop link or discount code for the community to use to learn new things or brush up on old skills ll start leetcode premium annual thanks20201 pluralsight annual off standard and premium cloud guru annual utm_medium hmepgtkovrp amp utm_campaign blackfriday off basic and plus dataquest not sure yet stay tuned vik tweeted about discounts to monthly plan but the only discount see is off annual have personally used all four resources leetcode premium is great to build sql skills and learn things like algorithms pluralsight actually acquired acg recently and used acg to pass few aws exams used dataquest when they first launched probably over years ago so not confident in this resource but hoping others can chime in thanks and happy holidays,0,post your data engineering black friday resource here,this is just thread where you can drop link or discount code for the community to use to learn new thing or brush up on old skill ll start leetcode premium annual thanks20201 pluralsight annual off standard and premium cloud guru annual utm_medium hmepgtkovrp amp utm_campaign blackfriday off basic and plus dataquest not sure yet stay tuned vik tweeted about discount to monthly plan but the only discount see is off annual have personally used all four resource leetcode premium is great to build sql skill and learn thing like algorithm pluralsight actually acquired acg recently and used acg to pas few aws exam used dataquest when they first launched probably over year ago so not confident in this resource but hoping others can chime in thanks and happy holiday
what are some tasks you wish to delegate,as title says what kind of tasks you face constantly wishing that there was maybe some junior in the team who could even benefit from doing it,0,what are some task you wish to delegate,a title say what kind of task you face constantly wishing that there wa maybe some junior in the team who could even benefit from doing it
which languages frameworks is must to know learn for six figure salary de job,if you re hire manager or an hr what most important for you,0,which language framework is must to know learn for six figure salary de job,if you re hire manager or an hr what most important for you
building api templates for customers,hi de folks here is my problem statement the company work for has an api which our customers can use to get information about how they are using our service our customers should be considered to fall into two categories some customers will be semi sophisticated they will be able to or have access to someone who is able to understand and use cloud services some customers will be unsophisticated picture someone who knows how to export to csv and pivot in excel we would like to enable as many of our customers to access our api store the results and use sql to join and aggregate the api responses together to answer their own business problems here is my question are there any products or solutions out there should know about which would allow us to templatize example api queries and subsequent data operations eg sql my best solution right now is to document the process of setting up flow in google cloud write python script hosted on google cloud instance to query our api store the results using google storage port the data over to be accessible in bigquery write sql profit are there better solutions that process is going to be heavy or unrealistic for most of our customers,0,building api template for customer,hi de folk here is my problem statement the company work for ha an api which our customer can use to get information about how they are using our service our customer should be considered to fall into two category some customer will be semi sophisticated they will be able to or have access to someone who is able to understand and use cloud service some customer will be unsophisticated picture someone who know how to export to csv and pivot in excel we would like to enable a many of our customer to access our api store the result and use sql to join and aggregate the api response together to answer their own business problem here is my question are there any product or solution out there should know about which would allow u to templatize example api query and subsequent data operation eg sql my best solution right now is to document the process of setting up flow in google cloud write python script hosted on google cloud instance to query our api store the result using google storage port the data over to be accessible in bigquery write sql profit are there better solution that process is going to be heavy or unrealistic for most of our customer
need help with designing infrastructure for new company,am lone de tasked with building new infrastructure for startup here are there needs and my ideas raw data is sourced from different hospitals delivered via sftp or email format is csv excel etc data is delivered on monthly or daily basis and can range from few gigs to hundreds of gigs my idea here is use airflow prefect for orchestration spark for heavy data processing and loading and use snowflake for our warehouse kubernetes maybe but seems like lot for one dude running the show all raw data must be transformed into standard data model the company uses data scientists must be able to access data via sql to run ml models on some heavy processing must be done such deduplication and record merging again think snowflake is good choice here can handle large amounts of data that is readily accessible the company also wants the data to be versioned honestly have no experience here but first thing that comes to mind is routine backups and audit tables but sure there are other tools out there for this versioning is ds can reproduce ml output and fda requirements lastly we need data quality checks checks for completeness and expected distributions have some experience with great expectations and seems like the right tool for the job here how does all this sounds any other recommendations for infrastructure,0,need help with designing infrastructure for new company,am lone de tasked with building new infrastructure for startup here are there need and my idea raw data is sourced from different hospital delivered via sftp or email format is csv excel etc data is delivered on monthly or daily basis and can range from few gig to hundred of gig my idea here is use airflow prefect for orchestration spark for heavy data processing and loading and use snowflake for our warehouse kubernetes maybe but seems like lot for one dude running the show all raw data must be transformed into standard data model the company us data scientist must be able to access data via sql to run ml model on some heavy processing must be done such deduplication and record merging again think snowflake is good choice here can handle large amount of data that is readily accessible the company also want the data to be versioned honestly have no experience here but first thing that come to mind is routine backup and audit table but sure there are other tool out there for this versioning is d can reproduce ml output and fda requirement lastly we need data quality check check for completeness and expected distribution have some experience with great expectation and seems like the right tool for the job here how doe all this sound any other recommendation for infrastructure
how to design data architecture for fast ad hoc analytics if the data model is not known priori,we have several clients for whom we need to create bi tool looks like good opportunity to look ahead and try to build saas tool since we already have the clients to pilot with the idea at its simplest client connects their datasource and our tool shows dashboard with insights we already have mvp which our clients like but works only with manually uploaded csv files with small amounts of data our next step is to design scalable data architecture we have very little hands on experience with this currently we are just doing some research the problem we see is that we do not know data schemas priori each client has different dataset with different schema so we cannot simply design normalized data model for fast ad hoc queries which will work with everybody or can we what are some good approaches to this datasources may be log of http requests event tracking json relational database csv file challenges we identified so far client data schema may change in time change of datatypes renaming adding removing columns etc we need to keep track of changes in time eg changes of attributes for an entity forgive me if use incorrect terminology still learning,0,how to design data architecture for fast ad hoc analytics if the data model is not known priori,we have several client for whom we need to create bi tool look like good opportunity to look ahead and try to build saas tool since we already have the client to pilot with the idea at it simplest client connects their datasource and our tool show dashboard with insight we already have mvp which our client like but work only with manually uploaded csv file with small amount of data our next step is to design scalable data architecture we have very little hand on experience with this currently we are just doing some research the problem we see is that we do not know data schema priori each client ha different dataset with different schema so we cannot simply design normalized data model for fast ad hoc query which will work with everybody or can we what are some good approach to this datasources may be log of http request event tracking json relational database csv file challenge we identified so far client data schema may change in time change of datatypes renaming adding removing column etc we need to keep track of change in time eg change of attribute for an entity forgive me if use incorrect terminology still learning
snowplow meet up festive gathering snowplow,hi join us for festive meet up in london on wed th december interesting talks drinks food and networking raffle and crazy golf amp x200b date time pm gmt venue shoreditch balls old street london registration here utm_medium post amp utm_campaign xmas meetup amp utm_content,0,snowplow meet up festive gathering snowplow,hi join u for festive meet up in london on wed th december interesting talk drink food and networking raffle and crazy golf amp x200b date time pm gmt venue shoreditch ball old street london registration here utm_medium post amp utm_campaign xmas meetup amp utm_content
pipeline documenting,curious how the everyone handles pipeline documentation in this context referring to documenting the pipeline itself use case source where data is stored during its lifecycle transformation specs etc as opposed to data validation data quality checks on the data itself,0,pipeline documenting,curious how the everyone handle pipeline documentation in this context referring to documenting the pipeline itself use case source where data is stored during it lifecycle transformation spec etc a opposed to data validation data quality check on the data itself
strategies have you used to sync postgresql database with redshift,what strategies have all used in the past to sync data between postgres instance into redshift cluster amp x200b context at my job we have postgres instance that serves as an application backend and redshift cluster for warehousing both of these are hosted on aws we would like to be able to replicate data from postgres to redshift and model it in our warehouse in the past have used presto or now trino as data lake solution which conveniently has built in connector that solved our syncing issues ve done little research and seems that aws dms could be solution but wonder how well it handles on going replication and am curious if anyone here as used it another option would be to leverage the copy command from postgres but fell like that approach will not scale well for our amount of data amp x200b what systems have you built in the past that have worked well or not for you,0,strategy have you used to sync postgresql database with redshift,what strategy have all used in the past to sync data between postgres instance into redshift cluster amp x200b context at my job we have postgres instance that serf a an application backend and redshift cluster for warehousing both of these are hosted on aws we would like to be able to replicate data from postgres to redshift and model it in our warehouse in the past have used presto or now trino a data lake solution which conveniently ha built in connector that solved our syncing issue ve done little research and seems that aws dm could be solution but wonder how well it handle on going replication and am curious if anyone here a used it another option would be to leverage the copy command from postgres but fell like that approach will not scale well for our amount of data amp x200b what system have you built in the past that have worked well or not for you
does anyone know about test questions in the book designing data intensive applications by martin kleppmann ddia to revise concepts learned,hi am looking for textbook like questions in the book designing data intensive applications by martin kleppmann this is very popular book feel someone must have prepared the questions on the concepts inside the book am looking for some questions or some tests on the same,0,doe anyone know about test question in the book designing data intensive application by martin kleppmann ddia to revise concept learned,hi am looking for textbook like question in the book designing data intensive application by martin kleppmann this is very popular book feel someone must have prepared the question on the concept inside the book am looking for some question or some test on the same
data engineering salaries in sweden,ve finished my tech and managerial leadership rounds at swedish tech product mid size startup and have my salary discussion coming up soon looking for starting point range for the same ve checked out glassdoor and www lonestatistik se without much helps any all pointers will be very appreciated thanks about me data engineer senior with years of work ex with european us based tech companies work specifically in data engineering data modeling and business intelligence,0,data engineering salary in sweden,ve finished my tech and managerial leadership round at swedish tech product mid size startup and have my salary discussion coming up soon looking for starting point range for the same ve checked out glassdoor and www lonestatistik se without much help any all pointer will be very appreciated thanks about me data engineer senior with year of work ex with european u based tech company work specifically in data engineering data modeling and business intelligence
how to build on premise data lake using open source tools,hello folks follow this community for quite while now and found it as an incredible helpful source to get started with data engineering however am recently bit stuck on the topic on how to build data lake on premise using open source tools let me give you some context am currently working part time at young company while also studying in graduate data science program currently am the only one working on this task and my knowledge of data engineering is limited to these topics so far know how to build and use relational databases recently used postgresql for this experimented with etl orchestration frameworks such as airflow and argo workflows events at former company worked at got into touch with graphql where queried the data needed which was stored at aws experimented recently with object storage such as minio recently tried to combine minio with trino to query the data in the object storage with rather limited success the company am working for right now is operating in domain where it is not possible to upload the data to one of the common cloud providers my task is to find data management solution the constraints are that it should take in any kind of data that could be binary data image data csv json ets currently the data is stored in files on server where it is used to build pocs by other team members there is no senior data engineer on the team ideally everything should work in kubernetes know this could be better basis to start with but this is what have right now try to find as much information as possible on the internet or books like these the enterprise big data lake trino the definitive guide designing data intensive applications data pipelines pocket reference building machine learning pipelines but am at stage where many authors may it be online or offline in texts or videos either just explain the very basics what is an object storage or they talk about very sophisticated architectures which can be found at airbnb uber and like often at one of the big cloud providers what need is the step in the middle and on premise how do build data lake that holds data of different levels of transformations and make this available to the team members currently would do the following using tools that work in kubernetes but ignore this for now am building poc which runs locally setup minio and create different zones such as raw refined trusted etc with these different zones could manage the access rights and make data available at different levels of processing available for the respective users setup the injestion etls using either argo or kubeflow the data from these pipelines will land in the raw zone transform data from the raw zone to the subsequent zones using argo or kubeflow etls make the data in the different zones searchable queryable how the last point is my biggest blocking point read that you need to catalogue your data using metadata to make it queryable but what is the best practice for this would add some metadata during the injestion period and store this and the file path to the object in object store in an relational db to make it queryable maybe also an indicator of level of transformation so the user knows whether it is available in subsequent bucket would be immensely happy if some of you could provide me with some insights on how to build such thing or at least how to make my current setup queryable my online research is already pretty exhausted and the only thing find now is new tool that promises to give you all but isn open source thanks to all of you and looking forward to the discussion,0,how to build on premise data lake using open source tool,hello folk follow this community for quite while now and found it a an incredible helpful source to get started with data engineering however am recently bit stuck on the topic on how to build data lake on premise using open source tool let me give you some context am currently working part time at young company while also studying in graduate data science program currently am the only one working on this task and my knowledge of data engineering is limited to these topic so far know how to build and use relational database recently used postgresql for this experimented with etl orchestration framework such a airflow and argo workflow event at former company worked at got into touch with graphql where queried the data needed which wa stored at aws experimented recently with object storage such a minio recently tried to combine minio with trino to query the data in the object storage with rather limited success the company am working for right now is operating in domain where it is not possible to upload the data to one of the common cloud provider my task is to find data management solution the constraint are that it should take in any kind of data that could be binary data image data csv json ets currently the data is stored in file on server where it is used to build pocs by other team member there is no senior data engineer on the team ideally everything should work in kubernetes know this could be better basis to start with but this is what have right now try to find a much information a possible on the internet or book like these the enterprise big data lake trino the definitive guide designing data intensive application data pipeline pocket reference building machine learning pipeline but am at stage where many author may it be online or offline in text or video either just explain the very basic what is an object storage or they talk about very sophisticated architecture which can be found at airbnb uber and like often at one of the big cloud provider what need is the step in the middle and on premise how do build data lake that hold data of different level of transformation and make this available to the team member currently would do the following using tool that work in kubernetes but ignore this for now am building poc which run locally setup minio and create different zone such a raw refined trusted etc with these different zone could manage the access right and make data available at different level of processing available for the respective user setup the injestion etls using either argo or kubeflow the data from these pipeline will land in the raw zone transform data from the raw zone to the subsequent zone using argo or kubeflow etls make the data in the different zone searchable queryable how the last point is my biggest blocking point read that you need to catalogue your data using metadata to make it queryable but what is the best practice for this would add some metadata during the injestion period and store this and the file path to the object in object store in an relational db to make it queryable maybe also an indicator of level of transformation so the user know whether it is available in subsequent bucket would be immensely happy if some of you could provide me with some insight on how to build such thing or at least how to make my current setup queryable my online research is already pretty exhausted and the only thing find now is new tool that promise to give you all but isn open source thanks to all of you and looking forward to the discussion
how to handle gdpr requests for data stored in s3,is there any way to delete or update records stored in s3 file on aws we do get request to remove some data records from our entire data lake it easy to remove from database but how to remove it from file based things any idea any help is highly appreciated thanks in advanced,0,how to handle gdpr request for data stored in s3,is there any way to delete or update record stored in s3 file on aws we do get request to remove some data record from our entire data lake it easy to remove from database but how to remove it from file based thing any idea any help is highly appreciated thanks in advanced
the us dream,am from india want to move to us and settle there can get international job offer in data engineer role from india has anyone done that,0,the u dream,am from india want to move to u and settle there can get international job offer in data engineer role from india ha anyone done that
data transformation tools like dbt but visual,hi all please recommend if any visual transformation tools for data warehouses in elt space similar to dbt but visual,0,data transformation tool like dbt but visual,hi all please recommend if any visual transformation tool for data warehouse in elt space similar to dbt but visual
building personalized product recommender system with vector database,how are vector databases applied to product recommender systems for anyone interested this is use case from an commerce company link amp sk ea2fbd59333da4b5b3994d5f7083032 the overall architecture consists of two main parts write process the item feature vectors generated by the deep learning model are normalized and written into mysql mysql then reads the processed item feature vectors using the data synchronization tool etl and imports them into the vector database milvus read process the search service obtains user preference feature vectors based on user query keywords and user portraits queries similar vectors in milvus and recalls topk item vectors,0,building personalized product recommender system with vector database,how are vector database applied to product recommender system for anyone interested this is use case from an commerce company link amp sk ea2fbd59333da4b5b3994d5f7083032 the overall architecture consists of two main part write process the item feature vector generated by the deep learning model are normalized and written into mysql mysql then read the processed item feature vector using the data synchronization tool etl and import them into the vector database milvus read process the search service obtains user preference feature vector based on user query keywords and user portrait query similar vector in milvus and recall topk item vector
sql server set up linked server,hi sorry for relatively beginner question am trying to set up linked server on localhost db to an sql managed instance on azure can connect from the object explored to the managed instance however cannot set up linked server using the exact same server name and login info below is an image to show the problem any help would be good thanks,0,sql server set up linked server,hi sorry for relatively beginner question am trying to set up linked server on localhost db to an sql managed instance on azure can connect from the object explored to the managed instance however cannot set up linked server using the exact same server name and login info below is an image to show the problem any help would be good thanks
best learning resources,want to learn kafka airflow pyspark and git any recommendations,0,best learning resource,want to learn kafka airflow pyspark and git any recommendation
fb meta data engineering virtual onsite interviews,looking for tips and advice for fb meta onsite interviews am preparing leetcode mediums and lots of sql but not sure about the way it conducted some posts mentioned that they will ask to build data models etc can anyone shed light on how it goes any resources that can refer to prepare any insights from someone who recently attended will be very helpful other general tips are welcome too thanks,0,fb meta data engineering virtual onsite interview,looking for tip and advice for fb meta onsite interview am preparing leetcode medium and lot of sql but not sure about the way it conducted some post mentioned that they will ask to build data model etc can anyone shed light on how it go any resource that can refer to prepare any insight from someone who recently attended will be very helpful other general tip are welcome too thanks
is conformed curated harmonized layer necessary in lakehouse architecture,just started at company in the process of building their analytics platform using dbt delta lake to be used for reporting and analyst queries while there are multiple data layers in delta lake there is no one layer like conformed curated harmonized layer layer containing the all the data organized for analytical purposes rather than transactional raw tables using for example star snowflake schema or denormalization etc all of my previous work on data lakes stressed heavy importance on this layer however the current team in is somewhat against it the thinking is that it will be too much to maintain and is not necessary instead analyst tables views are built directly off of raw layer tables my gut is telling me that as the platform grows the data duplication disorganization downstream of raw will come back to haunt us but right now the architecture is working fine anyone have experience thoughts on this,0,is conformed curated harmonized layer necessary in lakehouse architecture,just started at company in the process of building their analytics platform using dbt delta lake to be used for reporting and analyst query while there are multiple data layer in delta lake there is no one layer like conformed curated harmonized layer layer containing the all the data organized for analytical purpose rather than transactional raw table using for example star snowflake schema or denormalization etc all of my previous work on data lake stressed heavy importance on this layer however the current team in is somewhat against it the thinking is that it will be too much to maintain and is not necessary instead analyst table view are built directly off of raw layer table my gut is telling me that a the platform grows the data duplication disorganization downstream of raw will come back to haunt u but right now the architecture is working fine anyone have experience thought on this
udacity data engineering nanodegree review should you enroll,udacity data engineering nanodegree review should you enroll,0,udacity data engineering nanodegree review should you enroll,udacity data engineering nanodegree review should you enroll
designing database as beginner,tl dr need tips learning ressources with sql where of the data would be manually inputed amp x200b ve decided to build crud application for family printing business where employers would input information customers details name company name phone number adress place order date item amount price stock new merchandise amount left would get calculated based on order believe we would get at most daily new data points small companies have some knowledge in python and just diving into sql no choice guess starting on leetcode am still trying to figure out the best approach to design such structure ve just bought designing data intensive applications so hopefully it will help mainly want to hear about good practices tips and learning ressources do need to think thoroughly about the data modeling or general overview is fine can possibly think of every single columns in advance would adding the data daily be storage problem or slow down the crud app how do go on about keeping the customers data secure is cloud platform needed or only good to have thanks for reading,0,designing database a beginner,tl dr need tip learning ressources with sql where of the data would be manually inputed amp x200b ve decided to build crud application for family printing business where employer would input information customer detail name company name phone number adress place order date item amount price stock new merchandise amount left would get calculated based on order believe we would get at most daily new data point small company have some knowledge in python and just diving into sql no choice guess starting on leetcode am still trying to figure out the best approach to design such structure ve just bought designing data intensive application so hopefully it will help mainly want to hear about good practice tip and learning ressources do need to think thoroughly about the data modeling or general overview is fine can possibly think of every single column in advance would adding the data daily be storage problem or slow down the crud app how do go on about keeping the customer data secure is cloud platform needed or only good to have thanks for reading
lesson learned meme good watermark bad here another de flavored meme as compensation,lesson learned meme good watermark bad here another de flavored meme as compensation,0,lesson learned meme good watermark bad here another de flavored meme a compensation,lesson learned meme good watermark bad here another de flavored meme a compensation
using different schemas in dbt for model,good evening was look to see if someone could assist me with solving an issue where looking to use model which contains tables from different schemas the problem is that when compiled all the tables are queried as if they were in the same schema checked the documentation but it didn really assist much,0,using different schema in dbt for model,good evening wa look to see if someone could assist me with solving an issue where looking to use model which contains table from different schema the problem is that when compiled all the table are queried a if they were in the same schema checked the documentation but it didn really assist much
wlb as fb data engineer,consultant that ll be working as data engineer at fb was wondering if anyone with knowledge of the role can comment on the work life balance,0,wlb a fb data engineer,consultant that ll be working a data engineer at fb wa wondering if anyone with knowledge of the role can comment on the work life balance
transitioning from software engineer to data engineer,want to work towards fully remote job as soon as possible maybe contracting job aware that might not be possible for my first foray just want some advice to see if working in the right direction and whether there anything can be doing better currently working part time as fully remote software developer ve been doing this for close to two years now set to graduate with bachelors in computer science in january to prepare for transition to data engineering ve been completing dataquest course which has given me good basis in sql and should start me off in building data pipeline once finish with dataquest plan to start working on my own data engineering related github projects also working on getting an aws cloud practitioner certification figure this might help me stand out as an entry level applicant what else can do to land that first job any good places for data engineer related interview questions what job titles should be looking for etl developer amp x200b tl dr soon to have bs in comp sci years remote software dev experience preparing for data engineering job with data quest aws certificate and personal projects how else can get ahead,0,transitioning from software engineer to data engineer,want to work towards fully remote job a soon a possible maybe contracting job aware that might not be possible for my first foray just want some advice to see if working in the right direction and whether there anything can be doing better currently working part time a fully remote software developer ve been doing this for close to two year now set to graduate with bachelor in computer science in january to prepare for transition to data engineering ve been completing dataquest course which ha given me good basis in sql and should start me off in building data pipeline once finish with dataquest plan to start working on my own data engineering related github project also working on getting an aws cloud practitioner certification figure this might help me stand out a an entry level applicant what else can do to land that first job any good place for data engineer related interview question what job title should be looking for etl developer amp x200b tl dr soon to have b in comp sci year remote software dev experience preparing for data engineering job with data quest aws certificate and personal project how else can get ahead
how to efficiently bulk analyze json data from rest api,hello am tasked with analyzing data from rest api json data basically am currently normalizing the data into tabular format and saving as csv files with the intent to later load them into database to then do aggregations or summarizations but ve been wondering if it would be better to leave the data as is json then load the data into nosql database the normalization process can be tedious as have to study the structure of the raw data and then utilize python pandas to normalize the thing is have no experience with nosql databases and wondering if can do aggregations and summarizations with them assuming so the company does have snowflake environment but havent looked at its json support am familiar with postgresql which think its recent versions does support json jsonb but not sure if will have access to postgres environment should just bite the bullet and pursue dumping the json files into nosql database or continue with normalizing the data into tabular format or pursue something else,0,how to efficiently bulk analyze json data from rest api,hello am tasked with analyzing data from rest api json data basically am currently normalizing the data into tabular format and saving a csv file with the intent to later load them into database to then do aggregation or summarization but ve been wondering if it would be better to leave the data a is json then load the data into nosql database the normalization process can be tedious a have to study the structure of the raw data and then utilize python panda to normalize the thing is have no experience with nosql database and wondering if can do aggregation and summarization with them assuming so the company doe have snowflake environment but havent looked at it json support am familiar with postgresql which think it recent version doe support json jsonb but not sure if will have access to postgres environment should just bite the bullet and pursue dumping the json file into nosql database or continue with normalizing the data into tabular format or pursue something else
the snapshot that couldn cdc kafka connect and debugging,might be of interest here story from the data engineering trenches one that taught me some lessons about debugging root cause analysis and kafka connect,0,the snapshot that couldn cdc kafka connect and debugging,might be of interest here story from the data engineering trench one that taught me some lesson about debugging root cause analysis and kafka connect
practical event driven microservices architecture,practical event driven microservices architecture,0,practical event driven microservices architecture,practical event driven microservices architecture
dimensional modelling on redshift,guys have sql server target system on which we stored the data previously also we used to do etl using ssis packages our source system is migrating and we need to rebuild the pipeline for our etl and also the dimensional model understood we could do etl on redshift as well as on glue and dwh on redshift just wanted some opinion on this matter what should prefer cost wise and also we have never used python in the team before and just sql for data transformation have researched on redshift and it does not enforce pk fk constraints for our dimensional model let me know how to fill in fact table and dimensional table in this scenario any opinions and resources will be highly appreciated,0,dimensional modelling on redshift,guy have sql server target system on which we stored the data previously also we used to do etl using ssis package our source system is migrating and we need to rebuild the pipeline for our etl and also the dimensional model understood we could do etl on redshift a well a on glue and dwh on redshift just wanted some opinion on this matter what should prefer cost wise and also we have never used python in the team before and just sql for data transformation have researched on redshift and it doe not enforce pk fk constraint for our dimensional model let me know how to fill in fact table and dimensional table in this scenario any opinion and resource will be highly appreciated
how do big companies like facebook handle their enormous user activity data,popular apps like fb ig twitter obviously have huge amounts of user activity data generated every second how do they set up their pipelines to handle this do their application servers just send data directly to an insanely huge distributed message system like kafka or is there more clever ways to do this,0,how do big company like facebook handle their enormous user activity data,popular apps like fb ig twitter obviously have huge amount of user activity data generated every second how do they set up their pipeline to handle this do their application server just send data directly to an insanely huge distributed message system like kafka or is there more clever way to do this
how to build pipeline for analytics if the source data is made available only in excel files,we are in the process of setting up pipeline that is expected to transform the data available in excel files and make it available for analytics purpose using sap analytics cloud there are around excel files maximum mb each that we receive every week from different departments which we would like to process we already have access to snowflake and alteryx that could be used as part of the pipeline my questions are as follows is there any other tool that we could use in between to create this pipeline considering the data size do we really need to have dwh or could we instead use python to process the data and then feed it as an input to sap analytics cloud eliminating both snowflake and alteryx in between would be also happy to hear any other suggestions based on your experience of how you handled excel files to build pipeline,0,how to build pipeline for analytics if the source data is made available only in excel file,we are in the process of setting up pipeline that is expected to transform the data available in excel file and make it available for analytics purpose using sap analytics cloud there are around excel file maximum mb each that we receive every week from different department which we would like to process we already have access to snowflake and alteryx that could be used a part of the pipeline my question are a follows is there any other tool that we could use in between to create this pipeline considering the data size do we really need to have dwh or could we instead use python to process the data and then feed it a an input to sap analytics cloud eliminating both snowflake and alteryx in between would be also happy to hear any other suggestion based on your experience of how you handled excel file to build pipeline
data pipelines to monitor defi,hi everyone here great article on how to use open source tools to create data pipelines for monitoring defi they used python api with dagster data pipeline framework,0,data pipeline to monitor defi,hi everyone here great article on how to use open source tool to create data pipeline for monitoring defi they used python api with dagster data pipeline framework
transition from bi to data engineer job prospects and career growth,hi all hope this is the correct sub to post this question and am hoping will get some clarity from experienced folks here am currently experiencing career stagnation in my role as bi consultant had actually started as analytics consulting company where used to talk to clients and the ds in my team there was some dashboard building stuff which did along with sql writing couple of years passed by landed role with my client with substantial pay hike but the work was again mostly insights for sales writing sql and dashboard feel like have these golden handcuffs constantly feel out of touch with ds world and with very little modeling work recently am not sure if can even qualify for ds role anymore am sticking to the job just because the pay is good am at cross roads to decide where to go from now done feel confidant enough to pursue pure ds role that leaves me with traditional bi and de work the place where am only see ds positions and no bi position at all no eu and non us or feels like all traditional bi jave become ds roles suddenly so was thinking of transitioning into de full time since know sql hadoop python airflow lol bit dockwr as well but again the job market is little low in my area wanted to know from the folks in this sub do you feel de role slowly being absorbed into mle roles are the lines being blurrred between de and ds is devops also now part of de also because of my anxiousness decided to ourselves aws solution architect certificate to gain cloud knowledge is this something you would expect from de am sorry if this post comes across as naive bit done have any mentor at my workplace and reddit seemed the only place where can get some honest feedback,0,transition from bi to data engineer job prospect and career growth,hi all hope this is the correct sub to post this question and am hoping will get some clarity from experienced folk here am currently experiencing career stagnation in my role a bi consultant had actually started a analytics consulting company where used to talk to client and the d in my team there wa some dashboard building stuff which did along with sql writing couple of year passed by landed role with my client with substantial pay hike but the work wa again mostly insight for sale writing sql and dashboard feel like have these golden handcuff constantly feel out of touch with d world and with very little modeling work recently am not sure if can even qualify for d role anymore am sticking to the job just because the pay is good am at cross road to decide where to go from now done feel confidant enough to pursue pure d role that leaf me with traditional bi and de work the place where am only see d position and no bi position at all no eu and non u or feel like all traditional bi jave become d role suddenly so wa thinking of transitioning into de full time since know sql hadoop python airflow lol bit dockwr a well but again the job market is little low in my area wanted to know from the folk in this sub do you feel de role slowly being absorbed into mle role are the line being blurrred between de and d is devops also now part of de also because of my anxiousness decided to ourselves aws solution architect certificate to gain cloud knowledge is this something you would expect from de am sorry if this post come across a naive bit done have any mentor at my workplace and reddit seemed the only place where can get some honest feedback
fal run python scripts directly from your dbt project,hi friends wanted to show you an open source project that we have been working on we love dbt and we want to do more with it specifically we want to use it for our data science workloads and run python on our dbt models so we built fal to help us do that here is quick demo of it some examples of what you can do with fal download dbt models using familiar ref syntax as pandas dataframes send slack messages on model runs do forecasts on metrics do sentiment analysis on support tickets let us know if you have any feedback or want to see more examples,0,fal run python script directly from your dbt project,hi friend wanted to show you an open source project that we have been working on we love dbt and we want to do more with it specifically we want to use it for our data science workload and run python on our dbt model so we built fal to help u do that here is quick demo of it some example of what you can do with fal download dbt model using familiar ref syntax a panda dataframes send slack message on model run do forecast on metric do sentiment analysis on support ticket let u know if you have any feedback or want to see more example
common mistakes when building analytical data models designing data model for analytics is not the same as doing it for transactional processing you optimize for access patterns that are very different from row level data retrieval used in oltp systems,common mistakes when building analytical data models designing data model for analytics is not the same as doing it for transactional processing you optimize for access patterns that are very different from row level data retrieval used in oltp systems,0,common mistake when building analytical data model designing data model for analytics is not the same a doing it for transactional processing you optimize for access pattern that are very different from row level data retrieval used in oltp system,common mistake when building analytical data model designing data model for analytics is not the same a doing it for transactional processing you optimize for access pattern that are very different from row level data retrieval used in oltp system
kedro projects and kedro starters for data engineering and data science projects,kedro projects and kedro starters for data engineering and data science projects,0,kedro project and kedro starter for data engineering and data science project,kedro project and kedro starter for data engineering and data science project
needed advice on data governance in etl,am making an etl completely based in aws the data is stored in multiple s3 buckets and redshift what all data governance rules should focus on or comply to like data masking roles catalog etc any help is highly appreciated thanks in advance,0,needed advice on data governance in etl,am making an etl completely based in aws the data is stored in multiple s3 bucket and redshift what all data governance rule should focus on or comply to like data masking role catalog etc any help is highly appreciated thanks in advance
decent de courses at udemy,hi the famous discount event started at udemy what are the de courses you re looking for to stockpile,0,decent de course at udemy,hi the famous discount event started at udemy what are the de course you re looking for to stockpile
data engineering or data analyst without prior experience,hello might be asking something that has been asked before here but let me try have bachelor in cis concentrated around information systems security graduated in and didnt like it much so worked in low skills labor jobs like data clerk and cashier two months ago got exposed to data engineering and did my research to see what they do and got me really interested subbed to datacamp and im studied sql love sql and database in general it always made sense to me than any other programming languages and now im studying python and it has been going well with me also doing projects using pandas numpy and sqlite so can say that went from zero in sql and python to helping my baby brother in his python sql highschool courses now im not from us and europe live in uae in dubai and going through linkedin didnt find internship or junior data engineering and now hesitating wether should continue my studies in data engineering or should study data analyst if get to continue studying data engineering would want to connect with teams that need data engineering and can do it for free as long as get to use that experience in building my cv any idea where can connect with people globally,0,data engineering or data analyst without prior experience,hello might be asking something that ha been asked before here but let me try have bachelor in ci concentrated around information system security graduated in and didnt like it much so worked in low skill labor job like data clerk and cashier two month ago got exposed to data engineering and did my research to see what they do and got me really interested subbed to datacamp and im studied sql love sql and database in general it always made sense to me than any other programming language and now im studying python and it ha been going well with me also doing project using panda numpy and sqlite so can say that went from zero in sql and python to helping my baby brother in his python sql highschool course now im not from u and europe live in uae in dubai and going through linkedin didnt find internship or junior data engineering and now hesitating wether should continue my study in data engineering or should study data analyst if get to continue studying data engineering would want to connect with team that need data engineering and can do it for free a long a get to use that experience in building my cv any idea where can connect with people globally
migrating on prem spark jobs to google dataproc,hello am looking for some advice am researching on how to migrating spark jobs to dataproc what are the changes like code refactoring cluster config required for it the google documentation only talks about changing references from hdfs to gs assuming data has been migrated to cloud storage could think about creating dataproc vm images similar to on prem hadoop with dependencies what are other changes required in your opinion does anyone has experienced difficulty on doing this any tips and advice would be great thanks,0,migrating on prem spark job to google dataproc,hello am looking for some advice am researching on how to migrating spark job to dataproc what are the change like code refactoring cluster config required for it the google documentation only talk about changing reference from hdfs to g assuming data ha been migrated to cloud storage could think about creating dataproc vm image similar to on prem hadoop with dependency what are other change required in your opinion doe anyone ha experienced difficulty on doing this any tip and advice would be great thanks
renesas electronics enters fpga market with new forgefpgas focusing on ultra low power and low cost,renesas electronics enters fpga market with new forgefpgas focusing on ultra low power and low cost,0,renesas electronics enters fpga market with new forgefpgas focusing on ultra low power and low cost,renesas electronics enters fpga market with new forgefpgas focusing on ultra low power and low cost
asking for raise in dec currently unicorn data engineer with lt year experience dfw area need second opinion if am asking too much too little for what do,key points small sized non tech related company in dfw ytd gross earning of million only members in it department boss is part time consultant is sysadmin makes salary ft no degree and myself salary ft new executive team that is starting to become more data driven instead of intuition driven responsible for retrieving data building reports for the executive team maintaining the sql server amp etl also responsible for rebuilding the schema for the first proper data warehouse with postgresql amp migrating out data operations from on prem to cloud don deal with any big data currently still doing my bachelors in mathematics with minor in data science currently sophomore had no prior data experience boss has been trying to convince cfo to pay me what worth was initiated himself not by me all the executives really appreciate what do haven received single complaint from any of them boss has been expressing interest in wanting me to be sent for conferences to further career growth intend not to consider leaving the company until perhaps year just so competitive candidate in the market proposal ask for salary with listed accomplishments or ask for and be have be budgeted for conferences amp x200b am asking too much too little just right or not asking the right things,0,asking for raise in dec currently unicorn data engineer with lt year experience dfw area need second opinion if am asking too much too little for what do,key point small sized non tech related company in dfw ytd gross earning of million only member in it department bos is part time consultant is sysadmin make salary ft no degree and myself salary ft new executive team that is starting to become more data driven instead of intuition driven responsible for retrieving data building report for the executive team maintaining the sql server amp etl also responsible for rebuilding the schema for the first proper data warehouse with postgresql amp migrating out data operation from on prem to cloud don deal with any big data currently still doing my bachelor in mathematics with minor in data science currently sophomore had no prior data experience bos ha been trying to convince cfo to pay me what worth wa initiated himself not by me all the executive really appreciate what do haven received single complaint from any of them bos ha been expressing interest in wanting me to be sent for conference to further career growth intend not to consider leaving the company until perhaps year just so competitive candidate in the market proposal ask for salary with listed accomplishment or ask for and be have be budgeted for conference amp x200b am asking too much too little just right or not asking the right thing
data observability pains real,recently joined startup in the data observability space on the much hated sales side but don have background in data engineering and spent the last months painfully trudging through sql courses anywhere could find them knowing that ll never fully understand the potential challenges that data engineering teams face wanted to humbly and embarrassingly ask if solutions like data diff which datafold has which our team will be launching soon alongside our main focus on alerting solve an issue that data teams actually care about in the proactive qa space this is probably not the most relevant for the sub but would rather not miss shot or info because of not asking instead of being embarrassed thanks in advance and mods if this isn relevant enough you can take it down,0,data observability pain real,recently joined startup in the data observability space on the much hated sale side but don have background in data engineering and spent the last month painfully trudging through sql course anywhere could find them knowing that ll never fully understand the potential challenge that data engineering team face wanted to humbly and embarrassingly ask if solution like data diff which datafold ha which our team will be launching soon alongside our main focus on alerting solve an issue that data team actually care about in the proactive qa space this is probably not the most relevant for the sub but would rather not miss shot or info because of not asking instead of being embarrassed thanks in advance and mod if this isn relevant enough you can take it down
building data warehouse from scratch for manufacturing company,will be creating datawarehouse from ground up kind of lost on where to begin since ve thought only be doing the maintenance for the current infrastructure setup am working for medium sized manufacturing company with customers currently our sales department manually generates daily weekly monthly yearly reports and they get these info from different data sources they want to do analytics and slice and dice type and we already bought tableau for the reporting is mysql cloud sql good for this dwh setup if no what your recommendation what the best practices how to begin with integrating an etl process to fetch all the historical data to staging layer what are the things we should install for these appreciate all your assistance,0,building data warehouse from scratch for manufacturing company,will be creating datawarehouse from ground up kind of lost on where to begin since ve thought only be doing the maintenance for the current infrastructure setup am working for medium sized manufacturing company with customer currently our sale department manually generates daily weekly monthly yearly report and they get these info from different data source they want to do analytics and slice and dice type and we already bought tableau for the reporting is mysql cloud sql good for this dwh setup if no what your recommendation what the best practice how to begin with integrating an etl process to fetch all the historical data to staging layer what are the thing we should install for these appreciate all your assistance
how expensive are certain easy computations for mainframe data warehouses,hi in summary here the situation large enterprise with terabytes of data on teradata heh but as this is batch run it cannot be used for real time information from many source databases to web applications there ongoing work to cdc this by building an intermediate layer converting it and making it match with all of the different sources warehouses some of which predate star wars which ultimately ends up as db2 on os mainframe an actual approach for data quality is pretty much nonexistant in this organization so the general idea now on the very short term is to perform simple data profiling calculations on this db2 which is now the new single source of truth and spitting it out to some reporting screen down the road management is going to look at the whole circus apache griffin etc now was told you cannot run these calculations every minutes or so because it would use too much resources average median length field min max quantile group by and then all of the above timediff of colomns unique frequency count null and perhaps some more there are about tables columns each and let say about billion records on my personal computer ve done these simple calculations in python on single processor on large datasets gt gb in few sec at most albeit it was just pandas dataframe or so how would this clog up data warehouse that is built for these kinds of tasks how would know if performing these calculations would be too much without actually performing them don have access just want to have general idea whether these commands cause most systems to clog up with the amount of records or whether it really doesn matter still me recently employed data scientist who was put on data engineering task and has never seen his colleagues in real life,0,how expensive are certain easy computation for mainframe data warehouse,hi in summary here the situation large enterprise with terabyte of data on teradata heh but a this is batch run it cannot be used for real time information from many source database to web application there ongoing work to cdc this by building an intermediate layer converting it and making it match with all of the different source warehouse some of which predate star war which ultimately end up a db2 on o mainframe an actual approach for data quality is pretty much nonexistant in this organization so the general idea now on the very short term is to perform simple data profiling calculation on this db2 which is now the new single source of truth and spitting it out to some reporting screen down the road management is going to look at the whole circus apache griffin etc now wa told you cannot run these calculation every minute or so because it would use too much resource average median length field min max quantile group by and then all of the above timediff of colomns unique frequency count null and perhaps some more there are about table column each and let say about billion record on my personal computer ve done these simple calculation in python on single processor on large datasets gt gb in few sec at most albeit it wa just panda dataframe or so how would this clog up data warehouse that is built for these kind of task how would know if performing these calculation would be too much without actually performing them don have access just want to have general idea whether these command cause most system to clog up with the amount of record or whether it really doesn matter still me recently employed data scientist who wa put on data engineering task and ha never seen his colleague in real life
inter cloud streaming with kafka,have need to provide some kind of push service from my org that other organisations can subscribe to the data load can get quite heavy messages per org per day and currently we are forcing the other orgs to subscribe to our kinesis stream ergo they also need aws we now want to expand this so that orgs with other clouds gcp azure can subscribe to our stream my thought is that we could use managed kafka but what not able to determine is whether or not kafka running on other cloud providers can subscribe to kafka stream from aws assuming it can confluent for example any org that wanted to subscribe can use whatever kafka service they like to connect regardless of their cloud provider assuming that as concept kafka makes sense for intercloud streaming what would the implications be around things like latency have exactly zero experience with kafka and minimal with kinesis to be honest so keen to hear if kafka makes sense for this use case,0,inter cloud streaming with kafka,have need to provide some kind of push service from my org that other organisation can subscribe to the data load can get quite heavy message per org per day and currently we are forcing the other orgs to subscribe to our kinesis stream ergo they also need aws we now want to expand this so that orgs with other cloud gcp azure can subscribe to our stream my thought is that we could use managed kafka but what not able to determine is whether or not kafka running on other cloud provider can subscribe to kafka stream from aws assuming it can confluent for example any org that wanted to subscribe can use whatever kafka service they like to connect regardless of their cloud provider assuming that a concept kafka make sense for intercloud streaming what would the implication be around thing like latency have exactly zero experience with kafka and minimal with kinesis to be honest so keen to hear if kafka make sense for this use case
what the big deal with graph databases,why graph databases are so hot and why you should check them out,0,what the big deal with graph database,why graph database are so hot and why you should check them out
why should you use graph database,what is graph database what all the buzz about graph databases well it not just buzz the graph database market is exploding and so is the usage of graph databases in operational systems the limitations of the relational database management system rdbms model such as strict schemas and strong consistency along with the explosion of data and data types has led software developers and data architects to explore alternative data models for the past couple decades relational databases were not designed to traverse relationships between things they were intended to catalog and retrieve structured data also as data volumes increase relational databases require much more complex engineering and administration and their performance and reliability degrade significantly the not only sql or nosql database movement emerged in the early it gave developers and architects greater freedom to store and query data in new and innovative ways versus dealing with the restrictions and limitations of rdbms and sql the database types within nosql include key value stores wide column stores document databases and graph databases one of the key advantages of using graph database is the ability to model real world situations where there is lots of data and there are many complex connections between the data not only reflected in the graph model but also as new connections are discovered the foundation of modern data graph forms the foundation of modern data and analytics with capabilities to enhance and improve user collaboration machine learning models and explainable ai source graph databases enable the traversal of relationships and connections or nodes and edges within complex data structures often referred to as graph data the technical model of graph database is the focus on relationships and ability to store the graph querying relationships is fast because they are stored within the database itself moreover relationships can be easily visualized using graph databases for an application if the connections between data are as important or more important than the data itself to the dominant queries then you need database that natively supports relationships this particularly applies to navigational and pathfinding queries graph databases have an inherent advantage in cutting through big data clutter and finding the information that really matters graph analysis enables the use of multiple simultaneous interpretive models in parallel over the same data allowing business users to describe dynamic and complex domains more naturally and efficiently why should care graph is the present and future we are in the midst of the decade of data period where our world will produce an unfathomable amount of machine data metrics measurements and telemetry data that is emitted from everything from servers to robots and satellites and the pace of that data generation is increasing exponentially today this data is virtually untapped source of extraordinary business insights data so dense and rich it will dwarf traditional business intelligence in terms of its potential value companies that learn to harness and leverage this dense and rich data will be the clear winners in their respective industries the will be the era of data companies boosting massive markets database startups data movement startups data quality startups data lineage startups machine learning startups will be the zeitgeist of the decade as they shape the next wave of massive innovation graph has the capability of becoming the de facto and leading model for nosql databases data is becoming more connected as data becomes more and more connected or linked information will be increasingly derived from the links between data instead of the data itself as new use cases emerge data models need to be very flexible the popular graph databases are those that store data using flexible models to extract information out of linked data if the data contains lot of many to many relationships and the primary objective is quickly finding connections patterns and relationships then graph databases are superior to other technologies including relational databases key value column or document databases according to gartner inc of analytics will be graph based by the graph database is one of the top technologies used to detect fraud uncover fraud rings and identify sophisticated scams including ecommerce fraud corruption and money laundering graph databases use pattern recognition classification statistical analysis and machine learning models to identify fraud from massive amounts of data in addition to bfsi banking financial services and insurance other operational use cases include recommendation engines network analysis knowledge graphs traceability logistics healthcare intelligence systems and enterprise ai applications people places things events locations graph databases find relationships between them across diverse data assets in an instant for new intelligence,0,why should you use graph database,what is graph database what all the buzz about graph database well it not just buzz the graph database market is exploding and so is the usage of graph database in operational system the limitation of the relational database management system rdbms model such a strict schema and strong consistency along with the explosion of data and data type ha led software developer and data architect to explore alternative data model for the past couple decade relational database were not designed to traverse relationship between thing they were intended to catalog and retrieve structured data also a data volume increase relational database require much more complex engineering and administration and their performance and reliability degrade significantly the not only sql or nosql database movement emerged in the early it gave developer and architect greater freedom to store and query data in new and innovative way versus dealing with the restriction and limitation of rdbms and sql the database type within nosql include key value store wide column store document database and graph database one of the key advantage of using graph database is the ability to model real world situation where there is lot of data and there are many complex connection between the data not only reflected in the graph model but also a new connection are discovered the foundation of modern data graph form the foundation of modern data and analytics with capability to enhance and improve user collaboration machine learning model and explainable ai source graph database enable the traversal of relationship and connection or node and edge within complex data structure often referred to a graph data the technical model of graph database is the focus on relationship and ability to store the graph querying relationship is fast because they are stored within the database itself moreover relationship can be easily visualized using graph database for an application if the connection between data are a important or more important than the data itself to the dominant query then you need database that natively support relationship this particularly applies to navigational and pathfinding query graph database have an inherent advantage in cutting through big data clutter and finding the information that really matter graph analysis enables the use of multiple simultaneous interpretive model in parallel over the same data allowing business user to describe dynamic and complex domain more naturally and efficiently why should care graph is the present and future we are in the midst of the decade of data period where our world will produce an unfathomable amount of machine data metric measurement and telemetry data that is emitted from everything from server to robot and satellite and the pace of that data generation is increasing exponentially today this data is virtually untapped source of extraordinary business insight data so dense and rich it will dwarf traditional business intelligence in term of it potential value company that learn to harness and leverage this dense and rich data will be the clear winner in their respective industry the will be the era of data company boosting massive market database startup data movement startup data quality startup data lineage startup machine learning startup will be the zeitgeist of the decade a they shape the next wave of massive innovation graph ha the capability of becoming the de facto and leading model for nosql database data is becoming more connected a data becomes more and more connected or linked information will be increasingly derived from the link between data instead of the data itself a new use case emerge data model need to be very flexible the popular graph database are those that store data using flexible model to extract information out of linked data if the data contains lot of many to many relationship and the primary objective is quickly finding connection pattern and relationship then graph database are superior to other technology including relational database key value column or document database according to gartner inc of analytics will be graph based by the graph database is one of the top technology used to detect fraud uncover fraud ring and identify sophisticated scam including ecommerce fraud corruption and money laundering graph database use pattern recognition classification statistical analysis and machine learning model to identify fraud from massive amount of data in addition to bfsi banking financial service and insurance other operational use case include recommendation engine network analysis knowledge graph traceability logistics healthcare intelligence system and enterprise ai application people place thing event location graph database find relationship between them across diverse data asset in an instant for new intelligence
why copying parquet file into snowflake when the parquet schema is missed,feel confuse about the next scenario have parquet file into s3 copy the parquet file into snowflake specifying compression snappy and format parquet the file is copied in my table and can see the raw value however this value is json the parquet schema is los so im wondering know the benefits of using parquet but indeed don know why copying parquet files into snowflake is good when you miss an important bit of information like the schema am doing something wrong is logic question doubt,0,why copying parquet file into snowflake when the parquet schema is missed,feel confuse about the next scenario have parquet file into s3 copy the parquet file into snowflake specifying compression snappy and format parquet the file is copied in my table and can see the raw value however this value is json the parquet schema is los so im wondering know the benefit of using parquet but indeed don know why copying parquet file into snowflake is good when you miss an important bit of information like the schema am doing something wrong is logic question doubt
efficient data processing,hi all would like to bring your attention to an open source solution versatile data kit there is new article that was just published on the topic you can have look here the tool can help anybody who is working with databases by allowing you to create data pipelines either sql python or both data engineers data ingestions data transformations provides out of the box templates for building dimensional models publishing data jobs data analysts who are performing complex data transformations and are looking for ways on automating them data scientists who without realising are already doing sophisticated data processing or are already benefiting from data structured in well known data models as the kimball dimensional model hope this would also help some of you let me know if you need any help,0,efficient data processing,hi all would like to bring your attention to an open source solution versatile data kit there is new article that wa just published on the topic you can have look here the tool can help anybody who is working with database by allowing you to create data pipeline either sql python or both data engineer data ingestion data transformation provides out of the box template for building dimensional model publishing data job data analyst who are performing complex data transformation and are looking for way on automating them data scientist who without realising are already doing sophisticated data processing or are already benefiting from data structured in well known data model a the kimball dimensional model hope this would also help some of you let me know if you need any help
discussion for prospective data engineer,hi all am thinking about enrolling in data engineer training program as possible career switch have worked as field engineer in the high voltage industry for several years was hoping to discuss with someone the day to day aspects of work life and what to expect in this industry any help is appreciated,0,discussion for prospective data engineer,hi all am thinking about enrolling in data engineer training program a possible career switch have worked a field engineer in the high voltage industry for several year wa hoping to discus with someone the day to day aspect of work life and what to expect in this industry any help is appreciated
post files to rest api vs uploading files to sftp which would make more sense for large quantities of files,hey all need to setup monthly delivery of files made up of jpeg mp3 mp4 etc it would typically be about files per delivery for about gb to gb worth of data have both options available just trying to figure our what would make the most sense or be the easier more efficient option think have an idea but would like to hear the pros and cons if possible thanks,0,post file to rest api v uploading file to sftp which would make more sense for large quantity of file,hey all need to setup monthly delivery of file made up of jpeg mp3 mp4 etc it would typically be about file per delivery for about gb to gb worth of data have both option available just trying to figure our what would make the most sense or be the easier more efficient option think have an idea but would like to hear the pro and con if possible thanks
at my wit end with ssis,venting more than anything but does anybody actually use ssis and enjoy it get that it supposed to be low code solution but find that creating packages eats up so much more time with all the gotchas that come with the tool like today trying to create simple pipeline importing daily sales information via text file and it fighting me on converting pricing column for potential loss of data ve cleaned out every issue with conditional split already and even loaded the dataset to db with the field as text shouldn be any issues from what see and yet the import still bombs with necessary conversions to numeric it just so frustrating when you get hung up on petty bullshit like this thinking future state ll be doing elt with transformations done in sql at least until progress further in my learning with python any other ssis users feel this way,0,at my wit end with ssis,venting more than anything but doe anybody actually use ssis and enjoy it get that it supposed to be low code solution but find that creating package eats up so much more time with all the gotchas that come with the tool like today trying to create simple pipeline importing daily sale information via text file and it fighting me on converting pricing column for potential loss of data ve cleaned out every issue with conditional split already and even loaded the dataset to db with the field a text shouldn be any issue from what see and yet the import still bomb with necessary conversion to numeric it just so frustrating when you get hung up on petty bullshit like this thinking future state ll be doing elt with transformation done in sql at least until progress further in my learning with python any other ssis user feel this way
open source metrics store with cube building single source of truth for metrics that integrates with superset tableau and your custom front end app at the same time,open source metrics store with cube building single source of truth for metrics that integrates with superset tableau and your custom front end app at the same time,0,open source metric store with cube building single source of truth for metric that integrates with superset tableau and your custom front end app at the same time,open source metric store with cube building single source of truth for metric that integrates with superset tableau and your custom front end app at the same time
data models give companies the good oil for data governance,the following guide shows how you can craft data governance practice to align with your company overall business goals for establishing the processes that guard the data throughout its lifecycle and defining the policies for accessing data with the help of well articulated roles and metrics data models give companies the good oil for data governance the approach represented in more details in the guide above could be called the four pillars of data model governance these will help you gauge the effectiveness of data models to connect data management and data definition data coherence data consistency data compatibility data compliance,0,data model give company the good oil for data governance,the following guide show how you can craft data governance practice to align with your company overall business goal for establishing the process that guard the data throughout it lifecycle and defining the policy for accessing data with the help of well articulated role and metric data model give company the good oil for data governance the approach represented in more detail in the guide above could be called the four pillar of data model governance these will help you gauge the effectiveness of data model to connect data management and data definition data coherence data consistency data compatibility data compliance
oracle consultant,hi all recently graduated in cs and now looking for work found two opportunities in line with my possibilities for now don want to relocate and the offers are nearby and or remote friendly one is full stack developer position mainly python and js programming streaming applications focused specialized in python and like it but ignorant of js the full stack part it not so much in line with my aspirations the other is an oracle consultant position will work on elt and datawarehouse implementation for prominent local company using odi probably some bi project soon after my academic background is ai ds liked it but soon found out that there is no titanic dataset in the real world actual data tend to be awful recently discovered de and now in love with it want to become hybrid figure half ds half de maybe the term is analytics engineer would like to work with big data techs like kafka spark and many more in the long term the main question is which of the two is more coherent with my perspectives the first one is more generic and programming skills are always helpful the second one is the most consistent but afraid will fossilize my career opportunities by specializing in oracle any advice opinion is welcome,0,oracle consultant,hi all recently graduated in c and now looking for work found two opportunity in line with my possibility for now don want to relocate and the offer are nearby and or remote friendly one is full stack developer position mainly python and j programming streaming application focused specialized in python and like it but ignorant of j the full stack part it not so much in line with my aspiration the other is an oracle consultant position will work on elt and datawarehouse implementation for prominent local company using odi probably some bi project soon after my academic background is ai d liked it but soon found out that there is no titanic dataset in the real world actual data tend to be awful recently discovered de and now in love with it want to become hybrid figure half d half de maybe the term is analytics engineer would like to work with big data tech like kafka spark and many more in the long term the main question is which of the two is more coherent with my perspective the first one is more generic and programming skill are always helpful the second one is the most consistent but afraid will fossilize my career opportunity by specializing in oracle any advice opinion is welcome
etl vs el broken down in generations,etl tools have been around for while but they have considerably evolved in the past few years as part of their efforts to keep up with the development of data infrastructures we distinguish three generations of etl elt tools st generation standard etl extract transform load tools they follow processes dictated by stringent storage bandwidth and computation constraints that characterized the nd generation elt extract load transform processes result from the arrival of cloud data warehouses and the lifting of storage and bandwidth constraints rd generation third generation etl tools provide greater number of connectors thanks to their ability to standardize connectors and leverage their community what are your thoughts here great analysis of the evolution of etl elt it says we re entering new etl era with airbyte opensource highlightedupdateurns urn ali aactivity a6866487535854583808 approach leading the charge link to article,0,etl v el broken down in generation,etl tool have been around for while but they have considerably evolved in the past few year a part of their effort to keep up with the development of data infrastructure we distinguish three generation of etl elt tool st generation standard etl extract transform load tool they follow process dictated by stringent storage bandwidth and computation constraint that characterized the nd generation elt extract load transform process result from the arrival of cloud data warehouse and the lifting of storage and bandwidth constraint rd generation third generation etl tool provide greater number of connector thanks to their ability to standardize connector and leverage their community what are your thought here great analysis of the evolution of etl elt it say we re entering new etl era with airbyte opensource highlightedupdateurns urn ali aactivity a6866487535854583808 approach leading the charge link to article
how to calculate excel formulas efficiently,we are using libreoffice soffice headless to calculate excel formulas but it is not scalable is there better way to do this we use python aws thank you,0,how to calculate excel formula efficiently,we are using libreoffice soffice headless to calculate excel formula but it is not scalable is there better way to do this we use python aws thank you
gcp professional data engineer certification course materials,hi de community looking for the materials for the gcp professional data engineer exam preparation has anybody gone through this process does anybody have the suggestions so far what ve found is the course by janani ravi which is quite outdated and google official course track on coursera both are very broad in my opinion and explain the basic topics as well looking for something that azure de aws certified architect would need the course that is focused solely on the data engineering aspects of gcp without explaining the basics of cloud computing and architecture thanks,0,gcp professional data engineer certification course material,hi de community looking for the material for the gcp professional data engineer exam preparation ha anybody gone through this process doe anybody have the suggestion so far what ve found is the course by janani ravi which is quite outdated and google official course track on coursera both are very broad in my opinion and explain the basic topic a well looking for something that azure de aws certified architect would need the course that is focused solely on the data engineering aspect of gcp without explaining the basic of cloud computing and architecture thanks
anyone else attending sap teched,am so underwhelmed with what they ve shown felt like it was roughly random big data products showcase anyone else got the same feeling,0,anyone else attending sap teched,am so underwhelmed with what they ve shown felt like it wa roughly random big data product showcase anyone else got the same feeling
what type of testing or validation do you do with data pipelines,just want to hear some thoughts from the community as to what you use to test your data pipelines familiar with great expectations but curious if there were any best practices around testing for pipeline code does anyone do extensive unit testing if so what kind of tests do you run,0,what type of testing or validation do you do with data pipeline,just want to hear some thought from the community a to what you use to test your data pipeline familiar with great expectation but curious if there were any best practice around testing for pipeline code doe anyone do extensive unit testing if so what kind of test do you run
hi dataengineering your memes are pretty good want to give back to your community so here an mlops meme involving des,hi dataengineering your memes are pretty good want to give back to your community so here an mlops meme involving des,0,hi dataengineering your meme are pretty good want to give back to your community so here an mlops meme involving de,hi dataengineering your meme are pretty good want to give back to your community so here an mlops meme involving de
data ingestion best practises,hello all am very new to data ingestion have to ingest data from various sources like sap salesforce etc using azure data factory can someone let me know or comment links about how should build an ingestion framework that can be reused across the whole enterprise your comments experiences are also welcome,0,data ingestion best practises,hello all am very new to data ingestion have to ingest data from various source like sap salesforce etc using azure data factory can someone let me know or comment link about how should build an ingestion framework that can be reused across the whole enterprise your comment experience are also welcome
bridging the gap dba to de,so ve been dba for the last years split pretty evenly between oracle and sql server current role is in sql server and prefer it to oracle as career path but have fiddled with postgres mysql mariadb months with hadoop years ago etc my current employer is invested in azure but it still pretty new overall like to start learning more about and gaining skills in the de field and it is tough to decide where to begin ve done python decent bit for api and data collection tasks and lots of powershell in my current role for gathering data for monitoring tuning purposes but figure python is probably bit more in the de side any tips on how to organize my learning so that don feel like just randomly jumping between topics thanks,0,bridging the gap dba to de,so ve been dba for the last year split pretty evenly between oracle and sql server current role is in sql server and prefer it to oracle a career path but have fiddled with postgres mysql mariadb month with hadoop year ago etc my current employer is invested in azure but it still pretty new overall like to start learning more about and gaining skill in the de field and it is tough to decide where to begin ve done python decent bit for api and data collection task and lot of powershell in my current role for gathering data for monitoring tuning purpose but figure python is probably bit more in the de side any tip on how to organize my learning so that don feel like just randomly jumping between topic thanks
how got my first data engineering job,how got my first data engineering job,0,how got my first data engineering job,how got my first data engineering job
looking for kubeflow tutorials,does any one have any suggestions on good kubeflow tutorials,0,looking for kubeflow tutorial,doe any one have any suggestion on good kubeflow tutorial
oreilly learning,does oreilly learning subscription worth it is it good learning investment for data engineers,0,oreilly learning,doe oreilly learning subscription worth it is it good learning investment for data engineer
airflow be like,airflow be like,0,airflow be like,airflow be like
have had lot of people asking me for advice on passing gcp data engineering certification hope this blog helps the community and for those wanting solid de certification under their belt,have had lot of people asking me for advice on passing gcp data engineering certification hope this blog helps the community and for those wanting solid de certification under their belt,0,have had lot of people asking me for advice on passing gcp data engineering certification hope this blog help the community and for those wanting solid de certification under their belt,have had lot of people asking me for advice on passing gcp data engineering certification hope this blog help the community and for those wanting solid de certification under their belt
where can learn snowflake,the text,0,where can learn snowflake,the text
best practices for mongodb key value arrays,currently moving mongodb dumps from google cloud storage to bigquery as part of an elt architecture python api is super and defining the schema is really easy problem have is some collections have nested key value arrays person axy id bty id bigquery can handle this as the keys to access the people are random ids so don conform to schema can convert it to list and move the ids into the nested object but some of these files are gbs so the transformation step takes quite bit of time to stream transform and then write any suggestions appreciated,0,best practice for mongodb key value array,currently moving mongodb dump from google cloud storage to bigquery a part of an elt architecture python api is super and defining the schema is really easy problem have is some collection have nested key value array person axy id bty id bigquery can handle this a the key to access the people are random id so don conform to schema can convert it to list and move the id into the nested object but some of these file are gb so the transformation step take quite bit of time to stream transform and then write any suggestion appreciated
after exactly one month and three days of starting my azure data engineer preparation have good news,after exactly one month and three days of starting my azure data engineer preparation have good news,0,after exactly one month and three day of starting my azure data engineer preparation have good news,after exactly one month and three day of starting my azure data engineer preparation have good news
when should use beam spark over sql and vice versa for transforming data,am loading some data every morning from csv into big query data warehouse the data comes in daily and needs to be inserted as new rows into the warehouse the schema is always the same load the csv into cloud storage delete the rows that contain null values with filter transform string eg john doe ron swanson tim turner into list john doe ron swanson tim turner wondering if should use apache beam for this with an etl style approach or if could do an elt style approach with sql or an etl style with sql for the experienced de in here how would you go about this do you do most of your transforms in sql when should use beam over sql or sql over beam thank you,0,when should use beam spark over sql and vice versa for transforming data,am loading some data every morning from csv into big query data warehouse the data come in daily and need to be inserted a new row into the warehouse the schema is always the same load the csv into cloud storage delete the row that contain null value with filter transform string eg john doe ron swanson tim turner into list john doe ron swanson tim turner wondering if should use apache beam for this with an etl style approach or if could do an elt style approach with sql or an etl style with sql for the experienced de in here how would you go about this do you do most of your transforms in sql when should use beam over sql or sql over beam thank you
dbt cloud any way to obtain updated manifest json file,is there any way to obtain an updated manifest json file directly from dbt cloud after project has been run,0,dbt cloud any way to obtain updated manifest json file,is there any way to obtain an updated manifest json file directly from dbt cloud after project ha been run
prefect and powershell windows shell,hi has someone worked with prefect and used task to execute powershell script could only find github post suggesting that prefect cannot run powershell scripts,0,prefect and powershell window shell,hi ha someone worked with prefect and used task to execute powershell script could only find github post suggesting that prefect cannot run powershell script
data science architect engineer,ve reviewed couple variations on this topic but my question is less about what these roles do as think have reasonable understanding of job responsibilities and more about which would provide the best compensation career path for mid career transition from dba basically ve been multi platform dba but mostly oracle for years and being given the opportunity to transition to new but adjacent career path so wondering as we go into the advantages disadvantages of these or other data paths havent listed in addition to my dba role have some familiarity with python data visualization tableau ssrs etls ssis boomi and linux admin in case these are relevant,0,data science architect engineer,ve reviewed couple variation on this topic but my question is le about what these role do a think have reasonable understanding of job responsibility and more about which would provide the best compensation career path for mid career transition from dba basically ve been multi platform dba but mostly oracle for year and being given the opportunity to transition to new but adjacent career path so wondering a we go into the advantage disadvantage of these or other data path havent listed in addition to my dba role have some familiarity with python data visualization tableau ssrs etls ssis boomi and linux admin in case these are relevant
resources for going from de to the only de,hey all ve been working as de for about years and pretty confident with all the core skills can etl stuff message stuff log stuff horizontally scale stuff etc recently accepted role where going to be the main de for fresh startup super excited about the role however was wondering if you had any resources on more architecting good data systems instead of how to do xyz comfortable helping them scale up to some degree but was self taught and feel gap in the higher level thinking of systems design and architecture bonus points if you have mlops de references as well as that ll be part of it but the main crux is just how to think of current business problem current company and see years into the future,0,resource for going from de to the only de,hey all ve been working a de for about year and pretty confident with all the core skill can etl stuff message stuff log stuff horizontally scale stuff etc recently accepted role where going to be the main de for fresh startup super excited about the role however wa wondering if you had any resource on more architecting good data system instead of how to do xyz comfortable helping them scale up to some degree but wa self taught and feel gap in the higher level thinking of system design and architecture bonus point if you have mlops de reference a well a that ll be part of it but the main crux is just how to think of current business problem current company and see year into the future
hey data engineers we re opening our managed clickhouse service for free preview and looking for early adopters,we are pleased to announce our next wave of invites the limited number of invites are now up for grabs if you are developing on clickhouse or kafka and want cloud agnostic managed service request an invite to the all new today doublecloud is new company incorporated in the usa and the eu and part of the yandex company group we are the creators of the first managed clickhouse service back in which is currently managing more than clusters in the production of clickhouse and kafka for the more than th thousand customers with hundreds of petabytes of total volume user data request invite here amp x200b,0,hey data engineer we re opening our managed clickhouse service for free preview and looking for early adopter,we are pleased to announce our next wave of invite the limited number of invite are now up for grab if you are developing on clickhouse or kafka and want cloud agnostic managed service request an invite to the all new today doublecloud is new company incorporated in the usa and the eu and part of the yandex company group we are the creator of the first managed clickhouse service back in which is currently managing more than cluster in the production of clickhouse and kafka for the more than th thousand customer with hundred of petabyte of total volume user data request invite here amp x200b
how to integrate fortran technical analysis stage or any other compiled language into python pipeline for automated stock trading,so have been building few trading bots for while not super complex but get the gist of it all of them have been in python ingest both streaming and historical stock data from like an alpaca or polygon and do some transformations with pandas and some very basic technical analysis with python libraries like ta and bta lib then push trade signals with some combination of aws lambda and or tradingview and so on up until now have managed to do this almost completely in python maybe some js here and there but nothing significant now recently have been thinking about partnering with guy who is absolutely great at data science and analysis he is an older well seasoned guy who only operates in fortran as his background was engineering we are now trying to see how we can work with each other he has worked with both large data sets and streamed sensors data however he has no knowledge of internet protocols or these fancy apis or how to ultimately implement his analysis into actual order execution we are in the process of exploring if and how we can work together so as we are currently thinking through it would be setting up the ingest pipeline and the storage infrastructure he would be doing the technical analysis don know if fortran has some good http libraries for actual trade signaling and order execution so am guessing would be taking care of that part as well also haven worked with compiled languages in my pipelines so my ask is advice on how to set this up how can have the fortran technical analysis part communicate with both the ingest especially streaming data and the signaling and trade execution parts what sort of things and libraries should be looking out for would introducing microservices into my otherwise monolothic architecture help or does pub sub work better doing pub sub with fortran is that thing couldn find any documentation on working fortran with aws lambda or exposing endpoints with fortran so does this mean would have to introduce another intermediary store for generated signals and then work with those with python honestly any criticism or advice would be greatly appreciated know could try to learn fortran but am trying to avoid that if possible it takes really long time to know the ins and outs of language so even just high level description like ingest with this store like that expose with this analyze with that push with this execute with that would help greatly really want to understand how to work with different languages especially compiled at different parts of pipeline so am guessing this question can also be applied to golang or java thank you for taking time to read this,0,how to integrate fortran technical analysis stage or any other compiled language into python pipeline for automated stock trading,so have been building few trading bot for while not super complex but get the gist of it all of them have been in python ingest both streaming and historical stock data from like an alpaca or polygon and do some transformation with panda and some very basic technical analysis with python library like ta and bta lib then push trade signal with some combination of aws lambda and or tradingview and so on up until now have managed to do this almost completely in python maybe some j here and there but nothing significant now recently have been thinking about partnering with guy who is absolutely great at data science and analysis he is an older well seasoned guy who only operates in fortran a his background wa engineering we are now trying to see how we can work with each other he ha worked with both large data set and streamed sensor data however he ha no knowledge of internet protocol or these fancy apis or how to ultimately implement his analysis into actual order execution we are in the process of exploring if and how we can work together so a we are currently thinking through it would be setting up the ingest pipeline and the storage infrastructure he would be doing the technical analysis don know if fortran ha some good http library for actual trade signaling and order execution so am guessing would be taking care of that part a well also haven worked with compiled language in my pipeline so my ask is advice on how to set this up how can have the fortran technical analysis part communicate with both the ingest especially streaming data and the signaling and trade execution part what sort of thing and library should be looking out for would introducing microservices into my otherwise monolothic architecture help or doe pub sub work better doing pub sub with fortran is that thing couldn find any documentation on working fortran with aws lambda or exposing endpoint with fortran so doe this mean would have to introduce another intermediary store for generated signal and then work with those with python honestly any criticism or advice would be greatly appreciated know could try to learn fortran but am trying to avoid that if possible it take really long time to know the in and out of language so even just high level description like ingest with this store like that expose with this analyze with that push with this execute with that would help greatly really want to understand how to work with different language especially compiled at different part of pipeline so am guessing this question can also be applied to golang or java thank you for taking time to read this
need an etl developer data engineer mentor,please would like to connect with mentor to help me in my etl data engineering journey please reach out to me as applied for role that really like currently work as bi developer and looking to transition to data engineering thank you,0,need an etl developer data engineer mentor,please would like to connect with mentor to help me in my etl data engineering journey please reach out to me a applied for role that really like currently work a bi developer and looking to transition to data engineering thank you
analytics teams need more than traditional documentation,hey folks documenting processes allows business teams to scale documenting code allows software engineering teams to scale similarly documenting data enables the same scale for analytics teams but how confluence doesn support documenting technical resources while software documentation best practices don support procedural and business documentation easily let alone documenting data so what now in our recent blog post written by sarah krasnik you ll learn why analytics teams need more than traditional documentation you can find the full post here,0,analytics team need more than traditional documentation,hey folk documenting process allows business team to scale documenting code allows software engineering team to scale similarly documenting data enables the same scale for analytics team but how confluence doesn support documenting technical resource while software documentation best practice don support procedural and business documentation easily let alone documenting data so what now in our recent blog post written by sarah krasnik you ll learn why analytics team need more than traditional documentation you can find the full post here
treatment of thounsands hundred of xml files,hello am actually trying to process more than small xml files with pyspark on dataproc cluster use databricks to read the file which are on cloud storage and create spark dataframe from all the files my main problem is that the masternode is reading one by one the file and it is taking minutes for files which means hours for the whole process want to find way to be more efficient does anyone have suggestion thank you in advance,0,treatment of thounsands hundred of xml file,hello am actually trying to process more than small xml file with pyspark on dataproc cluster use databricks to read the file which are on cloud storage and create spark dataframe from all the file my main problem is that the masternode is reading one by one the file and it is taking minute for file which mean hour for the whole process want to find way to be more efficient doe anyone have suggestion thank you in advance
how relevant is dimensional modeling today,am currently taking course on data warehousing and the instructor introduced dimensional modeling my understanding is that the purpose of dimensional model is to make it easy to summarize sum average etc facts stored in fact tables with storage getting cheaper more granular data and columnar data stores being prevelant faster retrieval and joins are dimensional models becoming obsolete,0,how relevant is dimensional modeling today,am currently taking course on data warehousing and the instructor introduced dimensional modeling my understanding is that the purpose of dimensional model is to make it easy to summarize sum average etc fact stored in fact table with storage getting cheaper more granular data and columnar data store being prevelant faster retrieval and join are dimensional model becoming obsolete
new to de got invited for an opportunity,from software engineering background with some exposure on data analysis and etl light jobs and recruiter reached out to me with this job we talked about the position and stuff finally got scheduled to be interviewed by the head of data team for me it looks like this is backdoor access in the data engineering world for those of you who are in the data engineering field does this job description please check the link below sounds like what data engineer will do plsk6venvoydwi yrthxkbza pzbxpl x2ty4 adding to the job description above the recruiter was asking thing about kafka and processing millions of rows feel free to drop your thoughts ll take it into account will also appreciate if you can quantify from just so have numbers to base on,0,new to de got invited for an opportunity,from software engineering background with some exposure on data analysis and etl light job and recruiter reached out to me with this job we talked about the position and stuff finally got scheduled to be interviewed by the head of data team for me it look like this is backdoor access in the data engineering world for those of you who are in the data engineering field doe this job description please check the link below sound like what data engineer will do plsk6venvoydwi yrthxkbza pzbxpl x2ty4 adding to the job description above the recruiter wa asking thing about kafka and processing million of row feel free to drop your thought ll take it into account will also appreciate if you can quantify from just so have number to base on
takeaways from the future of metadata aft,takeaways from the future of metadata aft,0,takeaway from the future of metadata aft,takeaway from the future of metadata aft
the ideal data architecture,if you could build real time data architecture from scratch what tools would you use context structured data only real time data process records per day needs data has to be sourced from db2,0,the ideal data architecture,if you could build real time data architecture from scratch what tool would you use context structured data only real time data process record per day need data ha to be sourced from db2
unable to use subprocess check_output inside airflow,hi need to use gsutil inside an airflow dag and that has to be done by treating it as command line program ie calling via subprocess check output while can call gsutil using the terminal and with subprocess within python shell the same script gives an when called through an airflow dag amp x200b pasting error for reference traceback most recent call last file home lt user name gt local bin gsutil line in lt module gt from pkg_resources import load_entry_point file usr local lib python3 site packages pkg_resources init__ py line in lt module gt call_aside file usr local lib python3 site packages pkg_resources init__ py line in call_aside args kwargs file usr local lib python3 site packages pkg_resources init__ py line in initialize_master_working_set working_set workingset build_master file usr local lib python3 site packages pkg_resources init__ py line in build_master ws require requires__ file usr local lib python3 site packages pkg_resources init__ py line in require needed self resolve parse_requirements requirements file usr local lib python3 site packages pkg_resources init__ py line in resolve raise distributionnotfound req requirers pkg_resources distributionnotfound the gsutil distribution was not found and is required by the application am missing something understand that have the option of using google cloud module for python but the higher ups insist on using the gsutil command for this job,0,unable to use subprocess check_output inside airflow,hi need to use gsutil inside an airflow dag and that ha to be done by treating it a command line program ie calling via subprocess check output while can call gsutil using the terminal and with subprocess within python shell the same script give an when called through an airflow dag amp x200b pasting error for reference traceback most recent call last file home lt user name gt local bin gsutil line in lt module gt from pkg_resources import load_entry_point file usr local lib python3 site package pkg_resources init__ py line in lt module gt call_aside file usr local lib python3 site package pkg_resources init__ py line in call_aside args kwargs file usr local lib python3 site package pkg_resources init__ py line in initialize_master_working_set working_set workingset build_master file usr local lib python3 site package pkg_resources init__ py line in build_master w require requires__ file usr local lib python3 site package pkg_resources init__ py line in require needed self resolve parse_requirements requirement file usr local lib python3 site package pkg_resources init__ py line in resolve raise distributionnotfound req requirers pkg_resources distributionnotfound the gsutil distribution wa not found and is required by the application am missing something understand that have the option of using google cloud module for python but the higher ups insist on using the gsutil command for this job
master data management,hello want to learn about master data management could anyone advise me good resources about it my current level of understanding is at definition level with no practical experience,0,master data management,hello want to learn about master data management could anyone advise me good resource about it my current level of understanding is at definition level with no practical experience
best resources to learn sql regex regular functions and their usages,preparing for internship assessments figured should know regex for those pattern matching questions if you got any book or resources you can share it help great deal,0,best resource to learn sql regex regular function and their usage,preparing for internship assessment figured should know regex for those pattern matching question if you got any book or resource you can share it help great deal
bigquery equivalent of snowflake dynamic masking policy,hi all recently moved from snowflake to bigquery and looking for way to mask data which for example replace phone number with asterisks for unauthorized roles more about snowflake data masking here thank you,0,bigquery equivalent of snowflake dynamic masking policy,hi all recently moved from snowflake to bigquery and looking for way to mask data which for example replace phone number with asterisk for unauthorized role more about snowflake data masking here thank you
moldy data and dashboards,,0,moldy data and dashboard,
what books do you recommend to someone that very new de,come from more of an analyst background but somehow managed to get job as de yeah idk how that happened but it been good so far pretty comfortable with python and sql most of my work for now is just prepping and uploading data to snowflake for others to use clearly going to have to do more complex stuff in the future and rather not make my manager regret sticking out for me lol what books or resources helped you back when you were just starting off,0,what book do you recommend to someone that very new de,come from more of an analyst background but somehow managed to get job a de yeah idk how that happened but it been good so far pretty comfortable with python and sql most of my work for now is just prepping and uploading data to snowflake for others to use clearly going to have to do more complex stuff in the future and rather not make my manager regret sticking out for me lol what book or resource helped you back when you were just starting off
sample databases or apis to import data into amplitude and segment,hi everyone novice and trying to explore both amplitude and segment for our product analytics needs can someone kindly guide me towards any freely available data sources or databases to pull into these tools to compare the analytics hope conveyed my requirements aware of free apis such as pokeapi etc but how can use them to pull data with http apis please guide,0,sample database or apis to import data into amplitude and segment,hi everyone novice and trying to explore both amplitude and segment for our product analytics need can someone kindly guide me towards any freely available data source or database to pull into these tool to compare the analytics hope conveyed my requirement aware of free apis such a pokeapi etc but how can use them to pull data with http apis please guide
what technologies are you learning or are interested in learning,lately work has been slower and ll have more time to learn some things so wanted to get some ideas what technologies are you thinking on learning or are currently learning also interested in getting opinions on what newer technologies might be important to get familiarized with for example ve been hearing lot about dbt and want to learn more ve also been wanting to explore the data streaming side so thinking about kafka pub sub etc,0,what technology are you learning or are interested in learning,lately work ha been slower and ll have more time to learn some thing so wanted to get some idea what technology are you thinking on learning or are currently learning also interested in getting opinion on what newer technology might be important to get familiarized with for example ve been hearing lot about dbt and want to learn more ve also been wanting to explore the data streaming side so thinking about kafka pub sub etc
question on basics of segmentation and compaction concept from designing data intensive applications book,reading through designing data intensive applications and am on chapter where the author describes segmentation and compaction the basics of it at least the argument here is that we can run out of memory if we keep on appending to one segment curious how does segmentation help here if you have one big file of size and divide it into some parts isn the total size the same how would this help us if we run out of memory maybe this is something covered later in the book but it bit confusing to me guess can see how maybe you run out of memory on one disk and you use segmentation to write to another but it not making sense how this saves us from the issue of running out of memory there probably some fuller picture thing missing here thanks in advance,0,question on basic of segmentation and compaction concept from designing data intensive application book,reading through designing data intensive application and am on chapter where the author describes segmentation and compaction the basic of it at least the argument here is that we can run out of memory if we keep on appending to one segment curious how doe segmentation help here if you have one big file of size and divide it into some part isn the total size the same how would this help u if we run out of memory maybe this is something covered later in the book but it bit confusing to me guess can see how maybe you run out of memory on one disk and you use segmentation to write to another but it not making sense how this save u from the issue of running out of memory there probably some fuller picture thing missing here thanks in advance
do mdm tools work,my company is looking at implementing an mdm tool informatica to help manage our customer and marketing data generate universal customer id resolve duplicates etc we re b2c company have some reservations and feel like it wouldn necessarily solve our problems anyone have experience here,0,do mdm tool work,my company is looking at implementing an mdm tool informatica to help manage our customer and marketing data generate universal customer id resolve duplicate etc we re b2c company have some reservation and feel like it wouldn necessarily solve our problem anyone have experience here
what is the best workflow architecture to incorporate python and machine learning into looker dashboard reports,this is something curious about but don have much knowledge coming from more strictly data science statistics background but basically want to be able to incorporate more powerful python transformations prior to the data being loaded into looker from the db how is the best way about doing that know looker has lookml with its own packages but really like to be able to just use python and potentially call in whatever api want an option that seem to find is to have data sent into my database into clean pristine table and then extracted from there into the python workflow and redownloaded into different table that looker references accesses and uses for reporting leaving the original table with un transformed data is that generally recommended workflow any other options recommendations or considerations,0,what is the best workflow architecture to incorporate python and machine learning into looker dashboard report,this is something curious about but don have much knowledge coming from more strictly data science statistic background but basically want to be able to incorporate more powerful python transformation prior to the data being loaded into looker from the db how is the best way about doing that know looker ha lookml with it own package but really like to be able to just use python and potentially call in whatever api want an option that seem to find is to have data sent into my database into clean pristine table and then extracted from there into the python workflow and redownloaded into different table that looker reference access and us for reporting leaving the original table with un transformed data is that generally recommended workflow any other option recommendation or consideration
which is more promising,more generally web development track vs data track view poll,0,which is more promising,more generally web development track v data track view poll
informatica alternatives pros cons,finance guy getting the tech pitch from informatica and my concerns are that it seems more of closed system than the alternatives and the cost associated with the gui interface slowing production am off base what other alternatives do you prefer,0,informatica alternative pro con,finance guy getting the tech pitch from informatica and my concern are that it seems more of closed system than the alternative and the cost associated with the gui interface slowing production am off base what other alternative do you prefer
best practices for logging events,hi everyone currently thinking about how to track events for web app created specifically would want to track button clicks for b2c web application with up to millions of users every day that click the button up to hundreds of times per day for now it will be about users with about interactions with the buttons per user per day can currently see the following three options amp x200b hard code tracking on web app and store it in the database with post requests pro gives me complete freedom and perfect mapping to users con can create lot of load and demand on the db with each request google analytics tag manager implementation with data layer variable for user ids pro simple and good enough for current state no demand on production db con not sure how accurate the user id will be would have to be joined with production db data rd party tools segment etc pro already solved so no ad hoc issues no demand on production db con might be too expensive would have to be joined with production db data love to get your thoughts from data engineering perspective thanks lot,0,best practice for logging event,hi everyone currently thinking about how to track event for web app created specifically would want to track button click for b2c web application with up to million of user every day that click the button up to hundred of time per day for now it will be about user with about interaction with the button per user per day can currently see the following three option amp x200b hard code tracking on web app and store it in the database with post request pro give me complete freedom and perfect mapping to user con can create lot of load and demand on the db with each request google analytics tag manager implementation with data layer variable for user id pro simple and good enough for current state no demand on production db con not sure how accurate the user id will be would have to be joined with production db data rd party tool segment etc pro already solved so no ad hoc issue no demand on production db con might be too expensive would have to be joined with production db data love to get your thought from data engineering perspective thanks lot
big data projects,find very few resources websites for working on big data side projects can anyone help me to come up with some good ideas for working on big data projects if permitted you can also share high level architecture of your currently working projects,0,big data project,find very few resource website for working on big data side project can anyone help me to come up with some good idea for working on big data project if permitted you can also share high level architecture of your currently working project
best way to transfer multiple tables from hadoop to oracle,have couple hundred tables to copy from hadoop to oracle what would be the easiest way to do one time copy from hadoop to oracle,0,best way to transfer multiple table from hadoop to oracle,have couple hundred table to copy from hadoop to oracle what would be the easiest way to do one time copy from hadoop to oracle
how to refactor slow pipelines,there are many slow steps of slow pipeline suspect could refactor to improve performance and even clarity but the cost of refactoring the code is high because it takes so long to properly test want to test each update in isolation but that leads to very slow iterative process how do people typically deal with this constraint it been the same everywhere ve worked,0,how to refactor slow pipeline,there are many slow step of slow pipeline suspect could refactor to improve performance and even clarity but the cost of refactoring the code is high because it take so long to properly test want to test each update in isolation but that lead to very slow iterative process how do people typically deal with this constraint it been the same everywhere ve worked
best way to send data out of snowflake to rest api,hey am working with team who has an api setup to receive data need to send snowflake daily to the api the data is about million rows and columns would the best way to do this is just python script to send the data,0,best way to send data out of snowflake to rest api,hey am working with team who ha an api setup to receive data need to send snowflake daily to the api the data is about million row and column would the best way to do this is just python script to send the data
databricks vs snowflake,databricks vs snowflake,0,databricks v snowflake,databricks v snowflake
is talend still in business,anybody in regular contact with talend working on aws dms replacement project and talend was on my short list four call me forms and an email direct to the chief revenue officer and haven heard anything it been days all other vendors responded within hours anyone have clue what going on,0,is talend still in business,anybody in regular contact with talend working on aws dm replacement project and talend wa on my short list four call me form and an email direct to the chief revenue officer and haven heard anything it been day all other vendor responded within hour anyone have clue what going on
is de just all metrics all the time,when was first looking into de as formal career path thought it would be more like work had done previously writing parsers to extract data from pdfs developing code to connect two pieces of software together via apis automating reports building tools to make interacting with the data easier optimizing sql queries to run faster writing wrapper to connect to multiple databases so an analyst can get the data they need etc instead it just all metrics at least in my day to day it things like why has this metric been stuck at for the last week let ideate on possible metrics for the new product roll out what are some ways we can reduce this metric etc either that or it maintenance we re changing the value of field in table so let make sure that doesn break any of our existing pipelines does this match your experience is this reflective of what de is,0,is de just all metric all the time,when wa first looking into de a formal career path thought it would be more like work had done previously writing parser to extract data from pdfs developing code to connect two piece of software together via apis automating report building tool to make interacting with the data easier optimizing sql query to run faster writing wrapper to connect to multiple database so an analyst can get the data they need etc instead it just all metric at least in my day to day it thing like why ha this metric been stuck at for the last week let ideate on possible metric for the new product roll out what are some way we can reduce this metric etc either that or it maintenance we re changing the value of field in table so let make sure that doesn break any of our existing pipeline doe this match your experience is this reflective of what de is
how would you manage the schema evolution in this case,receiving daily three files one parquet one json and txt file the information they contain are related have pipeline that can ingest this data without problems however the challenge for me is trying to figure out how to handle any schema change on these files my concerns are mostly if they change column name field will need to change code all around the pipeline how can detect in advance this schema change how would you manage this stuff,0,how would you manage the schema evolution in this case,receiving daily three file one parquet one json and txt file the information they contain are related have pipeline that can ingest this data without problem however the challenge for me is trying to figure out how to handle any schema change on these file my concern are mostly if they change column name field will need to change code all around the pipeline how can detect in advance this schema change how would you manage this stuff
how are olap cubes made,am taking course to learn the basics of data warehousing and the instructor introduced olap cubes the assignment requires connecting to an olap server and using predefined cube to create pivot tables while am able to work with cubes to create pivot tables do not understand how cubes are made as it was not covered in the course coming from sql background understand how tables are made on server and how they get populated either via application logic or through etl processes can someone explain like am five the end to end process of how an olap cube is created on an olap server,0,how are olap cube made,am taking course to learn the basic of data warehousing and the instructor introduced olap cube the assignment requires connecting to an olap server and using predefined cube to create pivot table while am able to work with cube to create pivot table do not understand how cube are made a it wa not covered in the course coming from sql background understand how table are made on server and how they get populated either via application logic or through etl process can someone explain like am five the end to end process of how an olap cube is created on an olap server
column order in aws redshift,are there any easy ways methods to reorder column in aws redshift know that the alter table after col is not supported but there should be some or the other way via which we can reorder columns in redshift any help is highly appreciated thanks in advance,0,column order in aws redshift,are there any easy way method to reorder column in aws redshift know that the alter table after col is not supported but there should be some or the other way via which we can reorder column in redshift any help is highly appreciated thanks in advance
why are there no entry level data engineering jobs,used to have great passion to learn data engineering but lot of people adviced not to do it and waste my because there is not such thing as entry level data engineer so will not find de job they adviced me to learn java or golang instead because there are more entry level jobs for those technologies so my question is to data engineers and people who have been in the de field for long time why are there no entry level roles in de job market,0,why are there no entry level data engineering job,used to have great passion to learn data engineering but lot of people adviced not to do it and waste my because there is not such thing a entry level data engineer so will not find de job they adviced me to learn java or golang instead because there are more entry level job for those technology so my question is to data engineer and people who have been in the de field for long time why are there no entry level role in de job market
best sources to learn snowflake and get certified,after joining this forum and finding out that was living under rock utm_medium web2x amp context ve decided to give sowflake try would you guys recommend any good reference for someone with good background in de mostly python sql spark cloudera and gcp am particularly interested in anything focused on preparing for the certifications something like voucher for their online training catalog like the one they have in databricks would be great as well thanks,0,best source to learn snowflake and get certified,after joining this forum and finding out that wa living under rock utm_medium web2x amp context ve decided to give sowflake try would you guy recommend any good reference for someone with good background in de mostly python sql spark cloudera and gcp am particularly interested in anything focused on preparing for the certification something like voucher for their online training catalog like the one they have in databricks would be great a well thanks
how to simulate real time big data source for personal project,hi all currently learning kafka and want to build personal project to use kafka to ingest big data in real time am thinking mb per second continuously so it would be like gb per hour because want to learn processing big data with kafka my problem is how do simulate real time data source like that am guessing there is no open source real time data with that kind of volume any advice thank you,0,how to simulate real time big data source for personal project,hi all currently learning kafka and want to build personal project to use kafka to ingest big data in real time am thinking mb per second continuously so it would be like gb per hour because want to learn processing big data with kafka my problem is how do simulate real time data source like that am guessing there is no open source real time data with that kind of volume any advice thank you
booked my azure dp data engineer certification exam for tomorrow any advice you would have is welcome,am starting my new job from monday and since had been studying azure data engineer certification for month thought might as well get it over with those of you who have successfully completed the exam what topics do you recommend should be fully prepared with,0,booked my azure dp data engineer certification exam for tomorrow any advice you would have is welcome,am starting my new job from monday and since had been studying azure data engineer certification for month thought might a well get it over with those of you who have successfully completed the exam what topic do you recommend should be fully prepared with
project hamilton micro framework for creating dataframes looking for feedback,hi one of authors that created in terms of why posting after some feedback on the project itself in addition to where to take this project would this be useful tool of interest to people in data engineering why why not what is hamilton it an open source library for easily creating pandas dataframes it was birthed to help team tame their code base it is production grade code and has been running at stitch fix for years the core of the idea is that people write functions that look like this def column_c column_a pd series column_b pd series gt pd series some doc string return column_a column_b instead of df column_c df column_a df column_b think it cool paradigm and helps simplify and empower team to do more without worrying about how it all glued together for the backstory on how it came to be please see our blog post on it if you have thoughts or suggestions all ears thanks in advance if this isn content that appropriate for this subreddit apologies ll happily take suggestions,0,project hamilton micro framework for creating dataframes looking for feedback,hi one of author that created in term of why posting after some feedback on the project itself in addition to where to take this project would this be useful tool of interest to people in data engineering why why not what is hamilton it an open source library for easily creating panda dataframes it wa birthed to help team tame their code base it is production grade code and ha been running at stitch fix for year the core of the idea is that people write function that look like this def column_c column_a pd series column_b pd series gt pd series some doc string return column_a column_b instead of df column_c df column_a df column_b think it cool paradigm and help simplify and empower team to do more without worrying about how it all glued together for the backstory on how it came to be please see our blog post on it if you have thought or suggestion all ear thanks in advance if this isn content that appropriate for this subreddit apology ll happily take suggestion
are small freelance projects good way to get experience while working in another role full time,hey there currently honing my skills from an education practice perspective on coursera and free online data sets however wondering if it is feasible to get started on small freelance work from gig sites like upwork or fiverr to have paid work experience to talk about has anyone has success in finding part time work as data engineer as well,0,are small freelance project good way to get experience while working in another role full time,hey there currently honing my skill from an education practice perspective on coursera and free online data set however wondering if it is feasible to get started on small freelance work from gig site like upwork or fiverr to have paid work experience to talk about ha anyone ha success in finding part time work a data engineer a well
debugging puppeteer on vm,using puppeteer to crawl website and my program runs perfectly on my own computer however when run it using docker on vm it throwing an error want to be able to run my code on the vm but somehow see the browser on my own computer so can try to debug what going on can someone help me figure out where to get started know how to ssh into the vm but it the browser part that unsure of if relevant the vm is gce instance with linux and on mac,0,debugging puppeteer on vm,using puppeteer to crawl website and my program run perfectly on my own computer however when run it using docker on vm it throwing an error want to be able to run my code on the vm but somehow see the browser on my own computer so can try to debug what going on can someone help me figure out where to get started know how to ssh into the vm but it the browser part that unsure of if relevant the vm is gce instance with linux and on mac
what is the workflow of adding new variables to an existing database mongodb collection,imagine have bunch of json files that want to add to mongodb collection json file could look like this json file name tom height weight also for every document in the collection would like to create new variable for example the bmi so that the final collection would look something like this mongodb collection name tom height weight bmi name sara height weight bmi could do this in for loop and for every json file could add it to the collection and create the new variable pseudocode for file in json_files add_to_collection create_bmi no issues until this point now imagine that after adding multiple json files want to create another variable say the bmr for the sake of the example let assume that have all the required variables to calculate it plus other json files will arrive in the future what are the best practices to do this is it fine to create an additional process that loops through the whole collection and adds the new variable or should create new collection instead or something else that am missing also should the create_bmi method be called after all jsons have been added so that may reuse some code when adding new variables or is it fine to have two different processes one to add already known variables and another one to add possible variables in the future understand this might be quite basic for this subreddit but not very familiar with best practices in this context thank you ps doing all this in python but looking for generic guidelines and best practices,0,what is the workflow of adding new variable to an existing database mongodb collection,imagine have bunch of json file that want to add to mongodb collection json file could look like this json file name tom height weight also for every document in the collection would like to create new variable for example the bmi so that the final collection would look something like this mongodb collection name tom height weight bmi name sara height weight bmi could do this in for loop and for every json file could add it to the collection and create the new variable pseudocode for file in json_files add_to_collection create_bmi no issue until this point now imagine that after adding multiple json file want to create another variable say the bmr for the sake of the example let assume that have all the required variable to calculate it plus other json file will arrive in the future what are the best practice to do this is it fine to create an additional process that loop through the whole collection and add the new variable or should create new collection instead or something else that am missing also should the create_bmi method be called after all jsons have been added so that may reuse some code when adding new variable or is it fine to have two different process one to add already known variable and another one to add possible variable in the future understand this might be quite basic for this subreddit but not very familiar with best practice in this context thank you p doing all this in python but looking for generic guideline and best practice
final part preparing for the azure data engineer certificate dp203 notes for optimizing data storage and processing,hello everyone for those of you who are seeing this post for the first time am preparing to for the dp203 using udemy course and am also documenting my learning so thought might as well share with the community ve already posted eight parts part part part part part part part part and par kind of found the topics little less interesting and may have not documented certain things without going into too much detail so probably use other resources as well to learn about these services the larger topics covered are from variety of services learning about the monitor service on azure cache feature in your dedicated sql pool monitoring in data factory metrics that are available in stream analytics monitoring stream job partitions in event hubs here is the link to the pdf well this is it this was the final part in this series hope whoever goes through the series leaves with better understanding of the azure services covered in the series thank you everyone,0,final part preparing for the azure data engineer certificate dp203 note for optimizing data storage and processing,hello everyone for those of you who are seeing this post for the first time am preparing to for the dp203 using udemy course and am also documenting my learning so thought might a well share with the community ve already posted eight part part part part part part part part part and par kind of found the topic little le interesting and may have not documented certain thing without going into too much detail so probably use other resource a well to learn about these service the larger topic covered are from variety of service learning about the monitor service on azure cache feature in your dedicated sql pool monitoring in data factory metric that are available in stream analytics monitoring stream job partition in event hub here is the link to the pdf well this is it this wa the final part in this series hope whoever go through the series leaf with better understanding of the azure service covered in the series thank you everyone
query log files from azure blob storage,have application that saves its log files as json file in the azure blob storage around files until now want to allow the team to be able query these files what is the best practice to do this am thinking to periodically load the log files into snowflake table where the team can query it but feel this may not be the best solution,0,query log file from azure blob storage,have application that save it log file a json file in the azure blob storage around file until now want to allow the team to be able query these file what is the best practice to do this am thinking to periodically load the log file into snowflake table where the team can query it but feel this may not be the best solution
databricks responds to snowflake tpc ds,databricks responds to snowflake tpc ds,0,databricks responds to snowflake tpc d,databricks responds to snowflake tpc d
big picture hadoop hdfs spark yarn very overwhelmed,hi guys am relatively new data engineer and my primary stack at work is using spark when google searching and trying to debug solutions and queries usually come across these terms hadoop hdfs yarn hbase and it gets quite overwhelming trying to understand the bigger picture of this ecosystem could someone please provide history of how all of these things relate together to provide bigger picture so can build stronger mental model,0,big picture hadoop hdfs spark yarn very overwhelmed,hi guy am relatively new data engineer and my primary stack at work is using spark when google searching and trying to debug solution and query usually come across these term hadoop hdfs yarn hbase and it get quite overwhelming trying to understand the bigger picture of this ecosystem could someone please provide history of how all of these thing relate together to provide bigger picture so can build stronger mental model
suggested bootcamps with no upfront cost,ve heard of bootcamps that have little to no upfront cost but instead will take cut of your salary for the first months year as payment anyone have any recommendations currently have background in analytics in telecom along with an as in comp sci however it hard to move into data eng when my role doesn use too many of the tools that are required of data eng positions,0,suggested bootcamps with no upfront cost,ve heard of bootcamps that have little to no upfront cost but instead will take cut of your salary for the first month year a payment anyone have any recommendation currently have background in analytics in telecom along with an a in comp sci however it hard to move into data eng when my role doesn use too many of the tool that are required of data eng position
airflow s3 trigger,have requirement to process files that arrive on s3 as and when it arrives the file may arrive anytime between am pm everyday there are around such feeds everyday which is the better solution aws services s3 triggers lambda that sends message to sqs have single dag that keeps running and polling sqs and triggers corresponding processing dag airflow sensor use s3 sensor for all dag start processing once file arrive has someone tried this which is feasible,0,airflow s3 trigger,have requirement to process file that arrive on s3 a and when it arrives the file may arrive anytime between am pm everyday there are around such feed everyday which is the better solution aws service s3 trigger lambda that sends message to sqs have single dag that keep running and polling sqs and trigger corresponding processing dag airflow sensor use s3 sensor for all dag start processing once file arrive ha someone tried this which is feasible
reading orc file from http,hi can anyone help me to figure out how can read orc files from http the orc files are on hdfs storage an entirely different system which can access only through http endpoint application is in java spring since http response is like an input stream am not able to figure out how to read records from that stream thanks,0,reading orc file from http,hi can anyone help me to figure out how can read orc file from http the orc file are on hdfs storage an entirely different system which can access only through http endpoint application is in java spring since http response is like an input stream am not able to figure out how to read record from that stream thanks
get sucked in coursera assignment,hello guys currently in middle of course in coursera which is data mining pipeline and need to complete simple assignment but they keep gave me in every trial have been trying to solve it for about days would be very grateful if someone could help me fix the problem format png amp auto webp amp aa0ea630947b29911330c746fcbc864bde123fa1 format png amp auto webp amp dd902620df7dff619cffa886b0b554a4a9c0a902,0,get sucked in coursera assignment,hello guy currently in middle of course in coursera which is data mining pipeline and need to complete simple assignment but they keep gave me in every trial have been trying to solve it for about day would be very grateful if someone could help me fix the problem format png amp auto webp amp aa0ea630947b29911330c746fcbc864bde123fa1 format png amp auto webp amp dd902620df7dff619cffa886b0b554a4a9c0a902
orm are the cigarettes of the data engineering world,orm are the cigarettes of the data engineering world,0,orm are the cigarette of the data engineering world,orm are the cigarette of the data engineering world
online resources on apache beam and dataflow,ll be working soon as de in my company and the work is going to be focused on building dataflow pipelines using apache beam are there any particularly useful resources online that can use to quickly scale up my knowledge on beam and terraform fine going through the mountain that is the official beam documentation but would prefer something much more concise,0,online resource on apache beam and dataflow,ll be working soon a de in my company and the work is going to be focused on building dataflow pipeline using apache beam are there any particularly useful resource online that can use to quickly scale up my knowledge on beam and terraform fine going through the mountain that is the official beam documentation but would prefer something much more concise
learn pyspark or scala without any prior programming experience,the title sums it up is it ok to do it totally new to programming and sql can anyone give me days plan or practice to become solid data engineer,0,learn pyspark or scala without any prior programming experience,the title sum it up is it ok to do it totally new to programming and sql can anyone give me day plan or practice to become solid data engineer
anyone here using aws data pipeline,reading up on it now to automate some processes we re using loading data into redshift and eventually saving the output in s3 data pipeline seems to be useful curious to hear your thoughts how you re using it is there some overlap with tools like airflow thanks,0,anyone here using aws data pipeline,reading up on it now to automate some process we re using loading data into redshift and eventually saving the output in s3 data pipeline seems to be useful curious to hear your thought how you re using it is there some overlap with tool like airflow thanks
generating documentation of data fields,hi everyone wondering what people are using to create documentation of data fields of tables created during the data processing steps we want to avoid documenting the data frame by manually annotating each field of the resulting data frame by different person from the person who wrote the code this person needs to read understand the code and understand the calculation that created the fields ideally the programmer would add comments directly to the processing code that later would be parsed to generate the documentation automatically bit how doxygen creates documentation for code does something like that exist amp x200b we have been looking at datedo but not convinced that people will be keep the documentation up to date as new versions of the data frames are created because of bug fixes new ideas and calculation behind field creation or changes in the input data thanks,0,generating documentation of data field,hi everyone wondering what people are using to create documentation of data field of table created during the data processing step we want to avoid documenting the data frame by manually annotating each field of the resulting data frame by different person from the person who wrote the code this person need to read understand the code and understand the calculation that created the field ideally the programmer would add comment directly to the processing code that later would be parsed to generate the documentation automatically bit how doxygen creates documentation for code doe something like that exist amp x200b we have been looking at datedo but not convinced that people will be keep the documentation up to date a new version of the data frame are created because of bug fix new idea and calculation behind field creation or change in the input data thanks
my team is looking for data pro who could help us with writing articles,hi all data startup founder my team is thinking of releasing good quality analytics articles or even research on topics about modern data stack and tools obviously such content shouldn be created by copywriters or marketers probably someone among you loves blogging and writing we will never take ownership of written materials and will never hide the name of the author no need to say that we consider it as paid collaboration,0,my team is looking for data pro who could help u with writing article,hi all data startup founder my team is thinking of releasing good quality analytics article or even research on topic about modern data stack and tool obviously such content shouldn be created by copywriter or marketer probably someone among you love blogging and writing we will never take ownership of written material and will never hide the name of the author no need to say that we consider it a paid collaboration
what is the size of your db and which fs you work with,have new real time db to design and wondering which file system should use happy to hear your opinion view poll,0,what is the size of your db and which f you work with,have new real time db to design and wondering which file system should use happy to hear your opinion view poll
extracting json schema from parquet file,is there any way can get json schema from parquet file using python code want to extract schema of some parquet files so that can use that schema in my terraform job to create table in bigquery,0,extracting json schema from parquet file,is there any way can get json schema from parquet file using python code want to extract schema of some parquet file so that can use that schema in my terraform job to create table in bigquery
laptop recommendation,want to buy new laptop will macbook air with m1 chip is good choice over any windows laptop within same price range,0,laptop recommendation,want to buy new laptop will macbook air with m1 chip is good choice over any window laptop within same price range
can someone explain like nvidia rapids to me and its relation with spark and dask,ve tried to do bit of reading about this but get confused because coiled claim rapids as if it victory for dask over spark but from looking at nvidia rapids website rapids ai it seems like rapids is just way for either spark or dask or other things to run on gpus so spark and dask broadly speaking follow the driver node worker nodes split apply combine strategy rapids is powered by gpus which inherently are very efficient at parallel computations so is rapids paradigm shift suggesting we no longer need driver worker clusters,0,can someone explain like nvidia rapid to me and it relation with spark and dask,ve tried to do bit of reading about this but get confused because coiled claim rapid a if it victory for dask over spark but from looking at nvidia rapid website rapid ai it seems like rapid is just way for either spark or dask or other thing to run on gpus so spark and dask broadly speaking follow the driver node worker node split apply combine strategy rapid is powered by gpus which inherently are very efficient at parallel computation so is rapid paradigm shift suggesting we no longer need driver worker cluster
industry benchmarks and competing with integrity snowflake blog,industry benchmarks and competing with integrity snowflake blog,0,industry benchmark and competing with integrity snowflake blog,industry benchmark and competing with integrity snowflake blog
advice on interpreting file format,am trying to access some data on blood pressure recorder so that can export the data as common format for use in excel etc want to use the recorder for study and using the trash software bundled with it will be pain the file format is awp which seems to be proprietary format with no explanation of what it does the file is unencrypted and so can be opened in notedpad most initial values are just stored as plain text for example yearbegin amp x200b after this the actual data is stored in way cannot understand e4090d001e00007a004a005700430001000000000 c1 e4090d01000000850047005400430001000000000 c2 these look like hex strings but nothing use seems to come up with anything that makes sense before abandon this idea does anyone here have any idea if there might be an easy way to decrypt this information,0,advice on interpreting file format,am trying to access some data on blood pressure recorder so that can export the data a common format for use in excel etc want to use the recorder for study and using the trash software bundled with it will be pain the file format is awp which seems to be proprietary format with no explanation of what it doe the file is unencrypted and so can be opened in notedpad most initial value are just stored a plain text for example yearbegin amp x200b after this the actual data is stored in way cannot understand e4090d001e00007a004a005700430001000000000 c1 e4090d01000000850047005400430001000000000 c2 these look like hex string but nothing use seems to come up with anything that make sense before abandon this idea doe anyone here have any idea if there might be an easy way to decrypt this information
what are your de related side hustles,after reading the hour work week it really made me want to quit the and start some company of my own want to leverage my de skills to create some semi passive income im curious if anyone else has done this and what has been successful,0,what are your de related side hustle,after reading the hour work week it really made me want to quit the and start some company of my own want to leverage my de skill to create some semi passive income im curious if anyone else ha done this and what ha been successful
how do you manage output paths,this might not be relevant to everyone here but in my pipelines we produce number of intermediary data paths pretty much all into s3 either as checkpoints within tasks or as hand off points between tasks all told the daily pipeline maintain all spark or pyspark btw probably outputs or so data artifacts with of these being permanent external data artifacts and the rest only existing for couple of weeks for debugging purposes we don currently have good way to manage these output paths if someone on my team needs to debug branch they re developing in they just kind of have to know which task to look in and look through the logs to get paths it saving to this is tedious and is high in the tribal knowledge aspect love some sort of system that tracks outputs of paths that engineers can see into that isn just like logging what do you all do do you know of any frameworks libraries systems that track this for you,0,how do you manage output path,this might not be relevant to everyone here but in my pipeline we produce number of intermediary data path pretty much all into s3 either a checkpoint within task or a hand off point between task all told the daily pipeline maintain all spark or pyspark btw probably output or so data artifact with of these being permanent external data artifact and the rest only existing for couple of week for debugging purpose we don currently have good way to manage these output path if someone on my team need to debug branch they re developing in they just kind of have to know which task to look in and look through the log to get path it saving to this is tedious and is high in the tribal knowledge aspect love some sort of system that track output of path that engineer can see into that isn just like logging what do you all do do you know of any framework library system that track this for you
how to deconcentrate data with negative values,tried adding constant and then log doesnt help binning is an option if all hope fails need it for visualizing better and if possible keep the negative signs count mean std min max name change dtype float64,0,how to deconcentrate data with negative value,tried adding constant and then log doesnt help binning is an option if all hope fails need it for visualizing better and if possible keep the negative sign count mean std min max name change dtype float64
how to create the technical strategy for data engineering team,ve been asked in few interviews about how to create technical strategy for data engineering team as the topic is really broad don think the interviewer wants to know only about the technical stack ingestion with nifi processing with apache spark etc how would you build such document what aspects would you consider what do you think is being evaluated here,0,how to create the technical strategy for data engineering team,ve been asked in few interview about how to create technical strategy for data engineering team a the topic is really broad don think the interviewer want to know only about the technical stack ingestion with nifi processing with apache spark etc how would you build such document what aspect would you consider what do you think is being evaluated here
base practice or concept for data staging,hi there just started make etl project on development instance but still confused about data staging data staging is same with transaction database but is different instance or extract some data with specific column or extract data from join table what tools or technology use put data to database mysql or other rdbms or extract to file like flat file csv because currently extract from transaction database from join table and put data to warehouse database,0,base practice or concept for data staging,hi there just started make etl project on development instance but still confused about data staging data staging is same with transaction database but is different instance or extract some data with specific column or extract data from join table what tool or technology use put data to database mysql or other rdbms or extract to file like flat file csv because currently extract from transaction database from join table and put data to warehouse database
require some open source data pipeline package code in python for self study,have an interview where am suppose to read and understand etl pipeline repo of the company written in python my job is to suggest improvement if any and do some extensions the thing is that have only used python for some home projects as script but never used it extensively or professionally am well versed with few other programming language such as php java script plsql please suggest me few data pipeline projects that can study and get hang of the flow,0,require some open source data pipeline package code in python for self study,have an interview where am suppose to read and understand etl pipeline repo of the company written in python my job is to suggest improvement if any and do some extension the thing is that have only used python for some home project a script but never used it extensively or professionally am well versed with few other programming language such a php java script plsql please suggest me few data pipeline project that can study and get hang of the flow
advice on college major and minor for data analyst,am currently undergraduate sophomore at my college and am soon going to declare my major want to make sure that am picking the right degree major minor for becoming business intelligence analyst specialist and data analyst the degree is called mis management information systems the degree is mixture between business and information technology all the business classes already finished like marketing accounting finance management etc the soon to be technical classes will take are python database management systems sql system analysis data warehousing etl data analytics business intelligence project management and an internship are those classes worth taking for the data field already now those are classes that are good in the data field but just want to make sure are any of those particular subjects difficult to learn as in are the concepts hard to understand if you need comparsion for something find difficult it something that requires tremendous hours of studying to fully understand the concept of what is being taught in class like computer engineering majors how would you rank them from being the most difficult to being the easiest if it turns out that don like data analytics once graduate what other carrer fields can go into that aren business intelligence specialist or data analyst as for the minor my college recommends minor in marketing applied economics supply chain or finance out of those options which one will be the best and most broad in the data analyst field with that certain combination of the mis major and whatever minor you recommend get what other types of career fields can go into,0,advice on college major and minor for data analyst,am currently undergraduate sophomore at my college and am soon going to declare my major want to make sure that am picking the right degree major minor for becoming business intelligence analyst specialist and data analyst the degree is called mi management information system the degree is mixture between business and information technology all the business class already finished like marketing accounting finance management etc the soon to be technical class will take are python database management system sql system analysis data warehousing etl data analytics business intelligence project management and an internship are those class worth taking for the data field already now those are class that are good in the data field but just want to make sure are any of those particular subject difficult to learn a in are the concept hard to understand if you need comparsion for something find difficult it something that requires tremendous hour of studying to fully understand the concept of what is being taught in class like computer engineering major how would you rank them from being the most difficult to being the easiest if it turn out that don like data analytics once graduate what other carrer field can go into that aren business intelligence specialist or data analyst a for the minor my college recommends minor in marketing applied economics supply chain or finance out of those option which one will be the best and most broad in the data analyst field with that certain combination of the mi major and whatever minor you recommend get what other type of career field can go into
confluent solution engineer interview,hi guys have an interview for an entry level solution engineer role at confluent leading cloud based event streaming platform the role is focused on getting users to upgrade from opensource kafka to payg confluent cloud and guiding them through the journey to becoming happy customers amp x200b this isn my background ve busted my butt off doing an amazon cloud practitioner essentials course which took forever but was free and clued me up quite well on cloud amp x200b wanted to ask you guys if you have any interesting questions can ask the solution engineering interviewer that would demonstrate curiosity especially in relation to current industry trends amp x200b and also what kind of questions can expect to be asked for such position thanks,0,confluent solution engineer interview,hi guy have an interview for an entry level solution engineer role at confluent leading cloud based event streaming platform the role is focused on getting user to upgrade from opensource kafka to payg confluent cloud and guiding them through the journey to becoming happy customer amp x200b this isn my background ve busted my butt off doing an amazon cloud practitioner essential course which took forever but wa free and clued me up quite well on cloud amp x200b wanted to ask you guy if you have any interesting question can ask the solution engineering interviewer that would demonstrate curiosity especially in relation to current industry trend amp x200b and also what kind of question can expect to be asked for such position thanks
do these tasks seem doable for recent graduate,so recently graduated and this is the task got given from one of my first interviews really struggling with the pandas and sql sections python create function fibonacci to compute nth number of the fibonacci sequence fibonacci sequence create function inv_fibonacci to compute the inverse of fibonacci that is inv_fibonacci fibonacci consider pandas dataframe activity with the following columns customerid int date date depositamount float withdrawalamount float create function balances activity which returns pandas dataframe containing customerid date openingbalance closingbalance where closingbalance openingbalance depositamount withdrawalamount and openingbalance the pervious day closing balance for every customer in activity there should be record for each date from the earliest to the latest in activity assume an initial opening balance of note customers will not have activity records on every date sql there are tables outlined here create sql queries to answer the following questions create table to show number of depositors by year month output columns yearmonth depositors create table to show response rate of each offer positive response is communication which results in deposit on the day of or the day after the communication output columns offername positiveresponses numcomms responserate,0,do these task seem doable for recent graduate,so recently graduated and this is the task got given from one of my first interview really struggling with the panda and sql section python create function fibonacci to compute nth number of the fibonacci sequence fibonacci sequence create function inv_fibonacci to compute the inverse of fibonacci that is inv_fibonacci fibonacci consider panda dataframe activity with the following column customerid int date date depositamount float withdrawalamount float create function balance activity which return panda dataframe containing customerid date openingbalance closingbalance where closingbalance openingbalance depositamount withdrawalamount and openingbalance the pervious day closing balance for every customer in activity there should be record for each date from the earliest to the latest in activity assume an initial opening balance of note customer will not have activity record on every date sql there are table outlined here create sql query to answer the following question create table to show number of depositor by year month output column yearmonth depositor create table to show response rate of each offer positive response is communication which result in deposit on the day of or the day after the communication output column offername positiveresponses numcomms responserate
help with skewed data with negative values,hey newbie here ve got feature change which is relative and the difference between values is really small but it really crucial as am building predictive model based on this feature want to interpolate it and wanna know if machine learning models are sensitive enough to catch these small differences count mean std min max name change dtype float64,0,help with skewed data with negative value,hey newbie here ve got feature change which is relative and the difference between value is really small but it really crucial a am building predictive model based on this feature want to interpolate it and wanna know if machine learning model are sensitive enough to catch these small difference count mean std min max name change dtype float64
how to test python data pipeline functions,currently in the processes of writing unit and integration tests for set of data processing functions that runs on aws instances struggling to write meaningful tests for some of the more complex function that consist of pulling together transforming and joining large pandas dataframes my current strategy is to build collection of example input dataframe and expected outputs and compare them at the end however the size of these inputs and outputs are large and so checking line by line that these are correct before committing them won scale was wondering what the best way to test these kinds of pipeline functions are and any tools that help to run these kinds of tests currently just building pytest script together to run and manage these tests like more simple unit tests is there better strategy for this,0,how to test python data pipeline function,currently in the process of writing unit and integration test for set of data processing function that run on aws instance struggling to write meaningful test for some of the more complex function that consist of pulling together transforming and joining large panda dataframes my current strategy is to build collection of example input dataframe and expected output and compare them at the end however the size of these input and output are large and so checking line by line that these are correct before committing them won scale wa wondering what the best way to test these kind of pipeline function are and any tool that help to run these kind of test currently just building pytest script together to run and manage these test like more simple unit test is there better strategy for this
how to get into entry level data engineering jobs,am looking for some sort of roadmap and then good place to look for these jobs,0,how to get into entry level data engineering job,am looking for some sort of roadmap and then good place to look for these job
made pancakedb new type of columnar db that uses less storage and read time than snappy parquet while offering efficient incremental writes,made pancakedb new type of columnar db that uses less storage and read time than snappy parquet while offering efficient incremental writes,0,made pancakedb new type of columnar db that us le storage and read time than snappy parquet while offering efficient incremental writes,made pancakedb new type of columnar db that us le storage and read time than snappy parquet while offering efficient incremental writes
how to replace all values with one integer and all nan values with in pandas,basically binning or other as thanks,0,how to replace all value with one integer and all nan value with in panda,basically binning or other a thanks
how to switch from etl testing role to de role,have years of work experience in etl testing and now thinking to switch to data engineering role in azure have little experience in building pipelines in data factory for load and table migrations have intermediate sql and python skills could anyone give me roadmap for de role so can focus only on important things note have an azure subscription provided by the company so cost is not constraint thanks in advance,0,how to switch from etl testing role to de role,have year of work experience in etl testing and now thinking to switch to data engineering role in azure have little experience in building pipeline in data factory for load and table migration have intermediate sql and python skill could anyone give me roadmap for de role so can focus only on important thing note have an azure subscription provided by the company so cost is not constraint thanks in advance
has anyone done an aws live classroom training before,specifically asking for the data warehousing on aws one summary costs around usd day instructor led training course in person or virtual want to know is it worth it general thoughts on it,0,ha anyone done an aws live classroom training before,specifically asking for the data warehousing on aws one summary cost around usd day instructor led training course in person or virtual want to know is it worth it general thought on it
options for client data retrieval etl,we are planning to build product to provide dashboards to clients for context this is consultant client relationship with multiple clients would it be advisable to port the data into our the consultant environment and work with it etl presentation or work directly from the client environment if cloning the data outside the client environment is not ideal how would one go about performing the necessary data transformations to feed into bi platform something like cloud etl or something apologies if this isn the right place to post am bit new to this,0,option for client data retrieval etl,we are planning to build product to provide dashboard to client for context this is consultant client relationship with multiple client would it be advisable to port the data into our the consultant environment and work with it etl presentation or work directly from the client environment if cloning the data outside the client environment is not ideal how would one go about performing the necessary data transformation to feed into bi platform something like cloud etl or something apology if this isn the right place to post am bit new to this
