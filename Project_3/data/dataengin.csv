all_awardings,allow_live_comments,author,author_flair_background_color,author_flair_css_class,author_flair_richtext,author_flair_template_id,author_flair_text,author_flair_text_color,author_flair_type,author_fullname,author_is_blocked,author_patreon_flair,author_premium,awarders,can_mod_post,contest_mode,created_utc,domain,full_link,gildings,id,is_created_from_ads_ui,is_crosspostable,is_meta,is_original_content,is_reddit_media_domain,is_robot_indexable,is_self,is_video,link_flair_background_color,link_flair_richtext,link_flair_template_id,link_flair_text,link_flair_text_color,link_flair_type,locked,media_only,no_follow,num_comments,num_crossposts,over_18,parent_whitelist_status,permalink,pinned,post_hint,preview,pwls,retrieved_on,score,selftext,send_replies,spoiler,stickied,subreddit,subreddit_id,subreddit_subscribers,subreddit_type,thumbnail,title,total_awards_received,treatment_tags,upvote_ratio,url,whitelist_status,wls,media,media_embed,removed_by_category,secure_media,secure_media_embed,thumbnail_height,thumbnail_width,url_overridden_by_dest,media_metadata,gallery_data,is_gallery,author_cakeday,poll_data,crosspost_parent,crosspost_parent_list,edited,banned_by,collections
[],False,urs123,transparent,,[],fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b,Data Engineer,dark,text,t2_10z4rv,False,False,False,[],False,False,1642058431,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s2tim7/amazon_ads_api/,{},s2tim7,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s2tim7/amazon_ads_api/,False,self,"{'enabled': False, 'images': [{'id': 'VbgCuezFM4YbdvXlY-WHDgcLcrv7hsv1cKxd6W38YGI', 'resolutions': [{'height': 56, 'url': 'https://external-preview.redd.it/bVrv40Fka2YLNVy1IOAt8Agjn9IsiJn5J_78mWhf16I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=83d596f6737d3fc71d7367d772dad2c41f97b1a6', 'width': 108}, {'height': 113, 'url': 'https://external-preview.redd.it/bVrv40Fka2YLNVy1IOAt8Agjn9IsiJn5J_78mWhf16I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=13fb69cd615fb3f65dcd6098ca998ad888ed9073', 'width': 216}, {'height': 168, 'url': 'https://external-preview.redd.it/bVrv40Fka2YLNVy1IOAt8Agjn9IsiJn5J_78mWhf16I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c3969de9194f5dc5c43fc2d42cb0f8b066b5b091', 'width': 320}, {'height': 336, 'url': 'https://external-preview.redd.it/bVrv40Fka2YLNVy1IOAt8Agjn9IsiJn5J_78mWhf16I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4dbf42cb5723d28761fb1f86a13b5c46446e1883', 'width': 640}, {'height': 504, 'url': 'https://external-preview.redd.it/bVrv40Fka2YLNVy1IOAt8Agjn9IsiJn5J_78mWhf16I.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7f2ff5b193a00e8ebafa03c095cc26bd4553e91d', 'width': 960}, {'height': 567, 'url': 'https://external-preview.redd.it/bVrv40Fka2YLNVy1IOAt8Agjn9IsiJn5J_78mWhf16I.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3a6c95e146929f6f2792a26f095cc72a3f2495d9', 'width': 1080}], 'source': {'height': 630, 'url': 'https://external-preview.redd.it/bVrv40Fka2YLNVy1IOAt8Agjn9IsiJn5J_78mWhf16I.jpg?auto=webp&amp;s=ffff4b5f810818b8e0db4487bdca89e66174e0b9', 'width': 1199}, 'variants': {}}]}",6,1642058441,1,"I need to extract data from Amazon Ads and DSP campaigns. The business has the reports in Amazon platform, but they would like to create more in detail reports and cross compare, so I need to take the underlying data of said reports or, at least, download and extract the data from the reports.

Does anyone have any experience with this / these API(s)?

Link to API doc below, I am reading it, but it's slow (and painful). Also, Google did not help - there are articles advertising 3rd party solutions of extracting data, so I know it is doable, at least.

[APIs Documentation](https://advertising.amazon.com/about-api)",True,False,False,dataengineering,t5_36en4,49772,public,self,Amazon Ads API,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s2tim7/amazon_ads_api/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,manish_ks,,,[],,,,text,t2_h6c526me,False,False,False,[],False,False,1642055138,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s2skpg/castledio_is_the_fastest_reverse_etl_platform/,{},s2skpg,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/s2skpg/castledio_is_the_fastest_reverse_etl_platform/,False,self,"{'enabled': False, 'images': [{'id': 'gidq1CJjO89rOM1s939aa_DVoIG7GEB-EiagDwUASc0', 'resolutions': [{'height': 54, 'url': 'https://external-preview.redd.it/JhCn3JvBxY7TlXlHwOG5Y7rPQTQixm12kCsMWRB4njE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6da2666e90304b22146fb92a6b6dfe34423e4d1f', 'width': 108}, {'height': 108, 'url': 'https://external-preview.redd.it/JhCn3JvBxY7TlXlHwOG5Y7rPQTQixm12kCsMWRB4njE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e6dabecd12ad1504bc58f8e3bc3edecefc6a144d', 'width': 216}, {'height': 160, 'url': 'https://external-preview.redd.it/JhCn3JvBxY7TlXlHwOG5Y7rPQTQixm12kCsMWRB4njE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d6354754883a52b37c98542c77fbf941a8e3a9bb', 'width': 320}, {'height': 320, 'url': 'https://external-preview.redd.it/JhCn3JvBxY7TlXlHwOG5Y7rPQTQixm12kCsMWRB4njE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f0b5d29f4b374f9285ed74752fa5224c3dd21337', 'width': 640}, {'height': 480, 'url': 'https://external-preview.redd.it/JhCn3JvBxY7TlXlHwOG5Y7rPQTQixm12kCsMWRB4njE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d2f60706c4113d5520b7db8298a35f837db78e4d', 'width': 960}, {'height': 540, 'url': 'https://external-preview.redd.it/JhCn3JvBxY7TlXlHwOG5Y7rPQTQixm12kCsMWRB4njE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d777b0e8b6c91c5a9c1240798cbf6e1faa5d4da0', 'width': 1080}], 'source': {'height': 600, 'url': 'https://external-preview.redd.it/JhCn3JvBxY7TlXlHwOG5Y7rPQTQixm12kCsMWRB4njE.jpg?auto=webp&amp;s=8ca8cbad9e81175cdb53a48520c1bdaa1dcd9487', 'width': 1200}, 'variants': {}}]}",6,1642055148,1,"Castled is 6x to 13x faster than other Reverse ETL platforms like Census and Hightouch! The project is [open source](https://github.com/castledio/castled).

Census published a blog a month ago in which they claimed to be the fastest Reverse ETL solution. They even encouraged other solutions to publish their respective performance benchmarks using their dataset.

While Castled has a different take on the need for speed in a data integration solution, we thought it would be interesting to take up this challenge. [Check out this blog to know more](https://medium.com/castled/fastest-reverse-etl-platform-census-vs-hightouch-vs-castled-3d2975dd4e55)",True,False,False,dataengineering,t5_36en4,49771,public,self,Castled.io is the fastest Reverse ETL platform till date,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s2skpg/castledio_is_the_fastest_reverse_etl_platform/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Junior_Abies_2213,,,[],,,,text,t2_g06clmjl,False,False,False,[],False,False,1642055026,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s2sjhz/how_to_avoid_jdbc_in_azure_synapse_analytics/,{},s2sjhz,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s2sjhz/how_to_avoid_jdbc_in_azure_synapse_analytics/,False,,,6,1642055036,1,"Hi, currently I am doing ETL process using synapse Analytics. We are reading delta files from data lake and doing a incremental load on SQL table using spark pool. Jdbc throughout is very low and want to avoid it any suggestions.
Using spark dataframe because transformation are very complex and everything is parameterized.",True,False,False,dataengineering,t5_36en4,49770,public,self,How to avoid jdbc in Azure Synapse Analytics using spark pool for SQL Updates and Inserts,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s2sjhz/how_to_avoid_jdbc_in_azure_synapse_analytics/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,LowProgram6449,,,[],,,,text,t2_d7m50dkt,False,False,False,[],False,False,1642054490,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s2sdrb/help_for_projects_data_source/,{},s2sdrb,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s2sdrb/help_for_projects_data_source/,False,,,6,1642054500,1,"Hi! I am trying to build a project for my uni's FYP and decided that I will go all-in so that it can double up as my personal portfolio as well. My idea is to build a modern batch end-to-end data pipeline with containers orchestrated with Airflow, from ingestion to visualization.

Originally I was hoping to collaborate with a company for the data source (to develop a POC for the pipeline), but as time goes on I began to feel that it is incredibly difficult for companies to share their data to a student. Therefore, I began to look into the possibility of using open source data or Live API.

The problem is, I am facing difficulties of choosing the most appropriate data sources. I hoped my project could accomplish and demonstrate these characteristics below:

\- Integration from multiple diverse data sources

\- Batch import every X time (means the data should be timestamped and constantly updating)

\- Data sources rich enough for a few visualizations (possibly ML)

Can anybody advise me what data source that I can potentially use to accomplish this? Thankss!",True,False,False,dataengineering,t5_36en4,49769,public,self,Help for Project's Data Source,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s2sdrb/help_for_projects_data_source/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,barnyard9,,,[],,,,text,t2_i66qgsyw,False,False,False,[],False,False,1642046636,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s2pzax/forums_for_asking_homework_help_for_big_data/,{},s2pzax,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/s2pzax/forums_for_asking_homework_help_for_big_data/,False,,,6,1642046646,1,"I am ofc not asking to do my homework, but  sometimes  I get confused and need to ask confusions, what forums are good for it?",True,False,False,dataengineering,t5_36en4,49764,public,self,Forums for asking homework help for big data?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s2pzax/forums_for_asking_homework_help_for_big_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,soricellia,,,[],,,,text,t2_jrmn04,False,False,False,[],False,False,1642042048,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s2oev1/how_to_level_up_my_data_engineering_game/,{},s2oev1,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/s2oev1/how_to_level_up_my_data_engineering_game/,False,,,6,1642042059,1,"Hey there,

I work at a smallish company in the ecom business. I use azure synapse and power bi for all of my reporting needs.

Originally, I had used dataflows in power bi to report on data loaded from a traditional sql database, but as our datasets have grown we've quickly outscaled that solution as any report over 1gb failed to refresh in Microsoft's BI service appropriately. Scaling was a nightmare and I quickly asked salsa to cool it on this feature. 

Today, I load data from various sources, then generate KPIs and dimensions in synapse to load into power bi. This makes the refresh times much quicker as I am refreshing data already prepared and  aggregated. This also means I can scale my solution out to multiple clients with relative ease.

One thing to note is i do most of my work through the synapse UI. I create all of our dataflows and pipelines through the synapse UI. It is very 'low code' and I'm not sure if this is optimal or even how to make it a better solution. 

When I look at posts on here, everyone is honing their craft with Python notebooks or using some programming solution to get it done. 

My questions to my fellow redditors:
Is it even possible to get a data engineering job with some SQL, BI and azure synapse knowledge? I've built our data warehouse from scratch, but I worry because I'm learning to float and I'm being asked to swim. I don't really have a mentor and I'm kinda making it up as I go.

What are some good resources to help up my game and give me the best opportunity possible to build cool things in this space? As I said, I am building everything from scratch myself. I am being asked to lead this front and I have no clue what I'm doing. I'm desperate for resources to learn from.

Thanks in advance",True,False,False,dataengineering,t5_36en4,49758,public,self,how to level up my data engineering game?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s2oev1/how_to_level_up_my_data_engineering_game/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,AnxiouslyCalming,,,[],,,,text,t2_84t6nl3q,False,False,False,[],False,False,1642041874,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s2ocuj/event_traits_best_practices_segmentrudderstack/,{},s2ocuj,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/s2ocuj/event_traits_best_practices_segmentrudderstack/,False,,,6,1642041884,1,"Hey everyone,

Trying to get a grasp over our event traits being sent to Segment and soon RudderStack. I've been auditing our event data and it seems like our tracking plan basically mashes as much data into traits that don't really have anything to do with the event. Here's an example:

    track(""VIDEO_LIKED"", {
      videoId: 123,
      ...
      videoAuthorUserType: ""Premium Member""
    });

Our product person claims they need the data but we already have video author information in MixPanel. It seems to me like MixPanel should be able to derive the videoAuthorUserType no? Is the above bad practice? Any tips on best practices for shaping event data?

It seems to be some of this data is being flattened together to compensate for a lack of a feature on certain destinations but that should probably be fixed with a transformation to the destination no?

Appreciate any advice or tips.",True,False,False,dataengineering,t5_36en4,49757,public,self,Event Traits Best Practices (Segment/Rudderstack),0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s2ocuj/event_traits_best_practices_segmentrudderstack/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Natural-Carrot-723,,,[],,,,text,t2_gd818se3,False,False,False,[],False,False,1642038830,youtube.com,https://www.reddit.com/r/dataengineering/comments/s2nafh/merge_sort_part_5/,{},s2nafh,False,False,False,False,False,False,False,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s2nafh/merge_sort_part_5/,False,rich:video,"{'enabled': False, 'images': [{'id': 'XNoLubgr0puVghZMjfrJ6iHHi0G21-2DY6vHGq9oD1k', 'resolutions': [{'height': 81, 'url': 'https://external-preview.redd.it/NdVxO-SMrY7GJzpcMVOHBotGInisqGTnpl7V7OtW7xo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=edcb282a0a9503932d9b661a60420a6b085beda4', 'width': 108}, {'height': 162, 'url': 'https://external-preview.redd.it/NdVxO-SMrY7GJzpcMVOHBotGInisqGTnpl7V7OtW7xo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=af5be141f5515c4ac5a0ef6ca60c8014d3d45557', 'width': 216}, {'height': 240, 'url': 'https://external-preview.redd.it/NdVxO-SMrY7GJzpcMVOHBotGInisqGTnpl7V7OtW7xo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=92ca51e8fe9e6788bc7ff966709adbc240818460', 'width': 320}], 'source': {'height': 360, 'url': 'https://external-preview.redd.it/NdVxO-SMrY7GJzpcMVOHBotGInisqGTnpl7V7OtW7xo.jpg?auto=webp&amp;s=79549de32b9538163553d3bc89629be5d5cf13b4', 'width': 480}, 'variants': {}}]}",6,1642038847,1,,True,False,False,dataengineering,t5_36en4,49753,public,https://b.thumbs.redditmedia.com/gDrRKNRg7-82sPqKd9lbaFGeA8irjbyx1LLuP3yHtik.jpg,merge sort part 5,0,[],1.0,https://youtube.com/watch?v=CumaDwT7iFA&amp;feature=share,all_ads,6,"{'oembed': {'author_name': 'WRITING ALGORITHMS.....', 'author_url': 'https://www.youtube.com/channel/UCnhMkaVPneIrHzd1k3WUAeg', 'height': 200, 'html': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/CumaDwT7iFA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/CumaDwT7iFA/hqdefault.jpg', 'thumbnail_width': 480, 'title': 'merge sort part 5', 'type': 'video', 'version': '1.0', 'width': 267}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/CumaDwT7iFA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'scrolling': False, 'width': 267}",reddit,"{'oembed': {'author_name': 'WRITING ALGORITHMS.....', 'author_url': 'https://www.youtube.com/channel/UCnhMkaVPneIrHzd1k3WUAeg', 'height': 200, 'html': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/CumaDwT7iFA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/CumaDwT7iFA/hqdefault.jpg', 'thumbnail_width': 480, 'title': 'merge sort part 5', 'type': 'video', 'version': '1.0', 'width': 267}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/CumaDwT7iFA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/s2nafh', 'scrolling': False, 'width': 267}",105.0,140.0,https://youtube.com/watch?v=CumaDwT7iFA&amp;feature=share,,,,,,,,,,
[],False,No_Engine1637,,,[],,,,text,t2_7hgjxjf5,False,False,False,[],False,False,1642031028,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s2kki7/does_this_architecture_make_any_sense/,{},s2kki7,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s2kki7/does_this_architecture_make_any_sense/,False,,,6,1642031038,1,"I'm trying to teach myself data engineering, I learn better doing projects so I thought about doing a rather complex project that involves streaming data and it might be too ambitious for a total noob like myself.

What I'm trying to do is to create an architecture that grabs tweets, news and the prices of several cryptocurrencies and then try to give a forecast of their prices taking the sentiment of the tweets and classified texts as vectors from the news as predicting variables, this forecast gets updated with new predictions in near real time, and I thought something like this, but to be honest I'm not sure whether it makes sense or not.

I wonder if it would be possible to do forecasting in the same process of doing the sentiment analysis and text classification vectorization with Spark, or if I'd need to store the vectors and the value of the sentiment analysis in a NoSQL database and then doing the forecasting taking the information from this database.

I'd really appreciate any insights or recommendations!",True,False,False,dataengineering,t5_36en4,49742,public,self,Does this architecture make any sense?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s2kki7/does_this_architecture_make_any_sense/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,q666,,,[],,,,text,t2_46uca,False,False,False,[],False,False,1642024599,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s2i4ho/what_software_architecture_in_an_etl_oriented/,{},s2i4ho,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s2i4ho/what_software_architecture_in_an_etl_oriented/,False,,,6,1642024610,1,"I'm working on a monolithic PySpark project and have now an opportunity to refactor and redesign things a bit so was wondering what's your approach for code organization and composition, do you introduce any additional abstractions for your data transformers?

It seems like this topic in the context of pure data-oriented projects is quite poor... There is a ton of books and talks around software design but it often seems not applicable when building regular data transformers.   
I started to dig into some potential functional programming approaches and I came across **polylith** but also no success with data context examples there.

Do you prefer an OOP/class-based approach or FP?",True,False,False,dataengineering,t5_36en4,49740,public,self,What software architecture in an ETL oriented project is working out for you?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s2i4ho/what_software_architecture_in_an_etl_oriented/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,cpardl,,,[],,,,text,t2_fb1s1pke,False,False,False,[],False,False,1642014702,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s2e9ct/data_lakes_and_lake_houses_conversation_with/,{},s2e9ct,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s2e9ct/data_lakes_and_lake_houses_conversation_with/,False,self,"{'enabled': False, 'images': [{'id': '_yoscnXb1RFxtTuBZnbzDMJHCehvluaPTHyNfrCbMkw', 'resolutions': [{'height': 60, 'url': 'https://external-preview.redd.it/Deqlntlo9qb5vgHPqHMHWcvNrdzsWAxeZ0GCv4dKMO8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0e577c0d8f3a992b1dc57675a3d26252ad81247a', 'width': 108}, {'height': 121, 'url': 'https://external-preview.redd.it/Deqlntlo9qb5vgHPqHMHWcvNrdzsWAxeZ0GCv4dKMO8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1763e65c857ad2771645d4ea3c2e850f17bfccef', 'width': 216}, {'height': 180, 'url': 'https://external-preview.redd.it/Deqlntlo9qb5vgHPqHMHWcvNrdzsWAxeZ0GCv4dKMO8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7d90b0dbd576e3eba2f0ad686163a85121f65603', 'width': 320}, {'height': 360, 'url': 'https://external-preview.redd.it/Deqlntlo9qb5vgHPqHMHWcvNrdzsWAxeZ0GCv4dKMO8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f5971a1ba699a90ffd3b8a8cfbdc3fd309954682', 'width': 640}, {'height': 540, 'url': 'https://external-preview.redd.it/Deqlntlo9qb5vgHPqHMHWcvNrdzsWAxeZ0GCv4dKMO8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0f89730a35d8dbeffc76af7828edd89f296f4ce7', 'width': 960}, {'height': 607, 'url': 'https://external-preview.redd.it/Deqlntlo9qb5vgHPqHMHWcvNrdzsWAxeZ0GCv4dKMO8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b71ab8f712333f16f45b3c81feba003de6126689', 'width': 1080}], 'source': {'height': 900, 'url': 'https://external-preview.redd.it/Deqlntlo9qb5vgHPqHMHWcvNrdzsWAxeZ0GCv4dKMO8.jpg?auto=webp&amp;s=34de8ecb2dda95ced12ebd7c21053e1e7981ba1d', 'width': 1600}, 'variants': {}}]}",6,1642014712,1,"I had an amazing conversation with Vinoth about data lake and lakehouse technologies. He gave one of the clearest explanations of what a lakehouse is. Vinoth is an amazing guy and has crazy experience in large scale data systems and he was kind enough to share part of this experience with me.

You can listen to the conversation here: [Data warehouses, data lakes, lakehouses and large scale data systems with Vinoth from Apache Hudi.](https://datastackshow.com/podcast/the-difference-between-data-lakes-and-data-warehouses-with-vinoth-chandar-of-apache-hudi/)",True,False,False,dataengineering,t5_36en4,49730,public,self,Data lakes and lake houses conversation with Vinoth Chandar creator of the Apache Hudi project,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s2e9ct/data_lakes_and_lake_houses_conversation_with/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,columns_ai,,,[],,,,text,t2_el5l5mnh,False,False,False,[],False,False,1642014049,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s2dzos/introduce_an_opensource_project_in_data/,{},s2dzos,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s2dzos/introduce_an_opensource_project_in_data/,False,self,"{'enabled': False, 'images': [{'id': 'LxHbDqrIIwpZXzPI9nHxcq2R7rPjD7dy8z_TrtcSpis', 'resolutions': [{'height': 108, 'url': 'https://external-preview.redd.it/oGyPTDvHaz8QnGbRD2XZuIK8T5mmieVaUvvgayu40nA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dfb8b20b352e059871509c9187c57b4be5e0dbdf', 'width': 108}, {'height': 216, 'url': 'https://external-preview.redd.it/oGyPTDvHaz8QnGbRD2XZuIK8T5mmieVaUvvgayu40nA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5e4487690b5da87fbfd40da62eea50a8648c71a8', 'width': 216}, {'height': 320, 'url': 'https://external-preview.redd.it/oGyPTDvHaz8QnGbRD2XZuIK8T5mmieVaUvvgayu40nA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2daf509c1d3a1f5f4a04e9aeac2c8f8aabab51ce', 'width': 320}, {'height': 640, 'url': 'https://external-preview.redd.it/oGyPTDvHaz8QnGbRD2XZuIK8T5mmieVaUvvgayu40nA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=32a4fdaca453c010352bfd292518c3289584efb1', 'width': 640}, {'height': 960, 'url': 'https://external-preview.redd.it/oGyPTDvHaz8QnGbRD2XZuIK8T5mmieVaUvvgayu40nA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=857b2ce74dae9f63fcdefb283cfc9db1dd41eaff', 'width': 960}], 'source': {'height': 1024, 'url': 'https://external-preview.redd.it/oGyPTDvHaz8QnGbRD2XZuIK8T5mmieVaUvvgayu40nA.jpg?auto=webp&amp;s=6cc74073464a720c82a52d9395b814c104ace811', 'width': 1024}, 'variants': {}}]}",6,1642014060,1,"Introduce this open-source project under Apache License 2.0, hope it's useful and interesting to someone.

[https://github.com/varchar-io/nebula](https://github.com/varchar-io/nebula)

This project connects to streaming data from streaming engines like Kafka or cloud storage like AWS S3 and gives you a built-in UI to analyze data into graphics. It's easy to run either in Kubernetes or hand-crafted clusters with VMs. 

If interested, check out more details on the project page...",True,False,False,dataengineering,t5_36en4,49730,public,self,Introduce an open-source project in data engineering,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s2dzos/introduce_an_opensource_project_in_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,boggle_thy_mind,,,[],,,,text,t2_9d1jjuxh,False,False,False,[],False,False,1642013816,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s2dw46/are_files_stored_in_a_docker_container_secure/,{},s2dw46,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s2dw46/are_files_stored_in_a_docker_container_secure/,False,,,6,1642013827,1,"I created a docker image to pick some data from an API and then store it in our data lake, but as an intermediate step, it stores and processes the data ""locally"" before uploading. In this particular case the data is not really sensitive, but it still made me think - **is the data stored in a docker container secure?** is it immediately destroyed after the container is done? can it be accessed by the service I'm running it on, Azure Container Registry in this case? Or persist after the container is done?

In case of processing PPI, are there any considerations when processing and storing the data even temporarily in a container?",True,False,False,dataengineering,t5_36en4,49729,public,self,Are files stored in a docker container secure?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s2dw46/are_files_stored_in_a_docker_container_secure/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,secodaHQ,,,[],,,,text,t2_aiinah9q,False,False,False,[],False,False,1642013707,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s2dulf/introducing_revision_history_via_git/,{},s2dulf,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s2dulf/introducing_revision_history_via_git/,False,self,"{'enabled': False, 'images': [{'id': 'OHIn4vydv4LZMJSiPEaVBtbBdL3JE5BKtUaQ5JwvKiU', 'resolutions': [{'height': 65, 'url': 'https://external-preview.redd.it/nIruldxIVZCWDSdrYmFGcIPIxjVwjazCdTszC8Zq7rk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ec49c2069ed3804f9366b11b884f75f4fb144f13', 'width': 108}, {'height': 131, 'url': 'https://external-preview.redd.it/nIruldxIVZCWDSdrYmFGcIPIxjVwjazCdTszC8Zq7rk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=96502418ad69845339770b758a8b5ee81ea1db3d', 'width': 216}, {'height': 194, 'url': 'https://external-preview.redd.it/nIruldxIVZCWDSdrYmFGcIPIxjVwjazCdTszC8Zq7rk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=59465d527cd959e6bacc6786cc2498280a00e47e', 'width': 320}, {'height': 389, 'url': 'https://external-preview.redd.it/nIruldxIVZCWDSdrYmFGcIPIxjVwjazCdTszC8Zq7rk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=62cf93d0cc8dddc604eba0f751ac6160b98e235e', 'width': 640}, {'height': 584, 'url': 'https://external-preview.redd.it/nIruldxIVZCWDSdrYmFGcIPIxjVwjazCdTszC8Zq7rk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f77dffc18666e4610c000dadf4c20f2936339516', 'width': 960}, {'height': 657, 'url': 'https://external-preview.redd.it/nIruldxIVZCWDSdrYmFGcIPIxjVwjazCdTszC8Zq7rk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2580239d9188e8d765a83680b25c21e8d8cbf6c1', 'width': 1080}], 'source': {'height': 740, 'url': 'https://external-preview.redd.it/nIruldxIVZCWDSdrYmFGcIPIxjVwjazCdTszC8Zq7rk.jpg?auto=webp&amp;s=990a4bc205a3719418d04e0fde0b87dd34d2b61d', 'width': 1216}, 'variants': {}}]}",6,1642013718,1,"Secoda can now be synced with a Git repository, letting you customize how you develop and deploy Secoda and help your team adhere to application development lifecycle best practices. Sync Secoda to a Git repository, so you can manage Secoda workspace as code.

Data teams have been speaking about managing data as code and treating data as a product. Lots of conversations in 2021 circulated around bringing software development best practices into data. With this new feature, we allow data teams to manage their data catalogue the same way software engineering teams manage products. Although this is a V1 of the feature, we think it's a monumental improvement in the workflow in Secoda. In the future, we're hoping to build the ability to create full Dev/Staging/Production Secoda states to bring the best practices for version control and knowledge management to your data knowledge.

With this approach, data engineers can manage the version history of Secoda as well as manage their own data in their own instance. Teams can start monitoring the changes across Secoda and retract any changes before they impact the master branch. In the future, this will allow data teams to run tests &amp; perform QA on the staging instance while end-users can access the application on the production instance.

Here's the full article on this feature: [https://www.secoda.co/blog/introducing-revision-history-via-git](https://www.secoda.co/blog/introducing-revision-history-via-git)",True,False,False,dataengineering,t5_36en4,49729,public,self,Introducing: Revision history via Git,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s2dulf/introducing_revision_history_via_git/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,donscrooge,,,[],,,,text,t2_7yotno4i,False,False,False,[],False,False,1642012557,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s2defm/life_as_a_data_engineer_in_spotify/,{},s2defm,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/s2defm/life_as_a_data_engineer_in_spotify/,False,,,6,1642012568,1,"Hi all! Hope my post is relevant to the group. I am thinking of applying for a position ay Spotify and would like to ask if anyone has any experience of what working there is like. Keep in mind that I am referring for the emea region and I am really familiar with the Dutch working condions(work life balance, etc). Every input will be highly appreciated! And a happy &amp; healthy new year!",True,False,False,dataengineering,t5_36en4,49729,public,self,Life as a Data Engineer in Spotify,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s2defm/life_as_a_data_engineer_in_spotify/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,traveling_wilburys,,,[],,,,text,t2_56nxuw5r,False,False,False,[],False,False,1642012492,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s2ddl0/has_anyone_worked_with_scality_here/,{},s2ddl0,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s2ddl0/has_anyone_worked_with_scality_here/,False,,,6,1642012502,1,"I recently joined a team that's using Scality for storage.  What's the difference between Scality and AWS, and what's the advantage of using one over the other?",True,False,False,dataengineering,t5_36en4,49729,public,self,Has anyone worked with Scality here?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s2ddl0/has_anyone_worked_with_scality_here/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,hankwebb,,,[],,,,text,t2_1r6nzjz3,False,False,False,[],False,False,1642011443,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s2cyhg/is_there_a_learning_resource_that_explains_what/,{},s2cyhg,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/s2cyhg/is_there_a_learning_resource_that_explains_what/,False,,,6,1642011453,1,"I feel like when someone is explaining their data stack, it is a word salad of brand names. I have a hard time lining up a particular technology (e.g., Snowflake, AWS Glue, Databricks) with its function within the enterprise.

As a bonus, I would love some primer that explains what all the functional areas are and how they work together. Not sure that exists though!

Much appreciated.",True,False,False,dataengineering,t5_36en4,49727,public,self,Is there a learning resource that explains what the current enterprise/open source solutions do?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s2cyhg/is_there_a_learning_resource_that_explains_what/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Dependent-Archer3546,,,[],,,,text,t2_a0dw6qvt,False,False,False,[],False,False,1642010312,blog.coderise.io,https://www.reddit.com/r/dataengineering/comments/s2chqv/setting_up_cicd_for_azure_data_factory_using/,{},s2chqv,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/s2chqv/setting_up_cicd_for_azure_data_factory_using/,False,link,"{'enabled': False, 'images': [{'id': 'ALstA5GfwgxCPNaT4vN6QEJAt9VG8UwGIIZJfk1DVrc', 'resolutions': [{'height': 50, 'url': 'https://external-preview.redd.it/DPmhxM8uGzAOenZyvz8LdOKx5Q7G5C9eNX7yOaH98WE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=afd3574675cc16c5fe3001a896987616dc8103c7', 'width': 108}, {'height': 100, 'url': 'https://external-preview.redd.it/DPmhxM8uGzAOenZyvz8LdOKx5Q7G5C9eNX7yOaH98WE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cdca803e1aa291e39b3eb625a3e2c64fb69e8f93', 'width': 216}, {'height': 149, 'url': 'https://external-preview.redd.it/DPmhxM8uGzAOenZyvz8LdOKx5Q7G5C9eNX7yOaH98WE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ec3064aacd75d0b75385163a468239037e656054', 'width': 320}, {'height': 298, 'url': 'https://external-preview.redd.it/DPmhxM8uGzAOenZyvz8LdOKx5Q7G5C9eNX7yOaH98WE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=731e59d800079fe3e3dd2abb6ac2bdec7cff7d71', 'width': 640}, {'height': 448, 'url': 'https://external-preview.redd.it/DPmhxM8uGzAOenZyvz8LdOKx5Q7G5C9eNX7yOaH98WE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8d7f22c9e3eb0edb8fddbd5edbaa12c219ffbfed', 'width': 960}, {'height': 504, 'url': 'https://external-preview.redd.it/DPmhxM8uGzAOenZyvz8LdOKx5Q7G5C9eNX7yOaH98WE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fc906d0ffa39cc31605e8ba9f29d20cbce5f22ee', 'width': 1080}], 'source': {'height': 552, 'url': 'https://external-preview.redd.it/DPmhxM8uGzAOenZyvz8LdOKx5Q7G5C9eNX7yOaH98WE.jpg?auto=webp&amp;s=28a6a59ace2eb881bfd825e27f329fb61951018f', 'width': 1182}, 'variants': {}}]}",6,1642010322,1,,True,False,False,dataengineering,t5_36en4,49725,public,https://b.thumbs.redditmedia.com/fW044EJ8zAukVYaNzF3o6bpdp2bvLEulb1RL5DcNsUY.jpg,Setting up CICD for Azure Data Factory using Azure DevOps Pipelines,0,[],1.0,https://blog.coderise.io/setup-cicd-for-azure-data-factory-using-azure-devops-pipelines/,all_ads,6,,,,,,65.0,140.0,https://blog.coderise.io/setup-cicd-for-azure-data-factory-using-azure-devops-pipelines/,,,,,,,,,,
[],False,jeanlaf,,,[],,,,text,t2_11542k,False,False,False,[],False,False,1642007767,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s2bgsp/what_do_you_think_about_our_seriesb_deck_at/,{},s2bgsp,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s2bgsp/what_do_you_think_about_our_seriesb_deck_at/,False,self,"{'enabled': False, 'images': [{'id': 'rUYpGcBQN1C-gtjDLXOGTU-z2Y0RBB91ojqvOnTTBxQ', 'resolutions': [{'height': 60, 'url': 'https://external-preview.redd.it/G5wY4LdZl5b75moWorYvJ4lyaxGEkndlVePdfVgkqq8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a15d7b15b7fb1e9968ff6039b814b6c80167b852', 'width': 108}, {'height': 121, 'url': 'https://external-preview.redd.it/G5wY4LdZl5b75moWorYvJ4lyaxGEkndlVePdfVgkqq8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=14e7b88d0a310321c9a0a84f6f199c4c21083938', 'width': 216}, {'height': 180, 'url': 'https://external-preview.redd.it/G5wY4LdZl5b75moWorYvJ4lyaxGEkndlVePdfVgkqq8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=511f4983be172ad434eae7f246d1548153f6c5e2', 'width': 320}, {'height': 360, 'url': 'https://external-preview.redd.it/G5wY4LdZl5b75moWorYvJ4lyaxGEkndlVePdfVgkqq8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0b824a9fcf6d23e2b35765865e2cae18f8b5774f', 'width': 640}, {'height': 540, 'url': 'https://external-preview.redd.it/G5wY4LdZl5b75moWorYvJ4lyaxGEkndlVePdfVgkqq8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7aae6c026043be2e01c9953eccaddc31a87eeef9', 'width': 960}, {'height': 607, 'url': 'https://external-preview.redd.it/G5wY4LdZl5b75moWorYvJ4lyaxGEkndlVePdfVgkqq8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7bc427be02ba304cf2c626a2dc699315bc3c43da', 'width': 1080}], 'source': {'height': 720, 'url': 'https://external-preview.redd.it/G5wY4LdZl5b75moWorYvJ4lyaxGEkndlVePdfVgkqq8.jpg?auto=webp&amp;s=7eadba1c277f31d8591b7e432c3c6070f88fd909', 'width': 1280}, 'variants': {}}]}",6,1642007777,1,"At Airbyte, we just raised a $150M series-B. We also openly shared our investor deck and pitch.

https://airbyte.com/blog/the-deck-we-used-to-raise-our-150m-series-b

In our deck, we share how being open-source and community-powered is key for our growth. We want to solve the long tail of integrations problem with our participative model, remain a non-opinionated ELT tool to address everyone's needs, and provide a fair compute-based pricing in Airbyte Cloud.

We are curious to know what the data engineering community thinks about our strategy moving forward? What are you most excited about? What are we missing?",True,False,False,dataengineering,t5_36en4,49720,public,self,What do you think about our Series-B deck at Airbyte?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s2bgsp/what_do_you_think_about_our_seriesb_deck_at/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Phantazein,,,[],,,,text,t2_mo4lb,False,False,False,[],False,False,1642000403,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s28kon/modeling_11_relationship_in_accumulating_snapshot/,{},s28kon,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s28kon/modeling_11_relationship_in_accumulating_snapshot/,False,,,6,1642000414,1,"I'm struggling a little with building a dimensional model around an accumulating snapshot fact table. I'm modeling a process for project timelines so the fact table would have start\_date, funding\_date, end\_date, etc. and the main dimension table would be project details, which would include information like title. The problem is a project only goes through this process one time so the fact and dimension table would always have a 1:1 relationship which feels wrong. 

What would be best practice?",True,False,False,dataengineering,t5_36en4,49711,public,self,Modeling 1:1 Relationship in Accumulating Snapshot Table,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s28kon/modeling_11_relationship_in_accumulating_snapshot/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,PaulSandwich,,,[],,,,text,t2_x2wqm,False,False,False,[],False,False,1641999322,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s285hw/databrickspyspark_is_there_a_way_to_apply_the/,{},s285hw,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s285hw/databrickspyspark_is_there_a_way_to_apply_the/,False,,,6,1641999332,1,"Like the title says, I've got json I'm pulling from an API. I read that json into databricks and convert to parquet. They I append that to our parquet reporting tables.   
   
However, there's a conditional array with a bool. If every entry for that batch is null, then databricks is unable to infer what type is ought to be a defaults to string. This breaks our downstream appending of the single batches to the larger dataset.   
   
I've found lots of suggestions for turning everything into a string, and that works, but I'd prefer to 'pickle' a schema object from a 'happy path' sample of the json and apply it to all incoming batches. Is that a thing?",True,False,False,dataengineering,t5_36en4,49708,public,self,Databricks/PySpark: Is there a way to apply the schema from a 'complete' json object to new objects to stop null values from defaulting to string?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s285hw/databrickspyspark_is_there_a_way_to_apply_the/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Automatic-Carpenter6,,,[],,,,text,t2_72sftj7a,False,False,False,[],False,False,1641997962,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s27npk/who_does_data_engineering_for_top_sports_teams/,{},s27npk,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s27npk/who_does_data_engineering_for_top_sports_teams/,False,,,6,1641997973,1,"I'm on the job hunt and would love to be working with sports data. I had a look on LinkedIn and some of the top sports teams in the UK and Europe (eg Liverpool, Mercedes F1 etc) don't have any data engineeri employees.

Do these companies go direct to the likes of AWS and Microsoft and work with their best engineers to build their solutions, or do they do work with specific consultancies?

Any tips for trying to get into the sports industry would be greatly appreciated.",True,False,False,dataengineering,t5_36en4,49703,public,self,Who does data engineering for top sports teams?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s27npk/who_does_data_engineering_for_top_sports_teams/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,phmark19,,,[],,,,text,t2_33924v0v,False,False,False,[],False,False,1641992144,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s25mrp/my_first_google_cloud_data_pipeline/,{},s25mrp,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s25mrp/my_first_google_cloud_data_pipeline/,False,,,6,1641992155,1,"I have a batch data pipeline project w/c requires the data on a near realtime (10 minutes to 30 minutes interval) basis.  
This pipeline will move the data from one system to another using REST APIs, with minimal transformation required.  
The volume of the data I'll be processing was very trivial, at the moment we are receiving max 300 rows per day w/c was expected to increase for about 500 - 1000 rows per day the next coming months.  
Lastly, I will be storing my data into a backup storage (for recovery, analytics, etc.) after uploading it to the target system.  


I am using Python to develop my pipelines, but this is the first time for me to utilize cloud services.

So with that in mind, there are 3 solutions which I was thinking:  
1.  Use Cloud Functions, Cloud Storage, &amp; Cloud Scheduler stack  
1.1 GCF 1 will request the data from system 1 then dump it into GCS  
1.2 GCF 2 will fetch the data from staging GCS then upload it into both system 2 and backup  GCS  


2. SAME with 1 but I will utilize our existing Big Query for backing up the data.  
2.1 GCF 1 will request the data from system 1 then dump it into GCS  
2.2 GCF 2 will fetch the data from staging GCS then upload it into both system 2 and big query  


3. SAME as 1 or 2 but replacing the scheduler with an existing Staled Airflow instance.  


I'm also planning to use dbt in the future, if that would affect the solution.  


What I'm aiming with my infrastructure are low cost, and ease of maintainability &amp; monitoring.  
Feel free to suggest another GCloud Service if that can also solve the problem.  
Any advice is very much appreciated.",True,False,False,dataengineering,t5_36en4,49693,public,self,My First Google Cloud Data Pipeline,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s25mrp/my_first_google_cloud_data_pipeline/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,phmark19,,,[],,,,text,t2_33924v0v,False,False,False,[],False,False,1641990541,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s255ed/creating_my_first_google_cloud_based_data_pipeline/,{},s255ed,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s255ed/creating_my_first_google_cloud_based_data_pipeline/,False,,,6,1641990552,1,"I have a batch data pipeline project w/c requires the data on a near realtime basis.  
This pipeline will move the data from one system to another, with minimal processing power required.  
The volume of the data I'll be processing was very trivial, at the moment we are receiving 300 rows per day w/c  was expected to increase for about 500 - 1000 rows per day.",True,False,False,dataengineering,t5_36en4,49690,public,self,Creating My First Google Cloud Based Data Pipeline,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s255ed/creating_my_first_google_cloud_based_data_pipeline/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Omar_88,,,[],,,,text,t2_7iuhjtv,False,False,False,[],False,False,1641989883,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s24yoe/anyone_moved_jobs_in_the_last_36_months_uk/,{},s24yoe,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/s24yoe/anyone_moved_jobs_in_the_last_36_months_uk/,False,,,6,1641989893,1,"Hi everyone,

I've been at my current company (AWS/GCP) for around 8 months and it's pretty fun - although it's more analytical engineering than core data engineering I'm still learning a lot and gaining domain knowledge. 

However, because I worked at an early adopter of the MS Azure stack I get daily job offers in the region of 70-90k - this is substantially higher than what I'm on, for reference 4 years ago I was working as an ops analyst earning £37k.

I wanted to ask has anyone jumped shipped due to the crazy market demand and taken on the salary increase.",True,False,False,dataengineering,t5_36en4,49690,public,self,anyone moved jobs in the last 3-6 months - UK?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s24yoe/anyone_moved_jobs_in_the_last_36_months_uk/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,dragonachu117,,,[],,,,text,t2_a7vbf8fe,False,False,False,[],False,False,1641989128,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s24qyo/adf_multiple_file_dependencies/,{},s24qyo,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s24qyo/adf_multiple_file_dependencies/,False,,,6,1641989139,1," 

I need to trigger my ADF pipeline when three files arrives in paths : container/folder1/file1.parquet container/folder2/file2.parquet container/folder3/file3.parquet

Only when these 3 subfolders gets new files(files will be overwritten) should the ADF pipeline trigger.

How can we achieve this?",True,False,False,dataengineering,t5_36en4,49690,public,self,ADF multiple file dependencies,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s24qyo/adf_multiple_file_dependencies/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,DolphinScheduler1,,,[],,,,text,t2_facmhtfm,False,False,False,[],False,False,1641984098,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s23dyo/apache_dolphinscheduler_202_release_announcement/,{},s23dyo,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s23dyo/apache_dolphinscheduler_202_release_announcement/,False,,,6,1641984109,1,"&amp;#x200B;

https://preview.redd.it/3cs9d9wyj8b81.png?width=900&amp;format=png&amp;auto=webp&amp;s=ec070bff5a5cfe5c8a90a19f180e16d4104de1e0

&amp;#x200B;

In the long-awaited, WorkflowAsCode function is finally launched in version 2.0.2 as promised, bringing good news to users who need to dynamically create and update workflows in batches.

In addition, the new version also adds the WeCom alarm group chat message push, simplifies the metadata initialization process, and fixes issues that existed in the former version, such as failure of service restart after forced termination, and the failure to add a Hive data source.

# New Function

## WorkflowAsCode

First of all, in terms of new functions, version 2.0.2 released PythonGatewayServer, which is a Workflow-as-code server started in the same way as apiServer and other services.

When PythonGatewayServer is enabled, all Python API requests are sent to PythonGatewayServer. Workflow-as-code lets users create workflows through the Python API, which is great news for users who need to create and update workflows dynamically and in batches. Workflows created with Workflow-as-code can be viewed in the web UI just like other workflows.

The following is a Workflow-as-code test case:

`# Define workflow properties, including name, scheduling period, start time, tenant, etc.with ProcessDefinition(`  
`name=""tutorial"",`  
`schedule=""0 0 0 * * ? *"",`  
`start_time=""2021-01-01"",`  
`tenant=""tenant_exists"",`  
`) as pd:`  
`# Define 4 tasks, which are all shell tasks, the required parameters of shell tasks are task name, command information, here are all the shell commands of echo`  
`task_parent = Shell(name=""task_parent"", command=""echo hello pydolphinscheduler"")`  
`task_child_one = Shell(name=""task_child_one"", command=""echo 'child one'"")`  
`task_child_two = Shell(name=""task_child_two"", command=""echo 'child two'"")`  
`task_union = Shell(name=""task_union"", command=""echo union"")`  


   `# Define dependencies between tasks`  
`# Here, task_child_one and task_child_two are first declared as a task group through python's list`  
`task_group = [task_child_one, task_child_two]`  
`# Use the set_downstream method to declare the task group task_group as the downstream of task_parent, and declare the upstream through set_upstream`  
`task_parent.set_downstream(task_group)`  


   `# Use the bit operator &lt;&lt; to declare the task_union as the downstream of the task_group, and support declaration through the bit operator &gt;&gt;`  
`task_union &lt;&lt; task_group`

When the above code runs, you can see workflow in the web UI as follows:

`--&gt; task_child_one`  
`/                    \`  
`task_parent --&gt;                        --&gt;  task_union`  
`\                   /`  
`--&gt; task_child_two`

## 2 Wecom alarm mode supports group chat message push

In the previous version, the WeChat alarm only supported the message notification; in version 2.0.2, when the user uses the Wecom alarm, it supports pushing the group chat message in the app to the user.

# 02 Optimization

## 1 Simplified metadata initialization process

When Apache DolphinScheduler is first installed, running create-dolphinscheduler.sh requires a step by step upgrade from the oldest version to the current version. In order to initialize the metadata process more conveniently and quickly, version 2.0.2 allows users to directly install the current version of the database script, which improves the installation speed.

## 2 Remove “+1” (days) in complement dates

Removed the “+1” day in the complement date to avoid user confusion when the UI date always displays +1 when the complement is added.

# 03 Bug Fixes

\[#7661\] fix logger memory leak in worker  
\[#7750\] Compatible with historical version data source connection information  
\[#7705\] Memory constraints cause errors when upgrading from 1.3.5 to 2.0.2  
\[#7786\] Service restart fails after a forced termination  
\[#7660\] Process definition version create time is wrong  
\[#7607\] Failed to execute PROCEDURE node  
\[#7639\] Add default configuration of quartz and zookeeper in common configuration items  
\[#7654\] In the dependency node, an error is reported when there is an option that does not belong to the current project  
\[#7658\] Workflow replication error  
\[#7609\] Workflow is always running when worker sendResult succeeds but the master does not receive error report  
\[#7554\] H2 in Standalone Server will automatically restart after a few minutes, resulting in abnormal data loss  
\[#7434\] Error reported when executing MySQL table creation statement  
\[#7537\] Dependent node retry delay does not work  
\[#7392\] Failed to add a Hive data source

* Download: [https://dolphinscheduler.apache.org/zh-cn/download/download.html](https://dolphinscheduler.apache.org/zh-cn/download/download.html)
* Release Note: [https://github.com/apache/dolphinscheduler/releases/tag/2.0.2](https://github.com/apache/dolphinscheduler/releases/tag/2.0.2)

# 04 Thanks

As always, we would like to thank all the contributors (in no particular order) who have worked to polish Apache DolphinScheduler 2.0.2 as a better platform. It is your wisdom and efforts to make it more in line with the needs of users.

&amp;#x200B;

https://preview.redd.it/xddysj81k8b81.png?width=700&amp;format=png&amp;auto=webp&amp;s=3883ea74ced2845b5933d4c6492798f6af4772f4

# The Way to Join

There are many ways to participate and contribute to the DolphinScheduler community, including:  
Documents, translation, Q&amp;A, tests, codes, articles, keynote speeches, etc.

We assume the first PR (document, code) to contribute to be simple and should be used to familiarize yourself with the submission process and community collaboration style.

* So the community has compiled the following list of issues suitable for novices: [https://github.com/apache/dolphinscheduler/issues/5689](https://github.com/apache/dolphinscheduler/issues/5689)
* List of non-newbie issues: [https://github.com/apache/dolphinscheduler/issues?q=is%3Aopen+is%3Aissue+label%3A%22volunteer+wanted%22](https://github.com/apache/dolphinscheduler/issues?q=is%3Aopen+is%3Aissue+label%3A%22volunteer+wanted%22)
* How to participate in the contribution: [https://dolphinscheduler.apache.org/en-us/community/development/contribute.html](https://dolphinscheduler.apache.org/en-us/community/development/contribute.html)
* Community Official Website  
[https://dolphinscheduler.apache.org/](https://dolphinscheduler.apache.org/)
* GitHub Code repository: [https://github.com/apache/dolphinscheduler](https://github.com/apache/dolphinscheduler)

Your Star for the project is important, don’t hesitate to lighten a Star for [Apache DolphinScheduler](https://github.com/apache/dolphinscheduler) ❤️",True,False,False,dataengineering,t5_36en4,49685,public,https://a.thumbs.redditmedia.com/WR2BwRlZsvyVKRekywkESN9pikwgq3-K-WaoDfTyfj4.jpg,Apache DolphinScheduler 2.0.2 Release Announcement: WorkflowAsCode Is Launched!,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s23dyo/apache_dolphinscheduler_202_release_announcement/,all_ads,6,,,,,,59.0,140.0,,"{'3cs9d9wyj8b81': {'e': 'Image', 'id': '3cs9d9wyj8b81', 'm': 'image/png', 'p': [{'u': 'https://preview.redd.it/3cs9d9wyj8b81.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f1b3cddd2807049d8346e295429db0146e58b856', 'x': 108, 'y': 45}, {'u': 'https://preview.redd.it/3cs9d9wyj8b81.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1cc67ea6fa8fafaf6d12bcc96ac4b804dac328c6', 'x': 216, 'y': 91}, {'u': 'https://preview.redd.it/3cs9d9wyj8b81.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7660d56801188731bf73bb7959dea9f0b2817263', 'x': 320, 'y': 136}, {'u': 'https://preview.redd.it/3cs9d9wyj8b81.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a2b2cf81a7ce1788a6c8cafa30ae8cef682ea058', 'x': 640, 'y': 272}], 's': {'u': 'https://preview.redd.it/3cs9d9wyj8b81.png?width=900&amp;format=png&amp;auto=webp&amp;s=ec070bff5a5cfe5c8a90a19f180e16d4104de1e0', 'x': 900, 'y': 383}, 'status': 'valid'}, 'xddysj81k8b81': {'e': 'Image', 'id': 'xddysj81k8b81', 'm': 'image/png', 'p': [{'u': 'https://preview.redd.it/xddysj81k8b81.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a4c0a8e6e9f908fe517f6d3b90c62fbbc3714d13', 'x': 108, 'y': 25}, {'u': 'https://preview.redd.it/xddysj81k8b81.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=48e728a452d8896787961e0373ce95498f113eee', 'x': 216, 'y': 50}, {'u': 'https://preview.redd.it/xddysj81k8b81.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e51c6122ef6fbbdc70e7228c1d3761bb02b8cc23', 'x': 320, 'y': 74}, {'u': 'https://preview.redd.it/xddysj81k8b81.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7548d25d4ed167c8cf20ace67ea30ef620b9f8ae', 'x': 640, 'y': 149}], 's': {'u': 'https://preview.redd.it/xddysj81k8b81.png?width=700&amp;format=png&amp;auto=webp&amp;s=3883ea74ced2845b5933d4c6492798f6af4772f4', 'x': 700, 'y': 163}, 'status': 'valid'}}",,,,,,,,,
[],False,Sweet_Ad06,,,[],,,,text,t2_c6z7bhpj,False,False,False,[],False,False,1641983691,forms.gle,https://www.reddit.com/r/dataengineering/comments/s23abs/here_is_a_survey_for_my_data_viz_project_please/,{},s23abs,False,True,False,False,False,True,False,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s23abs/here_is_a_survey_for_my_data_viz_project_please/,False,link,"{'enabled': False, 'images': [{'id': 'lSKwLmc7psVm8Nowhs7xY3wGhGkS6zs9rmiI4s83CMk', 'resolutions': [{'height': 56, 'url': 'https://external-preview.redd.it/BJwujlcZnd4FKALwUINnLrIggNj3MBIRQevXDwl6Hso.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1c10d8ed21a908d648c33a676de566f2a3f9e8f5', 'width': 108}, {'height': 113, 'url': 'https://external-preview.redd.it/BJwujlcZnd4FKALwUINnLrIggNj3MBIRQevXDwl6Hso.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=42a67bf1e5aee4ae6737f4e77aa152081ee01be3', 'width': 216}, {'height': 168, 'url': 'https://external-preview.redd.it/BJwujlcZnd4FKALwUINnLrIggNj3MBIRQevXDwl6Hso.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=23c633dc5dfe447817927d16455490dc33a9e4db', 'width': 320}, {'height': 336, 'url': 'https://external-preview.redd.it/BJwujlcZnd4FKALwUINnLrIggNj3MBIRQevXDwl6Hso.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c2cb4497e97b9d52141fcfe6021723335f849258', 'width': 640}, {'height': 504, 'url': 'https://external-preview.redd.it/BJwujlcZnd4FKALwUINnLrIggNj3MBIRQevXDwl6Hso.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=16361c8512582d0cfe79dd6053402cced0732848', 'width': 960}, {'height': 567, 'url': 'https://external-preview.redd.it/BJwujlcZnd4FKALwUINnLrIggNj3MBIRQevXDwl6Hso.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5d6774b6c860fc3fede3820a8f275ab054ca9101', 'width': 1080}], 'source': {'height': 630, 'url': 'https://external-preview.redd.it/BJwujlcZnd4FKALwUINnLrIggNj3MBIRQevXDwl6Hso.jpg?auto=webp&amp;s=db3f9365c59329571ee6e8c6bb97a601f7e59eea', 'width': 1200}, 'variants': {}}]}",6,1641983702,1,,True,False,False,dataengineering,t5_36en4,49685,public,https://b.thumbs.redditmedia.com/fLJOiJvrRZQce-otxyEuvFM7TTIWBsdIzhWWXTGqM9s.jpg,Here is a survey for my data viz project. Please fill the form.,0,[],1.0,https://forms.gle/fTg4j3J96dA9PpFH9,all_ads,6,,,,,,73.0,140.0,https://forms.gle/fTg4j3J96dA9PpFH9,,,,,,,,,,
[],False,Thejoffrey,,,[],,,,text,t2_xv2ez,False,False,False,[],False,False,1641980253,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s22eg6/databricks_certified_professional_data_engineer/,{},s22eg6,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s22eg6/databricks_certified_professional_data_engineer/,False,,,6,1641980263,1,Hello everyone! Has anyone here passed the Data Engineer certification on Databricks ? I would like to pass the test myself and I would appreciate any tips or recources to check out :) Thanks in advance !,True,False,False,dataengineering,t5_36en4,49682,public,self,Databricks Certified Professional Data Engineer,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s22eg6/databricks_certified_professional_data_engineer/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,NizBomb,,,[],,,,text,t2_2u2okm3w,False,False,False,[],False,False,1641980037,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s22ckb/skills_to_learn_to_transition_from_data_analysis/,{},s22ckb,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/s22ckb/skills_to_learn_to_transition_from_data_analysis/,False,,,6,1641980048,1," Hi All,

So I recently started a job in Data Analysis about 3 months ago at a small company after finishing my MSc Bioinformatics. I'm thinking about starting some freelancing projects if I can but if not then just some courses and personal projects to try and make the transition to more of a Data Engineering role in the future of my career. It seems to be both a more interesting (in terms of handling data) and more lucrative career path, although I'm in no rush to get there and have time to learn more skills to become more hireable!

I am confident in Python, NumPy and Pandas, and use a lot of SAS for work at the moment, but I was wondering what people would suggest for a skillset to make this transition?

Thank you!",True,False,False,dataengineering,t5_36en4,49682,public,self,Skills to learn to transition from Data Analysis to Data Engineering,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s22ckb/skills_to_learn_to_transition_from_data_analysis/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,leweyy,,,[],,,,text,t2_a1dhu,False,False,False,[],False,False,1641978376,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s21wyj/looking_for_an_example_project_which_is_an_etl/,{},s21wyj,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s21wyj/looking_for_an_example_project_which_is_an_etl/,False,,,6,1641978387,1,I've written a new project recently which focuses around these 3 main things. I want to compare my project to that of one which utilises these tools. I'm particularly interested in the project structure so a github repo would be perfect. Does anyone have any great examples?,True,False,False,dataengineering,t5_36en4,49683,public,self,"Looking for an example project which is an ETL workflow utilising: Python, SQAlchemy and extracting from an API,",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s21wyj/looking_for_an_example_project_which_is_an_etl/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Sweet_Ad06,,,[],,,,text,t2_c6z7bhpj,False,False,False,[],False,False,1641978347,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s21wo3/employees_of_faang_i_am_doing_a_survey_can_you/,{},s21wo3,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s21wo3/employees_of_faang_i_am_doing_a_survey_can_you/,False,,,6,1641978357,1,"I have few questions:

1. What is your designation in your company?
2. What technologies do you often work/interact with?
3.  What exactly do the big tech (FAANG) companies look for in candidates when hiring?
4.  What are some red flags specific to the FAANG companies?",True,False,False,dataengineering,t5_36en4,49683,public,self,"Employees of FAANG, I am doing a survey. Can you help me please?",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s21wo3/employees_of_faang_i_am_doing_a_survey_can_you/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,klee1,,,[],,,,text,t2_25nkczt3,False,False,False,[],False,False,1641972762,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s20gtf/redhat_data_engineering_intern/,{},s20gtf,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/s20gtf/redhat_data_engineering_intern/,False,,,6,1641972772,1,Has anyone interviewed for RedHat Data Engineering Intern role? Could you share your experience and how to prepare? Thank you!,True,False,False,dataengineering,t5_36en4,49678,public,self,RedHat Data Engineering Intern,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s20gtf/redhat_data_engineering_intern/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,ibnipun10,,,[],,,,text,t2_2ppylqyb,False,False,False,[],False,False,1641967235,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s1yy9j/transitioning_to_blockchain/,{},s1yy9j,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s1yy9j/transitioning_to_blockchain/,False,,,6,1641967245,1,"Hi,

I have around 12 yrs of experience into software developer + data engineering. I am pretty interested in blockchain and would like to transition to that domain. How easy or difficult it is to transition at this time? Are there any work involved of using blockchains in big data?",True,False,False,dataengineering,t5_36en4,49670,public,self,Transitioning to blockchain,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s1yy9j/transitioning_to_blockchain/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,lastmonty,,,[],,,,text,t2_2r8faz0v,False,False,False,[],False,False,1641965791,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s1yj71/example_tutorial_for_new_tool/,{},s1yj71,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s1yj71/example_tutorial_for_new_tool/,False,,,6,1641965801,1,"Hello all, 

We are very close to releasing a pipelining tool to open source. Since our team is more oriented towards ml or ds, the tutorial we currently have is ml focused and considered very popular in that space (it's the Titanic dataset).

Though there are certain data transformations that I can demo using pipeline,I was curious if data engineering community has some hallmark pipeline tutorials or examples that I can implement using our software. 

The ones I found on airflow(bash date) or prefect (airline radius) are a bit too simple. 

Thanks for your help. 
Cheers,",True,False,False,dataengineering,t5_36en4,49664,public,self,Example tutorial for new tool,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s1yj71/example_tutorial_for_new_tool/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Analyst4Hire,,,[],,,,text,t2_fpcx8smb,False,False,False,[],False,False,1641957485,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s1vuip/what_is_your_day_to_day_look_like/,{},s1vuip,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s1vuip/what_is_your_day_to_day_look_like/,False,,,6,1641957496,1,"I'm starting my first DE job next month, just wanted to know what your day to day was like? How do you get work, what do you use to get your job done and any advice you might have, thanks!",True,False,False,dataengineering,t5_36en4,49654,public,self,What is your day to day look like?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s1vuip/what_is_your_day_to_day_look_like/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Raptor_Legend_Hakeem,,,[],,,,text,t2_jahwoa1,False,False,False,[],False,False,1641957185,i.redd.it,https://www.reddit.com/r/dataengineering/comments/s1vqs4/please_roast_my_resume_data_analyst_intern_to/,{},s1vqs4,False,True,False,False,True,True,False,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s1vqs4/please_roast_my_resume_data_analyst_intern_to/,False,image,"{'enabled': True, 'images': [{'id': 'Ch_11JeXZQL-yTgu_o_mZ979upC1FND7h8i8Lz63f68', 'resolutions': [{'height': 139, 'url': 'https://preview.redd.it/oglh7ap2c6b81.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=dbb51a1867012526b6d7389aac1ae898501e77fb', 'width': 108}, {'height': 279, 'url': 'https://preview.redd.it/oglh7ap2c6b81.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=31534c212a3860266a59be89b8f8c7b6b2e62b2a', 'width': 216}, {'height': 414, 'url': 'https://preview.redd.it/oglh7ap2c6b81.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0aa84d807f4ae75fdf177c1c398866877aac5f78', 'width': 320}, {'height': 828, 'url': 'https://preview.redd.it/oglh7ap2c6b81.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a87606252e3370f0fa3d7c9bf7b77bf9a0907c3c', 'width': 640}, {'height': 1242, 'url': 'https://preview.redd.it/oglh7ap2c6b81.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5f8368f303c5f52079a1e4da8702602bfa1bca46', 'width': 960}, {'height': 1397, 'url': 'https://preview.redd.it/oglh7ap2c6b81.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a531e041222a0b9869a20d0aac0701bf92d4ad54', 'width': 1080}], 'source': {'height': 2200, 'url': 'https://preview.redd.it/oglh7ap2c6b81.png?auto=webp&amp;s=147597b6801576822c0fde91bc1128ddab85b22d', 'width': 1700}, 'variants': {}}]}",6,1641957195,1,,True,False,False,dataengineering,t5_36en4,49654,public,https://b.thumbs.redditmedia.com/eLPRNa-uHnS3SFnSe_AOOyXzwjbys2UDvSJBv6xSDNM.jpg,Please Roast My Resume: Data Analyst Intern to Data Engineering Intern,0,[],1.0,https://i.redd.it/oglh7ap2c6b81.png,all_ads,6,,,,,,140.0,140.0,https://i.redd.it/oglh7ap2c6b81.png,,,,,,,,,,
[],False,Raptor_Legend_Hakeem,,,[],,,,text,t2_jahwoa1,False,False,False,[],False,False,1641956996,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s1vocb/please_roast_my_resume_data_analyst_intern_to/,{},s1vocb,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s1vocb/please_roast_my_resume_data_analyst_intern_to/,False,,,6,1641957007,1,"Inspired by the previous data analyst post about transitioning into a fulltime DE role. Was hoping to get some criticism on my resume as I begin searching for summer internship opportunities.

Should I include more personal projects/ club experience in lieu of professional experience? I have a few more projects in my GitHub repo that are data science/ data engineering related, was wondering if those would be more relevant than the instructional assistant role.

Thanks in advance",True,False,False,dataengineering,t5_36en4,49654,public,self,Please Roast My Resume: Data Analyst Intern to Data Engineering Intern,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s1vocb/please_roast_my_resume_data_analyst_intern_to/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Albertulysses,,,[],,,,text,t2_815ar4pk,False,False,False,[],False,False,1641953112,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s1ubuu/pandas_coding_convention_for_column_naming/,{},s1ubuu,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s1ubuu/pandas_coding_convention_for_column_naming/,False,,,6,1641953123,1,"Hi Y'all,

I'm interested in hearing the community's opinion on some Pandas coding convention.
My problem:
I have to make new columns based on other columns. 
The rules are basically the same so I created one function that does this. That's simple.
However the new columns need new names that replace the previous suffix. 
For example given:
df['Column1_value']

I'll want to name the new column:
df['Column1_result']

Simple enough, but I have 30ish columns which all would look like:

df['Column1_result']  = funct(df['Column1_value'])

Except the names are slightly different.

So in theory I can write a function that takes the name of the string and returns it with the ending being changed. 
Which means I can loop instead of declaring each column.
My intuition is saying don't do this.

Anyone have a good alternative or real reason why I wouldn't do this?",True,False,False,dataengineering,t5_36en4,49649,public,self,Pandas coding convention for column naming,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s1ubuu/pandas_coding_convention_for_column_naming/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Natural-Carrot-723,,,[],,,,text,t2_gd818se3,False,False,False,[],False,False,1641952847,youtube.com,https://www.reddit.com/r/dataengineering/comments/s1u8b6/merge_sort_2/,{},s1u8b6,False,False,False,False,False,False,False,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s1u8b6/merge_sort_2/,False,rich:video,"{'enabled': False, 'images': [{'id': 'XNoLubgr0puVghZMjfrJ6iHHi0G21-2DY6vHGq9oD1k', 'resolutions': [{'height': 81, 'url': 'https://external-preview.redd.it/NdVxO-SMrY7GJzpcMVOHBotGInisqGTnpl7V7OtW7xo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=edcb282a0a9503932d9b661a60420a6b085beda4', 'width': 108}, {'height': 162, 'url': 'https://external-preview.redd.it/NdVxO-SMrY7GJzpcMVOHBotGInisqGTnpl7V7OtW7xo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=af5be141f5515c4ac5a0ef6ca60c8014d3d45557', 'width': 216}, {'height': 240, 'url': 'https://external-preview.redd.it/NdVxO-SMrY7GJzpcMVOHBotGInisqGTnpl7V7OtW7xo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=92ca51e8fe9e6788bc7ff966709adbc240818460', 'width': 320}], 'source': {'height': 360, 'url': 'https://external-preview.redd.it/NdVxO-SMrY7GJzpcMVOHBotGInisqGTnpl7V7OtW7xo.jpg?auto=webp&amp;s=79549de32b9538163553d3bc89629be5d5cf13b4', 'width': 480}, 'variants': {}}]}",6,1641952858,1,,True,False,False,dataengineering,t5_36en4,49649,public,https://b.thumbs.redditmedia.com/gDrRKNRg7-82sPqKd9lbaFGeA8irjbyx1LLuP3yHtik.jpg,merge sort (2),0,[],1.0,https://youtube.com/watch?v=Y_zDfMAuE-Y&amp;feature=share,all_ads,6,"{'oembed': {'author_name': 'WRITING ALGORITHMS.....', 'author_url': 'https://www.youtube.com/channel/UCnhMkaVPneIrHzd1k3WUAeg', 'height': 200, 'html': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/Y_zDfMAuE-Y?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/Y_zDfMAuE-Y/hqdefault.jpg', 'thumbnail_width': 480, 'title': 'merge sort (2)', 'type': 'video', 'version': '1.0', 'width': 267}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/Y_zDfMAuE-Y?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'scrolling': False, 'width': 267}",reddit,"{'oembed': {'author_name': 'WRITING ALGORITHMS.....', 'author_url': 'https://www.youtube.com/channel/UCnhMkaVPneIrHzd1k3WUAeg', 'height': 200, 'html': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/Y_zDfMAuE-Y?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/Y_zDfMAuE-Y/hqdefault.jpg', 'thumbnail_width': 480, 'title': 'merge sort (2)', 'type': 'video', 'version': '1.0', 'width': 267}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/Y_zDfMAuE-Y?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/s1u8b6', 'scrolling': False, 'width': 267}",105.0,140.0,https://youtube.com/watch?v=Y_zDfMAuE-Y&amp;feature=share,,,,,,,,,,
[],False,Natural-Carrot-723,,,[],,,,text,t2_gd818se3,False,False,False,[],False,False,1641952108,youtube.com,https://www.reddit.com/r/dataengineering/comments/s1tz75/merge_sort_algorithm_1/,{},s1tz75,False,False,False,False,False,False,False,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s1tz75/merge_sort_algorithm_1/,False,rich:video,"{'enabled': False, 'images': [{'id': 'XNoLubgr0puVghZMjfrJ6iHHi0G21-2DY6vHGq9oD1k', 'resolutions': [{'height': 81, 'url': 'https://external-preview.redd.it/NdVxO-SMrY7GJzpcMVOHBotGInisqGTnpl7V7OtW7xo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=edcb282a0a9503932d9b661a60420a6b085beda4', 'width': 108}, {'height': 162, 'url': 'https://external-preview.redd.it/NdVxO-SMrY7GJzpcMVOHBotGInisqGTnpl7V7OtW7xo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=af5be141f5515c4ac5a0ef6ca60c8014d3d45557', 'width': 216}, {'height': 240, 'url': 'https://external-preview.redd.it/NdVxO-SMrY7GJzpcMVOHBotGInisqGTnpl7V7OtW7xo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=92ca51e8fe9e6788bc7ff966709adbc240818460', 'width': 320}], 'source': {'height': 360, 'url': 'https://external-preview.redd.it/NdVxO-SMrY7GJzpcMVOHBotGInisqGTnpl7V7OtW7xo.jpg?auto=webp&amp;s=79549de32b9538163553d3bc89629be5d5cf13b4', 'width': 480}, 'variants': {}}]}",6,1641952119,1,,True,False,False,dataengineering,t5_36en4,49649,public,https://b.thumbs.redditmedia.com/gDrRKNRg7-82sPqKd9lbaFGeA8irjbyx1LLuP3yHtik.jpg,Merge sort algorithm (1),0,[],1.0,https://youtube.com/watch?v=sUcIJakPD60&amp;feature=share,all_ads,6,"{'oembed': {'author_name': 'WRITING ALGORITHMS.....', 'author_url': 'https://www.youtube.com/channel/UCnhMkaVPneIrHzd1k3WUAeg', 'height': 200, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/sUcIJakPD60?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/sUcIJakPD60/hqdefault.jpg', 'thumbnail_width': 480, 'title': 'Merge sort  algorithm (1)', 'type': 'video', 'version': '1.0', 'width': 356}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/sUcIJakPD60?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'scrolling': False, 'width': 356}",reddit,"{'oembed': {'author_name': 'WRITING ALGORITHMS.....', 'author_url': 'https://www.youtube.com/channel/UCnhMkaVPneIrHzd1k3WUAeg', 'height': 200, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/sUcIJakPD60?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/sUcIJakPD60/hqdefault.jpg', 'thumbnail_width': 480, 'title': 'Merge sort  algorithm (1)', 'type': 'video', 'version': '1.0', 'width': 356}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/sUcIJakPD60?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/s1tz75', 'scrolling': False, 'width': 356}",105.0,140.0,https://youtube.com/watch?v=sUcIJakPD60&amp;feature=share,,,,,,,,,,
[],False,Awkward_Salary2566,,,[],,,,text,t2_dlbrtadc,False,False,False,[],False,False,1641948592,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s1spyc/did_i_arrive_at_dead_end_with_my_position_in/,{},s1spyc,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s1spyc/did_i_arrive_at_dead_end_with_my_position_in/,False,,,6,1641948602,1,"Hello,

I was first BI hire in company (at least for our country, but we are very independent). 

Because of type of the business, usage and ""agile"" thinking I chose the database of choice PostgreSQL, on-premise. Database is scaling amazingly well (for SME), there are literally 0 problems, small to none maintenance needed after properly setting the config. 

Now, because of this boom in hiring, I decided to try job market and I horrifying discovered that 80% job positions are requiring some cloud experience. 

What are my options for keeping up to date with cloud, given that I am keeping same position? 

I still fail to see how for exactly our type of business would db and ETL process on cloud be advantage (very predictable and stable usage and processes, hosted on computer with specs far higher than needed, far less unplanned shortages as AWS's us-east1 (for other reasons we have our own generators, 2 independent reliable ISPs, high end recovery mechanisms, ...) 

Only thing I can think of is to create some bullshit ML side-project, where we need cloud for burst computing power. 

Any suggestions? I have theoretically full power as I am in charge of everything BI related.

Tl;dr: I love my on-premise Postgres, job market wants me to switch to cloud, how to get best of both? 

&amp;#x200B;

Also, is it hard to lie about your cloud experience? I mean everybody is saying that its maintenance free and things just work, so in theory it should be super easy to learn",True,False,False,dataengineering,t5_36en4,49645,public,self,Did I arrive at dead end with my position in current company?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s1spyc/did_i_arrive_at_dead_end_with_my_position_in/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Thriven,,,[],,,,text,t2_8mnqe,False,False,False,[],False,False,1641947823,i.imgur.com,https://www.reddit.com/r/dataengineering/comments/s1sfu5/me_as_an_etl_engineer_watching_people_build_data/,{},s1sfu5,False,True,False,False,False,True,False,False,#ff66ac,[],dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1,Meme,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s1sfu5/me_as_an_etl_engineer_watching_people_build_data/,False,link,"{'enabled': False, 'images': [{'id': 'uQYebbFBSvw9AkwmLwphmoWsxEzlDuBcztE1PoZOWTs', 'resolutions': [{'height': 60, 'url': 'https://external-preview.redd.it/klvPw-9CRP5DQaUL_OVdMVBXj603pznEjSpLsjdRb2I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7f5370e44be08c14da3ace0b8af92b414c84f41f', 'width': 108}, {'height': 121, 'url': 'https://external-preview.redd.it/klvPw-9CRP5DQaUL_OVdMVBXj603pznEjSpLsjdRb2I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6565737e772e3675ab0ca277d804ab153ff1f769', 'width': 216}, {'height': 180, 'url': 'https://external-preview.redd.it/klvPw-9CRP5DQaUL_OVdMVBXj603pznEjSpLsjdRb2I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bc3b6bf4a105efd34a15d3f1f94b9d2c40b23d8e', 'width': 320}, {'height': 360, 'url': 'https://external-preview.redd.it/klvPw-9CRP5DQaUL_OVdMVBXj603pznEjSpLsjdRb2I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=40f28615f39f32ea306c07b77848f318d554e0a0', 'width': 640}, {'height': 540, 'url': 'https://external-preview.redd.it/klvPw-9CRP5DQaUL_OVdMVBXj603pznEjSpLsjdRb2I.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5e60b41cd56a5e5fe4680902a58120047b8e6d4e', 'width': 960}], 'source': {'height': 540, 'url': 'https://external-preview.redd.it/klvPw-9CRP5DQaUL_OVdMVBXj603pznEjSpLsjdRb2I.jpg?auto=webp&amp;s=86e933d637bf6834199cd0955656ce070be56fc7', 'width': 960}, 'variants': {}}]}",6,1641947834,1,,True,False,False,dataengineering,t5_36en4,49643,public,https://a.thumbs.redditmedia.com/-RewoS7KJN9v7NzO5Mdp94SGGT1noxA8awoBWpEAUR0.jpg,Me as an ETL engineer watching people build data tables with no regard to what goes in them.,0,[],1.0,https://i.imgur.com/9ZJkPvV.gifv,all_ads,6,,,,,,78.0,140.0,https://i.imgur.com/9ZJkPvV.gifv,,,,,,,,,,
[],False,OcclumencyOnReddit,,,[],,,,text,t2_74rlic8,False,False,False,[],False,False,1641944180,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s1r3xs/bigquery_fivetran_big_data_possible/,{},s1r3xs,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s1r3xs/bigquery_fivetran_big_data_possible/,False,,,6,1641944191,1,"Hi,

How do people get around the fact that fivetran cannot read data from BigQuery or other bigdata sources? Am I missing something?",True,False,False,dataengineering,t5_36en4,49641,public,self,BigQuery -&gt; FiveTran -&gt; (Big Data) Possible?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s1r3xs/bigquery_fivetran_big_data_possible/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,woke_uncle,,,[],,,,text,t2_aaqlewmw,False,False,False,[],False,False,1641941311,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s1q190/what_should_i_learn_as_a_data_engineer/,{},s1q190,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/s1q190/what_should_i_learn_as_a_data_engineer/,False,,,6,1641941321,1,"Following a CS conversion masters, I was hired as a technical account manager a few months ago at a startup. I started automating some processes for them, which my boss took an interest in, and after a few conversations I'm now in charge of building ETL pipelines with Python for internal data (JIRA tickets and the like) into SQL databases, analysing the data, and developing PowerBI dashboards. It's early days but, if I make some good progress with it over the next couple of months, I'll likely switch to being an actual full time data engineer at the company and not a TAM with some engineering responsibilities.

This is obviously a great learning opportunity, but as the company has no formal data engineers working there and the dev team are all product-focused I have nobody directly above me for guidance on this specific project. While it's exciting to be managing this project end to end at such an early point in my career, but I want to make sure I'm setting myself up for success later down the line.

So I figured I'd ask here:

* What are some technical skills/technologies that I should aim to build up while working on this project? (It's not big data, so things like Spark have limited applications) 
* What are some organisational skills that would be worth practising? (E.g. documentation)
* If you work with data engineers and analysts, what do you wish they knew/did differently? 

Thank you!",True,False,False,dataengineering,t5_36en4,49637,public,self,What should I learn as a data engineer?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s1q190/what_should_i_learn_as_a_data_engineer/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Simonaque,transparent,,[],19bba012-ac9d-11eb-b77b-0eec37c01719,Data Analyst,dark,text,t2_cj1g2,False,False,False,[],False,False,1641935847,i.redd.it,https://www.reddit.com/r/dataengineering/comments/s1nvao/please_critique_my_resume_data_analyst/,{},s1nvao,False,True,False,False,True,True,False,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s1nvao/please_critique_my_resume_data_analyst/,False,image,"{'enabled': True, 'images': [{'id': 'hyyGbm9lV3rPTWACbde063bD1AQ_zkn6nF-AHbaaQcU', 'resolutions': [{'height': 139, 'url': 'https://preview.redd.it/ymkuldyjk4b81.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=91d35c45e40f184144664258a1f922dc671b8c52', 'width': 108}, {'height': 279, 'url': 'https://preview.redd.it/ymkuldyjk4b81.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=310b6bf281458bfd791cb83e54cd9e50166cb3ef', 'width': 216}, {'height': 414, 'url': 'https://preview.redd.it/ymkuldyjk4b81.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0cde293c398687e95763ba94627c1a4b3c912fe8', 'width': 320}, {'height': 828, 'url': 'https://preview.redd.it/ymkuldyjk4b81.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=03b2bc4b36f585511481cdeda939d90d68e279ea', 'width': 640}, {'height': 1242, 'url': 'https://preview.redd.it/ymkuldyjk4b81.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f3802c1d2b63ba9ad9731dc9ac550c803fdd5aeb', 'width': 960}, {'height': 1397, 'url': 'https://preview.redd.it/ymkuldyjk4b81.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8be5821625eab657c7bfb01912cf2ff884979382', 'width': 1080}], 'source': {'height': 3300, 'url': 'https://preview.redd.it/ymkuldyjk4b81.jpg?auto=webp&amp;s=85cca7c9bdc613afd157b14590c5ef638e022b40', 'width': 2550}, 'variants': {}}]}",6,1641935857,1,,True,False,False,dataengineering,t5_36en4,49631,public,https://a.thumbs.redditmedia.com/zH3ionYrwvLc9QzArFptaPJiiNFjHg6O6ly8YcRhX28.jpg,Please Critique my Resume: Data Analyst transitioning to Data Engineer,0,[],1.0,https://i.redd.it/ymkuldyjk4b81.jpg,all_ads,6,,,,,,140.0,140.0,https://i.redd.it/ymkuldyjk4b81.jpg,,,,,,,,,,
[],False,RandomWalk55,,,[],,,,text,t2_gs0mp007,False,False,False,[],False,False,1641934459,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s1nb6u/data_engineers_what_do_you_actually_do_and_whats/,{},s1nb6u,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s1nb6u/data_engineers_what_do_you_actually_do_and_whats/,False,,,6,1641934469,1,"I'm trying to decide if the pivot from AppDev to Data Engineering is in my future and would like to get a sense of what it involves in the real world. I'm hoping for a broad-brush breakdown of job responsibilities and what languages/technologies/systems are the most important to get your job done.

Bonus points if you're willing to give a rough-swag of your experience and compensation.

Thanks people, I appreciate you taking the time to share your expertise.",True,False,False,dataengineering,t5_36en4,49628,public,self,"Data Engineers, what do you actually do and what's your tech stack?",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s1nb6u/data_engineers_what_do_you_actually_do_and_whats/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,lclhr,,,[],,,,text,t2_1644xa,False,False,False,[],False,False,1641933267,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s1mudl/installing_airflow_error/,{},s1mudl,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/s1mudl/installing_airflow_error/,False,,,6,1641933277,1,"Hello Everyone,

I'm trying to install airflow to do some practice in it.

I have a ubuntu 20.04 LTS server.

When installing airflow I can see there are 2 errors:

ERROR: openapi-schema-validator 0.2.0 has requirement jsonschema&lt;5.0.0,&gt;=4.0.0, but you'll have jsonschema 3.2.0 which is incompatible.

ERROR: flask-appbuilder 3.4.3 has requirement SQLAlchemy&lt;1.4.0, but you'll have sqlalchemy 1.4.29 which is incompatible.

However, i can use airflow in the terminal: When I try to use the DB init I got another error. (Attached picture)

Same error when i try to run the webserver too.

&amp;#x200B;

I've been struggling with it the last 2 hours but no clue.

&amp;#x200B;

Any help?

Thanks",True,False,False,dataengineering,t5_36en4,49624,public,self,installing Airflow error,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s1mudl/installing_airflow_error/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,LeftHelicopter5297,,,[],,,,text,t2_gfx5s46h,False,False,False,[],False,False,1641929101,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s1l6g6/got_my_first_job_as_a_junior_de_whats_next/,{},s1l6g6,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/s1l6g6/got_my_first_job_as_a_junior_de_whats_next/,False,,,6,1641929112,1,"Hi,

I somehow got hired as a junior DE. For the past few months everything I did was focused on getting the job. Now that I have received one I feel a little bit lost - how to develop my career now? 

About me:

\-it's my very first job in the field, not even analytics position before that,  
\-the employer is a F500 consulting company  
\-I grind stratascratch and feel more and more comfortable with hard questions,  
\-I don't have any degree yet - I'm studying CS  part-time,  
\-I'm a beginner in python, doing [pythoninstitute.org](https://pythoninstitute.org) courses at the moment as I am not able to solve even the easiest questions on LeetCode  
\-I'm in the EU  
\-the money isn't great but I'm OK with that for now

&amp;#x200B;

This is what I am thinking for the next 6 months:  
\-continue learning python and get a certificate from pythoninstitute - then maybe move to some projects  
\-do 1 hard SQL question/day   
\-get the databricks spark certificate  


Here's my questions:  
\-what should my next objective be?  
\-do I pause responding to recruiters on linkedin or not?  
\-when do I start interviewing again?

Thank you for your kind help",True,False,False,dataengineering,t5_36en4,49617,public,self,Got my first job as a junior DE - what's next?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s1l6g6/got_my_first_job_as_a_junior_de_whats_next/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,statAnomaly,,,[],,,,text,t2_8wnvbibx,False,False,False,[],False,False,1641925751,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s1jtl3/introducing_fabricator_a_declarative_feature/,{},s1jtl3,False,False,False,False,False,False,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s1jtl3/introducing_fabricator_a_declarative_feature/,False,,,6,1641925762,1,[removed],True,False,False,dataengineering,t5_36en4,49613,public,self,Introducing Fabricator: A Declarative Feature Engineering Framework,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s1jtl3/introducing_fabricator_a_declarative_feature/,all_ads,6,,,automod_filtered,,,,,,,,,,,,,,,
[],False,Sbramahi,,,[],,,,text,t2_d3snu0ec,False,False,False,[],False,False,1641921629,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s1i5no/azure_dp203_guide/,{},s1i5no,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/s1i5no/azure_dp203_guide/,False,,,6,1641921640,1,"Hey guys!

I'm currently in the process of studying for DP-203, the Azure Data Engineer Certification. However, the MS resources (as usual) are too detailed and overwhelming since I have no prior technical experience at all.. 

Can anyone recommend sources to study from? Any courses out there for beginners that provide a simple explanation to things? 

Any resources would be much appreciated ☺️",True,False,False,dataengineering,t5_36en4,49606,public,self,Azure DP-203 Guide,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s1i5no/azure_dp203_guide/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,igaloly,,,[],,,,text,t2_joxdwrl,False,False,False,[],False,False,1641920716,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s1hsmy/which_messaging_broker_should_i_use_for_my_use/,{},s1hsmy,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/s1hsmy/which_messaging_broker_should_i_use_for_my_use/,False,,,6,1641920727,1,"We have a client SDK that sends logs to Elasticsearch.

Sometimes, the SDK might send a batch of millions of logs to Elastic.

I want to enrich those logs.

I want to do this after inserting and asynchronously to not block the insert.

As a solution, I thought about sending the logs also to a messaging broker.

I want other microservice to pull from the broker, enrich the logs and update Elastic.

I want the pull to be of a batch of thousands of logs to perform the enrichment faster.

I thought about using Postgres as a broker because we already use PG, it's performant, and the learning curve is zero.

We use the same PG to store all of our data.

I thought about implementing a table named \`logs\` that the SDK will insert into, and the microservice will pull a batch from it every X minutes.

I am concerned about the performance deterioration that'll happen to PG as a whole because of the large volume of reads and inserts that'll be in the \`logs\` table.

&amp;#x200B;

1. **What do you think about the idea of using Postgres, and my concern?**
2. **Which alternative will suit my use case better: Kafka, Redis streams, RabbitMQ, or another one?**",True,False,False,dataengineering,t5_36en4,49606,public,self,Which messaging broker should I use for my use case?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s1hsmy/which_messaging_broker_should_i_use_for_my_use/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,ApatheticRart,,,[],,,,text,t2_73cw9sv5,False,False,False,[],False,False,1641919727,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s1he6t/fight_or_flight_response_for_new_position/,{},s1he6t,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/s1he6t/fight_or_flight_response_for_new_position/,False,,,6,1641919738,1,"This will be long so I'll TL;DR at the bottom. 

I am trying to get some outside perspective on my career from the sub. I have been feeling the pressure of my position and it has been triggering a flight or fight response from me. Half of me says to dig in and learn, and another part of me says to get out asap and find a new path. 

For starters I work for the fed. I began my career in mid 2016, shortly after graduating college. I was hired into a rotational management program that lasted 18 months. After completing the program I took my first official position within marketing but in a reporting role. This was my first exposure to SQL. The position had great potential for me to learn how to manipulate data for reporting purposes under a great boss, but that never happened. A reorg later and I was basically a report jockey. I would run some basic stuff and generate some pre made reports and then send out emails. It was extremely easy, but I was bored and wanted to get into a job with real skills... 

That led to me getting a job for the data warehousing org. I was not qualified by any means, and they knew that, but they were willing to bring me on. Initially they started me in one of the data marts. They told me they wanted me to work on a few things but that the primary goal was to get trained and learn development. Now there was essentially no onboarding, no training, no real guidance at all. I sat at my desk for weeks, months with nothing. I was trying to get a grasp but the world of development has so many facets and with the layer of government red tape over every process I was at a loss. After a few months and another reorg later I land on a different team within the same org. Now I am handling audits and agile PM work, no development. 

After the better part of a year of doing that I end up in a temporary assignment as a developer on a different team. Now this is related to the first position I had but very different in terms of what tools they used and the processes they used to promote the code changes. However I have a manager who knows my situation and was great with working with me and giving me projects I could do early on. I wasn't treated as a full on dev since the position was temporary but I was promoting code and learning. 

Well that lasted a handful of months and fell off to not having much work. Well what do you know, another reorg, a big one. Our whole organization was wiped and rebranded, a full on top down reorg. I was not selected early on to lateral into the equivalent position in the new org. 

From March 2021 - Sept 2021 I was unassigned. I still had to sign on, join some meetings, but I had no work, and nothing to do. The manager of my team was also in the same position so he was checked out. Everything was leading to me being laid off in October. At this point I haven't really had much contact with my previous team in a while. I wasn't technically on their team, and they had new managers, I was on an island trying to find a job. 

In the last round of interviews before being laid off, I got several interviews. Before I could even attend them all, I get a call from the new head of the new version of our organization. No interview, just says you got the job if you want it, but I need to know within a few hours. So right then I have to decide to stay in the role, but take the new position which ended up being a promotion for me, or risk it on the interview I had just done and the 2 more that week. I ended up accepting the position because I found it interesting and didn't want to change AGAIN. 

I have now been on this team since Oct 2021. I have roughly 6 months of actual dev work and I am in a full on data engineer role. I have inherited 17 applications that originated in the early 2000s and have also been through reorgs and conversions and I am on a 24/7 on call rotation every month or so. There have been several major changes through out the reorg process to how code is promoted and there is very poor/ little onboarding or training material. The manager I was working with previously did not get rehired into the same role, a new manager with no experience in what we do is now the manager. 

This is so long but typing this has been cathartic. 

TL;DR - I am not qualified for the role I am in. I have essentially no support from team members or my manager any more and I don't have much means of training at work. I feel like I am just waiting for something to fail or a project to come to me and blow up in my face. I find this field interesting, but I just don't know if I can get there. Part of me says to dig in and try but another part of me says I'm too far behind and I can't ever be great at this job. I don't even know how to begin to be good at the job. 

For those that are interested in what tools I am using...  
Ab Initio, UNIX (putty) , SQL (oracle, teradata) some other things too but that is the primary ETL stuff.",True,False,False,dataengineering,t5_36en4,49606,public,self,Fight or Flight Response for new position,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s1he6t/fight_or_flight_response_for_new_position/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,International-Life17,,,[],,,,text,t2_8xxbnh8b,False,False,False,[],False,False,1641917240,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s1gezp/please_evaluate_my_resume_for_job_search/,{},s1gezp,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/s1gezp/please_evaluate_my_resume_for_job_search/,False,,,6,1641917251,1,"Hey everyone, need your help on evaluating my resume.

I recently switched to a job which is more of ops than data engineering. But I want go back to being a  DE. Have experience in designing and developing batch and streaming apps (stateless and stateful event sourced apps)

But since I made a few short switches recently, I guess my resume looks bad (as in not stable to hiring managers) and my applications are getting rejected even through referrals.

Though I am skeptical on this because I have seen many people moving to faang with more frequent switches. (your thoughts on this?)

So I believe there is something wrong in my resume, please take a look and share your suggestions on what needs to be added/removed to make it better for job searches. Would be very helpful and I'd greatly appreciate it.

&amp;#x200B;

1. I had intentionally removed most of my previous orgs' experience because it would make my resume longer (I thought my recent work experience would make more sense).
2. And I removed my education details and redacted the recent org I work with.

Thanks everyone.",True,False,False,dataengineering,t5_36en4,49602,public,self,Please evaluate my resume for job search,0,[],0.99,https://www.reddit.com/r/dataengineering/comments/s1gezp/please_evaluate_my_resume_for_job_search/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,mwlon,,,[],,,,text,t2_8cr1i,False,False,False,[],False,False,1641914369,github.com,https://www.reddit.com/r/dataengineering/comments/s1fav1/pancakedb_a_new_event_ingestion_solution_for/,{},s1fav1,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s1fav1/pancakedb_a_new_event_ingestion_solution_for/,False,link,"{'enabled': False, 'images': [{'id': 'ZStUFxvnt5u1d8kDz-aqJl3SRf9a3E5dWeaiTGn9i-I', 'resolutions': [{'height': 54, 'url': 'https://external-preview.redd.it/Bt3cwaGPWeEb_TNRfqS-kfrV3z9ygWDSa9q3jLOo6LQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=80d1ce05dc856a58ae168c398deaeaf2c5b7f3c4', 'width': 108}, {'height': 108, 'url': 'https://external-preview.redd.it/Bt3cwaGPWeEb_TNRfqS-kfrV3z9ygWDSa9q3jLOo6LQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8d31c2122d029b2716f56a6ec13989fc52a4f836', 'width': 216}, {'height': 160, 'url': 'https://external-preview.redd.it/Bt3cwaGPWeEb_TNRfqS-kfrV3z9ygWDSa9q3jLOo6LQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8b043ff83933a7264e9936ec228fd20a97b98add', 'width': 320}, {'height': 320, 'url': 'https://external-preview.redd.it/Bt3cwaGPWeEb_TNRfqS-kfrV3z9ygWDSa9q3jLOo6LQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=eb6da8f83898f8abc63391c66d5021bc9734c59e', 'width': 640}, {'height': 480, 'url': 'https://external-preview.redd.it/Bt3cwaGPWeEb_TNRfqS-kfrV3z9ygWDSa9q3jLOo6LQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ca63f0b4e21db8c57f1b82601fdf8ec2efed0dbd', 'width': 960}, {'height': 540, 'url': 'https://external-preview.redd.it/Bt3cwaGPWeEb_TNRfqS-kfrV3z9ygWDSa9q3jLOo6LQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e7e0c6ee5553d613afcf5fa2469606a01b60e169', 'width': 1080}], 'source': {'height': 600, 'url': 'https://external-preview.redd.it/Bt3cwaGPWeEb_TNRfqS-kfrV3z9ygWDSa9q3jLOo6LQ.jpg?auto=webp&amp;s=95516f98d6a87b109921a2cac92ec4a63eb4a989', 'width': 1200}, 'variants': {}}]}",6,1641914380,1,,True,False,False,dataengineering,t5_36en4,49601,public,https://a.thumbs.redditmedia.com/QycnDHTE-I4_c18VZYusfNwcdcqNxbkC7v3-vZTPar8.jpg,"PancakeDB, a new event ingestion solution for streaming writes and batch reads",0,[],1.0,https://github.com/pancake-db/pancake-db,all_ads,6,,,,,,70.0,140.0,https://github.com/pancake-db/pancake-db,,,,,,,,,,
[],False,marshr9523,transparent,,[],19bba012-ac9d-11eb-b77b-0eec37c01719,Data Analyst,dark,text,t2_y3x7xvk,False,False,False,[],False,False,1641913274,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s1evh0/6_hour_data_engineer_assessment_at_5x/,{},s1evh0,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/s1evh0/6_hour_data_engineer_assessment_at_5x/,False,,,6,1641913285,1,"Has anyone here appeared for the technical round at 5x for the data engineer position? They've told me on call that it would be a 6 hr technical round, which would involve taking a test simulating day to day work of a 5x DE.

Has anyone appeared for it, or has any tips for it ?

Thanks!",True,False,False,dataengineering,t5_36en4,49598,public,self,6 Hour Data Engineer Assessment at 5x,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s1evh0/6_hour_data_engineer_assessment_at_5x/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,agile_crossover,,,[],,,,text,t2_2625b5in,False,False,False,[],False,False,1641908130,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s1d1pt/architecture_design_problem/,{},s1d1pt,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/s1d1pt/architecture_design_problem/,False,,,6,1641908140,1,"Is there a place/resource for data engineering specific design patterns? 

Specific problem here: trying to build a system that can launch different versions (commits) of a pipeline at will — mostly for testing of various commits or for releases. These different pipelines would need to spin up associated services and then spin down after completed or upon request in case of failure — curious if a design pattern for this existed already ? — leads me to the general question of resolved for DE design patterns exist?

Thanks!",True,False,False,dataengineering,t5_36en4,49592,public,self,Architecture / Design Problem,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s1d1pt/architecture_design_problem/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,FoolishEnquiry,,,[],,,,text,t2_dnxyju9v,False,False,False,[],False,False,1641906114,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s1ce8y/best_practice_to_alert_users_if_databricks/,{},s1ce8y,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s1ce8y/best_practice_to_alert_users_if_databricks/,False,,,6,1641906125,1,"Hello, I was wondering about the best practice to notify users (via email or slack) about a successful/unsuccessful notebook run.

I read about job alerts, but is there a way to send notifications to slack?

Thanks in advance",True,False,False,dataengineering,t5_36en4,49588,public,self,Best practice to alert users if databricks finished loading data.,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s1ce8y/best_practice_to_alert_users_if_databricks/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,izner82,,,[],,,,text,t2_67s1slvl,False,False,False,[],False,False,1641896354,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s19rv3/using_debit_card_with_5_balance_in_aws/,{},s19rv3,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s19rv3/using_debit_card_with_5_balance_in_aws/,False,,,6,1641896365,1,"I'm planning to play with aws a little meaning there's alot of uncertainties and a lot of unexpected charges that might occur, as far as my research goes aws budget could only alert you when the threshold for a specific service has been breached but it's not really going to do much more. 

I'm planning to use a debit card with a very low amount of balance so when things go wrong they couldn't charge me anymore as it doesn't contain enough money, i've tried searching but i haven't seen anyone who's done the same - now i'm getting paranoid whether what's wrong with my idea, can anyone point it out?",True,False,False,dataengineering,t5_36en4,49570,public,self,Using debit card with &lt;5$ balance in aws,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s19rv3/using_debit_card_with_5_balance_in_aws/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,SnowPlowOpenSource,,,[],,,,text,t2_enkpz7c3,False,False,False,[],False,False,1641895840,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s19nl7/snowplow_product_office_hours_19_january_2022/,{},s19nl7,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s19nl7/snowplow_product_office_hours_19_january_2022/,False,,,6,1641895850,1,"Learn how to use Snowplow Micro to implement end to end behavioral data tracking testing in your development practices

[19\/01\/2022, 17:00 GMT](https://preview.redd.it/52h30dae91b81.jpg?width=1200&amp;format=pjpg&amp;auto=webp&amp;s=4c44b8329600a95943eb676b445ed2c1d8b643a8)

As usual, you'll have an opportunity to ask questions at the end.

Look forward to seeing you on Jan 19th, 17:00 GMT.",True,False,False,dataengineering,t5_36en4,49570,public,https://b.thumbs.redditmedia.com/3dcGAQJQ2w_m7VGpX1mAhpZTW3n-XJBG1Wb-aYt10GA.jpg,Snowplow: Product Office Hours 19 January 2022,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s19nl7/snowplow_product_office_hours_19_january_2022/,all_ads,6,,,,,,73.0,140.0,,"{'52h30dae91b81': {'e': 'Image', 'id': '52h30dae91b81', 'm': 'image/jpg', 'p': [{'u': 'https://preview.redd.it/52h30dae91b81.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ab691426268cb42e9119e70b6c0c5af880dfe680', 'x': 108, 'y': 56}, {'u': 'https://preview.redd.it/52h30dae91b81.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=353b88d1e92e0b463321f3ba9b5b4b09412fe1a3', 'x': 216, 'y': 113}, {'u': 'https://preview.redd.it/52h30dae91b81.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1e8042306e7a3a35e811531c855f7a8d15834a8e', 'x': 320, 'y': 167}, {'u': 'https://preview.redd.it/52h30dae91b81.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5c55d5dcea2b082094b89f0af24a666f289f5017', 'x': 640, 'y': 334}, {'u': 'https://preview.redd.it/52h30dae91b81.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8f3c92c8848e1642b90cb3c9a5e5cbd3cb676a02', 'x': 960, 'y': 502}, {'u': 'https://preview.redd.it/52h30dae91b81.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=48cfdc44015026a71ac1b994ba9857de777b425e', 'x': 1080, 'y': 565}], 's': {'u': 'https://preview.redd.it/52h30dae91b81.jpg?width=1200&amp;format=pjpg&amp;auto=webp&amp;s=4c44b8329600a95943eb676b445ed2c1d8b643a8', 'x': 1200, 'y': 628}, 'status': 'valid'}}",,,,,,,,,
[],False,SnowPlowOpenSource,,,[],,,,text,t2_enkpz7c3,False,False,False,[],False,False,1641895295,snowplowanalytics.com,https://www.reddit.com/r/dataengineering/comments/s19itv/snowplow_product_office_hours_january_2022/,{},s19itv,False,True,False,False,False,True,False,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/s19itv/snowplow_product_office_hours_january_2022/,False,,,6,1641895306,1,,True,False,False,dataengineering,t5_36en4,49568,public,default,Snowplow: Product Office Hours January 2022,0,[],1.0,https://snowplowanalytics.com/events/office-hours-january-22/?utm_source=reddit&amp;utm_medium=post&amp;utm_campaign=product-office-hours&amp;utm_content=january-22,all_ads,6,,,,,,,,https://snowplowanalytics.com/events/office-hours-january-22/?utm_source=reddit&amp;utm_medium=post&amp;utm_campaign=product-office-hours&amp;utm_content=january-22,,,,,,,,,,
[],False,DarkestKnight_23,,,[],,,,text,t2_d1v0zccf,False,False,False,[],False,False,1641882017,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s1668j/modern_dwh_vs_kimball/,{},s1668j,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s1668j/modern_dwh_vs_kimball/,False,,,6,1641882028,1,"So a lot of Kimball's principles like strict normalisation and star schemas are being replaced by wide tables, which take advantage of cheap storage/columnar querying and are faster to query compared to multiple joins. However, over time wouldn't wide tables be a disaster if certain  dimensions (which were initially considered type 0) get crammed into the the wide tables only for them to later have to change? How do you guys decide between the efficiency and speed of wide tables vs the strict data validation and robustness of a star schema setup?",True,False,False,dataengineering,t5_36en4,49540,public,self,Modern DWH vs Kimball,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s1668j/modern_dwh_vs_kimball/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,smart_potato34,,,[],,,,text,t2_g0nhkcyi,False,False,False,[],False,False,1641880161,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s15n8b/business_intelligence_engineer_interview_at_amazon/,{},s15n8b,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s15n8b/business_intelligence_engineer_interview_at_amazon/,False,,,6,1641880171,1,"What should I expect? I was told it is going to be a SQL, some python scripting and Datawarehousing logic questions. (I'm not familiar with Amazon Cloud products).

How many questions will I get and how many should I get right? Thanks",True,False,False,dataengineering,t5_36en4,49536,public,self,Business Intelligence Engineer Interview at Amazon,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s15n8b/business_intelligence_engineer_interview_at_amazon/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,No_Substance_9411,,,[],,,,text,t2_9vkf58ou,False,False,False,[],False,False,1641873963,reddit.com,https://www.reddit.com/r/dataengineering/comments/s13ql8/help_with_aws_lambda_functions/,{},s13ql8,False,True,False,False,False,True,False,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s13ql8/help_with_aws_lambda_functions/,False,,,6,1641873973,1,,True,False,False,dataengineering,t5_36en4,49525,public,https://b.thumbs.redditmedia.com/p0FVFCCh1WBlyd0Ydj9edIqryIYwDPEdtmUpOo95OFw.jpg,Help with AWS lambda functions,0,[],1.0,https://www.reddit.com/gallery/s13ql8,all_ads,6,,,,,,64.0,140.0,https://www.reddit.com/gallery/s13ql8,"{'h5nj2b4qgza81': {'e': 'Image', 'id': 'h5nj2b4qgza81', 'm': 'image/jpg', 'p': [{'u': 'https://preview.redd.it/h5nj2b4qgza81.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=29fa3b34301156ba9f0769e4b8f845f239aeb1c9', 'x': 108, 'y': 49}, {'u': 'https://preview.redd.it/h5nj2b4qgza81.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b508ef05b2b7e1148797b166a205ce80aab667ec', 'x': 216, 'y': 99}, {'u': 'https://preview.redd.it/h5nj2b4qgza81.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7a078c10240c66c23b134b92e5d0c53a545aae9c', 'x': 320, 'y': 146}, {'u': 'https://preview.redd.it/h5nj2b4qgza81.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ca5843b627a803e76989335534fd4d8fb2cf3d5e', 'x': 640, 'y': 293}, {'u': 'https://preview.redd.it/h5nj2b4qgza81.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0f9614796a5aaf5f64cd81b30286ea171f52bf6e', 'x': 960, 'y': 440}, {'u': 'https://preview.redd.it/h5nj2b4qgza81.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4b7edddfe395301496810ee133be423e21bc1ffe', 'x': 1080, 'y': 495}], 's': {'u': 'https://preview.redd.it/h5nj2b4qgza81.jpg?width=4608&amp;format=pjpg&amp;auto=webp&amp;s=66cad9a719161ccc9b1676decd92a3e001976199', 'x': 4608, 'y': 2112}, 'status': 'valid'}, 'ovchv0fqgza81': {'e': 'Image', 'id': 'ovchv0fqgza81', 'm': 'image/jpg', 'p': [{'u': 'https://preview.redd.it/ovchv0fqgza81.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cd782b29be0d191b6c7533ee3bcc5dba158f14f9', 'x': 108, 'y': 49}, {'u': 'https://preview.redd.it/ovchv0fqgza81.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9361698c7fefc36636e4d45462106d1dcb7e789a', 'x': 216, 'y': 99}, {'u': 'https://preview.redd.it/ovchv0fqgza81.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c6088b49b608d185d4c6f296be2b6b8a0e963a51', 'x': 320, 'y': 146}, {'u': 'https://preview.redd.it/ovchv0fqgza81.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b7a5ddf0f2775ae6b71701367421b556a41ffe07', 'x': 640, 'y': 293}, {'u': 'https://preview.redd.it/ovchv0fqgza81.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c227351028e849b0d6718e645a93bc73f8368252', 'x': 960, 'y': 440}, {'u': 'https://preview.redd.it/ovchv0fqgza81.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=83d21c26e877c401ae09c6aaed344cde4f77a3b6', 'x': 1080, 'y': 495}], 's': {'u': 'https://preview.redd.it/ovchv0fqgza81.jpg?width=4608&amp;format=pjpg&amp;auto=webp&amp;s=e9b231f3e01ea23bfcd8653d2a14237405d8ccb8', 'x': 4608, 'y': 2112}, 'status': 'valid'}}","{'items': [{'caption': 'I am getting this error. I want to get data in Json and then save it s3 bucket. can some one help ', 'id': 101291400, 'media_id': 'h5nj2b4qgza81'}, {'id': 101291401, 'media_id': 'ovchv0fqgza81'}]}",True,,,,,,,
[],False,fabricmoo,,,[],,,,text,t2_d8wq0wkd,False,False,False,[],False,False,1641863393,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s105u5/job_market_for_international_applicants_in_usaeu/,{},s105u5,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/s105u5/job_market_for_international_applicants_in_usaeu/,False,,,6,1641863404,1,"Hello folks, looking to the seniors of this industry or anyone from a similar international background for some advice on my path.

From a SEA country, got my bachelor's in actuarial/Stats in the US, lost OPT job when COVID struck. Have since been working in my home country. I've held 2 relevant roles: data analyst/consultant (6 months) and data scientist (9 months to date).

The data field as a whole is really new where I'm at, and both the positions have been at startups so I've been the only person on the team, so I've had the chance to wear the ""full stack"" data guy hat, so to speak. Engineering wise, I've helped build out some clients pipelines using GCP and services like Stitch, Fivetran. At my current role I built the full analytics pipeline (moving data into sql, connecting it to dashboards for analysis etc) off their production NoSQL database.

Most of the work I've done is in python scripts and sql, with the orchestration being achieved via airflow/Cron jobs and the like.

I've really fallen in love with the engineering side of data over the heavy math and stats side of things, so right now I'd like to move overseas since the field isn't established here (along with other reasons of course, but that's not for this sub). I loved my time in the US and really felt at home there, so I  would like to return there.

I know it's going to be tough, and I'm somewhat familiar with the difficulties of getting a H1B.

I guess my question is, what's the outlook in this particular field for people like me? Are companies considering international applicants? Non CS background seems to be a big hurdle in the US. What would be my best option to break into the field? Do I really need to invest in a Masters and hope to land something off of OPT?

Other international applicants/seniors familiar with overseas visa situations: how'd you guys do it? What can I do with my experience?

TLDR: how can I break into the US/EU market as an applicant from a SEA country?",True,False,False,dataengineering,t5_36en4,49491,public,self,Job market for international applicants (in USA/EU),0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s105u5/job_market_for_international_applicants_in_usaeu/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,discoinfiltrator,,,[],,,,text,t2_4hjq9,False,False,False,[],False,False,1641856798,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s0xr0d/weighing_a_meta_de_offer/,{},s0xr0d,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s0xr0d/weighing_a_meta_de_offer/,False,,,6,1641856809,1,"I just got an offer for a DE position at Meta and was interested in knowing if others could shed some light on the role to help in my decision making process and check a few assumptions I've made. 

Briefly, what I have managed to understand is:
1) Meta DE work is, mostly, closer to the role of an analytics engineer involving a lot less development and more SQL/dashboard building 
2) Work life balance can be hard to achieve 

For the first point I'm less worried. I do enjoy that kindof work and am comfortable in that kind of role. My main concern here is work life balance. Reviews on Blind and Glassdoor are kind of all over the place with regards to this but many mention long hours and high pressure. I get that this can vary a lot by team, but I'd appreciate any insights any of you might have.",True,False,False,dataengineering,t5_36en4,49455,public,self,Weighing a Meta DE offer,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s0xr0d/weighing_a_meta_de_offer/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,dead-on-arrival-,,,[],,,,text,t2_4hu6n9ua,False,False,False,[],False,False,1641855537,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s0x928/transition_to_data_architect_roles/,{},s0x928,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s0x928/transition_to_data_architect_roles/,False,,,6,1641855548,1,"Hi, I'm a Sr. Data Engineer with ~4 years of experience looking to transition into the realm of Data Architecture. I've found passion in building scalable systems and cloud architectural components that support data engineers, data scientists and MLEs perform their duties better. 

How do I make an active transition into Architecture driven roles? Data architect roles are sparse, with not many options for less than 10 YOE. What kind of jobs do I look for and how do I let recruiters reaching out to me for DE roles that I'm more interested in architecture? 

Would love to hear the community's advice!",True,False,False,dataengineering,t5_36en4,49446,public,self,Transition to Data Architect roles,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s0x928/transition_to_data_architect_roles/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,happysunshinekidd,,,[],,,,text,t2_dltif81,False,False,False,[],False,False,1641854161,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s0wpv9/help_everything_is_a_model/,{},s0wpv9,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s0wpv9/help_everything_is_a_model/,False,,,6,1641854171,1,"Recently started working in a computer vision/robotics company as ML ops. We're building some in house tracking tools -- dvc has become a bottleneck and wandb is unreasonably expensive. 

Part of the plan is to have a django API servicing dataset retrieval/creation, model-reuse, pipeline introspection, etc. This was actually going well, until I started drafting up the.... models.

My Django models now reference my research teams models, which in turn depend on various other data models (remnants of an OOO filesystem management type approach), and its starting to drive me nuts.

Someone on my team casually suggested namespacing ML stuff as ""AIModel"" or ""MLMode"" but I still hate how this looks:

    class AIModel(models.Model):
        model_name = CharField(....)
...


Anyone have any tips on naming stuff in this day and age?",True,False,False,dataengineering,t5_36en4,49441,public,self,Help! Everything is a model!,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s0wpv9/help_everything_is_a_model/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,caksters,,,[],,,,text,t2_tux1p,False,False,False,[],False,False,1641852773,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s0w5ye/being_constantly_shut_down_by_more_senior_team/,{},s0w5ye,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/s0w5ye/being_constantly_shut_down_by_more_senior_team/,False,,,6,1641852784,1,"So I am a consultant (still junior) that is put on a project where we are building dashboard application for our client.
We are team of 2 data engineers, 1 data architect, 1 data strategy consultant (don’t know actual role name), 3 data modellers/analysts, solution owner.
On the engineering side we are 4 people (2 engineers, data architect, strategy guy) that are building data pipelines according to data modeller requirements for their consumption.

I am quite concerned with quality of work we are producing. I had to push so we can investigate and then set up local AWS glue environment so we dont have to actually use the GUI on AWS to manually run the pipelines every-time we are developing them. Now I feel like I am pushing for having some sort of testing standards in place so we can test our pipelines. I’ve been drawn in so many meetings where data architect needs to debug his code and he asks me to help to rewrite his spark logic in sql so he can check where are the mistakes. Like, isn’t this the reason you want to write automated tests where you can test for crap like this using some representative dummy data???

Everytime I mention something about writing black box tests for our pipeline, I am being dismissed that this would nice to have and they appreciate my idea, but we should focus on development work first and getting pipelines going as this is our priority… The problem with this approach is that once we finish a somewhat worki pipeline, we jump onto next pipeline development task and this is considered done.

Now I’ve been given a task to write a pipeline from s3 to redshift where I need to parse some nested jsons. At the moment I only have dummy data of 5 records. I asked if we can get a file that specifies what sort of data we are expecting in records because many fields contain nulls so I want to know whether it will contain arrays, dictionaries, strings. I was dismissed and told that this is not important. When more data comes in we will just infer schema from that and I should focus on task at hand and I should work with what I have (5 records). Like, am I being too anal by asking if we can somehow find out from data providers what data formats will be there so I know what table format I should create in redshift???

I don’t know, I have only 1 YOE and I feel this is definitely not how data engineering should be done. I am worried about the quality of work we are producing as well as I am not learning the best engineering practices here... I know solution owner is on the same page with me and the other data engineer as well. But any talks about quality checks, following more agile practices are being shut down by data architect and big data strategy consultant.

I will try my best to lead by example. I already showed how testing can be done locally. I will develop my pipeline with appropriate unit tests and black box tests for my pipeline. Hopefully this will convince more senior guys that this is not that difficult to do and it adds value.

My rant is over. Just wanted to write this somewhere so I can get it off my chest",True,False,False,dataengineering,t5_36en4,49434,public,self,Being constantly shut down by more senior team members when I mention adding some QA in our work,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s0w5ye/being_constantly_shut_down_by_more_senior_team/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,dataengineerdude,,,[],,,,text,t2_4wrevs7q,False,False,False,[],False,False,1641852650,confessionsofadataguy.com,https://www.reddit.com/r/dataengineering/comments/s0w41n/5_part_series_demystifying_data_warehouses_data/,{},s0w41n,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s0w41n/5_part_series_demystifying_data_warehouses_data/,False,link,"{'enabled': False, 'images': [{'id': '1Z1wa_Jyxu-aoYDcAWhhJ-egwNqC_t-Q0eBYgd_YyG0', 'resolutions': [{'height': 81, 'url': 'https://external-preview.redd.it/BmMjuoHb-tcJfdu8TI4raG8RpvhpnngXjKHTJebWfaM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3fcfe5c353c2f7bbeeafcf4fa6bb507d5c8673f7', 'width': 108}, {'height': 162, 'url': 'https://external-preview.redd.it/BmMjuoHb-tcJfdu8TI4raG8RpvhpnngXjKHTJebWfaM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=04be4362bf159ec05845bf2cb4e8b9cbf5a1fd53', 'width': 216}, {'height': 240, 'url': 'https://external-preview.redd.it/BmMjuoHb-tcJfdu8TI4raG8RpvhpnngXjKHTJebWfaM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=02e4ef36900377a2add645fdc5cf30ed61e2fed2', 'width': 320}, {'height': 480, 'url': 'https://external-preview.redd.it/BmMjuoHb-tcJfdu8TI4raG8RpvhpnngXjKHTJebWfaM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=14a9c179413144d6ef921adf384971c552402433', 'width': 640}, {'height': 720, 'url': 'https://external-preview.redd.it/BmMjuoHb-tcJfdu8TI4raG8RpvhpnngXjKHTJebWfaM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=574f5d813979549dda46800f0717f292d87b1b73', 'width': 960}], 'source': {'height': 773, 'url': 'https://external-preview.redd.it/BmMjuoHb-tcJfdu8TI4raG8RpvhpnngXjKHTJebWfaM.jpg?auto=webp&amp;s=b67aa2b807c6bf03a3c644ef6a378249c9a1276a', 'width': 1030}, 'variants': {}}]}",6,1641852660,1,,True,False,False,dataengineering,t5_36en4,49434,public,https://b.thumbs.redditmedia.com/VRpB2NN-3H1qQzabItZVuJHjJ8va0HZNcFcfpzWt8LA.jpg,5 Part Series - Demystifying Data Warehouses / Data Lakes / Lake Houses,0,[],1.0,https://www.confessionsofadataguy.com/5-part-series-demystifying-data-warehouses-data-lakes-lake-houses/,all_ads,6,,,,,,105.0,140.0,https://www.confessionsofadataguy.com/5-part-series-demystifying-data-warehouses-data-lakes-lake-houses/,,,,,,,,,,
[],False,RedSamRedSamRed,,,[],,,,text,t2_2ubpgep,False,False,False,[],False,False,1641852444,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s0w0z5/what_technology_will_give_me_the_most_chance_to/,{},s0w0z5,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s0w0z5/what_technology_will_give_me_the_most_chance_to/,False,,,6,1641852454,1,"Hi all, I'm an intern in an hardware company, I would like to find another internship for summer in data engineering. What technology/tool will give me the most chance to get an internship in data engineering as a newbie? Thanks for the help.",True,False,False,dataengineering,t5_36en4,49432,public,self,What technology will give me the most chance to get job?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s0w0z5/what_technology_will_give_me_the_most_chance_to/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,king_booker,,,[],,,,text,t2_ryny2,False,False,False,[],False,False,1641842202,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s0ryvu/can_we_have_a_monthly_discussion_thread/,{},s0ryvu,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s0ryvu/can_we_have_a_monthly_discussion_thread/,False,,,6,1641842213,1,"I wanted to post that I just did the datastax cassandra certification but I don't think it warrants its own thread. I was happy with it and wanted to share with the community

Anyway, yeah I put in a good 3-4 weeks in preparing. I had a bit of Cassandra experience but I wanted to learn. The datastax site is great and all of the contributors are really cool so thanks guys",True,False,False,dataengineering,t5_36en4,49352,public,self,Can we have a monthly discussion thread?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s0ryvu/can_we_have_a_monthly_discussion_thread/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,pnavid,,,[],,,,text,t2_ga69rrlq,False,False,False,[],False,False,1641837076,hightouch.io,https://www.reddit.com/r/dataengineering/comments/s0pxgj/airflow_alternatives_a_look_at_prefect_and_dagster/,{},s0pxgj,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s0pxgj/airflow_alternatives_a_look_at_prefect_and_dagster/,False,link,"{'enabled': False, 'images': [{'id': '1-Y5RMCgPzrzSY4kdqtVUDYcZnCC3HTIcGZGkM5mcoA', 'resolutions': [{'height': 60, 'url': 'https://external-preview.redd.it/SqhPVLNWDgGLrAVnTftVQ7qwgCpqlUP6ujGsS3dvM6E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=942eda8f2bab4bee714feb397ba0259e8fc826dc', 'width': 108}, {'height': 121, 'url': 'https://external-preview.redd.it/SqhPVLNWDgGLrAVnTftVQ7qwgCpqlUP6ujGsS3dvM6E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=404d6633b05356f307daf53e3a76365b2c7cd3c2', 'width': 216}, {'height': 180, 'url': 'https://external-preview.redd.it/SqhPVLNWDgGLrAVnTftVQ7qwgCpqlUP6ujGsS3dvM6E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6ebde868906db5689d5a2c2a89aace64026ae3db', 'width': 320}], 'source': {'height': 288, 'url': 'https://external-preview.redd.it/SqhPVLNWDgGLrAVnTftVQ7qwgCpqlUP6ujGsS3dvM6E.jpg?auto=webp&amp;s=d1da560e34767b4bef7a70f454adcee07335f912', 'width': 512}, 'variants': {}}]}",6,1641837086,1,,True,False,False,dataengineering,t5_36en4,49314,public,https://b.thumbs.redditmedia.com/5vstL9r_i2J0T3wO1v-uHnrdYw_XBlwT5wlzP2gmzIQ.jpg,Airflow Alternatives: A Look at Prefect and Dagster,0,[],1.0,https://hightouch.io/blog/airflow-alternatives-a-look-at-prefect-and-dagster/,all_ads,6,,,,,,78.0,140.0,https://hightouch.io/blog/airflow-alternatives-a-look-at-prefect-and-dagster/,,,,,,,,,,
[],False,Silver-Thing,,,[],,,,text,t2_2y9q9ok7,False,False,False,[],False,False,1641836846,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s0pu76/data_architects_of_reddit_how_do_you_keep_up_with/,{},s0pu76,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/s0pu76/data_architects_of_reddit_how_do_you_keep_up_with/,False,,,6,1641836857,1,"Recently, I've been more involved with data architecture at my job (I used to be a data engineer up to this point). I am mostly designing things based on my experience but I haven't really been following any people or blogs about data architecture yet. I wonder if you could suggest some valuable resources to me.",True,False,False,dataengineering,t5_36en4,49311,public,self,Data Architects of Reddit - how do you keep up with the latest trends?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s0pu76/data_architects_of_reddit_how_do_you_keep_up_with/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,anushbhatia,,,[],,,,text,t2_hv626pxm,False,False,False,[],False,False,1641835160,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s0p65j/should_i_work_on_snowflake_as_a_fresher/,{},s0p65j,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/s0p65j/should_i_work_on_snowflake_as_a_fresher/,False,,,6,1641835171,1,"Hello Peeps,

I am trained as a  data engineer in my company. I have been an role of the snowflake developer for a project should I take a decision to work in that or wait for another one?

**Trained On:**

Azure, ETL basics, Scala, Python, Data bricks, Spark, SQL, Linux and currently training on AWS.

Thanks for answering😇",True,False,False,dataengineering,t5_36en4,49293,public,self,Should I work on Snowflake as a Fresher?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s0p65j/should_i_work_on_snowflake_as_a_fresher/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Easy-Variation-5927,,,[],,,,text,t2_6k4tm1ai,False,False,False,[],False,False,1641833991,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s0ooxy/pipebase_020_released/,{},s0ooxy,False,True,False,False,False,True,True,False,#ff66ac,[],dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1,Meme,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s0ooxy/pipebase_020_released/,False,self,"{'enabled': False, 'images': [{'id': '114sgDv85LRSAeWByOoMqmIzLl3wBAmmIOkzXI6EY9U', 'resolutions': [{'height': 54, 'url': 'https://external-preview.redd.it/knCejV8bI7XhZrMMzSylLviddBpCiy4UpLZKAoexUO4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2cdd0bb5060d5d8d2780fea2da4c11e615e93227', 'width': 108}, {'height': 108, 'url': 'https://external-preview.redd.it/knCejV8bI7XhZrMMzSylLviddBpCiy4UpLZKAoexUO4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ad12938227476551388bab74038cc67aff7928cc', 'width': 216}, {'height': 160, 'url': 'https://external-preview.redd.it/knCejV8bI7XhZrMMzSylLviddBpCiy4UpLZKAoexUO4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=84cb4f80277c6609e0adeec14f9a21de98729ea3', 'width': 320}, {'height': 320, 'url': 'https://external-preview.redd.it/knCejV8bI7XhZrMMzSylLviddBpCiy4UpLZKAoexUO4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=52eb9d98977f1482ae96b3e31daf0a620ad8a74c', 'width': 640}, {'height': 480, 'url': 'https://external-preview.redd.it/knCejV8bI7XhZrMMzSylLviddBpCiy4UpLZKAoexUO4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ec1d4937de6a28c65fa7cbf6bdceb1a05347395f', 'width': 960}, {'height': 540, 'url': 'https://external-preview.redd.it/knCejV8bI7XhZrMMzSylLviddBpCiy4UpLZKAoexUO4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=69fca6354a05fd86e76afaadd605e45218456ddd', 'width': 1080}], 'source': {'height': 600, 'url': 'https://external-preview.redd.it/knCejV8bI7XhZrMMzSylLviddBpCiy4UpLZKAoexUO4.jpg?auto=webp&amp;s=88d76313b61b2bac71d60a9101769082e0d3c7c2', 'width': 1200}, 'variants': {}}]}",6,1641834002,1,"[`pipebase`](https://github.com/pipebase/pipebase) is a low code data integration framework.

In general, the framework allow developer customize data pipeline through [`manifest`] definition and wire a variety of system through [`pipeware`](https://github.com/pipebase/pipebase/tree/main/pipeware) plugins to sync/transform data.

Here is a [`Tutorial`](https://github.com/pipebase/pipebase/blob/main/cargo-pipe/README.md) as quick start, build your first hello world app (a timer) with CLI

And here is a list of [`Example`](https://github.com/pipebase/pipebase/tree/main/examples) demonstrate how to compose manifest (`pipe.yml`) and wire external system ex: `kafka`, `rabbitmq`, `mysql`, `cassandra`, `rocksdb`, `aws-s3`, `mqtt` etc.

Have fun !",True,False,False,dataengineering,t5_36en4,49283,public,self,pipebase 0.2.0 released,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s0ooxy/pipebase_020_released/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,pankswork,,,[],,,,text,t2_bu6ed,False,False,False,[],False,False,1641833838,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s0omwg/exporting_data_at_production_scale/,{},s0omwg,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s0omwg/exporting_data_at_production_scale/,False,,,6,1641833848,1,"Hi all,
I'm a pretty seasoned data engineer (can move and transform data pretty optimally) but I have a new use case.  I am working at a company thats product will be exporting large swaths of data to clients externally.  Traditionally I've built DWs that plug into BI tools or can serve simple exports, but I'm wondering about scaling of exports. If I need to serve 100GB a day to external clients, what is the ultimate tool to do that with?

Env:
We are using AWS, our raw and stg layers are in parquet files, and our DW will be in some sort of DB (very open to the DB at the moment, and am familiar with Row vs Columnar and can optimize based on desired use-case.)

Do I use spark to build exports via parquet files? Do I put in DW and query and send out from there? What are thoughts?

I know using something like Aurora or Snowflake to export will be very expensive as it will be cloud egress but idk how that holds up to cost of just running exports off data lake.

Thoughts?",True,False,False,dataengineering,t5_36en4,49281,public,self,Exporting Data at Production Scale,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s0omwg/exporting_data_at_production_scale/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Substantial_Gift_861,,,[],,,,text,t2_hu5lgzxj,False,False,False,[],False,False,1641833397,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s0oglv/what_skill_or_tools_you_need_to_know_in_order_to/,{},s0oglv,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s0oglv/what_skill_or_tools_you_need_to_know_in_order_to/,False,,,6,1641833408,1,Do you need to be good at analytics in order to be good in data engineering?,True,False,False,dataengineering,t5_36en4,49277,public,self,what skill or tools you need to know in order to be a data engineer?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s0oglv/what_skill_or_tools_you_need_to_know_in_order_to/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,city_boy__,,,[],,,,text,t2_2mp796xh,False,False,False,[],False,False,1641833188,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s0odvb/transition_from_sap_bw_to_data_engineering/,{},s0odvb,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/s0odvb/transition_from_sap_bw_to_data_engineering/,False,,,6,1641833199,1,"Hi All, 

I am working as a SAP BW consultant for 2.7 years now. I'm trying to get into hard core data engineering. There is very minimal data engineering taking place in SAP framework as it's very old and boring. After spending years I feel I don't see myself  growing I'm this technology. It did help me understand the basics of data warehouse and it's existence. 

I have been learning ML in parallel as well. But hardly can implement it in SAP environment. Is there ways I can grow in SAP or if I need to change my technology how do I start preparing. I have good basics of python and sql.",True,False,False,dataengineering,t5_36en4,49275,public,self,Transition from SAP BW to Data Engineering.,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s0odvb/transition_from_sap_bw_to_data_engineering/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Data-Sponge,,,[],,,,text,t2_icn1jw23,False,False,False,[],False,False,1641832298,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s0o1ci/building_a_data_warehouse_for_a_startup_as_new/,{},s0o1ci,False,False,False,False,False,False,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s0o1ci/building_a_data_warehouse_for_a_startup_as_new/,False,,,6,1641832308,1,[removed],True,False,False,dataengineering,t5_36en4,49267,public,self,Building a Data Warehouse for a Startup as New Grad,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s0o1ci/building_a_data_warehouse_for_a_startup_as_new/,all_ads,6,,,automod_filtered,,,,,,,,,,,,,,,
[],False,sm465,,,[],,,,text,t2_15ot9u,False,False,False,[],False,False,1641829649,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s0n04a/stuck_in_finding_a_job_in_uk_need_advice/,{},s0n04a,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/s0n04a/stuck_in_finding_a_job_in_uk_need_advice/,False,,,6,1641829660,1,"Hi, I'm 32M and currently live in the UK (Swansea) and it's about 2 months I'm looking for job, I'm new immigrant here but have no visa problem to 2025, I was a .Net back-end freelancer developer for about 6-7 years design systems and databases, built web APIs, Windows applications, and I'm familiar with python, wide ranges of databases such as SqlServer, MySql, PostgreSql, SQLite and also noSqls, recently (6 months) I've started to learn about Data Engineering and started to learn about ETL and cloud base databases.   


so here is the problem, I started to apply for paid internships or junior roles on LinkedIn, Reed, glassdoor almost everywhere, but the companies reject my application sometimes in under 5 minutes.  


could this community give me some advice about what should I do?",True,False,False,dataengineering,t5_36en4,49237,public,self,"Stuck in finding a job in UK, Need Advice",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s0n04a/stuck_in_finding_a_job_in_uk_need_advice/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,dxt0434,,,[],,,,text,t2_qex2g2y,False,False,False,[],False,False,1641825441,hodovi.ch,https://www.reddit.com/r/dataengineering/comments/s0lgwb/comparing_results_between_airflow_runs/,{},s0lgwb,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s0lgwb/comparing_results_between_airflow_runs/,False,,,6,1641825458,1,,True,False,False,dataengineering,t5_36en4,49175,public,default,Comparing results between Airflow runs,0,[],1.0,https://hodovi.ch/blog/comparing-results-between-airflow-runs/,all_ads,6,,,,,,,,https://hodovi.ch/blog/comparing-results-between-airflow-runs/,,,,,,,,,,
[],False,IncorporatedITGuy,,,[],,,,text,t2_75uyumrs,False,False,False,[],False,False,1641821078,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s0jyj1/any_case_for_using_snowflake_streams_if_dbt/,{},s0jyj1,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s0jyj1/any_case_for_using_snowflake_streams_if_dbt/,False,,,6,1641821089,1,"Snowflake is promoting Streams and Tasks a lot but DBT already provides Incremental Model and Scheduling capability ( directly or via Airflow).

I am curious if anyone using DBT + Snowflake stack found use case for Streams and Tasks.",True,False,False,dataengineering,t5_36en4,49126,public,self,Any case for using Snowflake Streams if DBT incremental models provide same functionality,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s0jyj1/any_case_for_using_snowflake_streams_if_dbt/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Inner-Loss-9002,,,[],,,,text,t2_8f48wixq,False,False,False,[],False,False,1641808086,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s0gdc7/please_critique_my_business_from_the_data/,{},s0gdc7,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s0gdc7/please_critique_my_business_from_the_data/,False,,,6,1641808096,1,"My customers are online retailers and the business model is a subscription service where my customers pay a monthly fee for me to maintain the data analytics (Power BI, daily/weekly PDFs, potential ML).

I currently see it working in AWS, where I hook up to their ERP system and bring it into my own database (Redshift) on a schedule. From there I can automate analytics PDFs in Lambda and connect to Power BI to maintain their dashboards.

Is handling this many different customers (20 paying 3k per month) viable or is this completely impractical?",True,False,False,dataengineering,t5_36en4,49014,public,self,Please critique my business from the data engineering perspective,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s0gdc7/please_critique_my_business_from_the_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,DolphinScheduler1,,,[],,,,text,t2_facmhtfm,False,False,False,[],False,False,1641807457,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s0g7qi/congratulations_apache_dolphinscheduler_has_been/,{},s0g7qi,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s0g7qi/congratulations_apache_dolphinscheduler_has_been/,False,,,6,1641807467,1,"&amp;#x200B;

https://preview.redd.it/u212wzauyta81.png?width=700&amp;format=png&amp;auto=webp&amp;s=502582cd1fc4ef2746bd46c14a7e55ada9d89431

Recently, TWOS officially announced the approval of 6 full members and 3 candidate members, Apache DolphinScheduler, a cloud-native distributed big data scheduler, was listed by TWOS.

Apache DolphinScheduler is a new-generation workflow scheduling platform that is distributed and easy to expand. It is committed to “solving the intricate dependencies among big data tasks and visualizing the entire data processing”. Its powerful visual DAG interface greatly improves the user experience and can configure workflow without complex code.

Since it was officially open-sourced in April 2019, Apache DolphinScheduler (formerly known as EasyScheduler) has undergone several architectural evolutions. So far, the relevant open-source codes have accumulated 7100+ Stars, with 280+ experienced code contributors, 110+ non-code contributors participating in the project, which includes PMCs and Committers of other Apache top-level projects. The Apache DolphinScheduler open source community continues to grow, and the WeChat user group has reached 6000+ people, and 600+ companies and institutions have adopted Apache DolphinScheduler in their production environment.

# TWOS

At the “2021 OSCAR Open Source Industry Conference”, China Academy of Telecommunication Research of MIIT (CAICT) officially established TWOS. TWOS is composed of open-source projects and open-source communities, which aims to guide the establishment of a healthy, credible, sustainable open source community, and build a communication platform providing a complete set of open source risk monitoring and ecological monitoring services.

To help enterprises reduce the risk of using open source software and promote the establishment of a credible open source ecosystem, CAICT has created a credible open-source standard system, which carries authoritative evaluation on enterprise open source governance capabilities, open-source project compliance, open-source community maturity, open-source tool detection capabilities, Open- source risk management capabilities of commercial products.

After being screened by TWOS evaluation criteria, Apache DolphinScheduler was approved to be a candidate member, which shows its recognition of Apache DolphinScheduler’s way of open-source operation, maturity, and contribution, and encourages the community to keep active.

On September 17, 2021, the first batch of members joined TWOS, including 25 full members such as openEuler, openGauss, MindSpore, openLookeng, etc., and 27 candidate members like Apache RocketMQ, Dcloud, Fluid, FastReID, etc., with a total of 52 members:

&amp;#x200B;

https://preview.redd.it/bogv6ayvyta81.png?width=700&amp;format=png&amp;auto=webp&amp;s=d72c73ccd2f6f4e5c6789e4deaeb27368db4572e

Only two communities were selected for the second batch of candidate members — Apache DolphinScheduler and PolarDB, an open-source cloud-native ecological distributed database contributed by Alibaba Cloud.

The Apache DolphinScheduler community is very honored to be selected as a candidate member of TWOS, which is an affirmation and incentive for the entire industry to build the community a better place. The community will make persistent efforts and strive to become a full member as soon as possible., and provide more value for China’s open-source ecological construction together, with all the TWOS members!",True,False,False,dataengineering,t5_36en4,49008,public,https://a.thumbs.redditmedia.com/IOdutSilA8NwqhCyZZJ60QF0od-14ifTbu-hZNf4Lc0.jpg,Congratulations! Apache DolphinScheduler Has Been Approved As A TWOS Candidate Member,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s0g7qi/congratulations_apache_dolphinscheduler_has_been/,all_ads,6,,,,,,59.0,140.0,,"{'bogv6ayvyta81': {'e': 'Image', 'id': 'bogv6ayvyta81', 'm': 'image/png', 'p': [{'u': 'https://preview.redd.it/bogv6ayvyta81.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=62aaead647751eeb7fc336145b5dfd9ba87d7a8e', 'x': 108, 'y': 37}, {'u': 'https://preview.redd.it/bogv6ayvyta81.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6be148df2f57f00ac0bb1023700a1cffeed23771', 'x': 216, 'y': 75}, {'u': 'https://preview.redd.it/bogv6ayvyta81.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dac3942cf8fb94c1d0dd8a4e7fff187c53b78fad', 'x': 320, 'y': 112}, {'u': 'https://preview.redd.it/bogv6ayvyta81.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b0f041adb887b6494b191c98ff526c903d683c8a', 'x': 640, 'y': 224}], 's': {'u': 'https://preview.redd.it/bogv6ayvyta81.png?width=700&amp;format=png&amp;auto=webp&amp;s=d72c73ccd2f6f4e5c6789e4deaeb27368db4572e', 'x': 700, 'y': 245}, 'status': 'valid'}, 'u212wzauyta81': {'e': 'Image', 'id': 'u212wzauyta81', 'm': 'image/png', 'p': [{'u': 'https://preview.redd.it/u212wzauyta81.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0f7a744ecf22b4ff1e4d66a34b92ba2e036f7846', 'x': 108, 'y': 45}, {'u': 'https://preview.redd.it/u212wzauyta81.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=352513209db7c992cc448e2494e23502543073da', 'x': 216, 'y': 91}, {'u': 'https://preview.redd.it/u212wzauyta81.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=461208eda42b50751d87a3f3a56943b73923f022', 'x': 320, 'y': 135}, {'u': 'https://preview.redd.it/u212wzauyta81.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5d1a7af2af236ee83a6f27b7994fff35c3a10b01', 'x': 640, 'y': 271}], 's': {'u': 'https://preview.redd.it/u212wzauyta81.png?width=700&amp;format=png&amp;auto=webp&amp;s=502582cd1fc4ef2746bd46c14a7e55ada9d89431', 'x': 700, 'y': 297}, 'status': 'valid'}}",,,,,,,,,
[],False,data_extractor,,,[],,,,text,t2_dye6wzsj,False,False,False,[],False,False,1641801300,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s0eoiu/convert_lat_and_long_to_city_name_in_bigquery/,{},s0eoiu,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s0eoiu/convert_lat_and_long_to_city_name_in_bigquery/,False,,,6,1641801311,1,"Hi guys the data that I am working on is in a shitty state, and i have to transform it more consumable form. Is there a way to convert lat and long to city directly on BQ or I should write cron job in python to transform the data?",True,False,False,dataengineering,t5_36en4,48991,public,self,Convert lat and long to city name in BigQuery,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s0eoiu/convert_lat_and_long_to_city_name_in_bigquery/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,vananth22,,,[],,,,text,t2_10v76s,False,False,False,[],False,False,1641788004,dataengineeringweekly.com,https://www.reddit.com/r/dataengineering/comments/s0ar6x/the_69th_edition_of_data_engineering_weekly/,{},s0ar6x,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s0ar6x/the_69th_edition_of_data_engineering_weekly/,False,link,"{'enabled': False, 'images': [{'id': '8A8VyaXs99yqpYNeNZIP9qlleyoekmvrvIZ3bPneWXM', 'resolutions': [{'height': 108, 'url': 'https://external-preview.redd.it/82HmuY_UcAyfAROS_zihOjoIlzhoyVesqdRJjR488sY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=447929128491931a8faec925029684395e99b9f6', 'width': 108}, {'height': 216, 'url': 'https://external-preview.redd.it/82HmuY_UcAyfAROS_zihOjoIlzhoyVesqdRJjR488sY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=884cc4a536329bf71d3ad9cf47b92053d0e6a0ee', 'width': 216}, {'height': 320, 'url': 'https://external-preview.redd.it/82HmuY_UcAyfAROS_zihOjoIlzhoyVesqdRJjR488sY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6ce0a66a002bce42ab1b184b6144f981cc494fde', 'width': 320}, {'height': 640, 'url': 'https://external-preview.redd.it/82HmuY_UcAyfAROS_zihOjoIlzhoyVesqdRJjR488sY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c0cdaa870788e81e157046823a1d040a1d86c667', 'width': 640}, {'height': 960, 'url': 'https://external-preview.redd.it/82HmuY_UcAyfAROS_zihOjoIlzhoyVesqdRJjR488sY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f2b7d702c8bd68b28989dd57afda65cb1bee5ae4', 'width': 960}, {'height': 1080, 'url': 'https://external-preview.redd.it/82HmuY_UcAyfAROS_zihOjoIlzhoyVesqdRJjR488sY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e13b0342989fdda6d4124bf9267ef05819e7ae54', 'width': 1080}], 'source': {'height': 1080, 'url': 'https://external-preview.redd.it/82HmuY_UcAyfAROS_zihOjoIlzhoyVesqdRJjR488sY.jpg?auto=webp&amp;s=0c7b2d3d3d676224a9d1efb80c9cbf8b1d563fdf', 'width': 1080}, 'variants': {}}]}",6,1641788015,1,,True,False,False,dataengineering,t5_36en4,48967,public,https://a.thumbs.redditmedia.com/G1gvW97Jcktz4791D4WMdBMUVxyz9b0H7BTrfz9Avk8.jpg,"The 69th edition of Data Engineering Weekly highlights one year of dbt, LakeHouse Architecture Reference, Airflow to Apache Dolphin comparison, and more!!",0,[],1.0,https://www.dataengineeringweekly.com/p/data-engineering-weekly-69,all_ads,6,,,,,,140.0,140.0,https://www.dataengineeringweekly.com/p/data-engineering-weekly-69,,,,,,,,,,
[],False,kaumaron,,,[],,,,text,t2_ossxw,False,False,False,[],False,False,1641777630,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s077i4/any_reasons_to_avoid_m1_macs_for_data_engineering/,{},s077i4,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s077i4/any_reasons_to_avoid_m1_macs_for_data_engineering/,False,,,6,1641777641,1,Took a new job and I've been asked to choose a new machine. I was disappointed to see that Apple doesn't sell Intel chips at all anymore because I would've clone my 2020 MBP. I know when M1 came out there were a number of compatibility issues with data science related packages. Have those been resolved? Do they not really impact your day to day?,True,False,False,dataengineering,t5_36en4,48961,public,self,Any reasons to avoid M1 Macs for data engineering?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s077i4/any_reasons_to_avoid_m1_macs_for_data_engineering/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Hefty_Investigator97,,,[],,,,text,t2_bpsuhp5h,False,False,False,[],False,False,1641771926,youtube.com,https://www.reddit.com/r/dataengineering/comments/s055nb/aws_community_builder_program_free_500_usd/,{},s055nb,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s055nb/aws_community_builder_program_free_500_usd/,False,rich:video,"{'enabled': False, 'images': [{'id': 'ZpWFQtUoVzA0cYyISPnmnBU8uZKjwUWcw3ZrXLpNhnc', 'resolutions': [{'height': 81, 'url': 'https://external-preview.redd.it/w67gSa1GGLSCk-oLLm9mGzLT22G24SJCjmLAv3HlJFQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4a2a47294075ec88185211af5d9edfd3d7df2a66', 'width': 108}, {'height': 162, 'url': 'https://external-preview.redd.it/w67gSa1GGLSCk-oLLm9mGzLT22G24SJCjmLAv3HlJFQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=325abe5d0e584578ba4e99d9fc7bba8a309422db', 'width': 216}, {'height': 240, 'url': 'https://external-preview.redd.it/w67gSa1GGLSCk-oLLm9mGzLT22G24SJCjmLAv3HlJFQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3a11d4638a555131a403f5c1d976727f38256327', 'width': 320}], 'source': {'height': 360, 'url': 'https://external-preview.redd.it/w67gSa1GGLSCk-oLLm9mGzLT22G24SJCjmLAv3HlJFQ.jpg?auto=webp&amp;s=58b9fe6286befddbae1a4eabdb0ac31e6ed739be', 'width': 480}, 'variants': {}}]}",6,1641771936,1,,True,False,False,dataengineering,t5_36en4,48957,public,https://b.thumbs.redditmedia.com/dt2onlH9nMbHtfAirvcQjXJeqXvgmPVQcxoYw3uk9cc.jpg,AWS Community Builder Program | Free 500 USD credits Benefits | How to...,0,[],1.0,https://youtube.com/watch?v=jEusettgk_g&amp;feature=share,all_ads,6,"{'oembed': {'author_name': 'Cloud &amp; Data Science', 'author_url': 'https://www.youtube.com/c/CloudDataScience', 'height': 200, 'html': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/jEusettgk_g?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/jEusettgk_g/hqdefault.jpg', 'thumbnail_width': 480, 'title': 'AWS Community Builder Program | Free 500 USD credits + Benefits | How to Apply', 'type': 'video', 'version': '1.0', 'width': 267}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/jEusettgk_g?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'scrolling': False, 'width': 267}",,"{'oembed': {'author_name': 'Cloud &amp; Data Science', 'author_url': 'https://www.youtube.com/c/CloudDataScience', 'height': 200, 'html': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/jEusettgk_g?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/jEusettgk_g/hqdefault.jpg', 'thumbnail_width': 480, 'title': 'AWS Community Builder Program | Free 500 USD credits + Benefits | How to Apply', 'type': 'video', 'version': '1.0', 'width': 267}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/jEusettgk_g?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/s055nb', 'scrolling': False, 'width': 267}",105.0,140.0,https://youtube.com/watch?v=jEusettgk_g&amp;feature=share,,,,,,,,,,
[],False,theporterhaus,#46d160,mod,[],fd5b074e-239e-11e8-a28b-0e0f8d9eda5a,mod | Sr. Data Engineer,light,text,t2_2tv9i42n,False,False,False,[],False,False,1641771819,i.redd.it,https://www.reddit.com/r/dataengineering/comments/s054b4/2022_mood/,{},s054b4,False,True,False,False,True,True,False,False,#ff66ac,[],dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1,Meme,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s054b4/2022_mood/,False,image,"{'enabled': True, 'images': [{'id': '2maTZOplZFHhcnaH-5jFUsnYFAKoxCYZLHcL_o9XV80', 'resolutions': [{'height': 79, 'url': 'https://preview.redd.it/s7olw2f01ra81.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=58f42cff31bd2cee30b67ad6f0ba3bee3f4c1856', 'width': 108}, {'height': 159, 'url': 'https://preview.redd.it/s7olw2f01ra81.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c59ccb53c83118d32b130a2c3f80fdc7289bfa04', 'width': 216}, {'height': 236, 'url': 'https://preview.redd.it/s7olw2f01ra81.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=43a730bbb4a2598104fda030d27c9ff05e8b98b6', 'width': 320}, {'height': 473, 'url': 'https://preview.redd.it/s7olw2f01ra81.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=232b06357b64ed54ae177097e2b0f1340a989611', 'width': 640}], 'source': {'height': 499, 'url': 'https://preview.redd.it/s7olw2f01ra81.jpg?auto=webp&amp;s=d7cdafe81f8f6eed42fbc3facbbd2810d875d4c5', 'width': 675}, 'variants': {}}]}",6,1641771829,1,,True,False,False,dataengineering,t5_36en4,48957,public,https://b.thumbs.redditmedia.com/yhrqT38aF9EtjrHUWtR8mhkxKo6BntNPjhvUmON9A6Q.jpg,2022 Mood,0,[],1.0,https://i.redd.it/s7olw2f01ra81.jpg,all_ads,6,,,,,,103.0,140.0,https://i.redd.it/s7olw2f01ra81.jpg,,,,,,,,,,
[],False,Tender_Figs,,,[],,,,text,t2_3uoce3bn,False,False,False,[],False,False,1641769750,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s04c4i/for_those_that_came_from_dabi_what_were_your/,{},s04c4i,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s04c4i/for_those_that_came_from_dabi_what_were_your/,False,,,6,1641769761,1,"Personally, I am finding myself less interested in the business and more interested in the technology and data. Is that a similar motivation or perspective for those who switched out of DA/BI?",True,False,False,dataengineering,t5_36en4,48954,public,self,"For those that came from DA/BI, what were your motivations?",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s04c4i/for_those_that_came_from_dabi_what_were_your/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,dataplumber_guy,,,[],,,,text,t2_dz3j9oc3,False,False,False,[],False,False,1641767830,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s03meg/file_error_alerts/,{},s03meg,False,False,False,False,False,False,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s03meg/file_error_alerts/,False,,,6,1641767840,1,[removed],True,False,False,dataengineering,t5_36en4,48952,public,self,File Error Alerts,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s03meg/file_error_alerts/,all_ads,6,,,automod_filtered,,,,,,,,,,,,,,,
[],False,Long_Weight_1562,,,[],,,,text,t2_au794432,False,False,False,[],False,False,1641760613,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/s00vsy/how_would_you_select_the_architecture_and_design/,{},s00vsy,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/s00vsy/how_would_you_select_the_architecture_and_design/,False,,,6,1641760623,1,"This is not some assignment question :D I'm dealing with this at work and I want to get some opinions on how others would handle it.

1. Ingest unstructured data from a single 3rd party data source (let's say 1 million rows/documents x 10 different tables/collections).

2. Some preliminary transformation like mapping custom field names to consistent internal ones, date time formatting and so on and then putting it into a db/data store. 

3. couple of potentially long running jobs (depends on querying external APIs which may have their own throttling limits) to be run on every single document from above but only on 2-3 of the tables/collections (so about 2-3 million rows).This is run only once every week or so. new fields are added from the result of the api queries.

4. A job where some pretrained DL models are ran against the rows for 4-5 tables (say about 5 new fields are added per row after the inferences). At the end, some aggregation happens and a new table is created where every row/document is a collection of sub-documents from 4-5 other collections (this point is important). 

5. Another job where the newly created table is processed and a bunch of new fields are created for every single subdoc (upto 50 per subdocument). Some of this ""processing"" also involves running ML/DL models on the docs. There are also training steps in the middle where some models need to train if there are enough data (or enough time has passed. let's just say monthly training). At the end of this job the contents of this new table is again aggregated based on one of the fields (let's say doc_owner) and put into another table. This is the actual data that is useful for the customer. This data maybe 200 fields wide.

6. job where the finally created data is exported to the customer's preferred warehouse/db/store (eg: firebase, snowflake, bigtable).

My question is how would approach such a problem? What tools would you use for each step?",True,False,False,dataengineering,t5_36en4,48942,public,self,How would you select the architecture and design a pipeline for this type of problem(included below)?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/s00vsy/how_would_you_select_the_architecture_and_design/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Sihal,,,[],,,,text,t2_x53nd,False,False,False,[],False,False,1641750461,reddit.com,https://www.reddit.com/r/dataengineering/comments/rzx1x6/jobs_deployment_on_apache_flink/,{},rzx1x6,False,True,False,False,False,True,False,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rzx1x6/jobs_deployment_on_apache_flink/,False,,,6,1641750471,1,,True,False,False,dataengineering,t5_36en4,48929,public,default,Jobs Deployment on Apache Flink,0,[],1.0,https://www.reddit.com/r/apacheflink/comments/rzwlmb/jobs_deployment/,all_ads,6,,,,,,,,https://www.reddit.com/r/apacheflink/comments/rzwlmb/jobs_deployment/,,,,,,,,,,
[],False,zdsvoboda,,,[],,,,text,t2_9jbrrex7,False,False,False,[],False,False,1641746881,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rzvqj5/new_opensource_elt_tool/,{},rzvqj5,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rzvqj5/new_opensource_elt_tool/,False,self,"{'enabled': False, 'images': [{'id': 'sv74BzKjgUQ5Q_iFZ8mJ4QTqfiq4qbwzEBTRGncr360', 'resolutions': [{'height': 60, 'url': 'https://external-preview.redd.it/coPRp84pUpf9oApnHGZrNqJYX9xC3qa8VwToHohIr50.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5937a051e9dc8c38483177c9dfa52106d47b61fe', 'width': 108}, {'height': 121, 'url': 'https://external-preview.redd.it/coPRp84pUpf9oApnHGZrNqJYX9xC3qa8VwToHohIr50.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b42e16622ea0ec3de9406618c570aa092471ceb6', 'width': 216}, {'height': 180, 'url': 'https://external-preview.redd.it/coPRp84pUpf9oApnHGZrNqJYX9xC3qa8VwToHohIr50.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4d4e1b3f74d5fe0e97670bae5761956c38135d7c', 'width': 320}, {'height': 360, 'url': 'https://external-preview.redd.it/coPRp84pUpf9oApnHGZrNqJYX9xC3qa8VwToHohIr50.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=025d9110a225ecce50fa7cc09bc6dec383b6ba19', 'width': 640}, {'height': 540, 'url': 'https://external-preview.redd.it/coPRp84pUpf9oApnHGZrNqJYX9xC3qa8VwToHohIr50.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=33f5d514c545b2e466d1bdc4fff44f953ae8ef26', 'width': 960}], 'source': {'height': 540, 'url': 'https://external-preview.redd.it/coPRp84pUpf9oApnHGZrNqJYX9xC3qa8VwToHohIr50.jpg?auto=webp&amp;s=88a703ad97848cde2bddf99e73fdec84353bb768', 'width': 960}, 'variants': {}}]}",6,1641746892,1,"I was looking for some declarative ELT tool for creating my analytics solutions, and DBT was the closest I've found. I liked its concept, but I came across quite a few limitations when I wanted to use it. I couldn't specify and create basic things like data types, indexes, primary/foreign keys, etc. In the end, I decided to implement my own - more straightforward and more flexible. I've published the result - [dbd on GitHub](https://github.com/zsvoboda/dbd). Perhaps, you can find it helpful. Your feedback is greatly appreciated!",True,False,False,dataengineering,t5_36en4,48919,public,self,New opensource ELT tool,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rzvqj5/new_opensource_elt_tool/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,--bubba--,,,[],,,,text,t2_7qgrqw3x,False,False,False,[],False,False,1641744741,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rzuz4i/missed_connection_to_the_guy_from_confluent_who/,{},rzuz4i,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rzuz4i/missed_connection_to_the_guy_from_confluent_who/,False,,,6,1641744751,1,"Mods - I'm sorry. I know this is a stupid post. I recently posted a thread about where Kafka fits into the modern data stack and someone offered to speak with me. I accidentally ignored the chat and I am upset because I was excited and this is a rare opportunity for me.

&amp;#x200B;

I will take down this garbage post as soon as they see this and message me. I know this is silly but please let me keep this up. Thanks.",True,False,False,dataengineering,t5_36en4,48915,public,self,"[Missed connection] To the guy from Confluent who messaged me offering to chat about my post ""Where does Apache Kafka fit into this map of the data stack? (Further question in comments)"" please message again!",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rzuz4i/missed_connection_to_the_guy_from_confluent_who/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,agewitbog,,,[],,,,text,t2_hr5bvfe5,False,False,False,[],False,False,1641727028,opguides.info,https://www.reddit.com/r/dataengineering/comments/rzpjwm/opinionated_guides_engineering/,{},rzpjwm,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rzpjwm/opinionated_guides_engineering/,False,,,6,1641727039,1,,True,False,False,dataengineering,t5_36en4,48894,public,default,Opinionated Guides: Engineering,0,[],1.0,https://opguides.info/engineering/engineering,all_ads,6,,,,,,,,https://opguides.info/engineering/engineering,,,,,,,,,,
[],False,codmanend,,,[],,,,text,t2_hr5dhmtv,False,False,False,[],False,False,1641726726,firebolt.io,https://www.reddit.com/r/dataengineering/comments/rzph5n/cloud_data_warehouse_guide/,{},rzph5n,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rzph5n/cloud_data_warehouse_guide/,False,link,"{'enabled': False, 'images': [{'id': 'Ort7WyJWv40V1PeYMK4eaWuB0jz2gHypND_uRxMMslU', 'resolutions': [{'height': 54, 'url': 'https://external-preview.redd.it/-TvSBwbtvXpbVBjD0nwz5u5BiZCbnZO7zm2HHt-F7sY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5bdfb2efe82d6e005e39f99c0610618041a75875', 'width': 108}, {'height': 108, 'url': 'https://external-preview.redd.it/-TvSBwbtvXpbVBjD0nwz5u5BiZCbnZO7zm2HHt-F7sY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8e0ea4dab8180c254d34377175b063d249a6a092', 'width': 216}, {'height': 160, 'url': 'https://external-preview.redd.it/-TvSBwbtvXpbVBjD0nwz5u5BiZCbnZO7zm2HHt-F7sY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fcd7260ff71627ea7ef1a474f6e510d1bcae7724', 'width': 320}, {'height': 320, 'url': 'https://external-preview.redd.it/-TvSBwbtvXpbVBjD0nwz5u5BiZCbnZO7zm2HHt-F7sY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ac77f4fe1b153f7bc355cc9c18729d21dc1f2aef', 'width': 640}, {'height': 480, 'url': 'https://external-preview.redd.it/-TvSBwbtvXpbVBjD0nwz5u5BiZCbnZO7zm2HHt-F7sY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5a3205d96c43f6ec0631a525f43f1c6444d4d440', 'width': 960}], 'source': {'height': 500, 'url': 'https://external-preview.redd.it/-TvSBwbtvXpbVBjD0nwz5u5BiZCbnZO7zm2HHt-F7sY.jpg?auto=webp&amp;s=25b161f53aca1d8fe126b9bad61853359410cc96', 'width': 1000}, 'variants': {}}]}",6,1641726736,1,,True,False,False,dataengineering,t5_36en4,48893,public,https://b.thumbs.redditmedia.com/OElpEqIC9CzWHVxQB3tobyGj7kN8YKR1Rg9HTgxH0MY.jpg,Cloud Data Warehouse Guide,0,[],1.0,https://www.firebolt.io/blog/cloud-data-warehouse,all_ads,6,,,,,,70.0,140.0,https://www.firebolt.io/blog/cloud-data-warehouse,,,,,,,,,,
[],False,Tharparkar1565,,,[],,,,text,t2_ea14eezz,False,False,False,[],False,False,1641709059,technologiesinindustry4.com,https://www.reddit.com/r/dataengineering/comments/rzl6os/javascript_data_types/,{},rzl6os,False,False,False,False,False,False,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rzl6os/javascript_data_types/,False,link,"{'enabled': False, 'images': [{'id': 'tliU_71DiYYcWsPAs3M5y5ZG-7N6tJrRpGdTKLtu4WU', 'resolutions': [{'height': 71, 'url': 'https://external-preview.redd.it/Jq_cInj0fD5Q821-NaHn2WeINJy-e2hjqrAbUjtYex8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f56ff5761ba7f0dc6242b12a7cb132749b142e20', 'width': 108}, {'height': 143, 'url': 'https://external-preview.redd.it/Jq_cInj0fD5Q821-NaHn2WeINJy-e2hjqrAbUjtYex8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ad37c34650adf6254dae9d9ab68cfa2a469173aa', 'width': 216}, {'height': 213, 'url': 'https://external-preview.redd.it/Jq_cInj0fD5Q821-NaHn2WeINJy-e2hjqrAbUjtYex8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=89b60c4c29dca5911207f8807ebd5cb5dd7d00bd', 'width': 320}, {'height': 426, 'url': 'https://external-preview.redd.it/Jq_cInj0fD5Q821-NaHn2WeINJy-e2hjqrAbUjtYex8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c077ea5e48234615942daa6c60ab7a6e89f1bacb', 'width': 640}], 'source': {'height': 426, 'url': 'https://external-preview.redd.it/Jq_cInj0fD5Q821-NaHn2WeINJy-e2hjqrAbUjtYex8.jpg?auto=webp&amp;s=011286e7c2ee140e6f836663e65cc1eb96dcdf37', 'width': 640}, 'variants': {}}]}",6,1641709069,1,,True,False,False,dataengineering,t5_36en4,48885,public,https://b.thumbs.redditmedia.com/GljPSt0dWs3lK6V13EZHoDwwgGy020xrwmMwO24-KSQ.jpg,JavaScript Data Types,0,[],1.0,https://www.technologiesinindustry4.com/2022/01/javascript-data-types.html,all_ads,6,,,reddit,,,93.0,140.0,https://www.technologiesinindustry4.com/2022/01/javascript-data-types.html,,,,,,,,,,
[],False,cryptobiosynthesis,,,[],,,,text,t2_eeigd8ze,False,False,False,[],False,False,1641699898,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rzij92/how_do_you_persist_historic_data_when_your/,{},rzij92,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rzij92/how_do_you_persist_historic_data_when_your/,False,,,6,1641699909,1,"Whether it's a new startup or an established company with legacy systems, many companies still don't do event sourcing, or if they do it's often quite limited.

There are numerous approaches to addressing this problem when event logs are not available:

* Change data capture (CDC). This is when each database operation gets written out to an event log directly from a database.

* Snapshotting. Periodically taking the value of particular rows/tables/entire databases

* Mining application logs. I mean.... if you have to?

* Whining about it to engineering until they finally stop overwriting important data

How do you approach this problem as a data engineer when you don't have event streams?",True,False,False,dataengineering,t5_36en4,48877,public,self,How do you persist historic data when your company doesn't store events?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rzij92/how_do_you_persist_historic_data_when_your/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,mwlon,,,[],,,,text,t2_8cr1i,False,False,False,[],False,False,1641699734,i.redd.it,https://www.reddit.com/r/dataengineering/comments/rzihf7/based_on_a_real_nearcatastrophe/,{},rzihf7,False,True,False,False,True,True,False,False,#ff66ac,[],dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1,Meme,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rzihf7/based_on_a_real_nearcatastrophe/,False,image,"{'enabled': True, 'images': [{'id': 'OoBRRJfXPgXxvO5FLHd_bZt_1o-l4M6smo4DWk4TQMw', 'resolutions': [{'height': 71, 'url': 'https://preview.redd.it/s09ctokh2la81.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=844642856708d9a19ec52b078b0f1c7b89c30ed5', 'width': 108}, {'height': 143, 'url': 'https://preview.redd.it/s09ctokh2la81.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=64ecd719b8b8bd808474e0e73184c9f419cdc7b2', 'width': 216}, {'height': 212, 'url': 'https://preview.redd.it/s09ctokh2la81.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5e2a2b1a78efc46e48aac0d2d973a250e362b8c2', 'width': 320}, {'height': 425, 'url': 'https://preview.redd.it/s09ctokh2la81.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=12bdea2dbf946784804f6843774942e08f280e2b', 'width': 640}], 'source': {'height': 516, 'url': 'https://preview.redd.it/s09ctokh2la81.png?auto=webp&amp;s=75c42ebf06e917cb93377f41d7bd7757814dc1d1', 'width': 776}, 'variants': {}}]}",6,1641699745,1,,True,False,False,dataengineering,t5_36en4,48877,public,https://b.thumbs.redditmedia.com/pnD6FD22B_zZFGYGrG_9VC8c2tT3VxIxr46X5kx8VhE.jpg,Based on a real near-catastrophe,0,[],1.0,https://i.redd.it/s09ctokh2la81.png,all_ads,6,,,,,,93.0,140.0,https://i.redd.it/s09ctokh2la81.png,,,,,,,,,,
[],True,--bubba--,,,[],,,,text,t2_7qgrqw3x,False,False,False,[],False,False,1641690162,assets.website-files.com,https://www.reddit.com/r/dataengineering/comments/rzfcnu/where_does_apache_kafka_fit_into_this_map_of_the/,{},rzfcnu,False,True,False,False,False,True,False,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rzfcnu/where_does_apache_kafka_fit_into_this_map_of_the/,False,image,"{'enabled': True, 'images': [{'id': 'h1fldyYbgq0ESucD8nDkh4oQZBAsjQEFqXHTl7oQmQk', 'resolutions': [{'height': 47, 'url': 'https://external-preview.redd.it/QlUTTsXJTPY8MVTC_5W2QspbPoiUGC8d9EPjSKNyl-Q.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=858ef0dd70afb6c1a2357586dd59537b3bae2423', 'width': 108}, {'height': 95, 'url': 'https://external-preview.redd.it/QlUTTsXJTPY8MVTC_5W2QspbPoiUGC8d9EPjSKNyl-Q.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=35b255294f04658e182547ac303005e0cfb025ea', 'width': 216}, {'height': 141, 'url': 'https://external-preview.redd.it/QlUTTsXJTPY8MVTC_5W2QspbPoiUGC8d9EPjSKNyl-Q.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=31c49fdc4182f9a0a7bd367979c6f0e668f685e2', 'width': 320}, {'height': 282, 'url': 'https://external-preview.redd.it/QlUTTsXJTPY8MVTC_5W2QspbPoiUGC8d9EPjSKNyl-Q.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d46a4ba403921ab2a9853770b330c735ad7e3fc2', 'width': 640}], 'source': {'height': 371, 'url': 'https://external-preview.redd.it/QlUTTsXJTPY8MVTC_5W2QspbPoiUGC8d9EPjSKNyl-Q.png?auto=webp&amp;s=3c82038410430c256381fb84085ea9f649ca5e22', 'width': 841}, 'variants': {}}]}",6,1641690173,1,,True,False,False,dataengineering,t5_36en4,48870,public,https://b.thumbs.redditmedia.com/_Se0uq1zbjSBc51qa4u9Dgzle6ABTKkUDQpgg-yeClI.jpg,Where does Apache Kafka fit into this map of the data stack? (Further question in comments),0,[],1.0,https://assets.website-files.com/608c6fda09a6be4a926844f1/60e8a913d241ba2d2af0df54_BWaYXCCSvsG0d7euhqTitFDR7ak-bDCSTHuFCLnf3jjqQGFV2OOIHGweq8VPh9EGefhMobn5DeYIEViWkTSWm9WmtfYQCrJooqv6UMJmRWnZtIZKEgVkVIQ2Udp6rrF-3NErG-UO.png,all_ads,6,,,,,,61.0,140.0,https://assets.website-files.com/608c6fda09a6be4a926844f1/60e8a913d241ba2d2af0df54_BWaYXCCSvsG0d7euhqTitFDR7ak-bDCSTHuFCLnf3jjqQGFV2OOIHGweq8VPh9EGefhMobn5DeYIEViWkTSWm9WmtfYQCrJooqv6UMJmRWnZtIZKEgVkVIQ2Udp6rrF-3NErG-UO.png,,,,,,,,,,
[],False,Bianca_di_Angelo,,,[],,,,text,t2_8cdm03wl,False,False,False,[],False,False,1641684108,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rzd43h/i_am_at_a_crossroad_and_cannot_postpone_the/,{},rzd43h,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rzd43h/i_am_at_a_crossroad_and_cannot_postpone_the/,False,,,6,1641684118,1,"A little background, I graduated almost 3 years ago and joined an early stage startup as a Software Developer. I learned a lot there about Software Development but just after a few months as my area of interest was more towards Data Engineering, I started taking those responsibilities and projects to became the ""Data Guy"" in the small tech team. I learned Apache Airflow, Postgresql and AWS services and honed them while I was there.

Due to COVID and serious budget cuts, I shifted to a more settled mid-sized company which had a Data Engineering team.  But in the 4 months that I have been here, my productivity is lower than it has ever been. The work is just plain boring, everything working extensively on SQL. The fun of engineering is completely gone and it feels like I have not learned anything new since I have joined. The requirements that usually come towards the team are either making some structure changes in the database table or in some report to better align with business.

I don't know what should I do, should I give myself more time to adjust to this work style or look for other opportunities again. I think with so early in my career, i should focus more on a job which would help me grow and expand my area of expertise.  
Now since I don't have any hands on experience in big data technologies, I don't qualify for openings in companies with promising DE teams and I fear if don't go to one which is using them then I will miss out again and the cycle will repeat the next time i wish to switch.

The make matter worse, I don't know if this is what data engineering is like and even if I learn Big data technologies and get a decent job, then i won't get bored as well there. 

Really need some guidance so i can take the best path forward. Any help is greatly appreciated.",True,False,False,dataengineering,t5_36en4,48868,public,self,I am at a crossroad and cannot postpone the decision any longer,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rzd43h/i_am_at_a_crossroad_and_cannot_postpone_the/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,elideli,,,[],,,,text,t2_xa00y,False,False,False,[],False,False,1641682068,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rzcbfw/beginner_recommend_me_a_pythonsql_project/,{},rzcbfw,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rzcbfw/beginner_recommend_me_a_pythonsql_project/,False,,,6,1641682078,1,Currently doing a 16-week part-time Python and SQL Boot Camp. We have the choice to work on one portfolio project and I would like to do something data engineering related but don’t know where to start. Any recommendations?,True,False,False,dataengineering,t5_36en4,48867,public,self,Beginner : recommend me a Python+SQL project,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rzcbfw/beginner_recommend_me_a_pythonsql_project/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,traveling_wilburys,,,[],,,,text,t2_56nxuw5r,False,False,False,[],False,False,1641679096,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rzb6fx/can_you_guys_give_an_example_of_how_you_optimized/,{},rzb6fx,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rzb6fx/can_you_guys_give_an_example_of_how_you_optimized/,False,,,6,1641679106,1,"I'm trying to understand ways in which I can improve spark workflow in my daily work. I work with datasets 100-300GB, mostly transforming existing columns based on business requirements. I'm trying to see situations in which I could improve the workflow, and am looking for examples from you guys when and how you had to do something similar.",True,False,False,dataengineering,t5_36en4,48864,public,self,Can you guys give an example of how you optimized spark workloads for performance,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rzb6fx/can_you_guys_give_an_example_of_how_you_optimized/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,falkerr,,,[],,,,text,t2_4gyscnsm,False,False,True,[],False,False,1641671109,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rz85bf/want_to_learn_aws_over_the_next_year_will_it_be/,{},rz85bf,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rz85bf/want_to_learn_aws_over_the_next_year_will_it_be/,False,,,6,1641671120,1,"I want to start learning AWS and doing data engineering on AWS. I am about to start doing a couple of tutorials that require starting the AWS free trial. However, I see some parts of the trial only last a couple of months (like Redshift). If that runs out, will I pay a lot to practice? 

I am busy and a slow learner so I was hoping to learn slowly over the next year and ease into it. The two months thing makes me question if I should even start this right now if I am not going to have a lot of time to do it over the next two months. Are there other longer trial cloud services I should try or is there a way to extend the trial? 

I’m not going to be heavily using the services, only when I want to practice things so would it not even be that expensive if I don’t have heavy usage? 

Anyone have any experience practicing AWS?",True,False,False,dataengineering,t5_36en4,48860,public,self,"Want to learn AWS over the next year, will it be expensive to practice?",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rz85bf/want_to_learn_aws_over_the_next_year_will_it_be/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Flat_Shower,,,[],,,,text,t2_5v6av4nm,False,False,False,[],False,False,1641670110,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rz7rdv/google_treats_sql_like_code_and_you_should_too/,{},rz7rdv,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rz7rdv/google_treats_sql_like_code_and_you_should_too/,False,,,6,1641670122,1,"I've been with Google for just over 2 years, and I'm trying to create as many teachable lessons as I can. I come from a diverse background of low-tech and healthcare, and hope that some of these principals that Google employs can be helpful for others. Enjoy!

[https://ilovedata.medium.com/why-google-treats-sql-like-code-and-you-should-too-53f97925037e](https://ilovedata.medium.com/why-google-treats-sql-like-code-and-you-should-too-53f97925037e)",True,False,False,dataengineering,t5_36en4,48860,public,self,"Google Treats SQL like code, and you should too!",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rz7rdv/google_treats_sql_like_code_and_you_should_too/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,randomusicjunkie,,,[],,,,text,t2_3tzpeuhd,False,False,False,[],False,False,1641669530,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rz7jev/naming_conventions_when_doing_the_transformations/,{},rz7jev,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rz7jev/naming_conventions_when_doing_the_transformations/,False,,,6,1641669541,1,"Once the data is loaded into the data warehouse, let's say we talk about an **ELT** use-case, how do you evolve the data and how do you name the different stages?

For example, 

\-----**RAW**: the raw data loaded into the DW/Lake from the sources with zero transformation

\-----**STAGING:** transformations that are not ready to be served to BI tools yet

\----- **?????**: Refined, aggregated tables for data marts / BI tools

(etc)",True,False,False,dataengineering,t5_36en4,48859,public,self,"Naming conventions when doing the transformations, cleaning, and wrangling on the data?",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rz7jev/naming_conventions_when_doing_the_transformations/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,nrskmn,,,[],,,,text,t2_bfd45sy5,False,False,False,[],False,False,1641666163,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rz690b/how_and_why_are_macs_preferred_for_data/,{},rz690b,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rz690b/how_and_why_are_macs_preferred_for_data/,False,,,6,1641666174,1,"PREFACE: this is not a Windows vs Mac debate. It's a question out of curiosity.

Context: 
I've been a DE for over 2 years and used MSSQL + AzureDF + Python over the Microsoft tech stack of SSMS, VS, GitHub, Powershell, and Jupyter (and a bit of Redshift). Company only provided Dell Windows laptops with decent specs.

Recently made a switch where the tech stack includes Python, Oracle, Snowflake, R, and NoSQL DBs over Docker, DataGrip, PyCharm, R-Studio, and GitHub. Company provides MacBook Pros which are amped out!

Question:
Why is it preferable to use Macs for Data Engineering (all this gibberish just to rewrite the post title?!!)

What, specifically to DE, is the benefit of using a Mac - a closed system which costs 2x more with support only from the manufacturer?

Especially in this day and age where almost all the data resides on remote servers or cloud and you login into those through RDPs or CLIs.

(I do get the overall Mac benefits and friggin love my MBP as a machine. Wayyy better than the Dells I had)",True,False,False,dataengineering,t5_36en4,48854,public,self,How and Why are Macs preferred for Data Engineering?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rz690b/how_and_why_are_macs_preferred_for_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,UniqueAway,,,[],,,,text,t2_1soxv3pz,False,False,False,[],False,False,1641662601,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rz4vr4/what_kind_of_a_data_engineering_job_is_this/,{},rz4vr4,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,1,0,False,all_ads,/r/dataengineering/comments/rz4vr4/what_kind_of_a_data_engineering_job_is_this/,False,self,"{'enabled': False, 'images': [{'id': 'B_T_k50tMc2MglGWvew2XA2Z9G0c09jqo0I_V2iJufE', 'resolutions': [{'height': 114, 'url': 'https://external-preview.redd.it/03zhhl-at1mzw8K93FGKxFk1l-XfhllSWkrgPG7MwyE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d0365d7c8de3eea3901549e18a9d04a783c4cc7f', 'width': 108}, {'height': 228, 'url': 'https://external-preview.redd.it/03zhhl-at1mzw8K93FGKxFk1l-XfhllSWkrgPG7MwyE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e0e1767fd4b5fc0de6f091e7ae3338ee695045a0', 'width': 216}, {'height': 338, 'url': 'https://external-preview.redd.it/03zhhl-at1mzw8K93FGKxFk1l-XfhllSWkrgPG7MwyE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=38cfa7adcd3c7f67cce05e5f85ce000fab04911b', 'width': 320}], 'source': {'height': 599, 'url': 'https://external-preview.redd.it/03zhhl-at1mzw8K93FGKxFk1l-XfhllSWkrgPG7MwyE.png?auto=webp&amp;s=d532570238696dd270eae3ac60f0a3130f37c5a5', 'width': 566}, 'variants': {}}]}",6,1641662611,1,"[https://i.imgur.com/US9kvRQ.png](https://i.imgur.com/US9kvRQ.png)

I feel like this is a combination of Data Science, Data Engineering and Machine Learning. I have applied to Software engineering position but they offered me this when they saw my resume. I am a math + cs grad with a short ML internship. 

Also will I be able to transition to Java Backend from this if I work for a year or two?",True,False,False,dataengineering,t5_36en4,48855,public,self,What kind of a data engineering job is this?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rz4vr4/what_kind_of_a_data_engineering_job_is_this/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,No_Engine1637,,,[],,,,text,t2_7hgjxjf5,False,False,False,[],False,False,1641662546,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rz4v15/what_are_the_advantages_of_using_nifi_and_kafka/,{},rz4v15,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rz4v15/what_are_the_advantages_of_using_nifi_and_kafka/,False,,,6,1641662556,1,"I was looking for architectures to do sentiment analysis in streaming with Spark and I came across this architecture:

https://preview.redd.it/ub2l8boszha81.png?width=1548&amp;format=png&amp;auto=webp&amp;s=ea3dcc106f84d8ac125f302fa9e1fb10b1a293d9

I was wondering, what are the advantages of using Nifi + Wifi with the Twitter API instead of directly connecting Spark to it? I assume it would be more fault tolerant like this, but I really just don't know and I'd appreciate some insights",True,False,False,dataengineering,t5_36en4,48855,public,https://b.thumbs.redditmedia.com/GQlF3xYVhpQZ8NBQura0hzXQ_vUrZIIYu3_y9OJ1iLY.jpg,What are the advantages of using Nifi and Kafka for data ingestion?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rz4v15/what_are_the_advantages_of_using_nifi_and_kafka/,all_ads,6,,,,,,47.0,140.0,,"{'ub2l8boszha81': {'e': 'Image', 'id': 'ub2l8boszha81', 'm': 'image/png', 'p': [{'u': 'https://preview.redd.it/ub2l8boszha81.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7e537a5c747959ea35c3d132b22c82fe725e5b89', 'x': 108, 'y': 36}, {'u': 'https://preview.redd.it/ub2l8boszha81.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3ef75a757ab2173799b22404453231eaa38e24a2', 'x': 216, 'y': 73}, {'u': 'https://preview.redd.it/ub2l8boszha81.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=70521f807c02981890b6170d9f51941a5bc6c6ed', 'x': 320, 'y': 109}, {'u': 'https://preview.redd.it/ub2l8boszha81.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1b9b5cd50c95ecf56a3cb53ed319876cc1c1331a', 'x': 640, 'y': 219}, {'u': 'https://preview.redd.it/ub2l8boszha81.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2d0007c165ba8fbf18a820cfbfe088f29940f4f3', 'x': 960, 'y': 328}, {'u': 'https://preview.redd.it/ub2l8boszha81.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e9689b7eacbf1ed8b09300ff3fff92d6e4ac70db', 'x': 1080, 'y': 369}], 's': {'u': 'https://preview.redd.it/ub2l8boszha81.png?width=1548&amp;format=png&amp;auto=webp&amp;s=ea3dcc106f84d8ac125f302fa9e1fb10b1a293d9', 'x': 1548, 'y': 530}, 'status': 'valid'}}",,,,,,,,,
[],False,porcelainsmile,,,[],,,,text,t2_7txotlcm,False,False,False,[],False,False,1641656534,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rz2mc7/read_6_chapters_of_kimball_what_now/,{},rz2mc7,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rz2mc7/read_6_chapters_of_kimball_what_now/,False,,,6,1641656545,1,"Hey All,

I am on my journey of preparing for Data Engineering interviews and Data Warehousing and Dimensional Modelling was one thing that I always felt intimidated by before getting started. But this time I took the leap after seeing a lot of people recommending Kimball as a great book to get started.

I read 6 chapters and learnt a good deal about what is Data Warehousing, Dimensional Modelling, Facts, Dimensions, SCDs and stuff. However, completing the entire book is still a colossal task and I think it would not be necessary to crack the interviews (correct me if I am wrong). I am still very new to this area and would want to maybe practice more by creating dimensional models now rather than just read read read.

So here I am, asking the most helpful DE community on the entire internet, what should be my next steps? How can I bolster my learning of DW/DM? What are some important topics that are asked in the interviews? Is there anything beyond the STAR schema? Is it important? Where can I find more practice material? And more...",True,False,False,dataengineering,t5_36en4,48852,public,self,Read 6 chapters of Kimball. What now?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rz2mc7/read_6_chapters_of_kimball_what_now/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Danielpuff123,,,[],,,,text,t2_4wcek3y6,False,False,False,[],False,False,1641652212,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rz12t6/data_engineering_101_what_is_a_data_warehouse/,{},rz12t6,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rz12t6/data_engineering_101_what_is_a_data_warehouse/,False,self,"{'enabled': False, 'images': [{'id': 'z8zPZ2tq0dbcx4-bpd_1Qt9BbpVej6YnmzkkRdealm8', 'resolutions': [{'height': 60, 'url': 'https://external-preview.redd.it/oVa-VFRCmQbQDKgnQmep7ucsw8TfPlBi8GIcZ2BS81Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d6cbdaa6aade6169fab35be6d4808a35d55b816c', 'width': 108}, {'height': 121, 'url': 'https://external-preview.redd.it/oVa-VFRCmQbQDKgnQmep7ucsw8TfPlBi8GIcZ2BS81Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d1ef138a4a5603ac111ba40b411029784cbf2dd0', 'width': 216}, {'height': 180, 'url': 'https://external-preview.redd.it/oVa-VFRCmQbQDKgnQmep7ucsw8TfPlBi8GIcZ2BS81Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=69ae91be260dea90a6e41c04543f2534e2f9464b', 'width': 320}, {'height': 360, 'url': 'https://external-preview.redd.it/oVa-VFRCmQbQDKgnQmep7ucsw8TfPlBi8GIcZ2BS81Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5d439cf035b4751e0f1fbab8bb77cc7c77bbdada', 'width': 640}, {'height': 540, 'url': 'https://external-preview.redd.it/oVa-VFRCmQbQDKgnQmep7ucsw8TfPlBi8GIcZ2BS81Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3eaa38aec47975e38a0bdb651557c086eb807c4e', 'width': 960}, {'height': 607, 'url': 'https://external-preview.redd.it/oVa-VFRCmQbQDKgnQmep7ucsw8TfPlBi8GIcZ2BS81Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a2794215a91a1db3621868e2ab6d10faef53c954', 'width': 1080}], 'source': {'height': 1284, 'url': 'https://external-preview.redd.it/oVa-VFRCmQbQDKgnQmep7ucsw8TfPlBi8GIcZ2BS81Q.jpg?auto=webp&amp;s=2ed5d43b0e28eaa430e366ec53cedc881a95e5af', 'width': 2282}, 'variants': {}}]}",6,1641652222,1,https://click.convertkit-mail2.com/4zul02d0lxb7uk7xgncx/3ohphkhq4r85meir/aHR0cHM6Ly93d3cuc3RhcnRkYXRhZW5naW5lZXJpbmcuY29tL3Bvc3Qvd2hhdC1pcy1hLWRhdGEtd2FyZWhvdXNlLw==,True,False,False,dataengineering,t5_36en4,48848,public,self,Data engineering 101 (What is a data warehouse),0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rz12t6/data_engineering_101_what_is_a_data_warehouse/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Bebop-n-Rocksteady,,,[],,,,text,t2_qichn,False,False,False,[],False,False,1641621932,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ryt1j6/home_lab_data_lake_ideas/,{},ryt1j6,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/ryt1j6/home_lab_data_lake_ideas/,False,,,6,1641621942,1,"Hello,
I wanted to reach out to see if anyone had any good data lake/data pipeline ideas for a home lab or links to some labs someone has already put together?

Thanks!",True,False,False,dataengineering,t5_36en4,48828,public,self,Home Lab Data Lake Ideas,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ryt1j6/home_lab_data_lake_ideas/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,StillClick,,,[],,,,text,t2_4iy847ry,False,False,False,[],False,False,1641621750,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ryszrs/key_skills_for_a_data_engineer_and_career_path/,{},ryszrs,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/ryszrs/key_skills_for_a_data_engineer_and_career_path/,False,,,6,1641621761,1,"Hi everyone, im looking for some advices about what to do (next steps) of my career path. Right now i'm working at a company as a data engineer (2 months), but it looks just in name, because in my day i just work coding in python (no ETL, no Cloud, no pipelines, or other skills that a data engineer should have). I came from a bi analyst rol (6 months) working in power bi reporting, sql queries, and i'm starting to get worried about i'm not developing the right skills to compete in market.

What advices would you do to me to develop a competitive profile in this market (bi analyst/data engineer). I know that eventually where i work i'm going into ETL (on premise), and they're looking for migrate into Cloud Services. And i'm wondering if it's normal that at the start of my new job i'm just working in a new data architecture in python (2months) for batch part of ETL process.

Thanks for reading",True,False,False,dataengineering,t5_36en4,48827,public,self,Key skills for a Data Engineer and Career path advices,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ryszrs/key_skills_for_a_data_engineer_and_career_path/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,maxinxin,,,[],,,,text,t2_6nsdc,False,False,False,[],False,False,1641620952,amazon.com,https://www.reddit.com/r/dataengineering/comments/rysr0j/free_aws_courses_on_amazoncom/,{},rysr0j,False,True,False,False,False,True,False,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rysr0j/free_aws_courses_on_amazoncom/,False,,,6,1641620963,1,,True,False,False,dataengineering,t5_36en4,48826,public,default,Free AWS courses on Amazon.com,0,[],1.0,https://www.amazon.com/aws-online-courses/b/?ie=UTF8&amp;node=14297978011&amp;ref_=sv_courses1_1,all_ads,6,,,,,,,,https://www.amazon.com/aws-online-courses/b/?ie=UTF8&amp;node=14297978011&amp;ref_=sv_courses1_1,,,,,,,,,,
[],False,thethrowupcat,,,[],,,,text,t2_h6cebdw,False,False,False,[],False,False,1641619256,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rys9ff/looking_for_an_easy_setup_for_blockchain_data/,{},rys9ff,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rys9ff/looking_for_an_easy_setup_for_blockchain_data/,False,,,6,1641619267,1,"Hey everyone. I am not a DE so I am a little unsure of what I need to do. I am comfortable with learning it all on my own mostly, but would like some general steps.

I'd like to access blockchain data (BTC or ETH for now) and load this into a warehouse. What options do I have to do this for free? I'd ultimately like to model some stuff using dbt and create an output into a self-hosted BI tool.",True,False,False,dataengineering,t5_36en4,48821,public,self,Looking for an easy setup for blockchain data,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rys9ff/looking_for_an_easy_setup_for_blockchain_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,inlovewithabackpack,,,[],,,,text,t2_13e7p2,False,False,False,[],False,False,1641613675,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ryqjyg/how_do_yall_manage_warehouse_schema_and_models/,{},ryqjyg,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/ryqjyg/how_do_yall_manage_warehouse_schema_and_models/,False,,,6,1641613822,1,"At a company with two environments for the data warehouse. At prior companies we've really just had a prod warehouse, so I'm curious about best practices of keeping the structures in sync. 

Have used Django, alembic and flyway for transaction db management for backend projects, and am wondering if a combination of similar db migration tooling with something like dbt is the way to go. 

Fellow data nerds, what are your thoughts?",True,False,False,dataengineering,t5_36en4,48820,public,self,How do y'all manage warehouse schema and models between multiple environments?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ryqjyg/how_do_yall_manage_warehouse_schema_and_models/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Hzzhzz,,,[],,,,text,t2_drg1nixg,False,False,False,[],False,False,1641611187,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ryps3z/from_data_analyst_to_data_engineer_apply_for/,{},ryps3z,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/ryps3z/from_data_analyst_to_data_engineer_apply_for/,False,,,6,1641611198,1,"
I have been a Data Analyst for the past 15 months at a famous ""fast paced"" tech and have complete ownership of ETL pipelines that I build for other analysts in my org (80% data engineering stuff). I use Spark, Hadoop, Airflow, python, sql and have good understanding of data modeling and data warehousing concepts. I also have worked as a software engineer for a year and I used Java extensively.

As the title suggests, I am looking to completely transition into core Data Engineering focused roles. I don't want to get hung up on job title but the compensation difference is too big for me to not reconsider things. It has been a great learning experience but I definitely feel under-compensated and exploited. 

Transitioning internally will be a slow process and I am sure my manager will not be too happy about me leaving the team, also not to forget the opportunity cost of continuing with my current salary. Should I still push for this, transition internally to gain more experience and look outside after? Or should I try to get a DE position outside (preferably in another big tech) with my current Data Analyst title? 

I'd really appreciate any advice!",True,False,False,dataengineering,t5_36en4,48818,public,self,From Data Analyst to Data Engineer: Apply for internal positions or look for opportunities outside?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ryps3z/from_data_analyst_to_data_engineer_apply_for/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,mistanervous,,,[],,,,text,t2_7ec0q,False,False,False,[],False,False,1641601182,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rymcs8/m1_macs_are_still_riddled_with_compatibility/,{},rymcs8,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rymcs8/m1_macs_are_still_riddled_with_compatibility/,False,,,6,1641601192,1,"I posted a while back asking about macs for data engineering, but neglected to specify that it is an m1 mac. After spending my first week troubleshooting tons of errors due to mismatching architecture in my packages, I would say to avoid m1 Macs for any local development unless you like spending weeks fiddling with every package",True,False,False,dataengineering,t5_36en4,48809,public,self,M1 macs are still riddled with compatibility issues,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rymcs8/m1_macs_are_still_riddled_with_compatibility/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,rajrondo,,,[],,,,text,t2_8bqa1,False,False,False,[],False,False,1641599140,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ryll6y/airflow_sensor_vs_scheduler/,{},ryll6y,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/ryll6y/airflow_sensor_vs_scheduler/,False,,,6,1641599150,1,"I'm an DE noob trying to setup our first pipeline. We have a weekly flow of hundreds to thousands of hour long videos into our s3 bucket. For every video that comes in, I need to  
1. Run it through a shell script that converts the video to constant frame rate. (already have the script, requires ffmpeg and cuda  
2. Store the output back into s3  


We're ok with somewhat delayed processing (1 day delay is fine), but have quite a bit of volume. In my head, using the Airflow s3 key sensor makes sense, but reading around, it seems like a lot of people recommend just sticking with the cron-like scheduling capability of airflow.  


Are there cons that I'm not thinking of with the sensor idea?",True,False,False,dataengineering,t5_36en4,48809,public,self,Airflow sensor vs scheduler,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ryll6y/airflow_sensor_vs_scheduler/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,bigchungusmode96,,,[],,,,text,t2_3ahwym06,False,False,False,[],False,False,1641598809,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rylgr6/alternatives_to_database_table_reinsertion_for/,{},rylgr6,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rylgr6/alternatives_to_database_table_reinsertion_for/,False,,,6,1641598820,1,"I have a dataset of millions of transactions that I am pulling from multiple data tables. I handle the data query, merge, cleaning &amp; processing in Python when I run a ML script. 

Now I also need a way to have the processed data accessible for a BI dashboard. The direct way I can think of is inserting the data back into a data table, but I'm wondering if there are better ways to do this? 

1. I'm not sure whether dashboards like Tableau can be set up to read in a Pickle file from a remote server directory but if it's possible this would be easy since my current process exports the data to a Pickle file.
2. I'm not a data engineer by trade, but my hunch was there should also be a way to run the Python data processing code in a data pipeline that would then store the data in a data table that is accessible to my ML code or the dashboard connection. (The benefit being I wouldn't have to run my ML script to get the processed data).",True,False,False,dataengineering,t5_36en4,48809,public,self,"Alternatives to database table reinsertion for large, post-processed dataset",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rylgr6/alternatives_to_database_table_reinsertion_for/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,TrainquilOasis1423,,,[],,,,text,t2_3wxhxt1i,False,False,False,[],False,False,1641591884,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ryiu4a/recommended_linkedin_learning_courses/,{},ryiu4a,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/ryiu4a/recommended_linkedin_learning_courses/,False,,,6,1641591896,1,My company is paying for LinkedIn learning and I have decided to take full advantage of this. Which courses should I prioritize? Do hiring manager even care about any of them?,True,False,False,dataengineering,t5_36en4,48803,public,self,recommended LinkedIn learning courses?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ryiu4a/recommended_linkedin_learning_courses/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,KingAfro851,,,[],,,,text,t2_5frff9z3,False,False,False,[],False,False,1641588142,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ryhdfd/de_coding_interview_at_metafacebook/,{},ryhdfd,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/ryhdfd/de_coding_interview_at_metafacebook/,False,,,6,1641588152,1,I have a coding interview at Meta(Facebook) for a DE role next week and I was hoping if anyone can help me out on the recent questions they might be asking. Leetcode and Glassdoor seem to have a lot of questions asked in 2021 but not any new recent ones. My interview will be based on SQL and Python! Anything would help to prepare for the interview. Thank you so much!,True,False,False,dataengineering,t5_36en4,48801,public,self,DE coding interview at Meta(Facebook),0,[],0.99,https://www.reddit.com/r/dataengineering/comments/ryhdfd/de_coding_interview_at_metafacebook/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,smbanaie,,,[],,,,text,t2_a7k05e30,False,False,False,[],False,False,1641585009,github.com,https://www.reddit.com/r/dataengineering/comments/ryg4e7/a_curated_list_of_dockercompose_files_prepared/,{},ryg4e7,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/ryg4e7/a_curated_list_of_dockercompose_files_prepared/,False,link,"{'enabled': False, 'images': [{'id': '3VaB9Gy6s0YCoWY559kM_CoDRyOQ80iUx8IzQeHN7ao', 'resolutions': [{'height': 54, 'url': 'https://external-preview.redd.it/WwIMNmR-dWg1pl1xKJW5GPN9WLEgorwl6t3I3nNvyUo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a215fa43e0bacf5c34e007002396b1d9d9072e5d', 'width': 108}, {'height': 108, 'url': 'https://external-preview.redd.it/WwIMNmR-dWg1pl1xKJW5GPN9WLEgorwl6t3I3nNvyUo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4cae72b2c36f387c7b4ff01e77d156148a02fb02', 'width': 216}, {'height': 160, 'url': 'https://external-preview.redd.it/WwIMNmR-dWg1pl1xKJW5GPN9WLEgorwl6t3I3nNvyUo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=39ff851fc25a997b7a4c35251b489e1f4174afc6', 'width': 320}, {'height': 320, 'url': 'https://external-preview.redd.it/WwIMNmR-dWg1pl1xKJW5GPN9WLEgorwl6t3I3nNvyUo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6a3df39ea6c54ca5917e0b6497b2b127d51f465d', 'width': 640}, {'height': 480, 'url': 'https://external-preview.redd.it/WwIMNmR-dWg1pl1xKJW5GPN9WLEgorwl6t3I3nNvyUo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=10851de12c477b2609a423e707f3e42d3600e953', 'width': 960}, {'height': 540, 'url': 'https://external-preview.redd.it/WwIMNmR-dWg1pl1xKJW5GPN9WLEgorwl6t3I3nNvyUo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2a27a279499dd789a98ebeaa293c8495f459861b', 'width': 1080}], 'source': {'height': 600, 'url': 'https://external-preview.redd.it/WwIMNmR-dWg1pl1xKJW5GPN9WLEgorwl6t3I3nNvyUo.jpg?auto=webp&amp;s=180781863cbd86a1411b5d377029b368c87e95ed', 'width': 1200}, 'variants': {}}]}",6,1641585020,1,,True,False,False,dataengineering,t5_36en4,48802,public,https://b.thumbs.redditmedia.com/-Cd31YB2vCctJRc1fHljwhwWwLkfMeBMhomWJMSSuQA.jpg,"a curated list of docker-compose files prepared for testing data engineering tools, databases, and open-source libraries.",0,[],1.0,https://github.com/irbigdata/data-dockerfiles,all_ads,6,,,,,,70.0,140.0,https://github.com/irbigdata/data-dockerfiles,,,,,,,,,,
[],False,I_am_not_doing_this,,,[],,,,text,t2_abbcm4w8,False,False,False,[],False,False,1641575764,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rych0l/asking_for_tips_for_my_internship/,{},rych0l,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rych0l/asking_for_tips_for_my_internship/,False,,,6,1641575775,1,"So I joined this start-up last week and have worked for 2 days now. Most of the time if I don't talk to my colleagues I seriously have no clue what I am supposed to do with all the assignments they give me.

What advices can you give me? I feel bad for not really doing any work. 

How can I understand all the tools/repo that they have built? Should I read the repo like a book?",True,False,False,dataengineering,t5_36en4,48791,public,self,Asking for tips for my internship,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rych0l/asking_for_tips_for_my_internship/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,jaspar1,,,[],,,,text,t2_k95rt,False,False,False,[],False,False,1641568155,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ry9ipf/does_anybody_use_aws_lambda_containerized_apps/,{},ry9ipf,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/ry9ipf/does_anybody_use_aws_lambda_containerized_apps/,False,,,6,1641568166,1,Aws recently in late 2020 added support to deploy ecr images in conjunction with aws lambda functions which is awesome but wondering if anyone uses this architecture in prod at enterprise level? How scalable is this?,True,False,False,dataengineering,t5_36en4,48781,public,self,Does anybody use aws lambda containerized apps using ECR at enterprise scale?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ry9ipf/does_anybody_use_aws_lambda_containerized_apps/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,secodaHQ,,,[],,,,text,t2_aiinah9q,False,False,False,[],False,False,1641568097,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ry9hxg/5_predictions_for_the_modern_data_stack_in_2022/,{},ry9hxg,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/ry9hxg/5_predictions_for_the_modern_data_stack_in_2022/,False,self,"{'enabled': False, 'images': [{'id': 'sN2cP26sLjL5ZzJm9Yh0F3UnqLlcJ2w51MqUL-yknBA', 'resolutions': [{'height': 65, 'url': 'https://external-preview.redd.it/wZN9pwR8iTjer8D-YlZ1S4zHshvYHxahfAM2hGZJp2M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9e93004d54dcd05ae3417702688488ca6edfafa6', 'width': 108}, {'height': 131, 'url': 'https://external-preview.redd.it/wZN9pwR8iTjer8D-YlZ1S4zHshvYHxahfAM2hGZJp2M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fe753abd79e5bd95c865ae8523f3e16065c03bf6', 'width': 216}, {'height': 194, 'url': 'https://external-preview.redd.it/wZN9pwR8iTjer8D-YlZ1S4zHshvYHxahfAM2hGZJp2M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=518c1d71ec935940ce9deb0873271acaae4a7598', 'width': 320}, {'height': 389, 'url': 'https://external-preview.redd.it/wZN9pwR8iTjer8D-YlZ1S4zHshvYHxahfAM2hGZJp2M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=81f3e406e4563aa4bedbbb7d6d00aed664c01588', 'width': 640}, {'height': 584, 'url': 'https://external-preview.redd.it/wZN9pwR8iTjer8D-YlZ1S4zHshvYHxahfAM2hGZJp2M.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8597bed87d7e14934c2069c92167a19be5c64b69', 'width': 960}, {'height': 657, 'url': 'https://external-preview.redd.it/wZN9pwR8iTjer8D-YlZ1S4zHshvYHxahfAM2hGZJp2M.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ce5a503adea225dabc14a1b27268b5988df82315', 'width': 1080}], 'source': {'height': 740, 'url': 'https://external-preview.redd.it/wZN9pwR8iTjer8D-YlZ1S4zHshvYHxahfAM2hGZJp2M.jpg?auto=webp&amp;s=a25a8fd342344222a794449fb5e870ab7ad2f9d5', 'width': 1216}, 'variants': {}}]}",6,1641568107,1,"We wrote this article with our 5 predictions for the modern data stack in 2022. In summary, these are the 5 main trends that we think we will see in the next year:

1. **The metrics layer is here**

On Oct 15 of 2021, Drew Banin shook up the data world with his PR titled “dbt should know about metrics”. With that announcement, dbt became the front runner to win the race towards the data OS. The implementation of the metric layer will be a major factor in making data accessible to more types of users. This is a very exciting announcement and will make it easier for companies to leverage data for business intelligence by improving the ease of creating queries and dashboards.

2. **The new “data workspace” will emerge** 

The way we read, share and consume information has changed drastically over the years, and it has the potential to continue to do so in the future. Therefore, an “all-in-one” data workspace tool may be a solution that responds not only to the needs of data teams today but also to how stakeholders consume data tomorrow.

3. **The reverse ETL race heats up**

Stakeholders outside of the data team are becoming more data literate and in doing so, are starting to require a different set of tools to work with data. This is partially why the reverse ETL space has become one of the fastest-growing data categories. One of the primary predictions is that open source reverse ETL will reach the same adoption as both Hightouch and Census in 2022 in the Reverse ETL space. This may seem like a bold claim, but one that we feel is backed by substantial evidence based on what has taken shape in the ETL space. 

In addition to the increased competition in the reverse ETL space, we believe that we could see a major acquisition in this space from a larger company like Twilio or Fivetran. The synergies between Reverse ETL and ETL are 

4. **Predictive models are coming**

I’m extremely excited about how predictions are going to start improving the accuracy of metrics in the modern data stack. Continual is one company that is making it easy to maintain predictions – from customer churn to inventory forecasts – directly in your cloud data warehouse. This approach makes me feel confident that predictive analytics and machine learning will continue to grow in popularity within the modern data stack as it continues to become easier to implement.

**5. Data operations takes shape**

Ironically, data teams frequently don’t have the information to help us to make decisions and take action in a data-driven way. We need data about the data we provide to make decisions about this data, also called metadata. 

For example, which tables are being relied upon the most by end-users? What is the business definition of this metric? Are any ETL pipelines delayed? 

Answers to these sorts of questions are increasingly important as data is becoming a product used beyond simple reporting to power a wide surface area of applications. The operations around data are almost as important as the data itself. But most teams today are building products without process, documentation, monitoring or analytics. 

In summary, we believe the future of the modern data stack will continue to make it easier than ever to extract even more value from your data. It’s an exciting time to be a data geek, and I’m happy to be riding this wave.

Anything that you think we missed? you can read the entire article here: https://www.secoda.co/blog/5-predictions-for-the-modern-data-stack-in-2022",True,False,False,dataengineering,t5_36en4,48781,public,self,5 predictions for the modern data stack in 2022,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ry9hxg/5_predictions_for_the_modern_data_stack_in_2022/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,ali_azg,,,[],,,,text,t2_177iae,False,False,False,[],False,False,1641567440,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ry98fp/how_to_deal_with_number_type_in_javascript/,{},ry98fp,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/ry98fp/how_to_deal_with_number_type_in_javascript/,False,,,6,1641567450,1,"Hello everyone,

In our company, the backend teams use combination of NodeJs and MongoDB.

Apparently in JS there is a data type named ""Number"", and this data type can be both DoubleType and IntegerType. And they write data in MongoDB.

The problem is, we in Data Engineer team, when we want to sync the data from MongoDB to HDFS using Spark, we sometimes face different data types and change of schema in fields that their types are Number. Because in Spark DoubleType and IntegerType are two different things, but in JS and MongoDB there is no difference between them.

I don't know what is the best approach to this problem. But I came up with a solution that we consider all Number types as DoubleType when syncing data, so we can avoid such changes in the future.

I want to know pros and cons of this solution, and if you know any better approach to this problem, please let me know.

Thank you all in advance.",True,False,False,dataengineering,t5_36en4,48779,public,self,"How to deal with ""Number"" type in JavaScript?",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ry98fp/how_to_deal_with_number_type_in_javascript/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,stackedhats,,,[],,,,text,t2_asezmvm8,False,False,False,[],False,False,1641566798,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ry909t/advice_for_how_to_hire_de_intern/,{},ry909t,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/ry909t/advice_for_how_to_hire_de_intern/,False,,,6,1641566809,1,"I'm a fairly new DE (1 YOE), and received permission to bring on an intern this summer.

The main goal would be getting me experience teaching and managing people and hopefully to find someone that we could hire full time next year.

I'm the only person in the firm wearing the DE/SWE hat right now.

I have some idea of what traits I would want in a hire, namely curious, enjoys programming and understanding how things work under the hood. 

I do not expect an intern to know jack shit, and will probably just ask candidates to do fizzbuzz and talk about projects they've done in the interview.

&amp;#x200B;

While I'm hardly a seasoned veteran, I've been growing into the role quite well, have a working data warehouse with solid logging and exception handling that hasn't broken after 2 months of daily pipeline runs, and expect further growth in the 5 months between now and when the intern would start.

I'm thinking I'll probably spin up a cloned warehouse environment with the PII stripped out (this is an investment firm so replacing client names with a list of baby names or just numbers won't affect data quality in a meaningful way), give them a copy of DDIA to read over the summer and have them work through some of the problems I've worked to solve. That would give them real experience and provide a way to familiarize them with the stack and databases.

Basically, I'm going to treat this as a learning experience for them rather than try to shove grunt work or busywork at them and feel good about myself.

&amp;#x200B;

So with that in mind, and the fact that ideally we could recruit some talent into the firm afterwards, what do you think I should be looking for/ asking for in the posting and interview process?

I also HATE the way most technical roles are hired currently and will fucking die on this hill if I have to in order to make it better.

&amp;#x200B;

My basic requirements are:

&amp;#x200B;

* CS major/minor (I could entertain DA majors but having an MSBA myself have a pretty low opinion of the usefulness of such programs).
* Experience with ANY OOP language (the foundation of learning your first language is what matters, not intimate knowledge of the idiosyncrasies of python)
* Ability to pass fizzbuzz (with numbers other than 3 and 5)
* It would be nice if they knew SQL, but let's be honest... it's not that hard to learn and the only way to actually get good at it is to use it
* Rising Senior
* Smart person who gets things done

&amp;#x200B;

Yeah... that's pretty much it, we're a finance firm with low data maturity and a lot of room to grow, we aren't using the shiniest new tools, but data people here have a lot of upside and value to deliver as a result.

Any advice on how to find and evaluate candidates for a DE internship at a small finance company?

What would you require from candidates? What traits would you select for?",True,False,False,dataengineering,t5_36en4,48779,public,self,Advice for how to hire DE intern,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ry909t/advice_for_how_to_hire_de_intern/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Zealousideal-Bus1698,,,[],,,,text,t2_808d2wbz,False,False,False,[],False,False,1641561397,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ry75lf/efficient_way_in_spark_to_compute_rows_read_and/,{},ry75lf,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/ry75lf/efficient_way_in_spark_to_compute_rows_read_and/,False,,,6,1641561409,1,"We are having explict count actions in our spark jobs which is taking lot of resources, imstead can we leverage spark internal metrics  sparkListeners eventLogs ?",True,False,False,dataengineering,t5_36en4,48774,public,self,Efficient way in Spark to compute rows Read and rows written in Spark jobs processing multiple datasets,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ry75lf/efficient_way_in_spark_to_compute_rows_read_and/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,DataAaron,,,[],,,,text,t2_i55id5cg,False,False,False,[],False,False,1641556428,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ry5rdi/what_other_roles_can_i_apply_for_if_i_cant_find/,{},ry5rdi,False,False,False,False,False,False,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/ry5rdi/what_other_roles_can_i_apply_for_if_i_cant_find/,False,,,6,1641556438,1,[removed],True,False,False,dataengineering,t5_36en4,48770,public,self,What other roles can I apply for if I can’t find an entry-level Data Engineering role?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ry5rdi/what_other_roles_can_i_apply_for_if_i_cant_find/,all_ads,6,,,automod_filtered,,,,,,,,,,,,,,,
[],False,DataAaron,,,[],,,,text,t2_i55id5cg,False,False,False,[],False,False,1641556211,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ry5pkk/what_are_roles_can_i_apply_for_if_im_unable_to/,{},ry5pkk,False,False,False,False,False,False,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/ry5pkk/what_are_roles_can_i_apply_for_if_im_unable_to/,False,,,6,1641556222,1,[removed],True,False,False,dataengineering,t5_36en4,48770,public,self,What are roles can I apply for if I’m unable to land an entry-level Data Engineer position?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ry5pkk/what_are_roles_can_i_apply_for_if_im_unable_to/,all_ads,6,,,reddit,,,,,,,,,,,,,,,
[],False,Tharparkar1565,,,[],,,,text,t2_ea14eezz,False,False,False,[],False,False,1641554878,technologiesinindustry4.com,https://www.reddit.com/r/dataengineering/comments/ry5dcf/dataset_augmentation_for_deep_learning/,{},ry5dcf,False,False,False,False,False,False,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/ry5dcf/dataset_augmentation_for_deep_learning/,False,link,"{'enabled': False, 'images': [{'id': 'fW9NnepdkZ2ZeWWTtUjIMUoQxnl6qj6JruQXN2qv4w8', 'resolutions': [{'height': 102, 'url': 'https://external-preview.redd.it/ovFz1OPYwX3vwxrdwR2x1tPqRnaUZu_NUBec_rxURUE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1ce019843641b694e6672b0ddb1ca3a77dc8b9f', 'width': 108}, {'height': 205, 'url': 'https://external-preview.redd.it/ovFz1OPYwX3vwxrdwR2x1tPqRnaUZu_NUBec_rxURUE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=393c40f5752c6701122873056b47a04adb9497cd', 'width': 216}, {'height': 304, 'url': 'https://external-preview.redd.it/ovFz1OPYwX3vwxrdwR2x1tPqRnaUZu_NUBec_rxURUE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=73c5f57b85abc51d8b4e33fc21a217117f59cd96', 'width': 320}, {'height': 608, 'url': 'https://external-preview.redd.it/ovFz1OPYwX3vwxrdwR2x1tPqRnaUZu_NUBec_rxURUE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ae0ba5ad63af339561bb01f0228380ac2e35c1ab', 'width': 640}, {'height': 912, 'url': 'https://external-preview.redd.it/ovFz1OPYwX3vwxrdwR2x1tPqRnaUZu_NUBec_rxURUE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6379141e58278cf5f77e8625042c4b1c81963e37', 'width': 960}], 'source': {'height': 951, 'url': 'https://external-preview.redd.it/ovFz1OPYwX3vwxrdwR2x1tPqRnaUZu_NUBec_rxURUE.jpg?auto=webp&amp;s=650ab28896a72c398df2e8939fdf6e1c75e3e6c0', 'width': 1001}, 'variants': {}}]}",6,1641554889,1,,True,False,False,dataengineering,t5_36en4,48768,public,https://b.thumbs.redditmedia.com/qM8099jDe8HYb-QzKqTRyfPUOwmckM8rFfHz-ve7sCY.jpg,Dataset augmentation for Deep Learning,0,[],1.0,https://www.technologiesinindustry4.com/2022/01/dataset-augmentation-for-deep-learning.html,all_ads,6,,,reddit,,,133.0,140.0,https://www.technologiesinindustry4.com/2022/01/dataset-augmentation-for-deep-learning.html,,,,,,,,,,
[],False,RealCaptainDaVinci,,,[],,,,text,t2_1wr17q2f,False,False,False,[],False,False,1641552266,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ry4p6s/my_job_title_is_data_engineer_but_i_dont_think_im/,{},ry4p6s,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/ry4p6s/my_job_title_is_data_engineer_but_i_dont_think_im/,False,,,6,1641552277,1,"I'm currently working as a data engineer in a team which builds and maintains ETL pipelines for internal data. I joined this team \~1.5 years ago, just after college, and from the little experience that I have it seems like DE solutions don't involve much ""engineering"".

For instance, most of our pipelines pull data via APIs from third-party SaaS systems and this too is now being taken care of by low/no-code ingestion tools. ETL steps are pretty much the same across projects. The data model involves work but they are pretty much set during project inception and are pretty much the usual facts, SCD 1&amp;2.

The issues / challenges that we usually face is third-party source APIs / systems not behaving as advertised and we basically just have to wait until their team fixes it. New projects should usually be exciting, but in this case I dread it due to the 10s of fields that have to be looked at and types/format determined on a manual basis.

New requests from business are usually around, ""can we add this new field to this table?"", ""can we create this view to match the existing report?"".  

So, overall I feel that the project my team handles is basically to add reporting ability to third-party systems. Which I feel should have been present in the third-party systems in the first place.

I'm yet to work on any serious optimization tasks, I'm yet to write any code that isn't the usual ETL steps. I'm not sure if this is what is involved in DE or maybe I'm just not a data person.",True,False,False,dataengineering,t5_36en4,48766,public,self,"My job title is data engineer, but I don't think I'm doing any engineering.",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ry4p6s/my_job_title_is_data_engineer_but_i_dont_think_im/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Samuel936,,,[],,,,text,t2_z6cc9g6,False,False,False,[],False,False,1641549157,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ry3xd9/am_i_a_good_fit_for_data_engineering/,{},ry3xd9,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/ry3xd9/am_i_a_good_fit_for_data_engineering/,False,,,6,1641549167,1,"Recently had an Amazon recruiter reach out to me and say they think I’d be a great fit for their DE position as a MGIS Major and Senior in university. 

My guess is due to the fact I had some SQL experience listed on there, but frankly I feel terribly under qualified for the position. I did SQL on Azure like 2 semesters ago as homework and got a basic MTA cert, but all that stuff I haven’t picked up since then. I know everything in can be learned, and I am confident in my abilities to develop skills. So I applied anyway despite the intimidating description.

Any tips or advice on approaching this position?",True,False,False,dataengineering,t5_36en4,48766,public,self,Am I a good fit for Data Engineering?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ry3xd9/am_i_a_good_fit_for_data_engineering/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,LowProgram6449,,,[],,,,text,t2_d7m50dkt,False,False,False,[],False,False,1641545274,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ry2ymi/need_advice_for_data_engineering_portfolio_project/,{},ry2ymi,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/ry2ymi/need_advice_for_data_engineering_portfolio_project/,False,,,6,1641545284,1,"I am trying to build a DE portfolio project and my idea is to build a small scale cloud-native pipeline. Can you guys recommend me approaches that might be suitable for this?

I am thinking of deploying containers that contain Data Ingestion (from Live API) scripts and dbt jobs for transformation and orchestrating it with Airflow. Is this an acceptable solution? What are the most suitable cloud providers or services that I can use to accomplish this?

Thank you!

P.S. I am more than willing to invest money to pay for the cloud costs",True,False,False,dataengineering,t5_36en4,48762,public,self,Need advice for Data Engineering portfolio project,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ry2ymi/need_advice_for_data_engineering_portfolio_project/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,maxinxin,,,[],,,,text,t2_6nsdc,False,False,False,[],False,False,1641540140,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ry1ori/need_help_with_career_choices_from_etl_dev_to_de/,{},ry1ori,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/ry1ori/need_help_with_career_choices_from_etl_dev_to_de/,False,,,6,1641540151,1,"My background: 
I have been working as ETL developer for the past 3 years, this was my first real data related job.

The company is in the healthcare industry and uses Microsoft stack. I am quite familiar with Sql server and SSIS, but I don't see SSIS as being a great career option in the long run, and the job is getting boring for me with repeated tasks. 

Like many others I am considering a move towards data engineering or data science position.
My company is transitioning into Azure and cloud computing and offering to give us training some time soon.

I have a few questions:

1. I know Python is crucial, I have started getting back into learning Python, but I have not had any hands on experience with it since finishing my degree.  What is a good place to find more ETL related Python knowledge?

2. Given that the current company is moving to Azure, is it better that I stay during the transition and learn everything azure related or move to using Python as primary ETL tool to land a new job? In other words, would the Azure knowledge be as important as getting better at Python for a new position?

3. I have friends that want to refer me to a DS/DA focused position, I have taken some related courses for masters degree but have little to no hands on experience,  I have heard from friends that you don't have to learn as many tools in the future and can fully rely on Python for data science compared to DE, but I am unsure if what I have done in the past few years going to be helpful for me to land a job.

Thanks in advance for any help!",True,False,False,dataengineering,t5_36en4,48759,public,self,Need help with career choices from ETL dev to DE.,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ry1ori/need_help_with_career_choices_from_etl_dev_to_de/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,yesuser001,,,[],,,,text,t2_7yh1iphp,False,False,False,[],False,False,1641526445,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rxxim0/snowflake_with_data_model_tools_need_info/,{},rxxim0,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rxxim0/snowflake_with_data_model_tools_need_info/,False,,,6,1641526455,1,"Hey Guys, I am looking for forward engineering data model tool for snowflake.

I did some research and narrowed down two out of a bunch
1)Dbm sql 
2)dbvisualizer

Do anyone have experience working with any of these tools, how good are they?
Do they cover all features to be utilized for snowflake db modeling? 
Any pros and cons of these tool in long term use case?
Did I miss any other tool out of these two?",True,False,False,dataengineering,t5_36en4,48745,public,self,Snowflake with data model tools - need info,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rxxim0/snowflake_with_data_model_tools_need_info/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Apprehensive_Can442,,,[],,,,text,t2_2jxuj03c,False,False,False,[],False,False,1641523162,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rxwd8t/what_is_the_best_way_to_manage_users_and/,{},rxwd8t,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rxwd8t/what_is_the_best_way_to_manage_users_and/,False,,,6,1641523172,1,"preferably:
- programmable
- with version control",True,False,False,dataengineering,t5_36en4,48744,public,self,What is the best way to manage users and permissions in my data warehouse?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rxwd8t/what_is_the_best_way_to_manage_users_and/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,SatRipper,,,[],,,,text,t2_gi9h13c,False,False,False,[],False,False,1641518443,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rxuoyl/how_hard_is_it_to_get_a_job_in_tech/,{},rxuoyl,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rxuoyl/how_hard_is_it_to_get_a_job_in_tech/,False,,,6,1641518454,1,"I’m going to be joining a consulting firm as a junior data engineer this year when I graduate from college. How difficult is it to break into tech? Also, how many YOE should you have when applying? Would i have a good chance with only 1 YOE at this job?",True,False,False,dataengineering,t5_36en4,48737,public,self,How hard is it to get a job in Tech?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rxuoyl/how_hard_is_it_to_get_a_job_in_tech/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,NormanNitro,,,[],,,,text,t2_iz2pc,False,False,False,[],False,False,1641513346,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rxssgs/unsure_about_moving_to_data_engineering/,{},rxssgs,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rxssgs/unsure_about_moving_to_data_engineering/,False,,,6,1641513356,1," I’ve been in my first role out of college (kind of a hodgepodge but could charitably be described as Dev Ops?) for almost 3 years. That’s definitely been too long, and I’ve felt a bit stagnant, so I’ve been looking for SWE positions where I can focus more on programming. By chance I’ve ended up with an offer for a Data Engineer position focused on writing ETL and I’m torn about what to do with it. The tech stack is all things that I’d be interested in working with (Java, Ruby, Spark, NoSQL, AWS), there’s a pay bump, the company’s doing well, and I like the mission and the people. I like working with data but the field is new to me, so I’m worried if I don’t end up enjoying it, would the skills be transferable? Or would the programming experience be applicable to other backend SWE roles? I've been doing some reading and I see some people moving from SWE to Data Engineering, but how viable is the other way?",True,False,False,dataengineering,t5_36en4,48731,public,self,Unsure about moving to Data Engineering,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rxssgs/unsure_about_moving_to_data_engineering/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Flat_Shower,,,[],,,,text,t2_5v6av4nm,False,False,False,[],False,False,1641511374,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rxs21j/wanting_honest_critique_of_my_writing/,{},rxs21j,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rxs21j/wanting_honest_critique_of_my_writing/,False,self,"{'enabled': False, 'images': [{'id': 'jOg_iXyvjCv6-oqjp5-t5OBKcqcGKchR8ywmjUtMRHU', 'resolutions': [{'height': 72, 'url': 'https://external-preview.redd.it/uWoFaLEvCZrszi2DSt0Ce2A0YU0kS4k0VF-rpJbY-vs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fac94736e15564bf49051c016ea7d9a78d843951', 'width': 108}, {'height': 145, 'url': 'https://external-preview.redd.it/uWoFaLEvCZrszi2DSt0Ce2A0YU0kS4k0VF-rpJbY-vs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4b500c25468127d9744cba53b1701e6674a0164d', 'width': 216}, {'height': 215, 'url': 'https://external-preview.redd.it/uWoFaLEvCZrszi2DSt0Ce2A0YU0kS4k0VF-rpJbY-vs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ba0d2af3a719fe992325235fc209ceb4b39ec889', 'width': 320}, {'height': 430, 'url': 'https://external-preview.redd.it/uWoFaLEvCZrszi2DSt0Ce2A0YU0kS4k0VF-rpJbY-vs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=73f9a3813832aaf294fd3af1efecda243e3e7b68', 'width': 640}, {'height': 645, 'url': 'https://external-preview.redd.it/uWoFaLEvCZrszi2DSt0Ce2A0YU0kS4k0VF-rpJbY-vs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=65d9f4c17bd7d684e0655eb9bb6f3c3b515e7261', 'width': 960}, {'height': 726, 'url': 'https://external-preview.redd.it/uWoFaLEvCZrszi2DSt0Ce2A0YU0kS4k0VF-rpJbY-vs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0c000578029db3e99d714b7b7f72097269f59094', 'width': 1080}], 'source': {'height': 807, 'url': 'https://external-preview.redd.it/uWoFaLEvCZrszi2DSt0Ce2A0YU0kS4k0VF-rpJbY-vs.jpg?auto=webp&amp;s=1a8aedf709bd96b5d5b27f110771963daec7691f', 'width': 1200}, 'variants': {}}]}",6,1641511385,1,"I've been watching SeattleDataGuys' YouTube videos, and he calls out that one of his passive income sources is writing on Medium. I've seen a few articles on Medium over the years, but have never really considered writing. I spent about an hour putting together a quick article, and I'd love some \[harsh\] criticism on my writing style.

[https://medium.com/@galenbusch/3-things-all-data-engineers-should-learn-from-google-7a8fe917597a](https://medium.com/@galenbusch/3-things-all-data-engineers-should-learn-from-google-7a8fe917597a)",True,False,False,dataengineering,t5_36en4,48730,public,self,Wanting honest critique of my writing,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rxs21j/wanting_honest_critique_of_my_writing/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Inside-Radish1259,,,[],,,,text,t2_i80c7msd,False,False,False,[],False,False,1641511182,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rxrzbc/incoming_data_engineer_intern_at_amazon_prep/,{},rxrzbc,False,False,False,False,False,False,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rxrzbc/incoming_data_engineer_intern_at_amazon_prep/,False,,,6,1641511192,1,[removed],True,False,False,dataengineering,t5_36en4,48729,public,self,Incoming Data Engineer Intern at Amazon - Prep,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rxrzbc/incoming_data_engineer_intern_at_amazon_prep/,all_ads,6,,,automod_filtered,,,,,,,,,,,,,,,
[],False,Important-Bug-8004,,,[],,,,text,t2_8bgk7n3m,False,False,False,[],False,False,1641509868,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rxrhft/apache_nifi_howto_video_course/,{},rxrhft,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rxrhft/apache_nifi_howto_video_course/,False,self,"{'enabled': False, 'images': [{'id': '7oMNtYozy8vAb0jfPwF43_PBXMbByqxrnhiayrWI2Ew', 'resolutions': [{'height': 81, 'url': 'https://external-preview.redd.it/gPgGSwKvKuxiK13-0Ma1ANtEky_fS25D-uUp2s1sdF8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f711ca3a87413f0cd6074415fe94c1609bcf6070', 'width': 108}, {'height': 162, 'url': 'https://external-preview.redd.it/gPgGSwKvKuxiK13-0Ma1ANtEky_fS25D-uUp2s1sdF8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=13927f0c6939391e7ae9a98f7201414c47a9e0e5', 'width': 216}, {'height': 240, 'url': 'https://external-preview.redd.it/gPgGSwKvKuxiK13-0Ma1ANtEky_fS25D-uUp2s1sdF8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7a217e3e283e219be095a2074238b18d89271b8e', 'width': 320}], 'source': {'height': 360, 'url': 'https://external-preview.redd.it/gPgGSwKvKuxiK13-0Ma1ANtEky_fS25D-uUp2s1sdF8.jpg?auto=webp&amp;s=b4b74a4d9f7228c0733d9a1ee557abd2b946ea22', 'width': 480}, 'variants': {}}]}",6,1641509878,1," 

Hei Everyone.

I am a BI developer who likes to learn new things.

I want to share what I am doing. I created a video on how to install and configure the Apache Nifi with some easy examples to test the SW.

[https://www.youtube.com/watch?v=zvmAVxIkoso](https://www.youtube.com/watch?v=zvmAVxIkoso)

Here is also my IT blog where the process is written down in depth.

[https://www.denistalksit.tech/post/how-to-install-and-configure-apache-nifi-on-linux-server-ubunto-20-04-3](https://www.denistalksit.tech/post/how-to-install-and-configure-apache-nifi-on-linux-server-ubunto-20-04-3)

Please like and sub if you think that it's ok content.

Thank you ! :)",True,False,False,dataengineering,t5_36en4,48729,public,self,Apache Nifi how-to video course,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rxrhft/apache_nifi_howto_video_course/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Dawido090,,,[],,,,text,t2_4as7wsm1,False,False,False,[],False,False,1641507548,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rxqlpm/sql_in_de/,{},rxqlpm,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rxqlpm/sql_in_de/,False,,,6,1641507559,1,I know it deepends but how well do you need to know sql engine and more administration commands? For example do you need to know very well how to have healthy server or it's handled by admins?,True,False,False,dataengineering,t5_36en4,48726,public,self,SQL in DE,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rxqlpm/sql_in_de/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,dev_null_scales,,,[],,,,text,t2_3hkf0t14,False,False,False,[],False,False,1641499231,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rxnhit/databricks_has_terrible_monitoringobservability/,{},rxnhit,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rxnhit/databricks_has_terrible_monitoringobservability/,False,,,6,1641499242,1,"Using Azure Databricks and we have some massive issues trying to instrument any kind of monitoring or metrics from it, the best we seem to get is a Ganglia page from 2004. 

We are currently implementing some hackery prometheus metric exporting from it using a shim outside of databricks, because they are trying to proxy spark into their system. 

Anyone else ran into issues trying to troubleshoot VM usage and such? I'm not sure how anyone gets good observability on this..",True,False,False,dataengineering,t5_36en4,48718,public,self,Databricks has terrible monitoring/observability or am I missing something?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rxnhit/databricks_has_terrible_monitoringobservability/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Business_You9930,,,[],,,,text,t2_9g2mgwx8,False,False,False,[],False,False,1641499077,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rxnf8n/question_as_a_de_do_you_have_access_to_all/,{},rxnf8n,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rxnf8n/question_as_a_de_do_you_have_access_to_all/,False,,,6,1641499087,1,"Howdy! My role is a data analyst, and as I was troubleshooting a pipeline failure with a DE on the IT Enterprise Data Engineering team at my company, I was hoping to see the production data that was used to be copied into a sql db from Azure Blob Storage - the reason I wanted to see it doesn't quite matter as my question is on data access...

He mentioned only the ""Ops Lead"" on their team had access to production data. I asked why and he said then he'd have access to payroll and other restricted/PII data. 

Is this typical? I figured any DE on the Enterprise DE team would have access to production data contained in the resources they managed. Isn't it in their role responsibilities to manage sensitive data? Isn't there the ability to restrict subdirectories or store payroll data as an example in another directory from source system data that business users like myself use and is not restricted/PII? Just curious as this seems limiting and unnecessary...",True,False,False,dataengineering,t5_36en4,48718,public,self,Question: As a DE do you have access to *all production in a storage container?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rxnf8n/question_as_a_de_do_you_have_access_to_all/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,happybirthday290,,,[],,,,text,t2_9zhy6t6t,False,False,False,[],False,False,1641498659,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rxn9cq/turning_petabytescale_raw_video_into_great/,{},rxn9cq,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rxn9cq/turning_petabytescale_raw_video_into_great/,False,self,"{'enabled': False, 'images': [{'id': 'bznOOwwSwHP6Z8jFH5nXpUiuXl6tYGcLnFDAuh92r8Y', 'resolutions': [{'height': 31, 'url': 'https://external-preview.redd.it/ZUr_cp3HnfQcha7Zfsr72hgiHeqp7zRbC-jg1M7zvho.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=82bc64fdd5d02b6d4398f6ca72e9302d3af87961', 'width': 108}, {'height': 62, 'url': 'https://external-preview.redd.it/ZUr_cp3HnfQcha7Zfsr72hgiHeqp7zRbC-jg1M7zvho.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f9d368d5bca6f06d0461ef4c8d6697577ea4db90', 'width': 216}, {'height': 92, 'url': 'https://external-preview.redd.it/ZUr_cp3HnfQcha7Zfsr72hgiHeqp7zRbC-jg1M7zvho.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b3c67e5469eeefcb33a4b21f07c0003525feb301', 'width': 320}, {'height': 184, 'url': 'https://external-preview.redd.it/ZUr_cp3HnfQcha7Zfsr72hgiHeqp7zRbC-jg1M7zvho.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=35f75264244a2e1b32b8d5aabacc83247f4f6c1b', 'width': 640}, {'height': 276, 'url': 'https://external-preview.redd.it/ZUr_cp3HnfQcha7Zfsr72hgiHeqp7zRbC-jg1M7zvho.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ab776a2aca87dcdd0f0c9ba97cb0dc1fa4dfbc04', 'width': 960}, {'height': 311, 'url': 'https://external-preview.redd.it/ZUr_cp3HnfQcha7Zfsr72hgiHeqp7zRbC-jg1M7zvho.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=824e5eaee8ca80311416a9d56962c6664f14998e', 'width': 1080}], 'source': {'height': 346, 'url': 'https://external-preview.redd.it/ZUr_cp3HnfQcha7Zfsr72hgiHeqp7zRbC-jg1M7zvho.jpg?auto=webp&amp;s=16971a68c9ccfe75eaa09a9e694110d78083cc0d', 'width': 1200}, 'variants': {}}]}",6,1641498670,1,"Imagine 10 cameras, running 24/7 at 30 FPS - that's **27 million frames** generated in a single day. Knowing what's in that data or finding the 1% of things that are actually interesting is hard. I genuinely think there's way too little attention put forth to how people should use their raw data effectively even though so many people choose to store petabytes of it, just in case.

I wrote a little article about taking raw video and turning that into an actionable computer vision model. Would love to have a discussion about this so comment away :)

[https://towardsdatascience.com/curating-a-dataset-from-raw-images-and-videos-c8b962eca9ba](https://towardsdatascience.com/curating-a-dataset-from-raw-images-and-videos-c8b962eca9ba)

I'm also the OP of [this other post from a few days back](https://www.reddit.com/r/dataengineering/comments/ruige9/sieve_we_processed_24_hours_of_security_footage/) but just wanted to have a more technical conversation here!",True,False,False,dataengineering,t5_36en4,48718,public,self,Turning petabyte-scale raw video into great datasets,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rxn9cq/turning_petabytescale_raw_video_into_great/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Vast_Balls,,,[],,,,text,t2_h383c0yw,False,False,True,[],False,False,1641496587,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rxmgdt/recommended_softwarecloud_options_for_fitting/,{},rxmgdt,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rxmgdt/recommended_softwarecloud_options_for_fitting/,False,,,6,1641496598,1,"Hello,

I have collected hundreds of CSVs of monthly loan and economic data, which continues to grow each month. The bulk of the data is the loans and tracks individuals loan performance over time, such as the payment amount made, whether the customer paid off more than they needed to, whether they went delinquent, refinanced, etc. It also has borrower characteristics like FICO scores and DTI ratios. What I would like to do is a build a model(s) to predict prepayments, delinquencies, refinances, etc. with consideration for macro conditions and borrower characteristics. If successful, this model could be implemented at my company to replace our vendor model.

Conceptually I have idea about how this might work. I have built many ML models with datasets that were small enough to work on my local machine, but the computing requirements of this are beyond that. My question may be more of a data engineering question, but I am wondering what the lowest cost method would be to store, manipulate, and fit models on this set. First for the proof of concept, and then potentially longer term for running loans through this model on a monthly basis.

Right now I am thinking of simply storing the data on some low cost cloud service like Amazon S3 and using Apache Spark via Databricks to manipulate, analyze, and fit models on it. Is this is a good idea? Or is it more or less than I would need, at least for the proof of concept? I work for a small company that has relatively weak and outdated data support so I am leading this alone but could get a little bit of money towards it.

Apologies if this is outside of the scope of this subreddit.

Thanks!",True,False,False,dataengineering,t5_36en4,48714,public,self,Recommended software/cloud options for fitting algorithms on ~1.5 TB of data?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rxmgdt/recommended_softwarecloud_options_for_fitting/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,PauloMarcl,,,[],,,,text,t2_1ry44h1z,False,False,False,[],False,False,1641495418,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rxlzuz/changing_company_after_3_month/,{},rxlzuz,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rxlzuz/changing_company_after_3_month/,False,,,6,1641495428,1,"I am currently in a new company since 3 months where things are going fine but not great. Also the job is more focused on backend / API development than creating data pipelines, in which I am more interested.

On a bad day at work, I responded to some other recruiters and finally got an offer which is 5-10% better than my current salary on a more data pipeline focused job.

Do you think it would be premature to change company after only 3 months ? I feel than I didn't let enough time to my current company to prove its value and that it could backfire as I would be changing companies too often",True,False,False,dataengineering,t5_36en4,48712,public,self,Changing company after 3 month,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rxlzuz/changing_company_after_3_month/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,EntrepreneurSea4839,,,[],,,,text,t2_7k6wly40,False,False,False,[],False,False,1641491428,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rxkfrh/getting_started_with_de_in_2022/,{},rxkfrh,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rxkfrh/getting_started_with_de_in_2022/,False,,,6,1641491439,1,"How to get started with Data engineering ? Knows about SQL, some python. Have DS exp.",True,False,False,dataengineering,t5_36en4,48707,public,self,Getting Started with DE in 2022,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rxkfrh/getting_started_with_de_in_2022/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,jacocal,,,[],,,,text,t2_3rawc1,False,False,False,[],False,False,1641484615,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rxhtu9/data_pipelining_in_databricks_delta_lake/,{},rxhtu9,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rxhtu9/data_pipelining_in_databricks_delta_lake/,False,,,6,1641484625,1,"Hi Everyone! I'm trying to do cleaning within a Delta Lake pipeline (bronze -&gt; silver). The silver pipeline runs on trigger while bronze is writing in stream through appending. The question is, how can I get a checkpoint for my silver pipeline to start transforming data from the latest checkpoint that it read on the previous batch?",True,False,False,dataengineering,t5_36en4,48694,public,self,Data pipelining in Databricks Delta Lake,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rxhtu9/data_pipelining_in_databricks_delta_lake/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,okletsg000,,,[],,,,text,t2_8cfgjmq1,False,False,False,[],False,False,1641484378,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rxhqp2/wanna_find_a_niche_or_offer_product_within/,{},rxhqp2,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rxhqp2/wanna_find_a_niche_or_offer_product_within/,False,,,6,1641484388,1,"As the title I wanna find a niche and also offer service product or a solution within data analytics. But need road help or ideas from you guys.

Today I work as a BI analyst and data engineering, the whole ELT process, I use only three tools to accomplish the ELT step and also as last step power bi or tableau. But I feel that everyone today can do my job or tasks therefore I wanna come up with something special that a few person have to compete with me. 

I can basic code in python also when using for analysis maybe I should only focus on Databricks? I don’t know. I really feel lost and a bit anxious. 

Any ideas or tips?",True,False,False,dataengineering,t5_36en4,48695,public,self,Wanna find a niche or offer product within analytics,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rxhqp2/wanna_find_a_niche_or_offer_product_within/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Existing-Job-5881,,,[],,,,text,t2_cun607s3,False,False,False,[],False,False,1641484114,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rxhn9j/scaling_data_ingestion/,{},rxhn9j,False,False,False,False,False,False,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rxhn9j/scaling_data_ingestion/,False,,,6,1641484127,1,[removed],True,False,False,dataengineering,t5_36en4,48694,public,self,Scaling Data Ingestion,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rxhn9j/scaling_data_ingestion/,all_ads,6,,,automod_filtered,,,,,,,,,,,,,,,
[],False,543254447,,,[],,,,text,t2_c3yqm,False,False,False,[],False,False,1641482860,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rxh6cq/airflow_development_style_for_personal_project/,{},rxh6cq,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rxh6cq/airflow_development_style_for_personal_project/,False,,,6,1641482871,1,"Just wondering how everyone deploys airflow for the purpose of personal projects. Would love to hear different approaches.

&amp;#x200B;

Do you install and configure everything in a VM? I guess this way you can use VS code and just write your DAG directly on the VM.

Or are you using docker? If so how do you write your dags, I would love to have syntax highlight lol.",True,False,False,dataengineering,t5_36en4,48693,public,self,Airflow - Development Style for Personal Project,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rxh6cq/airflow_development_style_for_personal_project/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Fragrant-Lobster4276,,,[],,,,text,t2_7wcm51fu,False,False,False,[],False,False,1641482002,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rxguh6/data_modelling_resources_for_streaming_use_cases/,{},rxguh6,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rxguh6/data_modelling_resources_for_streaming_use_cases/,False,,,6,1641482015,1,"In context of data modelling kimbal,inmon and data vault all seem to be focussed/optimised for batch workloads

Any specific resources equivalent to kimbal but focussed on streaming use cases?",True,False,False,dataengineering,t5_36en4,48693,public,self,Data Modelling resources for streaming use cases,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rxguh6/data_modelling_resources_for_streaming_use_cases/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,instamarq,,,[],,,,text,t2_a35iq247,False,False,False,[],False,False,1641481772,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rxgr5e/scalable_cloud_saas_approaches_and_tools/,{},rxgr5e,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rxgr5e/scalable_cloud_saas_approaches_and_tools/,False,,,6,1641481782,1,"I'm helping to build a service for my company that allows users to log in, give our application permission to retrieve data from an API for which they have an account and then warehouse said data for our users automatically.

We don't plan for many users yet obviously, but when more users jump on board, we'll need to make sure that we aren't overloading a single task node (the client facing web app will be separated from the task node actually performing the data extraction and loading). Currently using Azure for what we've built so far.

Any good blueprints out there with reasonable complexity? All recommendations welcome!",True,False,False,dataengineering,t5_36en4,48693,public,self,Scalable Cloud SaaS Approaches and Tools,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rxgr5e/scalable_cloud_saas_approaches_and_tools/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Born-Comment3359,,,[],,,,text,t2_5t56uq7x,False,False,False,[],False,False,1641481119,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rxgi3b/is_python_and_sql_enough_to_get_my_first_data/,{},rxgi3b,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rxgi3b/is_python_and_sql_enough_to_get_my_first_data/,False,,,6,1641481129,1,"I know Python and SQL, but I saw a lot of JDs requiring a lot more stuff. I am not sure if Python and SQL will be enough for my first job?",True,False,False,dataengineering,t5_36en4,48692,public,self,Is Python and SQL enough to get my first data engineering job with decent pay?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rxgi3b/is_python_and_sql_enough_to_get_my_first_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,caksters,,,[],,,,text,t2_tux1p,False,False,False,[],False,False,1641480521,youtu.be,https://www.reddit.com/r/dataengineering/comments/rxga7s/what_is_engineering_for_data_how_should_we_manage/,{},rxga7s,False,True,False,False,False,True,False,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rxga7s/what_is_engineering_for_data_how_should_we_manage/,False,rich:video,"{'enabled': False, 'images': [{'id': 'B6sji03N_A8V-cmreFXy5CHYH08m5BlZ_EDYxTonsUs', 'resolutions': [{'height': 81, 'url': 'https://external-preview.redd.it/6FW83eZN5hfW_n2kmBHZb6H2mguLevavnIpS6E2TVv0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=22be37e5982cc01c88fd1d05a6341afe4b39a310', 'width': 108}, {'height': 162, 'url': 'https://external-preview.redd.it/6FW83eZN5hfW_n2kmBHZb6H2mguLevavnIpS6E2TVv0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=121b9b44997c264f67548fa754f800e8c15c162d', 'width': 216}, {'height': 240, 'url': 'https://external-preview.redd.it/6FW83eZN5hfW_n2kmBHZb6H2mguLevavnIpS6E2TVv0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=eda63a05bf250e5d646016a7b12ec7328c55ef3f', 'width': 320}], 'source': {'height': 360, 'url': 'https://external-preview.redd.it/6FW83eZN5hfW_n2kmBHZb6H2mguLevavnIpS6E2TVv0.jpg?auto=webp&amp;s=2fab2efa2f15a0d0fce2a284e5c56843912ee35e', 'width': 480}, 'variants': {}}]}",6,1641480531,1,,True,False,False,dataengineering,t5_36en4,48692,public,https://b.thumbs.redditmedia.com/OBOiDEqcKxLXQy4_QyXsZOSbwyJDEJCSWD6LMWtG24U.jpg,What Is Engineering For Data? How should we manage data at scale?,0,[],1.0,https://youtu.be/jeMQld-6BtQ,all_ads,6,"{'oembed': {'author_name': 'Continuous Delivery', 'author_url': 'https://www.youtube.com/c/ContinuousDelivery', 'height': 200, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/jeMQld-6BtQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/jeMQld-6BtQ/hqdefault.jpg', 'thumbnail_width': 480, 'title': 'What Is Engineering For Data?', 'type': 'video', 'version': '1.0', 'width': 356}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/jeMQld-6BtQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'scrolling': False, 'width': 356}",,"{'oembed': {'author_name': 'Continuous Delivery', 'author_url': 'https://www.youtube.com/c/ContinuousDelivery', 'height': 200, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/jeMQld-6BtQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/jeMQld-6BtQ/hqdefault.jpg', 'thumbnail_width': 480, 'title': 'What Is Engineering For Data?', 'type': 'video', 'version': '1.0', 'width': 356}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/jeMQld-6BtQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/rxga7s', 'scrolling': False, 'width': 356}",105.0,140.0,https://youtu.be/jeMQld-6BtQ,,,,,,,,,,
[],False,askvinni,,,[],,,,text,t2_681y9mz2,False,False,False,[],False,False,1641479673,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rxfzi8/testing_candidates_on_thinking_like_a_data_person/,{},rxfzi8,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rxfzi8/testing_candidates_on_thinking_like_a_data_person/,False,,,6,1641479683,1,"Not my best work in the title but hear me out.

I understand that ""thinking like a data person"" isn't a very concrete soft skill to test for. Hell, I'd be hard pressed to define what it means exactly, but I'm hoping people in this sub will intuitively know what I mean by that. It's a mixture of critical thinking, working with ambiguity and incomplete information, and having a ""feel"" for possible issues arising in a data model.

As the market for candidates in the data space is quite small, we got a few very interesting applications from software engineers looking to move into BI Development and Data Engineering. It goes without saying that we're hiring for attitude and planning to invest some time in whomever we hire to get them up to speed, but since I never really hired this type of profile, I wanted to ask you about some of the questions you like to ask during the interview process and general interview strategies for these types of candidates.

Looking forward to the discussion arising from this!",True,False,False,dataengineering,t5_36en4,48691,public,self,"Testing candidates on ""thinking like a data person""",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rxfzi8/testing_candidates_on_thinking_like_a_data_person/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,avkmal,,,[],,,,text,t2_4qkusspa,False,False,False,[],False,False,1641472830,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rxdob6/unit_test_for_databricks/,{},rxdob6,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rxdob6/unit_test_for_databricks/,False,,,6,1641472840,1,"Hello data people! Anyone here using Databricks for their ETL pipeline? Did you write unit test for your code? If yes, how? I've only heard about Databricks connect. Is the best solution for unit testing in Databricks?",True,False,False,dataengineering,t5_36en4,48687,public,self,Unit test for Databricks,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rxdob6/unit_test_for_databricks/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,acp_2402,,,[],,,,text,t2_83y3qbvd,False,False,False,[],False,False,1641472439,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rxdk9d/can_we_copy_bak_file_to_sql_database_table_in/,{},rxdk9d,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rxdk9d/can_we_copy_bak_file_to_sql_database_table_in/,False,,,6,1641472450,1," I want to know if I can copy '.bak' (Backup dump) file to SQL Database table in Azure Data Factory without using SQL server management studio. If Yes, then how ? If No, what are the possible reasons?",True,False,False,dataengineering,t5_36en4,48686,public,self,Can we copy .bak file to SQL Database table in Azure Data Factory ?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rxdk9d/can_we_copy_bak_file_to_sql_database_table_in/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,AMGraduate564,,,[],,,,text,t2_57j4x5fp,False,False,False,[],False,False,1641469457,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rxco94/data_modelling_for_data_lakehouse_enterprisewide/,{},rxco94,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rxco94/data_modelling_for_data_lakehouse_enterprisewide/,False,,,6,1641469467,1,"The concept of the Enterprise Data Warehouse model got popularity in the good-ole days when everything was Database-based (can only process Structured data). Now we have Data Lakes and more importantly, Data Lakehouse (concept by Databricks), which is a flat file-based workflow (can process Structured/Semi-structured/Unstructured data).

What is the standard to follow for modeling the data for OLAP in a Data Lakehouse?",True,False,False,dataengineering,t5_36en4,48685,public,self,Data Modelling for Data Lakehouse: Enterprise-wide Dimensional model or No model needed at all?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rxco94/data_modelling_for_data_lakehouse_enterprisewide/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,ChrisWro,,,[],,,,text,t2_f1wodej1,False,False,False,[],False,False,1641468966,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rxcjjd/databricks_cluster_capacity/,{},rxcjjd,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rxcjjd/databricks_cluster_capacity/,False,,,6,1641468977,1,"Hello  
I'm looking for examples of how to properly configure Databricks cluster capacity (which machines chose for workers and driver, how many workers, etc). I have a typical batch job: I load data from CSV files and merge them into Delta tables.

I have the info on how many tables I will process, what is the size of input CSV files. I use Databricks API to launch a job with a fixed number of workers, in a job I run concurrent notebooks,  
and I want to precisely choose the right number of needed workers. 

Thanks in advance",False,False,False,dataengineering,t5_36en4,48684,public,self,Databricks cluster capacity,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rxcjjd/databricks_cluster_capacity/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,thecrixus,,,[],,,,text,t2_286kcv5s,False,False,False,[],False,False,1641468582,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rxcfgj/updating_database_when_an_excel_file_is_changed/,{},rxcfgj,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rxcfgj/updating_database_when_an_excel_file_is_changed/,False,,,6,1641468593,1,"Hi guys, for a personal project I'm manually maintaining an excel file where I insert/delete rows occasionally and it's tiring to restart a new database each time.

How would I automate this process such that the database gets updated whenever I make a change to the excel file? My database is MongoDB cloud.",True,False,False,dataengineering,t5_36en4,48684,public,self,Updating database when an Excel file is changed,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rxcfgj/updating_database_when_an_excel_file_is_changed/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Valkyrja-Kara,,,[],,,,text,t2_hcejskpo,False,False,False,[],False,False,1641468046,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rxca5d/salesforce_sandbox_config_for_de/,{},rxca5d,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rxca5d/salesforce_sandbox_config_for_de/,False,,,6,1641468057,1,Are there any specific configuration changes or settings that need to be made when setting up a full sandbox for integration and warehousing development and testing?,True,False,False,dataengineering,t5_36en4,48684,public,self,Salesforce Sandbox Config for DE,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rxca5d/salesforce_sandbox_config_for_de/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,PuzzleheadedPapaya9,,,[],,,,text,t2_3x0u1jtz,False,False,False,[],False,False,1641466253,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rxbs62/learning_scala/,{},rxbs62,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rxbs62/learning_scala/,False,,,6,1641466264,1,"Hi there,
I've been looking through data engineering vacancies on the internet and quite a few mention Scala as a preferred skill. I know a fair bit of Python but I don't have experience with Scala or any other jvm languages. I heard it's supposed to be beneficial for performance in some areas of data engineering. Do you guys know when and why Scala would be preferred over Python and also how I would go about learning Scala? I'd like to learn while doing a project but I'm not sure what kinds of projects are doable for a Scala beginner and what specific kinds of things it is used for.",True,False,False,dataengineering,t5_36en4,48682,public,self,Learning Scala,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rxbs62/learning_scala/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,borjavb,,,[],,,,text,t2_90dc4r5o,False,False,False,[],False,False,1641462079,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rxaovk/column_level_lineage_with_dbt_in_bigquery/,{},rxaovk,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rxaovk/column_level_lineage_with_dbt_in_bigquery/,False,self,"{'enabled': False, 'images': [{'id': 'sNKrflqkLOUhkpEuR7fiqYT0tFmDEnSCqhmM-ZRggOo', 'resolutions': [{'height': 68, 'url': 'https://external-preview.redd.it/QchdW46i64S1ilcIa00mveUm8i0zQrFog7E2KJVIRdM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4161e2c12e4f5330658335bf7253ada22bd9d875', 'width': 108}, {'height': 137, 'url': 'https://external-preview.redd.it/QchdW46i64S1ilcIa00mveUm8i0zQrFog7E2KJVIRdM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ef932e738f603295131842ab423849678d5899f0', 'width': 216}, {'height': 203, 'url': 'https://external-preview.redd.it/QchdW46i64S1ilcIa00mveUm8i0zQrFog7E2KJVIRdM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=869e86ed0fabf5d7c06a2e12de76a783243132e9', 'width': 320}, {'height': 406, 'url': 'https://external-preview.redd.it/QchdW46i64S1ilcIa00mveUm8i0zQrFog7E2KJVIRdM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=42bf152bfe2ea63dae535ffb87bcff429492a959', 'width': 640}, {'height': 610, 'url': 'https://external-preview.redd.it/QchdW46i64S1ilcIa00mveUm8i0zQrFog7E2KJVIRdM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=609594c1eb1f7df4feed81240baac99715a34b57', 'width': 960}, {'height': 686, 'url': 'https://external-preview.redd.it/QchdW46i64S1ilcIa00mveUm8i0zQrFog7E2KJVIRdM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=af078f334f358f8caa0788bc6667bb01e40829f0', 'width': 1080}], 'source': {'height': 763, 'url': 'https://external-preview.redd.it/QchdW46i64S1ilcIa00mveUm8i0zQrFog7E2KJVIRdM.jpg?auto=webp&amp;s=8a6e0007fa95ddfc18c9279d4dd846d13bbb61aa', 'width': 1200}, 'variants': {}}]}",6,1641462097,1,"[I've recently written a post](https://medium.com/data-monzo/mapping-our-data-journey-with-column-lineage-56209c00606d) about how we are using ZetaSQL to extract column level lineage in BigQuery, using the compiled models from dbt. I'll try to write in the upcoming weeks a more technical post around ZetaSQL and how to use it to parse SQL sentences.

[https://medium.com/data-monzo/mapping-our-data-journey-with-column-lineage-56209c00606d](https://medium.com/data-monzo/mapping-our-data-journey-with-column-lineage-56209c00606d)

https://preview.redd.it/no9pm2kgf1a81.png?width=2000&amp;format=png&amp;auto=webp&amp;s=3d00121404f6d686b8f37923a7e4ea445c6dcba0",True,False,False,dataengineering,t5_36en4,48680,public,https://b.thumbs.redditmedia.com/NUhcM7Hi3vEbJqpuyc1X6G2PMF8A2npl8OqhPwTWpxI.jpg,Column level lineage with dbt in BigQuery,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rxaovk/column_level_lineage_with_dbt_in_bigquery/,all_ads,6,,,,,,89.0,140.0,,"{'no9pm2kgf1a81': {'e': 'Image', 'id': 'no9pm2kgf1a81', 'm': 'image/png', 'p': [{'u': 'https://preview.redd.it/no9pm2kgf1a81.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7b42559f269f3b15a3cb36e97f632f3da388d32f', 'x': 108, 'y': 68}, {'u': 'https://preview.redd.it/no9pm2kgf1a81.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ac35102e71b70e32ba81c0fb9124ae5a6de2401f', 'x': 216, 'y': 137}, {'u': 'https://preview.redd.it/no9pm2kgf1a81.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6d28d3f1689597c7c59946ed244ce20d698be914', 'x': 320, 'y': 203}, {'u': 'https://preview.redd.it/no9pm2kgf1a81.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6dab73b36b0bbdc9f7daf463464e048543f5c0c7', 'x': 640, 'y': 407}, {'u': 'https://preview.redd.it/no9pm2kgf1a81.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1c79bbf4ecbf4fff17a5bb5bcc428a9314fc012d', 'x': 960, 'y': 611}, {'u': 'https://preview.redd.it/no9pm2kgf1a81.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=39e0dbfa78f6245868573ea1f3601729aaf5982b', 'x': 1080, 'y': 687}], 's': {'u': 'https://preview.redd.it/no9pm2kgf1a81.png?width=2000&amp;format=png&amp;auto=webp&amp;s=3d00121404f6d686b8f37923a7e4ea445c6dcba0', 'x': 2000, 'y': 1273}, 'status': 'valid'}}",,,,,,,,,
[],False,thistoowillbedeleted,,,[],,,,text,t2_i488ipxy,False,False,False,[],False,False,1641461579,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rxaknd/is_there_a_generic_dag_builder/,{},rxaknd,False,False,False,False,False,False,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rxaknd/is_there_a_generic_dag_builder/,False,,,6,1641461590,1,[removed],True,False,False,dataengineering,t5_36en4,48679,public,self,Is there a generic DAG builder?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rxaknd/is_there_a_generic_dag_builder/,all_ads,6,,,automod_filtered,,,,,,,,,,,,,,,
[],False,tehgalvanator,,,[],,,,text,t2_flpr4,False,False,False,[],False,False,1641460008,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rxa6me/career_in_de_with_mis/,{},rxa6me,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rxa6me/career_in_de_with_mis/,False,,,6,1641460047,1,"College senior here, I’ll be graduating with a degree in Business Administration: Management Information Systems concentration this summer. I took a Big Data class and really enjoyed wrangling a Yelp dataset. I recently found out about DE and I’d like to learn some more about the career. What can I do to increase my chances of landing a job in DE? I’ve taken a class in Big Data and Database Management Systems. 

Here is a link to my Big Data group project if anyone is curious to check it out: https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/3479084692020769/2306451512185921/2148413392763171/latest.html",True,False,False,dataengineering,t5_36en4,48679,public,self,Career in DE with MIS?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rxa6me/career_in_de_with_mis/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,No_Substance_9411,,,[],,,,text,t2_9vkf58ou,False,False,False,[],False,False,1641456545,i.redd.it,https://www.reddit.com/r/dataengineering/comments/rx9b99/true_or_false/,{},rx9b99,False,True,False,False,True,True,False,False,#ff66ac,[],dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1,Meme,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rx9b99/true_or_false/,False,image,"{'enabled': True, 'images': [{'id': 'ar7ZsMNu_If4M7e5LWgiYsQqQ05tcYakOzmWIEtPrl0', 'resolutions': [{'height': 156, 'url': 'https://preview.redd.it/xfc6ydijz0a81.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=97b37b95e7d15e15089078a08cf4a6464c471bf0', 'width': 108}, {'height': 312, 'url': 'https://preview.redd.it/xfc6ydijz0a81.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7faaafd95f490623146195234400ca1c0a551731', 'width': 216}, {'height': 462, 'url': 'https://preview.redd.it/xfc6ydijz0a81.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=905b1c6170a50c5a2b7126d3d4f6b2e9af933e0d', 'width': 320}, {'height': 925, 'url': 'https://preview.redd.it/xfc6ydijz0a81.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6e42233588a17ed9a6bd79018041df6f7a8b831f', 'width': 640}, {'height': 1388, 'url': 'https://preview.redd.it/xfc6ydijz0a81.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e271c628c68cdd92555d1e4a54cef19d327a14d5', 'width': 960}, {'height': 1562, 'url': 'https://preview.redd.it/xfc6ydijz0a81.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f44d2d68dd4babdd8df85494a4df23112260f344', 'width': 1080}], 'source': {'height': 1562, 'url': 'https://preview.redd.it/xfc6ydijz0a81.jpg?auto=webp&amp;s=181bea99eb0fc2a5ee6452385cfe9c41e54ca21e', 'width': 1080}, 'variants': {}}]}",6,1641456556,1,,True,False,False,dataengineering,t5_36en4,48676,public,https://b.thumbs.redditmedia.com/XQiIZmWGAr0k_Ck9fFkSQPDw4_9oRQ-SZe23kCW4Y4k.jpg,True or false?,0,[],1.0,https://i.redd.it/xfc6ydijz0a81.jpg,all_ads,6,,,,,,140.0,140.0,https://i.redd.it/xfc6ydijz0a81.jpg,,,,,,,,,,
[],False,GreedyCourse3116,,,[],,,,text,t2_991xw1st,False,False,False,[],False,False,1641453638,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rx8k98/please_guide_me_for_interview_study_material_i_am/,{},rx8k98,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rx8k98/please_guide_me_for_interview_study_material_i_am/,False,,,6,1641453649,1,"I was a Software Developer. I worked as a pseudo Data Engineer at my last job (did batch streaming python ETL scripts) but now I am moving to make a career in Data Engineering. At this moment, I have searched numerous articles online and I am overwhelmed on how to prepare for the interviews. So far according to my understanding, I need to get hands-on: 

1. Python
2. SQL
3. Data Modeling
4. Data Warehousing
5. Data Pipeline - Batch and Stream
6. Distributed System Fundamentals
7. System Design
8. Behavioral
9. Any major topic I missed ?

It can take months if I dive deep in all of the above sections. I am unemployed and I want to get a job sooner than later. 

I am preparing for 1, 2 and 8th point so far but how to find sufficient resources on rest of the points? Each book can take weeks to complete, should I target watching YouTube/Udemy videos instead? 

Please, I request, please someone guide me properly to ace interviews. I have been unemployed since pandemic started. I can commit more than 12 hours of studying and I want to crack interviews.",True,False,False,dataengineering,t5_36en4,48672,public,self,Please guide me for interview study material. I am extremely overwhelmed.,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rx8k98/please_guide_me_for_interview_study_material_i_am/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,brownstrom,,,[],,,,text,t2_3tsn4xyv,False,False,False,[],False,False,1641447851,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rx6xz2/scheduling_airflow_job_to_run_on_a_particular_day/,{},rx6xz2,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rx6xz2/scheduling_airflow_job_to_run_on_a_particular_day/,False,,,6,1641447862,1,"I tried looking this up online but couldn't find any concrete answer, hence posting it here. 

I am trying to run my DAG on every Wednesday at midnight and am not sure how to schedule that, as the options are only, once, weekly, monthly etc.

    with DAG(
 dag_id=""test_dag"",
 schedule_interval=""@every_wednesday"",
 default_args=default_args,
 start_date=datetime(2022, 1, 1),
 catchup=False,
)",True,False,False,dataengineering,t5_36en4,48666,public,self,Scheduling airflow job to run on a particular day,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rx6xz2/scheduling_airflow_job_to_run_on_a_particular_day/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,William_James001,,,[],,,,text,t2_i9emq64m,False,False,False,[],False,False,1641447255,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rx6rjv/where_to_get_data_for_the_it_exam_dump_suggest/,{},rx6rjv,False,False,False,False,False,False,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rx6rjv/where_to_get_data_for_the_it_exam_dump_suggest/,False,,,6,1641447266,1,[removed],True,False,False,dataengineering,t5_36en4,48666,public,self,"Where to get Data for the IT Exam dump? Suggest, please",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rx6rjv/where_to_get_data_for_the_it_exam_dump_suggest/,all_ads,6,,,reddit,,,,,,,,,,,,,,,
[],False,Analyst4Hire,,,[],,,,text,t2_fpcx8smb,False,False,False,[],False,False,1641436154,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rx39ez/first_data_engineer_job_how_should_i_prep/,{},rx39ez,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rx39ez/first_data_engineer_job_how_should_i_prep/,False,,,6,1641436165,1,"I'm coming from an Analyst role, I'm quiet competent in SQL and I'm able to feel my way through the dark with python. What prep should I focus on to hit the ground running day one?

&amp;#x200B;

I'm really cool with any opinions or suggestions, thank you!",True,False,False,dataengineering,t5_36en4,48653,public,self,First Data Engineer Job how should I prep,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rx39ez/first_data_engineer_job_how_should_i_prep/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,MissEliseCecilia,,,[],,,,text,t2_1k9tjv4,False,False,False,[],False,False,1641429699,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rx1naj/should_i_continue_with_an_interview_for_a_job_i/,{},rx1naj,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rx1naj/should_i_continue_with_an_interview_for_a_job_i/,False,,,6,1641429709,1,"
I am currently a sales analyst trying to transition to data engineering. Things are going well in an interview with a large company, but after a glowing first round they revealed 1. The position would be hybrid, forcing me to move to a HCOL city and 2. The salary and all benefits (bonus and 401k match) would be only a modest raise from what I make now when factoring in cost of living increases. She was resistant to negotiation.

The recruiter set up one more technical interview. I’m considering still going ahead to get practice for data engineering interviews , but is this very rude or a bad idea?",True,False,False,dataengineering,t5_36en4,48649,public,self,Should I continue with an interview for a job I know I won’t take?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rx1naj/should_i_continue_with_an_interview_for_a_job_i/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,cpardl,,,[],,,,text,t2_fb1s1pke,False,False,False,[],False,False,1641427040,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rx0nci/a_deep_dive_to_our_data_stack_architecture/,{},rx0nci,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rx0nci/a_deep_dive_to_our_data_stack_architecture/,False,self,"{'enabled': False, 'images': [{'id': 'fEIE9eRqnuo67k2iRMf-WtZ_LDl0mRuafgDdNLm7hNE', 'resolutions': [{'height': 51, 'url': 'https://external-preview.redd.it/ipnVJHKLbfInOwvhshqrgnFSE9IVDYpkuYcKz_tkT8c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=957f965bc76d0b47b3faee66f45da3855a8333c8', 'width': 108}, {'height': 102, 'url': 'https://external-preview.redd.it/ipnVJHKLbfInOwvhshqrgnFSE9IVDYpkuYcKz_tkT8c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ef6449d4b6b2cabef4e424a1227895b3fbc816ea', 'width': 216}, {'height': 151, 'url': 'https://external-preview.redd.it/ipnVJHKLbfInOwvhshqrgnFSE9IVDYpkuYcKz_tkT8c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=10a94c196a5a6d4824926083cca6120fd57cec87', 'width': 320}, {'height': 302, 'url': 'https://external-preview.redd.it/ipnVJHKLbfInOwvhshqrgnFSE9IVDYpkuYcKz_tkT8c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c9180dddcfaab9d996e08df24c190dd59bd914f1', 'width': 640}], 'source': {'height': 355, 'url': 'https://external-preview.redd.it/ipnVJHKLbfInOwvhshqrgnFSE9IVDYpkuYcKz_tkT8c.jpg?auto=webp&amp;s=6062d544a75e50ec60ccb9c5d5d8a0d48ffa14de', 'width': 750}, 'variants': {}}]}",6,1641427050,1,"Hey everyone.

We wrote a post, as part of a series of posts, about our data stack. It includes some dogfooding for obvious reasons and hopefully not a lot of inevitable bias. But I believe it will be helpful for anyone who's interested in designing/implementing a data stack.

You can find the post here: [https://rudderstack.com/blog/rudderstacks-data-stack-deep-dive](https://rudderstack.com/blog/rudderstacks-data-stack-deep-dive)

Feedback is more than welcome and happy to answer any questions.",True,False,False,dataengineering,t5_36en4,48644,public,self,"A deep dive to our data stack architecture, including a healthy dose of dogfooding",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rx0nci/a_deep_dive_to_our_data_stack_architecture/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,ambiguousoul,transparent,,[],fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b,Data Engineer,dark,text,t2_6lo9b75x,False,False,False,[],False,False,1641419577,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rwxrrd/data_virtualizationdenodo/,{},rwxrrd,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rwxrrd/data_virtualizationdenodo/,False,,,6,1641419588,1,"My work primarily involves this platform and the integrations around it (Data sources/ business applications/ authentication/ data integration etc.,). Through my work,  I am able to understand what data engineering is all about and I find it interesting so far. To give an idea about what my day-to-day work looks like; customer support/ consulting / handling cases/ troubleshooting calls / architectural suggestions/ possibilities; 

I want to understand how I should plan my career (2+ YOE),

1. How is Denodo seen in the DE circle and how widely is it used?
2.Will sticking to the same product/ technology have an impact later?
3. As I am from a non-coding background (comfortable with anything SQL related), what are the other skills that i should acquire to stay relevant in the DE domain?
4. In my case, the product is already built with all the features/ functionalities related to DE and I'm able to understand what's happening. But in this group, i see people talking about doing the extraction/ integration/ building the pipelines all by themselves using other tools/ softwares. I want to understand what type of/ specific coding skills will be required to move in the direction?

Any response will be a great help and i appreciate it!",True,False,False,dataengineering,t5_36en4,48635,public,self,Data virtualization-Denodo,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rwxrrd/data_virtualizationdenodo/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,rudboi12,,,[],,,,text,t2_4s2dogl9,False,False,False,[],False,False,1641415129,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rwwcf1/de_vs_debi/,{},rwwcf1,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rwwcf1/de_vs_debi/,False,,,6,1641415140,1,"I’m currently doing a masters in DS in europe and working as a DE intern at a mid-size tech company (300ish employees) and I’m considering a move to a better brand tech company as a DE-BI —-top startup with around 3k employees. Salary and perks are basically identical.

Only diference is the DE role. While both are in Data Management team, the one I’m at is way more technical than the DE-BI role. I’ve heard that some FANG have this kind of DE-BI role where you are basically a BI-Engineer since you work daily with dashboards and BI tools. In my current role we don’t do any BI, just pure DE with scala. 

My goal eventually is to become a MLE after some years of DE experience. Another thing to consider is that my company DS team is nothing compared to the startup i could join. The most we do is few recommenders and search optimization. The startup have an inhouse research team building crazy stuff with chatbots and RL, and other SOTA applications. 

Which one should I choose? I’m leaning towards the startup but don’t know if I’ll get enough technical experience to be able to transition to MLE in the same company.",True,False,False,dataengineering,t5_36en4,48629,public,self,DE vs DE-BI,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rwwcf1/de_vs_debi/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,therealiota,,,[],,,,text,t2_1oew8nd4,False,False,False,[],False,False,1641406328,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rwt4mo/best_book_to_learn_about_data_warehouse_and_data/,{},rwt4mo,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rwt4mo/best_book_to_learn_about_data_warehouse_and_data/,False,,,6,1641406339,1,"I have an interview in a few weeks for Data Engineering role. I haven’t worked with Data Warehousing yet. 

Additionally, I want to grasp more knowledge on developing data model for random situations like carpooling or bookstore etc. They will give me a situation and I would have to design its data model. 

I want to prepare and possibly books are a viable solution. 

Please recommend books for the same. 

Also if you have another recommendation on what to study or prepare, please enlighten me. So far I have practiced Python, SQL, designing pipeline techniques . 

Thank you!",True,False,False,dataengineering,t5_36en4,48619,public,self,Best book to learn about Data Warehouse and Data Modeling ?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rwt4mo/best_book_to_learn_about_data_warehouse_and_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,ckdatanerd,,,[],,,,text,t2_985ca477,False,False,False,[],False,False,1641401635,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rwrc52/how_hard_is_it_to_learn_aws_if_you_know_azure/,{},rwrc52,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rwrc52/how_hard_is_it_to_learn_aws_if_you_know_azure/,False,,,6,1641401645,1,"I have been working as a Data Engineer for 7 months now and I really like it. My company mainly uses Azure, so I lack exposure to AWS. I’m starting to slowly apply to new jobs, and I’m wondering what I should focus on learning in AWS, and how hard it will be. I taught myself (or learned from my boss) everything I know in Azure, but it is pretty user friendly so it wasn’t that hard. Does anybody have any suggestions on the best way to start learning important AWS services for a Data Engineer? Thanks!",True,False,False,dataengineering,t5_36en4,48615,public,self,How hard is it to learn AWS if you know Azure?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rwrc52/how_hard_is_it_to_learn_aws_if_you_know_azure/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,m123av,,,[],,,,text,t2_c6m5h81g,False,False,False,[],False,False,1641400566,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rwqxzy/is_the_purdue_pg_course_in_data_engineering_from/,{},rwqxzy,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rwqxzy/is_the_purdue_pg_course_in_data_engineering_from/,False,,,6,1641400577,1,Is the Purdue PG course in data engineering from Simplilearn any good?,True,False,False,dataengineering,t5_36en4,48615,public,self,Is the Purdue PG course in data engineering from Simplilearn any good?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rwqxzy/is_the_purdue_pg_course_in_data_engineering_from/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,ibgeek,,,[],,,,text,t2_32ect,False,False,True,[],False,False,1641400540,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rwqxol/thoughts_on_managing_independent_processes/,{},rwqxol,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rwqxol/thoughts_on_managing_independent_processes/,False,,,6,1641400550,1,"I have a system in which discrete event notifications are received for millions of users but relatively little data per user.  Each event message is tagged with the user.  When we process the events, we group them by user and our downstream analyses calculates features for each user for ML models.  We need to pull from other data sources like key-value stores to augment the events with additional data but the data is completely partitioned by user through the whole process.

I am currently considering having the raw event data be ingested by Kafka, partitioned based on a hash of the user id.  This would allow us to handle the data processing by running a single, independent process per Kafka partition.  (Think Kafka streaming.)

I am curious if anybody knows of any platforms that are good for deploying and managing the state of a large number of independent processes as a group.  Similar to grid schedulers like Sun Grid Engine from back in the day, I want to be able to say go execute 128 processes (say packaged as Docker images) on the cluster.  But these are not just going to run once -- they will run continuously.  If a process fails, I want the system to restart it and notify me.  I want to be able to check the status of the processes.

I'm hesitant to use something like Spark because it seems better suited for a small number of large data sets.  We don't need the ability to join across partitions and explicitly want to avoid enabling that.

Does anybody have similar use cases?  Any recommendations?  TIA!",True,False,False,dataengineering,t5_36en4,48615,public,self,Thoughts on managing independent processes,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rwqxol/thoughts_on_managing_independent_processes/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Born-Comment3359,,,[],,,,text,t2_5t56uq7x,False,False,False,[],False,False,1641393935,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rwoj6q/switching_to_de_career_any_advice_would_be/,{},rwoj6q,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rwoj6q/switching_to_de_career_any_advice_would_be/,False,,,6,1641393945,1,"Hi, I have 2 years of statistical programming experience in pharma, but I am thinking about switching to DE, cuz I feel like statistical programmers are being paid waaay less than data engineers, for example in my country stat programmers receive 20k USD anually on average, while data engineers can get 40-50k USD anually on average. So, I know Python/SQL pretty well (I don't have industry experience for them though), would you advice to stay in the field I am currently in or I'd better switch to DE? My main concern is that, although I have programming experience and I know Python/SQL, no one will hire me as a DE because I don't have relevant experience with it, and I will end up with nothing. 

Any advice??

Thanks.",True,False,False,dataengineering,t5_36en4,48609,public,self,"Switching to DE career, any advice would be appreciated.",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rwoj6q/switching_to_de_career_any_advice_would_be/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,WhiskeeFrank,,,[],,,,text,t2_6rkm9o7,False,False,False,[],False,False,1641393178,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rwo9ux/is_this_course_a_good_way_of_getting_an/,{},rwo9ux,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rwo9ux/is_this_course_a_good_way_of_getting_an/,False,self,"{'enabled': False, 'images': [{'id': 'e__h3bjnc3W4FRNvSa1WogPxxdPDiEiVg9toQeIgHtY', 'resolutions': [{'height': 60, 'url': 'https://external-preview.redd.it/ZSRiRaXfODVUXt__5V4UeKLp9I8DEx8cRf6u04vpUU0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ed6e3a4cf04902681cd0cb9f87a06e3963450df2', 'width': 108}, {'height': 121, 'url': 'https://external-preview.redd.it/ZSRiRaXfODVUXt__5V4UeKLp9I8DEx8cRf6u04vpUU0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8269614916282b3b224a9a808d4f503f01bc0fed', 'width': 216}, {'height': 180, 'url': 'https://external-preview.redd.it/ZSRiRaXfODVUXt__5V4UeKLp9I8DEx8cRf6u04vpUU0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=85bcc95775056e5e2567079d1162e3cdb21ec5fb', 'width': 320}, {'height': 360, 'url': 'https://external-preview.redd.it/ZSRiRaXfODVUXt__5V4UeKLp9I8DEx8cRf6u04vpUU0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fadbe20df7cf00bf92413cd34f41482b32736d9a', 'width': 640}, {'height': 540, 'url': 'https://external-preview.redd.it/ZSRiRaXfODVUXt__5V4UeKLp9I8DEx8cRf6u04vpUU0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=eea5fbe0792ef70fd546dd70e5f47217bdb43912', 'width': 960}, {'height': 607, 'url': 'https://external-preview.redd.it/ZSRiRaXfODVUXt__5V4UeKLp9I8DEx8cRf6u04vpUU0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=913447e581488db025ec7a65fad33ceda8cbdcc5', 'width': 1080}], 'source': {'height': 720, 'url': 'https://external-preview.redd.it/ZSRiRaXfODVUXt__5V4UeKLp9I8DEx8cRf6u04vpUU0.jpg?auto=webp&amp;s=f4cfd391ee1466f8f0d75855a3436fef10ed0689', 'width': 1280}, 'variants': {}}]}",6,1641393189,1,"Hi there,

I've been looking for ways to build up the required skills/knowledge to get an entry level DE job, and came across this course yesterday ([https://learndataengineering.com/p/academy](https://learndataengineering.com/p/academy)).

My python's pretty good, SQL's basic, and I've done a few DS and data wrangling projects at work and for personal projects.  But I don't have any practical experience in things like Docker, Kafka, Spark, MongoDB, etc.

So do you think this course would be good for me?  Has anyone done it before?  Any other suggestions/alternatives?

I think the main thing is it would give me outlines for a number of practical projects that I could adapt for my portfolio, but not sure if that's important for DE interviews..",True,False,False,dataengineering,t5_36en4,48609,public,self,Is this course a good way of getting an entry-level DE job?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rwo9ux/is_this_course_a_good_way_of_getting_an/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,No_Substance_9411,,,[],,,,text,t2_9vkf58ou,False,False,False,[],False,False,1641381964,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rwkr31/what_is_databricks_and_how_can_i_learn_it/,{},rwkr31,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rwkr31/what_is_databricks_and_how_can_i_learn_it/,False,,,6,1641381974,1,"I am currently enrolled in data engineering boot camp. We go over various technologies azure , pyspark , airflow , hoodop ,nosql,SQL, python. But not over something like databricks. I am in contact with lots of recent graduates who landed a job. Almost everyone recommend me to learn databricks. But I have no idea what it is and how to learn? There's not much good resources out there on YouTube or any other website.",True,False,False,dataengineering,t5_36en4,48596,public,self,What is databricks and how can I learn it ?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rwkr31/what_is_databricks_and_how_can_i_learn_it/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,mannu_11,,,[],,,,text,t2_3dsjwu75,False,False,False,[],False,False,1641380524,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rwkd6z/resource_for_building_data_models_and_data_marts/,{},rwkd6z,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rwkd6z/resource_for_building_data_models_and_data_marts/,False,,,6,1641380535,1,What are the best resources to learn to build data models and data marts!,True,False,False,dataengineering,t5_36en4,48595,public,self,Resource for Building Data Models and Data Marts,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rwkd6z/resource_for_building_data_models_and_data_marts/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,caksters,,,[],,,,text,t2_tux1p,False,False,False,[],False,False,1641377991,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rwjp7p/manager_left_will_be_assigned_a_new_one_how_can_i/,{},rwjp7p,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rwjp7p/manager_left_will_be_assigned_a_new_one_how_can_i/,False,,,6,1641378001,1,"I am very new to DE (&lt;1 yo and previously worked as analyst for 1 year). my previous manager left which is unfortunate for me because we got along really well. 
one reason for that was that he was very into technical side of engineering. Often during our catchups, I would show him the code that I wrote for clients projects and he would give me pointers how to do a better job, or we would talk about best engineering practices and share articles about technical data engineering topics.

Because he has left, I will be assigned a new manager. The current option is to assign the head of data engineering team as my permanent manager. My worry is that I will not get the technical feedback from him as he seems to be interested in strategy and data engineering vision for the company. Obviously nothing wrong with that, but I am worried that I will not learn as much in terms of technical skills and engineering side as I did before.

I was asked if I am happy for the head of DE to be my manager, I told them that I don’t mind. But I am really ibterested in learning the technical aspect of the job in my first years as engineer and that I am worried that I will not get the same guidance on my work as I did before.

I will have a meeting with head of DE and I would like to know how I can diplomatically address this issue with my concerns without offending anyone?",True,False,False,dataengineering,t5_36en4,48593,public,self,"Manager left, will be assigned a new one. How can I politely ask for a manager who is interested in technical side of things?",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rwjp7p/manager_left_will_be_assigned_a_new_one_how_can_i/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,saaaalut,,,[],,,,text,t2_cd04mw52,False,False,False,[],False,False,1641374500,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rwiti1/having_a_hard_time_getting_de_job_should_i_aim/,{},rwiti1,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rwiti1/having_a_hard_time_getting_de_job_should_i_aim/,False,,,6,1641374510,1,"I tried making personal projects and stuff, but everywhere job experience is a requirement even for internship So I was thinking should I look for a data analyst or back end dev job. I was confused which kinda of profile's skills will help when I ll later eventually transition in data engineering. Your help would be appreciated thanks!",True,False,False,dataengineering,t5_36en4,48586,public,self,"Having a hard time getting DE job, should I aim for a different position for my first job?",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rwiti1/having_a_hard_time_getting_de_job_should_i_aim/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Outside_Step8626,,,[],,,,text,t2_axrouatp,False,False,False,[],False,False,1641368047,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rwh8f8/has_someone_tried_this/,{},rwh8f8,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rwh8f8/has_someone_tried_this/,False,,,6,1641368057,1,It is something in lines with any source to any sink. There is no proper documentation related to its architecture.,True,False,False,dataengineering,t5_36en4,48580,public,self,Has someone tried this https://github.com/apache/incubator-seatunnel ?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rwh8f8/has_someone_tried_this/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,sharmaniti437,,,[],,,,text,t2_p3jl6tq,False,False,False,[],False,False,1641366707,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rwgw65/big_data_engineering_a_quick_guide_to_a_coveted/,{},rwgw65,False,False,False,False,False,False,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rwgw65/big_data_engineering_a_quick_guide_to_a_coveted/,False,self,"{'enabled': False, 'images': [{'id': 'RD18Z9YV1-H39PW3MadtGXpAiXXKgyH1n-ZCIuLHkIY', 'resolutions': [{'height': 61, 'url': 'https://external-preview.redd.it/Pg9iT4dUFNdw1oyvxEnG0YfU_CHnsn2pmlzIWiN_TGk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0188218221f86e20a9e9876c95a5b01d7f1e38e8', 'width': 108}, {'height': 123, 'url': 'https://external-preview.redd.it/Pg9iT4dUFNdw1oyvxEnG0YfU_CHnsn2pmlzIWiN_TGk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7a5ff5030e64b9fa5642cf099c615ec759e6d150', 'width': 216}, {'height': 182, 'url': 'https://external-preview.redd.it/Pg9iT4dUFNdw1oyvxEnG0YfU_CHnsn2pmlzIWiN_TGk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=462aa5f3244682dc7786704e2173d75f011fd895', 'width': 320}, {'height': 365, 'url': 'https://external-preview.redd.it/Pg9iT4dUFNdw1oyvxEnG0YfU_CHnsn2pmlzIWiN_TGk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=27ed16b6eade3322cb1d84263676a877a56d76bd', 'width': 640}], 'source': {'height': 400, 'url': 'https://external-preview.redd.it/Pg9iT4dUFNdw1oyvxEnG0YfU_CHnsn2pmlzIWiN_TGk.jpg?auto=webp&amp;s=2b3a023e402d82ef1309726c07060749b4b2fd42', 'width': 700}, 'variants': {}}]}",6,1641366717,1,[removed],True,False,False,dataengineering,t5_36en4,48578,public,https://b.thumbs.redditmedia.com/oyMBRK362bPvVsyA_ni60zkRV5oojKyRDpjs0i1rACU.jpg,Big data engineering –a quick guide to a coveted career,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rwgw65/big_data_engineering_a_quick_guide_to_a_coveted/,all_ads,6,,,reddit,,,78.0,140.0,,,,,,,,,,,
[],False,prassuja,,,[],,,,text,t2_5qvnxvbx,False,False,False,[],False,False,1641365166,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rwgiau/data_engineering_entry/,{},rwgiau,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rwgiau/data_engineering_entry/,False,,,6,1641365176,1,Looking for suggestions getting into the data engineering world. I have good SQL knowledge and intermediate python skills plus pandas. What courses and what kind of projects might be best for me break into DE field.,True,False,False,dataengineering,t5_36en4,48578,public,self,Data Engineering Entry,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rwgiau/data_engineering_entry/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,DarkestKnight_23,,,[],,,,text,t2_d1v0zccf,False,False,False,[],False,False,1641357708,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rwebpm/incrementing_tables_that_use_union_queries/,{},rwebpm,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rwebpm/incrementing_tables_that_use_union_queries/,False,,,6,1641357719,1,"I have a summary table that I need to refresh on a daily basis, only problem is that the summary table is a collection of 20+ unions and it doesn't look like merge operation can be performed on union queries. Any ideas how to proceed with this? Seems to me like I'll have to truncate and perform full table inserts for now, which is fine but could get problematic once the volume of data increases.",True,False,False,dataengineering,t5_36en4,48570,public,self,Incrementing tables that use union queries,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rwebpm/incrementing_tables_that_use_union_queries/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Lost_Context8080,,,[],,,,text,t2_gtuhxwu6,False,False,False,[],False,False,1641343389,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rw9gs4/better_ways_of_iterating_through_data_frames/,{},rw9gs4,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rw9gs4/better_ways_of_iterating_through_data_frames/,False,,,6,1641343400,1,"I work as a Junior Data Engineer and much of the data we migrate to our DBs are first stored in data frames in Python. Sometimes we do things like fetch the response from an API, to migrating data across different databases (or different kinds of databases), among other things. I am wondering if there are better ways for iterating through a data frame to do a few things: 

1. Writing the data in the dataframe to a table in a database. I know that pandas has the “to_sql()” function (which appears to do this) but I am looking for other options here. 

2. Doing operations on individual records in the dataframe.

I am mainly asking these questions because I would like to have other tools/techniques under my belt other than loops, especially when working with larger datasets.

Like I said, I am a junior engineer, so please forgive me if my terminology is incorrect.",True,False,False,dataengineering,t5_36en4,48564,public,self,Better ways of iterating through data frames other than loops?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rw9gs4/better_ways_of_iterating_through_data_frames/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,SeattleDataGuy,,,[],,,,text,t2_b003dzgv,False,False,False,[],False,False,1641342213,theseattledataguy.com,https://www.reddit.com/r/dataengineering/comments/rw929w/5_data_engineering_projects_to_add_to_your_resume/,{},rw929w,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rw929w/5_data_engineering_projects_to_add_to_your_resume/,False,link,"{'enabled': False, 'images': [{'id': 'aj1ichx-QxGySWUCDeDUY_6FgO_0UAtBhJlmhoWO0rs', 'resolutions': [{'height': 72, 'url': 'https://external-preview.redd.it/tIBszq-FdOdwJcs_76kudegXdTSBmMP_G8m2PMAh_uM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c9e03a2219be5c268c6ff6e269a7608f5a53268e', 'width': 108}, {'height': 144, 'url': 'https://external-preview.redd.it/tIBszq-FdOdwJcs_76kudegXdTSBmMP_G8m2PMAh_uM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=02f288a2cb779e10a746f968ce2301eec417ba73', 'width': 216}, {'height': 213, 'url': 'https://external-preview.redd.it/tIBszq-FdOdwJcs_76kudegXdTSBmMP_G8m2PMAh_uM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0fe680c8312fd561956ac83d76c513e656b7da0f', 'width': 320}, {'height': 426, 'url': 'https://external-preview.redd.it/tIBszq-FdOdwJcs_76kudegXdTSBmMP_G8m2PMAh_uM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fa8408a5474e9aa7ee93797180ef5cc8e51312e7', 'width': 640}, {'height': 640, 'url': 'https://external-preview.redd.it/tIBszq-FdOdwJcs_76kudegXdTSBmMP_G8m2PMAh_uM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d5cd3ea4672f2be4dcf5b6674de76ebf2400518b', 'width': 960}, {'height': 720, 'url': 'https://external-preview.redd.it/tIBszq-FdOdwJcs_76kudegXdTSBmMP_G8m2PMAh_uM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=72cb28b6d5f533a1123cd108b2a8ad512cd54e4f', 'width': 1080}], 'source': {'height': 1707, 'url': 'https://external-preview.redd.it/tIBszq-FdOdwJcs_76kudegXdTSBmMP_G8m2PMAh_uM.jpg?auto=webp&amp;s=b899df25782187699c8bde80a72319d86d558488', 'width': 2560}, 'variants': {}}]}",6,1641342223,1,,True,False,False,dataengineering,t5_36en4,48563,public,https://b.thumbs.redditmedia.com/i5EzRoCxtGAizDuF7jNAXNaTUD3xCF6rcv7iQIl0xCc.jpg,5 Data Engineering Projects To Add To Your Resume - Seattle Data Guy,0,[],1.0,https://www.theseattledataguy.com/5-data-engineering-projects-to-add-to-your-resume/,all_ads,6,,,,,,93.0,140.0,https://www.theseattledataguy.com/5-data-engineering-projects-to-add-to-your-resume/,,,,,,,,,,
[],False,kabzthegang,,,[],,,,text,t2_2r6kthl,False,False,False,[],False,False,1641332534,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rw5k98/beginner_de_project/,{},rw5k98,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rw5k98/beginner_de_project/,False,self,"{'enabled': False, 'images': [{'id': 'YZEmDtQnY_O8uiqcWNfPOEDTwbliCn_Qnwwq2I30YPs', 'resolutions': [{'height': 60, 'url': 'https://external-preview.redd.it/4_htHBfx62Ivwv3fqr3LMrOI7vhggNeDLOOEhjWnRAE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=95476fe511befd272261101fe814000958c95f50', 'width': 108}, {'height': 121, 'url': 'https://external-preview.redd.it/4_htHBfx62Ivwv3fqr3LMrOI7vhggNeDLOOEhjWnRAE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2980d248751ab05c64f1c99ac649fc2089fc8441', 'width': 216}, {'height': 180, 'url': 'https://external-preview.redd.it/4_htHBfx62Ivwv3fqr3LMrOI7vhggNeDLOOEhjWnRAE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f456cd54a2c7954de625ef88f722f2a9168be9f3', 'width': 320}], 'source': {'height': 270, 'url': 'https://external-preview.redd.it/4_htHBfx62Ivwv3fqr3LMrOI7vhggNeDLOOEhjWnRAE.jpg?auto=webp&amp;s=ebcd404cb5e60face836ec78143ba999b3fd6d22', 'width': 480}, 'variants': {}}]}",6,1641332544,1,"Hi all,

I've been trying to apply for DS/DA jobs, but realized that I may be more interested in the DE side of things (back end of how to structure data together so DS/DA can draw insights from). I have experience in python and SQL, but am kind of overwhelmed. Mainly, I am stuck on where to start for a beginner DE project showcasing that I know how to use Big Data technologies (S3, RedShift, etc.). I see a lot of people drawing pipelines/plans of their project.

In addition, I've been taking this course: [https://www.udemy.com/course/aws-data-analytics/](https://www.udemy.com/course/aws-data-analytics/) to understand what the software/technologies use are, but any tips on planning, starting, &amp; executing a DE project from scratch? From a blank slate is the problem since in undergrad, I was often ""babied"" by professors &amp; TAs where the base/structure of the code is set up. Thanks a lot. I appreciate any help!",True,False,False,dataengineering,t5_36en4,48555,public,self,Beginner DE Project,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rw5k98/beginner_de_project/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Mpickett83,,,[],,,,text,t2_dwx2dz5q,False,False,False,[],False,False,1641330348,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rw4q08/policies_around_downloading_installing_software/,{},rw4q08,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rw4q08/policies_around_downloading_installing_software/,False,,,6,1641330359,1,"Question for Data Engineers in Enterprise companies, what are your policies, procedures or limitations around downloading and installing software to perform your job locally or on a corporate server?",True,False,False,dataengineering,t5_36en4,48550,public,self,Policies around downloading installing software?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rw4q08/policies_around_downloading_installing_software/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,DoctorQuinlan,,,[],,,,text,t2_y5s32,False,False,False,[],False,False,1641329996,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rw4l1i/help_need_to_give_a_work_demonstration_on_a/,{},rw4l1i,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rw4l1i/help_need_to_give_a_work_demonstration_on_a/,False,,,6,1641330007,1,"I am on another round of interviews and they would like me to bring in a work sample of something I have done in the past. Problem is that everything I have done for my current job would be a info BREACH to show them.

Any suggestions on something else I could show instead? I don't mind putting in the work and creating something new. However, I'm not even sure where to begin... maybe something in Tableau? SSIS with SQL or Power BI would also be options.

Thanks, any insight is appreciated. This is really a dream job for me and I feel close, but I don't quite know what to try to build to show them, or if I should take a tableau report from work and change text/numbers. However, most are really just plain data dumps with few visuals.",True,False,False,dataengineering,t5_36en4,48550,public,self,HELP! Need to give a work demonstration on a project for my dream job... any tips?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rw4l1i/help_need_to_give_a_work_demonstration_on_a/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,infl1ct1on,,,[],,,,text,t2_5g4le,False,False,False,[],False,False,1641327878,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rw3sdt/pipeline_help_sas7bdat_s3_dashboard/,{},rw3sdt,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rw3sdt/pipeline_help_sas7bdat_s3_dashboard/,False,,,6,1641327889,1,"I am seeking assistance with a pipeline I am envisioning. I have numerous sas7bdat files that I need to transform (using R) and feed into a visualization app (RShiny). This has already been accomplished manually, but I would like to scale up the solution using proper data engineering principles.

Before I join and transform these SAS files, I am thinking it is advantageous to load them into an S3 bucket. These SAS files are currently scattered all over the place on our server, and I would like to store them in one location that I can draw from in the future. I have knowledge of SQL from using PROC SQL in SAS, but I am struggling to wrap my head around the advantage of converting these SAS files to proper SQL database and tables. Can anyone please provide me with a resource or explanation on why this is important? Our company is shifting away from SAS to R/Python, so I am in favor of moving away from SAS for that reason alone, but I feel like there is something I am missing.

Once all these files are in an S3 bucket, I would like to join and transform the data into our common data model and store this transformed data (single file at this point) into another S3 bucket I can draw upon in the future. What is the best practice or tool to use here? I'm unsure of the pros/cons of using something like AWS lambda over python code that is triggered with Airflow.

Once the data is in this second S3 bucket I'll easily be able to read it in with our visualization app, I am really most confused about getting the data into the first S3 bucket / proper SQL schema. If you have any recommendations for tools or python packages to explore, I greatly appreciate it!

I know there are procedures in SAS that can either write to an SQL database or (I think) even to an S3 bucket, but I would like to avoid these solutions as our company is completely moving away from SAS, and I just need to retain these old sas7bdat files. 

Thanks!",False,False,False,dataengineering,t5_36en4,48547,public,self,Pipeline help: sas7bdat -&gt; S3 -&gt; Dashboard,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rw3sdt/pipeline_help_sas7bdat_s3_dashboard/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,ornamental_skeleton,,,[],,,,text,t2_9qlkg5ez,False,False,False,[],False,False,1641325141,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rw2qwv/dba_looking_to_move_into_data_engineering_what/,{},rw2qwv,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rw2qwv/dba_looking_to_move_into_data_engineering_what/,False,,,6,1641325151,1,"Hi

Apologies if this question gets asked a lot and irks some folks. Reddit search isn't great and google with 'Reddit' at the end didn't work so well, plus I wanna hear from people already in the industry. 

I am currently a Production Database Administrator with experience in MS SQL (all version from 2005 onward), MySQL, Oracle, Postgres.  Most of my exp sits within MS SQL and I have very strong T-SQL and admin experience. 

Would this experience alone land me a decent Data Engineer role? I suspect not, or at least not one that pays well, but let me know if i'm wrong. 

What skills do you think I should prioritise learning to make myself more attractive to potential employers? From scouring the internet of job postings, it would suggest this is Python so i've already been learning, but i'd be keen to know if there is anything else. So far i've built a couple of random text apps and a web scraper. 

This might not be the best place to ask this particular question - What are some good Python projects that I could try to undertake that would directly relate to something that a Data Engineer might encounter in their day-to-day? I was thinking potentially a rudamentary ETL but I may be biting off more than I can chew.",True,False,False,dataengineering,t5_36en4,48543,public,self,DBA looking to move into Data Engineering - What skills to prioritise?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rw2qwv/dba_looking_to_move_into_data_engineering_what/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,sdhomer2017,,,[],,,,text,t2_bpym79u,False,False,False,[],False,False,1641323771,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rw284d/handling_multiple_dynamic_dependencies_in_azure/,{},rw284d,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rw284d/handling_multiple_dynamic_dependencies_in_azure/,False,,,6,1641323782,1,"
I've a question regarding dependency management in a complex set of data pipelines. I'm extracting data from say fifteen different source systems, some of those source systems have hundereds of tables, others only a handful. Each table is saved out to as parquet to a datalake. An event is fired for each complete table extracted. That event is caught and schedules a standard process is run to tidy up the data and produce a standardised parquet file also in the datalake.

From that data I produce a dozen different datasets using jobs in spark. The dependencies (inputs) for those datasets are a subset of the tables from the extract phase. Different datasets may have overlapping requirements, for example a salesforce.user table might be needed in three different output datasets. The dataset is created when the various dependencies are satisfied.

The technique for identifying when the dependencies are met and the spark job can be run is a polling pipeline in Azure Data Factory. 

Finally to my question :). Are there any standard patterns or tooling, orchestration tools etc to support this approach - or am i going around this completely the wrong way? The process as above has been implemented and works fine, but it seems overly complex and the polling activity in ADF is chargeable which seems a little off to me - we're paying to wait essentially.

Any ideas anyone?",True,False,False,dataengineering,t5_36en4,48541,public,self,Handling multiple dynamic dependencies in Azure Data Factory,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rw284d/handling_multiple_dynamic_dependencies_in_azure/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,hibluemonday,,,[],,,,text,t2_xrx5m,False,False,False,[],False,False,1641259872,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rvhwzy/what_skills_to_build_for_backend_software/,{},rvhwzy,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rvhwzy/what_skills_to_build_for_backend_software/,False,,,6,1641259882,1,"I transitioned into DE from a data analyst background. My short term career goal is to grow and cement my skills in the software engineering side of DE. I'd like to be able to bounce back and forth between backend SWE and DE roles further into my career.

What are some skills that would allow me to grow on the backend SWE side of things as a current Data Engineer?

The extent of my backend experience is developing a simple API with Django to serve data from a Postgres database.",True,False,False,dataengineering,t5_36en4,48492,public,self,What skills to build for backend software engineering?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rvhwzy/what_skills_to_build_for_backend_software/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,randomusicjunkie,,,[],,,,text,t2_3tzpeuhd,False,False,False,[],False,False,1641250662,i.redd.it,https://www.reddit.com/r/dataengineering/comments/rvel59/2021_will_it_change_in_2022/,{},rvel59,False,True,False,False,True,True,False,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rvel59/2021_will_it_change_in_2022/,False,image,"{'enabled': True, 'images': [{'id': 'cZ5HSBuVOjo3KkqxTjYFd7bwG88EUeoGkIqNY7Ofs1U', 'resolutions': [{'height': 52, 'url': 'https://preview.redd.it/uh7ss39czj981.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=228af8bed5ce2360fb0644678210ce4b2cfaae4d', 'width': 108}, {'height': 104, 'url': 'https://preview.redd.it/uh7ss39czj981.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0c14d9b3102157677b33fb6be8d1fd7f1f8a92e7', 'width': 216}, {'height': 154, 'url': 'https://preview.redd.it/uh7ss39czj981.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=33e09b98ab1493029bf070408d216613cec1761c', 'width': 320}, {'height': 308, 'url': 'https://preview.redd.it/uh7ss39czj981.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2bd6b11d42c541d1bdd3cab4b8dfb998dcac2150', 'width': 640}, {'height': 462, 'url': 'https://preview.redd.it/uh7ss39czj981.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=23824b2221a514d796214778b690de8021534241', 'width': 960}, {'height': 520, 'url': 'https://preview.redd.it/uh7ss39czj981.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=562abd269fbb9baedf783c21dfb78647bb272dd9', 'width': 1080}], 'source': {'height': 601, 'url': 'https://preview.redd.it/uh7ss39czj981.jpg?auto=webp&amp;s=346996fed5f2a42867f2c3a9bfe97be1354bc0f5', 'width': 1247}, 'variants': {}}]}",6,1641250672,1,,True,False,False,dataengineering,t5_36en4,48485,public,https://b.thumbs.redditmedia.com/2in_jgxNgNfiDdUWuOjumBkYtpmiwd_-qzo1q7m3Mms.jpg,"2021, will it change in 2022?",0,[],1.0,https://i.redd.it/uh7ss39czj981.jpg,all_ads,6,,,,,,67.0,140.0,https://i.redd.it/uh7ss39czj981.jpg,,,,,,,,,,
[],False,JanPrince002,,,[],,,,text,t2_8hmvs28y,False,False,False,[],False,False,1641241205,pythonstacks.com,https://www.reddit.com/r/dataengineering/comments/rvaz0j/the_best_machine_learning_courses_on_udemy_2022/,{},rvaz0j,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rvaz0j/the_best_machine_learning_courses_on_udemy_2022/,False,link,"{'enabled': False, 'images': [{'id': 'A2gpk6IF1ArU8-vY3_wBMStIb6fHgwWwymQS6f1Uo7c', 'resolutions': [{'height': 60, 'url': 'https://external-preview.redd.it/zXFdZp0DikZtq3fxs6DY683HnuV87tW8RpwuZAjdqu4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c1b656c4aa59ae1d0b928becb28f11bd67d0bbd6', 'width': 108}, {'height': 121, 'url': 'https://external-preview.redd.it/zXFdZp0DikZtq3fxs6DY683HnuV87tW8RpwuZAjdqu4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=66a9a2415c38a143801969f17cb060260f755726', 'width': 216}, {'height': 180, 'url': 'https://external-preview.redd.it/zXFdZp0DikZtq3fxs6DY683HnuV87tW8RpwuZAjdqu4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=030ae25ab48c032ef3f796c5226020d32e15fd43', 'width': 320}, {'height': 360, 'url': 'https://external-preview.redd.it/zXFdZp0DikZtq3fxs6DY683HnuV87tW8RpwuZAjdqu4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ff46635470476e1550fef707cffb8306c6d9b07c', 'width': 640}, {'height': 540, 'url': 'https://external-preview.redd.it/zXFdZp0DikZtq3fxs6DY683HnuV87tW8RpwuZAjdqu4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c5fd4126e88085e7e4a31bf432a3a3716b3a23c5', 'width': 960}, {'height': 607, 'url': 'https://external-preview.redd.it/zXFdZp0DikZtq3fxs6DY683HnuV87tW8RpwuZAjdqu4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=35bcdf2d4e028683a82744e93059e9875d8481df', 'width': 1080}], 'source': {'height': 1080, 'url': 'https://external-preview.redd.it/zXFdZp0DikZtq3fxs6DY683HnuV87tW8RpwuZAjdqu4.jpg?auto=webp&amp;s=1bd568bb59b59e058c44e8adb9032a10449157a2', 'width': 1920}, 'variants': {}}]}",6,1641241215,1,,True,False,False,dataengineering,t5_36en4,48476,public,https://b.thumbs.redditmedia.com/fLjo7YTWhntpWG6pklQB20rGlrxayCWrOrdNM06b9Zg.jpg,The Best Machine Learning Courses on Udemy (2022),0,[],1.0,https://www.pythonstacks.com/blog/post/machine-learning-courses/,all_ads,6,,,,,,78.0,140.0,https://www.pythonstacks.com/blog/post/machine-learning-courses/,,,,,,,,,,
[],False,randomusicjunkie,,,[],,,,text,t2_3tzpeuhd,False,False,False,[],False,False,1641239156,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rva6nc/how_did_you_become_a_sql_pro/,{},rva6nc,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rva6nc/how_did_you_become_a_sql_pro/,False,,,6,1641239167,1,"I can solve all Hackerrank/Leercode SQL problems but I feel like I don’t understand SQL as a whole, as such, I would not be able to handle a data cleansing/wrangling project.

Are there any practice SQL projects available? What would you recommend?",True,False,False,dataengineering,t5_36en4,48476,public,self,How did you become a SQL pro?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rva6nc/how_did_you_become_a_sql_pro/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Paulythress,,,[],,,,text,t2_l1vnoo,False,False,False,[],False,False,1641230046,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rv6nq4/anyone_going_to_data_council_this_year/,{},rv6nq4,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rv6nq4/anyone_going_to_data_council_this_year/,False,,,6,1641230057,1,First one I’ll be going to. Pretty excited! Ready to learn &amp; be exposed to new ideas!,True,False,False,dataengineering,t5_36en4,48464,public,self,Anyone going to Data Council this year?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rv6nq4/anyone_going_to_data_council_this_year/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,numpy_array,,,[],,,,text,t2_6culsex0,False,False,False,[],False,False,1641227063,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rv5hyf/are_there_any_data_engineer_discord_servers_out/,{},rv5hyf,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rv5hyf/are_there_any_data_engineer_discord_servers_out/,False,,,6,1641227074,1,Wondering if there is a discord community out there to discuss data engineering topics?,True,False,False,dataengineering,t5_36en4,48463,public,self,Are there any data engineer Discord servers out there?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rv5hyf/are_there_any_data_engineer_discord_servers_out/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,dataguy66,,,[],,,,text,t2_6n045l7s,False,False,False,[],False,False,1641226450,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rv5924/need_help_choosing_masters/,{},rv5924,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rv5924/need_help_choosing_masters/,False,,,6,1641226460,1,"Hello All,

I'm from India with 3+ YOE in data engineering.

I'm thinking of masters in US.

Please help me with best affordable masters programs (Bigdata inspecific, not into datascience) in US with FALL 2022 intake.

Lately I'm seeing these advertisement for accelerated masters program(one year). Do they serve the purpose?

Thanks in Advance.",True,False,False,dataengineering,t5_36en4,48462,public,self,Need help choosing masters,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rv5924/need_help_choosing_masters/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,etl_boi,,,[],,,,text,t2_f16v22yv,False,False,False,[],False,False,1641225481,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rv4v36/is_1_year_3_months_yoe_not_enough/,{},rv4v36,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rv4v36/is_1_year_3_months_yoe_not_enough/,False,,,6,1641225491,1,"Currently applying.  Live in NYC, but open to relocating anywhere in-office or remote, so I don’t think I’m being too picky.

I knew getting the first job would be a grind, but now that I’m looking for my second, I’m still having a hard time getting bites on my resume.

I have 1.25 YOE as a DE at an F500 and was pretty much kicked to mid-level super early on, so my projects are large and high-impact (to the tune of a few hundred million).  I have a ton of responsibility owning and implementing pipelines and data products for half dozen teams.  I regularly communicate with stakeholders and present to senior management.

Beyond that, we’ve hired 3 new grads after I joined, and I’ve taken on a huge, informal leadership role in acting as the de facto tech lead.

Our stack is PySpark for transforms with some proprietary tools similar to airflow, tableau, and fivetran.

I still have a lot of areas to improve upon, but I thought I’d be a pretty competitive candidate this time around now that I’m more “proven”.  Especially hearing how hot the market is for experienced people.

DE is typically a more senior software engineering position, and I’m wondering if recruiters see my YOE and just assume I have title inflation?",True,False,False,dataengineering,t5_36en4,48462,public,self,Is 1 year 3 months YOE not enough?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rv4v36/is_1_year_3_months_yoe_not_enough/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,datameshlearning,,,[],,,,text,t2_a926z8al,False,False,False,[],False,False,1641223257,medium.com,https://www.reddit.com/r/dataengineering/comments/rv414s/pretty_fair_run_down_10_reasons_why_you_should/,{},rv414s,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rv414s/pretty_fair_run_down_10_reasons_why_you_should/,False,link,"{'enabled': False, 'images': [{'id': '8wEjvn9iiHmGeLWWrWff5RLRh8G7TYBUZYQRY0remSs', 'resolutions': [{'height': 81, 'url': 'https://external-preview.redd.it/3Pugpd3hWNYWz7ygE07xKrDRmLkFdEZZ3B0X2Q3oCGI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9d6eb580f532e7028f73502f7cf588f47adef2b0', 'width': 108}, {'height': 163, 'url': 'https://external-preview.redd.it/3Pugpd3hWNYWz7ygE07xKrDRmLkFdEZZ3B0X2Q3oCGI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=84ae8e295a3a351917d4bd6549f8e854b6c591b8', 'width': 216}, {'height': 242, 'url': 'https://external-preview.redd.it/3Pugpd3hWNYWz7ygE07xKrDRmLkFdEZZ3B0X2Q3oCGI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ca26f6f655e888b7c59bd3e2d3e2f199b774ac4d', 'width': 320}, {'height': 485, 'url': 'https://external-preview.redd.it/3Pugpd3hWNYWz7ygE07xKrDRmLkFdEZZ3B0X2Q3oCGI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=739aad899b94423158b3f9ebb92fbeb2e73f5184', 'width': 640}, {'height': 728, 'url': 'https://external-preview.redd.it/3Pugpd3hWNYWz7ygE07xKrDRmLkFdEZZ3B0X2Q3oCGI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d0723569d347156c995fe3061b42a0b5dc2d22a1', 'width': 960}, {'height': 819, 'url': 'https://external-preview.redd.it/3Pugpd3hWNYWz7ygE07xKrDRmLkFdEZZ3B0X2Q3oCGI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=de8e2c0f0dc1195593be1a702b722ee506a1adc4', 'width': 1080}], 'source': {'height': 911, 'url': 'https://external-preview.redd.it/3Pugpd3hWNYWz7ygE07xKrDRmLkFdEZZ3B0X2Q3oCGI.jpg?auto=webp&amp;s=8f4d3af4ca5e1798471b2ed26d59c9c0bba62b29', 'width': 1200}, 'variants': {}}]}",6,1641223267,1,,True,False,False,dataengineering,t5_36en4,48458,public,https://a.thumbs.redditmedia.com/-sEGObc6rObPO_NWV20XFwlGV_0733y8Jwma1wBtl34.jpg,[Pretty fair run down] 10 reasons why you should not adopt Data Mesh,0,[],1.0,https://medium.com/google-cloud/10-reasons-why-you-should-not-adopt-data-mesh-7a0b045ea40f,all_ads,6,,,,,,106.0,140.0,https://medium.com/google-cloud/10-reasons-why-you-should-not-adopt-data-mesh-7a0b045ea40f,,,,,,,,,,
[],False,DataGeek0,,,[],,,,text,t2_dqawdl1y,False,False,False,[],False,False,1641221643,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rv3fra/d_lessons_from_the_field_in_building_your_mlops/,{},rv3fra,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,1,0,False,all_ads,/r/dataengineering/comments/rv3fra/d_lessons_from_the_field_in_building_your_mlops/,False,,,6,1641221653,1,"Hi r/dataengineering!

I wanted to share this upcoming Enterprise Data &amp; AI webinar with you. I put the info from the website below along with the link if you're interested. I'm interested in hearing about the Uber and The RealReal case studies.

\-------

**Featured Speaker: Harpreet Sahota, Data Scientist at Comet and host of ""The Artists of Data Science"" Podcast**

As machine learning expands and larger organizations begin deploying across bigger teams, the need to efficiently operationalize becomes critical for enterprises. In our discussions with leading organizations utilizing ML like The RealReal and Uber, we have compiled real-world case studies and organizational best practices for MLOps in the enterprise.

Join us for a discussion where we'll explore the benefits of MLOps and discuss when and how to deploy MLOps in your ML. We'll review three real world case studies that will answer key questions: 

* When to start implementing in MLOps?
* How to start implementing in MLOps?
* How to measure the value of your MLOps strategy

**Agenda:**

* 12:00pm-12:30: Featured Presentation
* 12:30-13:00pm: Your Q&amp;A and interaction

\----------

Link to website: [https://events.cognilytica.com/CLNjE3MHwyNA](https://events.cognilytica.com/CLNjE3MHwyNA)",True,False,False,dataengineering,t5_36en4,48457,public,self,"[D] Lessons From the Field in Building Your MLOps Strategy with Harpreet Sahota, Data Scientist at Comet - Jan 27, 2022 at 12 PM ET",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rv3fra/d_lessons_from_the_field_in_building_your_mlops/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,C4_S4,,,[],,,,text,t2_77o38px2,False,False,False,[],False,False,1641210028,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ruztn6/open_source_dw/,{},ruztn6,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/ruztn6/open_source_dw/,False,self,"{'enabled': False, 'images': [{'id': '5DIkAQQcaHpL2SHtgqULp7y3NHe9H50CZpHbxAXiphw', 'resolutions': [{'height': 56, 'url': 'https://external-preview.redd.it/sLPbDQUjWNsTbmWZY3tnzasU97c_6Z9UmFunbKYQVFY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0e89bfdddf94eed7210a968dd96984c4800da7f2', 'width': 108}, {'height': 113, 'url': 'https://external-preview.redd.it/sLPbDQUjWNsTbmWZY3tnzasU97c_6Z9UmFunbKYQVFY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=66c2b18d94de6b60e658ef58a07bdd44ec26bb8b', 'width': 216}, {'height': 168, 'url': 'https://external-preview.redd.it/sLPbDQUjWNsTbmWZY3tnzasU97c_6Z9UmFunbKYQVFY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=10e1a1af1afac5d0cd728db5fe312055daa2055e', 'width': 320}, {'height': 336, 'url': 'https://external-preview.redd.it/sLPbDQUjWNsTbmWZY3tnzasU97c_6Z9UmFunbKYQVFY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=39d7fecb4e79e575905d96fc6fdf01dce400af15', 'width': 640}, {'height': 504, 'url': 'https://external-preview.redd.it/sLPbDQUjWNsTbmWZY3tnzasU97c_6Z9UmFunbKYQVFY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dcc096ed5a1d63b1e6bae4645481ac618316d0cc', 'width': 960}, {'height': 567, 'url': 'https://external-preview.redd.it/sLPbDQUjWNsTbmWZY3tnzasU97c_6Z9UmFunbKYQVFY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3be2398108d618d5257c717aa35afcf82fc52c48', 'width': 1080}], 'source': {'height': 630, 'url': 'https://external-preview.redd.it/sLPbDQUjWNsTbmWZY3tnzasU97c_6Z9UmFunbKYQVFY.jpg?auto=webp&amp;s=aadc8ced94e111f2f71cf59c653ac0e5cabb719f', 'width': 1200}, 'variants': {}}]}",6,1641210039,1,"Its a bad idea use [Redash](https://redash.io) as ETL and Data warehouse? 
Im not a data engineer just looking for a low/mid scale solution for experiment.",True,False,False,dataengineering,t5_36en4,48448,public,self,Open source DW?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ruztn6/open_source_dw/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Fragrant-Lobster4276,,,[],,,,text,t2_7wcm51fu,False,False,False,[],False,False,1641207230,i.redd.it,https://www.reddit.com/r/dataengineering/comments/ruz2v1/dont_understand_this_job_post_no_pyspark_but/,{},ruz2v1,False,True,False,False,True,True,False,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/ruz2v1/dont_understand_this_job_post_no_pyspark_but/,False,image,"{'enabled': True, 'images': [{'id': 'odF6cjqnktMcxnaM-SjzghVluLC4YhavGifPE0mvftU', 'resolutions': [{'height': 76, 'url': 'https://preview.redd.it/qn0vtn37eg981.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f21fc32c5b8c7125fbc68ea90e8c0944c1b26261', 'width': 108}, {'height': 152, 'url': 'https://preview.redd.it/qn0vtn37eg981.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4b1f0c2dc93b4581909ac3334088372beec54258', 'width': 216}, {'height': 226, 'url': 'https://preview.redd.it/qn0vtn37eg981.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b58597a14b9cc6187af98465d7aa3c75f1219840', 'width': 320}, {'height': 453, 'url': 'https://preview.redd.it/qn0vtn37eg981.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6ba011f104b23e152a1040ea3f1a4f027d001932', 'width': 640}], 'source': {'height': 509, 'url': 'https://preview.redd.it/qn0vtn37eg981.jpg?auto=webp&amp;s=9884b48d144b780837915f7da8ac6be3170d09a0', 'width': 719}, 'variants': {}}]}",6,1641207240,1,,True,False,False,dataengineering,t5_36en4,48447,public,https://b.thumbs.redditmedia.com/LedlpEQl-ZVS3TmVt-DdTMQHMAzooJ3lVJnYhK5ZVJM.jpg,Dont understand this job post no pyspark but extensive experience of spark jobs with python?,0,[],1.0,https://i.redd.it/qn0vtn37eg981.jpg,all_ads,6,,,,,,99.0,140.0,https://i.redd.it/qn0vtn37eg981.jpg,,,,,,,,,,
[],False,inntaxapt,,,[],,,,text,t2_evih6rnx,False,False,False,[],False,False,1641195730,perfecto.io,https://www.reddit.com/r/dataengineering/comments/ruw8g6/automated_regression_testing_guide/,{},ruw8g6,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/ruw8g6/automated_regression_testing_guide/,False,link,"{'enabled': False, 'images': [{'id': 'djG-sgiY5D4_6lir3da2b6QceUl4kXVioFrVrCQDDS4', 'resolutions': [{'height': 72, 'url': 'https://external-preview.redd.it/1sLKtjJhMaqUaiQ8p5WzoOmQ8VQcFmHogWPepDk2fV4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=165657f1a7fa84217dc58d4b1943f8e7719bddb1', 'width': 108}, {'height': 144, 'url': 'https://external-preview.redd.it/1sLKtjJhMaqUaiQ8p5WzoOmQ8VQcFmHogWPepDk2fV4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=744b18813f997fd90df28df550be6f00815e7b46', 'width': 216}, {'height': 213, 'url': 'https://external-preview.redd.it/1sLKtjJhMaqUaiQ8p5WzoOmQ8VQcFmHogWPepDk2fV4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6c5b62e579fd4b0b413a7215345caadd96140c08', 'width': 320}], 'source': {'height': 400, 'url': 'https://external-preview.redd.it/1sLKtjJhMaqUaiQ8p5WzoOmQ8VQcFmHogWPepDk2fV4.jpg?auto=webp&amp;s=4d0d11ea3e092197f90785ce3d2e69565dce143c', 'width': 600}, 'variants': {}}]}",6,1641195740,1,,True,False,False,dataengineering,t5_36en4,48428,public,https://b.thumbs.redditmedia.com/TLvXdgdlUMKrfXz2XZZh5q39V3E1dUTi3J1ibBWsrzw.jpg,Automated regression testing guide,0,[],1.0,https://www.perfecto.io/blog/automated-regression-testing,all_ads,6,,,,,,93.0,140.0,https://www.perfecto.io/blog/automated-regression-testing,,,,,,,,,,
[],False,thebatgamer,,,[],,,,text,t2_n3ole,False,False,False,[],False,False,1641184298,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rusz5y/helpadvice_on_upgrading_business/,{},rusz5y,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rusz5y/helpadvice_on_upgrading_business/,False,,,6,1641184308,1,"Long story short, a friend of mine's parent who has a medium-size book reselling business recently passed away and he has to handle it all.

The business has 2 local stores that sell books and some toy products, an online store for books and they supply educational books to schools as well. They have around 20k+ SKUs but the accounts and inventory are just stored in various excel files which is a mess for us to understand. For example, they take part in book fairs, and let's say someone takes some books for the fair from the inventory/warehouse and some sell and some do not, but the excel files are only of sold books at the fair so we don't know which books were taken.

We basically need to provide structure and use something to handle the inventory and accounts in the cheapest and fastest possible way. Please advise us like what options are good and is like upgrading to a proper database system even a good idea?",True,False,False,dataengineering,t5_36en4,48416,public,self,Help/Advice on upgrading Business,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rusz5y/helpadvice_on_upgrading_business/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,vananth22,,,[],,,,text,t2_10v76s,False,False,False,[],False,False,1641183949,dataengineeringweekly.com,https://www.reddit.com/r/dataengineering/comments/rusv1e/the_68th_edition_of_data_engineering_weekly/,{},rusv1e,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rusv1e/the_68th_edition_of_data_engineering_weekly/,False,link,"{'enabled': False, 'images': [{'id': 'kjTk-4snICu4l3YWOcF9us-qQJ-ewuwlz1RFAR3Kajk', 'resolutions': [{'height': 108, 'url': 'https://external-preview.redd.it/F1xLnOfsgG-SaJMuGJu6lvBwzA4HDwuEWvrIrWCMzlQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=37d472a1570571a5bb93607cc2dd6f9701e57bb2', 'width': 108}, {'height': 216, 'url': 'https://external-preview.redd.it/F1xLnOfsgG-SaJMuGJu6lvBwzA4HDwuEWvrIrWCMzlQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=35f803a4611076b377c5827dae0e541d3e4134a7', 'width': 216}, {'height': 320, 'url': 'https://external-preview.redd.it/F1xLnOfsgG-SaJMuGJu6lvBwzA4HDwuEWvrIrWCMzlQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=66270bee5f0e2c5926b5143fc1575557b819827e', 'width': 320}], 'source': {'height': 634, 'url': 'https://external-preview.redd.it/F1xLnOfsgG-SaJMuGJu6lvBwzA4HDwuEWvrIrWCMzlQ.jpg?auto=webp&amp;s=303b3e00375b96f76201c45f4d7affff8be99b40', 'width': 634}, 'variants': {}}]}",6,1641183959,1,,True,False,False,dataengineering,t5_36en4,48416,public,https://b.thumbs.redditmedia.com/QU8du9MGc8iw6gnNM5remJJeinEwg7YUlIkvEku6Z9w.jpg,"The 68th edition of Data Engineering weekly featuring articles from Twitter, Netflix, Etsy, Shopify and more.",0,[],1.0,https://www.dataengineeringweekly.com/p/data-engineering-weekly-68,all_ads,6,,,,,,140.0,140.0,https://www.dataengineeringweekly.com/p/data-engineering-weekly-68,,,,,,,,,,
[],False,ocvagabond,,,[],,,,text,t2_z52pt,False,False,False,[],False,False,1641183018,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rusk9k/rough_estimate_for_total_compensation_as_data/,{},rusk9k,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rusk9k/rough_estimate_for_total_compensation_as_data/,False,,,6,1641183028,1,"Looking for insight into what to expect in this type of role. I’m not building the DBs or the pipelines, but due to my business knowledge regarding the data and industry knowledge in this specific field I have the ability to deliver insights and roadmap items in addition to building the SQL processes to transform the data ingested from our inbound pipelines. Initially managing a small team with the expectation to grow fairly rapidly. 

Based in major west coast metro (non Bay Area) but company is remote first. What compensation packages would you expect here?",True,False,False,dataengineering,t5_36en4,48415,public,self,Rough estimate for total compensation as Data Engineering manager,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rusk9k/rough_estimate_for_total_compensation_as_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Vaslo,,,[],,,,text,t2_e4wl2,False,False,False,[],False,False,1641173045,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rup59t/open_tools_to_replace_ssis_process_including/,{},rup59t,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rup59t/open_tools_to_replace_ssis_process_including/,False,,,6,1641173056,1,"I’d like to learn some other tools besides SSIS.  I’ve created a process which pulls data from Excel and a dozen CSVs into SQL Server tables and I do transformations using SSIS tools as well as stored procedures.  To replace this I would like to use tools like DBT and Airflow.

I’d like to use open tools for now as I’m an amateur engineer working in Finance and I won’t really get any monetary resources to explore this.  I realize this will probably be more work for me but what can I do?

What’s the best way to just import data into the process from Excel/CSV etc?   SSIS makes this simple with its source and destination tools.  I’m not sure what to use without that tool.  

It’s not that much data, only running at most once a day, maybe 10k lines with 10-15 lookup tables.  I have about 30 steps in the process and SSIS can do it in about 15-20 seconds.  And I am an untrained amateur so I’m sure that can be even better (though I’m very happy with 30 seconds to process a bunch of data sources!)

I guess I see it as something like
-Use airflow to orchestrate
-DBT to transform
-Postgres for warehouse
-Some way to move the data in and out from excel, CSV, etc

As I mentioned I can’t use tools like Snowflake which I’ll need monetary and IT resources for.

Thanks for your advice!",True,False,False,dataengineering,t5_36en4,48401,public,self,Open tools to replace SSIS process including incoming and outgoing data,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rup59t/open_tools_to_replace_ssis_process_including/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,iLiveoffWelfare,,,[],,,,text,t2_4o35cdem,False,False,False,[],False,False,1641171568,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ruome3/best_resources_to_practice_sql_for_data_engineer/,{},ruome3,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/ruome3/best_resources_to_practice_sql_for_data_engineer/,False,,,6,1641171579,1,"Currently work as a Data Engineer. Long story short, i'm severally underpaid and looking to start interviewing soon. Anyone have any helpful resources for SQL and/or data engineering concepts in general that will help me prepare for interviews? Thanks!",True,False,False,dataengineering,t5_36en4,48400,public,self,Best resources to practice SQL for Data Engineer interviews,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ruome3/best_resources_to_practice_sql_for_data_engineer/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,SterlingVII,,,[],,,,text,t2_qkblr4l,False,False,False,[],False,False,1641169516,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/runw7y/data_science_ms_appropriate_for_moving_into_data/,{},runw7y,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/runw7y/data_science_ms_appropriate_for_moving_into_data/,False,,,6,1641169526,1,"Hi everyone,

I'm currently a student and looking to move into a data engineering / ML engineering role. I am in the process currently of applying to M.S. in Data Science programs, and I noticed that most of the programs state that their purpose is to develop data scientists. Would they not be the proper outlet for someone looking to move into an engineering role under data science?

My goals, for example, are to develop data science and machine learning applications throughout my career. I am concerned that if I be honest about my intentions to use the degree to move into data engineering that the admissions committees may feel like my goals are not aligned with what they are looking for, even though they have the exact curriculum I need to meet my goals.

Does anyone have any experience in this area, have you pursued a data science degree and been upfront about having the goal of moving into an engineering role after graduation? Should I be concerned about being honest about my goals in my personal statements? When a university claims their program is aimed at creating data scientists, do they mean strictly data scientists or would this normally include professionals in a broader data science field such as data / ML engineers as well?

Thank you all for your time and perspectives, they're very much appreciated.",True,False,False,dataengineering,t5_36en4,48397,public,self,Data Science M.S. appropriate for moving into data / ML engineering?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/runw7y/data_science_ms_appropriate_for_moving_into_data/,all_ads,6,,,,,,,,,,,,True,,,,,,
[],False,happybirthday290,,,[],,,,text,t2_9zhy6t6t,False,False,False,[],False,False,1641155007,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ruige9/sieve_we_processed_24_hours_of_security_footage/,{},ruige9,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/ruige9/sieve_we_processed_24_hours_of_security_footage/,False,,,6,1641155017,1,"Hey everyone! I’m one of the creators of Sieve, and I’m excited to be sharing it!

Sieve is an API that helps you store, process, and automatically search your video data–instantly and efficiently. Just think 10 cameras recording footage at 30 FPS, 24/7. That would be 27 million frames generated in a single day. The videos might be searchable by timestamp, but finding moments of interest is like searching for a needle in a haystack.

We built this visual demo ([link here](https://sievedata.com/app/query?api_key=AIzaSyAfKwf0tuuNOHbYi_JX-ew_dXH6SzdxZWY)) a little while back which we’d love to get feedback on. It’s \~24 hours of security footage that our API processed in &lt;10 mins and has simple querying and export functionality enabled. We see applications in better understanding what data you have, figuring out which data to send to labeling, sampling datasets for training, and building multiple test sets for models by scenario.

To try it on your videos: [https://github.com/Sieve-Data/automatic-video-processing](https://github.com/Sieve-Data/automatic-video-processing)

Visual dashboard walkthrough: [https://youtu.be/\_uyjp\_HGZl4](https://youtu.be/_uyjp_HGZl4)",True,False,False,dataengineering,t5_36en4,48378,public,self,Sieve: We processed ~24 hours of security footage in &lt;10 mins (now semantically searchable per-frame!),0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ruige9/sieve_we_processed_24_hours_of_security_footage/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,8EF922136FD98,,,[],,,,text,t2_3dopqeow,False,False,False,[],False,False,1641142418,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rudprv/writing_psets_using_a_notepad_in_abinitio_etl/,{},rudprv,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rudprv/writing_psets_using_a_notepad_in_abinitio_etl/,False,,,6,1641142428,1,"Hi Guys,

Apologies for posting this here. I couldn't find the right subreddit for this.

I'm new to abinitio and have trouble understanding the .pset file structure. One of the requirements of the project I'm assigned is to create a pset file by hand (using a text editor). I checked a few .pset files and couldn't figure out the pattern in parameter initializations. Also I've been searching on [docs.abinitio.com](https://docs.abinitio.com) for the past few days. The best I came across was PDL, but it just has some basic variable declaration, not the in-depth guide I'm looking for. Please point me to a specific topic in the abintio docs or a 3rd party resource.

Thanks for your help!",True,False,False,dataengineering,t5_36en4,48360,public,self,Writing psets using a notepad in AbInitio ETL.,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rudprv/writing_psets_using_a_notepad_in_abinitio_etl/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,bart081116,,,[],,,,text,t2_qgnurjv,False,False,False,[],False,False,1641138652,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ruccm3/suggestions_to_level_up_my_stack_mostly_etlish/,{},ruccm3,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,1,0,False,all_ads,/r/dataengineering/comments/ruccm3/suggestions_to_level_up_my_stack_mostly_etlish/,False,,,6,1641138662,1,"I currently work as a freelance developer contracted to a few companies, mostly labelled as a ""web scraper"" although I've grown to dislike that label since I feel like I do so much more than that. My deliverable is basically an ETL system that takes data from a website/many websites, keeps all the data consistent, up to date etc. and then the database I create I either have that as the deliverable to create a sort of API system around it so they can extract the data.

While I am very confident in my pure data extraction abilities and I truly see myself as a veteran in this regard (knowing the structures of every type of website using every known framework to most efficiently collect the data). I would love to improve on all other aspects of my work.

My stack atm is Python/Flask with Celery, MongoDB. Deployed on AWS EC2 mainly. Very proficient in Pandas however I'm not really using it much these days (recent jobs the ""transform"" step has been as simple as editing a bit of JSON in 10 or so lines of code using simple dict operations and I never see the need to overcomplicate things).

I can't help but feel I'm being very inefficient with this process, while it works I'm sure my stack could be better. This is mainly so I can land better contracts that deal with bigger data. Any suggestions? Any input is welcome, especially if you deal with similar stuff. Also criticism is welcome as well, I am fairly young and fully self taught, learning every day.

(Since I feel like a lot of people will say SQL, I do welcome suggestions based on this too, I am proficient but far from an expert and my recent projects have definitely had a use for Mongo)",True,False,False,dataengineering,t5_36en4,48357,public,self,Suggestions to level up my stack (mostly ETL-ish related stuff)?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ruccm3/suggestions_to_level_up_my_stack_mostly_etlish/,all_ads,6,,,,,,,,,,,,True,,,,,,
[],False,Danielpuff123,,,[],,,,text,t2_4wcek3y6,False,False,False,[],False,False,1641115657,startdataengineering.com,https://www.reddit.com/r/dataengineering/comments/ru66v5/how_to_add_tests_to_your_data_pipelines/,{},ru66v5,False,True,False,False,False,True,False,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/ru66v5/how_to_add_tests_to_your_data_pipelines/,False,link,"{'enabled': False, 'images': [{'id': '7V6HKP8NgH2FJByqeTHL0csxZgqrQrXqRKD2aFtrEUQ', 'resolutions': [{'height': 60, 'url': 'https://external-preview.redd.it/z9pDwZ_wi5MyMofxwsbO4pdF1wmAq6Ohb9GUsISdJng.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=812718e9f7750b97ed97b328ffe6f5c8c30f6cc5', 'width': 108}, {'height': 121, 'url': 'https://external-preview.redd.it/z9pDwZ_wi5MyMofxwsbO4pdF1wmAq6Ohb9GUsISdJng.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=65eb70316eb4890dd66cbf6957bf29de09a57483', 'width': 216}, {'height': 179, 'url': 'https://external-preview.redd.it/z9pDwZ_wi5MyMofxwsbO4pdF1wmAq6Ohb9GUsISdJng.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=07bcf3f88e434da8fbeb011301111966e9ed9658', 'width': 320}, {'height': 359, 'url': 'https://external-preview.redd.it/z9pDwZ_wi5MyMofxwsbO4pdF1wmAq6Ohb9GUsISdJng.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1e968c3f1db40a98f1ec28c5f46a0af4634a36bf', 'width': 640}, {'height': 539, 'url': 'https://external-preview.redd.it/z9pDwZ_wi5MyMofxwsbO4pdF1wmAq6Ohb9GUsISdJng.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=588b4b54a4408d09ad764ed990424e1d85ca403e', 'width': 960}, {'height': 607, 'url': 'https://external-preview.redd.it/z9pDwZ_wi5MyMofxwsbO4pdF1wmAq6Ohb9GUsISdJng.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c9efa1fa5348294b4ab002fff0d649ff776ddb95', 'width': 1080}], 'source': {'height': 1386, 'url': 'https://external-preview.redd.it/z9pDwZ_wi5MyMofxwsbO4pdF1wmAq6Ohb9GUsISdJng.jpg?auto=webp&amp;s=e823f1584508d495f537c6e28e7ef3bb03d2ddd8', 'width': 2466}, 'variants': {}}]}",6,1641115668,1,,False,False,False,dataengineering,t5_36en4,48340,public,https://b.thumbs.redditmedia.com/i2aEIIYyuVC9SxQkM2lo8Lp0s8uN7nB_AWUiP5DmxUI.jpg,How to add tests to your data pipelines,0,[],1.0,https://www.startdataengineering.com/post/how-to-add-tests-to-your-data-pipeline/,all_ads,6,,,,,,78.0,140.0,https://www.startdataengineering.com/post/how-to-add-tests-to-your-data-pipeline/,,,,,,,,,,
[],False,GreedyCourse3116,,,[],,,,text,t2_991xw1st,False,False,False,[],False,False,1641115127,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ru62kw/please_suggest_a_book_for_data_engineering/,{},ru62kw,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/ru62kw/please_suggest_a_book_for_data_engineering/,False,,,6,1641115137,1,"I think it would be a good idea to grasp more knowledge about DE concepts, terms and data pipelines. 

I am interviewing to be a DE (I was a SDE for 5 years) and I have worked with Relational and Non-relational DBs in the past. I have knowledge of NLP and ML concepts too. 

I can prepare for the interviews through google articles but it does not give me satisfactory wisdom with DE. In interviews, I get lost when they ask me to create a data model from start to end. I need to learn more. 

 Can you please suggest a book ? If not book, then some series of articles or anything else?",True,False,False,dataengineering,t5_36en4,48340,public,self,Please suggest a book for Data Engineering concepts.,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ru62kw/please_suggest_a_book_for_data_engineering/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,MeditatingInMicroG,,,[],,,,text,t2_gr1vtasc,False,False,False,[],False,False,1641091622,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rtzl8z/bounty_to_solve_hypervector_data_embedding/,{},rtzl8z,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rtzl8z/bounty_to_solve_hypervector_data_embedding/,False,,,6,1641091633,1,"I need to know what is the most optimal way to embed natural data in a hypervector. Optimal in the sense of minimal data loss why preserving the properties of the vectors.   So let's say you have an 28x28x3 image of numbers 0-255, how to get that flattened vector into a binary hypervector optimally?  Likewise if you have a floating point vector of size 10, how to transfer to binary hypervector?  Also the binary hypervectors could also be -1 or 1 instead of 0 and 1.  Think of quantum data embeddings here where data is converted into the Bloch sphere. Except here we want to embed into the hypervector space, that may be useful. What I need to know at the end of the project is what function best converts natural data into the hypervectors with minimal data loss and maximal preservation of the hypervector properties.

If you feel like you are capable of solving this problem, please message u/meditatinginmicrog for bounty details.",True,False,False,dataengineering,t5_36en4,48316,public,self,Bounty to solve hypervector data embedding,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rtzl8z/bounty_to_solve_hypervector_data_embedding/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Tinkerbelle-158,,,[],,,,text,t2_9u9o4314,False,False,False,[],False,False,1641087702,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rtybbo/salary_resource_guidelines_for_data_engineering/,{},rtybbo,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rtybbo/salary_resource_guidelines_for_data_engineering/,False,,,6,1641087713,1,"Glassdoor is very inaccurate along with pay scale. Since inflation has grown, covid and general demand, I was hoping if anyone has any resources for salary and what data engineering roles salaries are within Toronto.

Any help would be extremely appreciated!!!",True,False,False,dataengineering,t5_36en4,48313,public,self,Salary resource guidelines for data engineering roles within Toronto?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rtybbo/salary_resource_guidelines_for_data_engineering/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,prame_,,,[],,,,text,t2_346p6ejs,False,False,False,[],False,False,1641050867,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rtlikr/project_ideas_for_endtoend_data_engineering/,{},rtlikr,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rtlikr/project_ideas_for_endtoend_data_engineering/,False,,,6,1641050877,1,"Hi everyone, 

I’m looking for entry-level jobs in DS in Europe currently and I’m looking to add some relevant projects to my resume. I’ve done some machine learning/data mining projects at university, but I’ve not managed to get out of Python and pandas. I would really like to get started with some good projects that showcase some basic data engineering pipeline knowledge. 

I have an interview next month and the technical round is to demonstrate a personal project and I really want to use this opportunity well. 

I’m looking for an end-to-end project that I can complete in about 2 weeks. I would also appreciate resources for learning while doing the project. Thanks in advance!",True,False,False,dataengineering,t5_36en4,48276,public,self,Project ideas for end-to-end data engineering project for a beginner,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rtlikr/project_ideas_for_endtoend_data_engineering/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,el_jeep0,transparent,,[],fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b,Data Engineer,dark,text,t2_1sa7zw4c,False,False,False,[],False,False,1641006792,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rta74n/what_kinds_of_factors_influenced_you_choosing_a/,{},rta74n,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rta74n/what_kinds_of_factors_influenced_you_choosing_a/,False,,,6,1641006803,1,"Hi All,   
In the past the the teams I have worked on chose one option or another based on factors like the following, wanted to hear some of the reasons you decided to go one way or another?  


Here are some examples from my experiences:

* Team knowledge base about the underlying laguage / tech and interoperability with existing stack (e.g. we chose to use beam because team has strong python background and larger org has many eng's w. strong go background so we can potentially draw on that as we start to tune performance in the future)
* Speed of the timeline in which we had to deliver results (opted to use Stitch instead of open source singer alternative because it allowed us to move various lines of business/RDBMS's onto snowflake DWH far faster and more reliably than if we custom wrote taps/targets for each using Singer open source framework its built off of)
* Data residency/ on prem requirements (opted against using Astronomer's in-cloud offering and instead decided to use self-managed Airflow because some of the data we were piping through was not allowed to reside in any of the countries the clouds they offered to deploy through were located in at the time)   


Note: I realize this was discussed in at least [one previous post 3 years back](https://www.reddit.com/r/dataengineering/comments/9z712i/fully_managed_services_gcp_vs_open_sourced/), was hoping for fresh perspectives!",True,False,False,dataengineering,t5_36en4,48258,public,self,What kinds of factors influenced you choosing a SAS/Managed solution v.s. the open source alternative?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rta74n/what_kinds_of_factors_influenced_you_choosing_a/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,BoiElroy,,,[],,,,text,t2_r8dyi,False,False,True,[],False,False,1640993457,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rt6525/unstructured_data_governance_for_ml/,{},rt6525,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rt6525/unstructured_data_governance_for_ml/,False,,,6,1640993468,1,"I want to leave this quite open ended. I want to hear from the community about best practices, tools, challenges, success stories of managing large volumes of unstructured data.

The context is juxtaposed to relational databases, when used well will automatically instill type constraints, relational constraints, functional dependencies in relations will imply the needed normalization. Identifying keys will help CDC and versioning. ACID allows for coherent isolated transactions even with high concurrent requests. So the point being, while it definitely is not easy, dbm systems are a mature technology, that have over decades fostered good practices and an ecosystem of supporting tools.

Unstructured data very quickly can become ""data swamps"" in cloud object storage. It presents a new set of problems and opportunities and I want to hear the communities war stories and accomplishments.

I know Delta Lake OSS and Delta Lake by Databricks is one successful project for managing data lakes. But I want to hear about what else is happening. There seem to be lots of interesting tools and practices I'd like people to add their experience to this thread :)",True,False,False,dataengineering,t5_36en4,48259,public,self,Unstructured Data Governance for ML,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rt6525/unstructured_data_governance_for_ml/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,crob_evamp,,,[],,,,text,t2_hxgb9i54,False,False,False,[],False,False,1640992249,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rt5qvg/advice_request_google_cloud_large_repeated_text/,{},rt5qvg,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rt5qvg/advice_request_google_cloud_large_repeated_text/,False,,,6,1640992260,1,"Hi all,

I've recently joined a team where a given transaction research task is being conducted, and is in need of optimization

Consider:
1. Bulk tables of transactions include ng user, dates, prices, and a text description. (100's mil rows, currently stored in self managed postgresql on google cloud)

2. Reference tables of regex statements, and providers those statements infer.

3. Analysts semi-regularly need to tune these match statements, and therefore refresh the summarized result match table.

Currently a django admin model supports analyst interaction with the regex table, and the ability to refresh the matches. 

Currently the computation is handled by postgresql. 

Overall the team is sure the current pipeline (including many other not mentioned tasks) is stale, and in need of significant revamp to support scalability, and part of my duties is to think about this matching process, and the refreshing process.

We are a google cloud organization, but have a team competent in self management or off the shelf tool usage. We would prefer not to pay for any additional services beyond google cloud tools. We use django extensively but have zero love for it. We have begun teasing apart some other tasks with composer / airflow, so that is available, fyi.

Any thoughts on improving the matching approach, and implementing any other cloud services? Key goals are to maintain analyst access to refine matches, and improve seamlessness of refreshing, while using a more suited tool to support future scaling. 

Thanks for reading!",True,False,False,dataengineering,t5_36en4,48257,public,self,"Advice request: google cloud large, repeated text matching design",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rt5qvg/advice_request_google_cloud_large_repeated_text/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,columns_ai,,,[],,,,text,t2_el5l5mnh,False,False,False,[],False,False,1640991972,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rt5npg/will_this_idea_be_useful_to_develop_further/,{},rt5npg,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rt5npg/will_this_idea_be_useful_to_develop_further/,False,,,6,1640991982,1,"Hello community, happy new year to everyone!

I prototyped an idea called ""storytelling whatever you see"", which is a chrome extension that helps users to turn any data they see on the web into a sharable story in a couple of clicks.

&amp;#x200B;

*Processing video a3mzf1awly881...*

\[on youtube\] [2 minute prototype demo](https://youtu.be/AKdGPzDm1Wo)

Ideally, it should support any data like spreadsheet/excel, CSV/json in HTTP URL, or even HTML table, unstructured data could be as creative as word counting, etc... this prototype covers spreadsheet only.

My question:

1. is this type of extension useful at all?
   1. if yes, in what scenario?
   2. if not, any possible changes could make it useful?
2. would you like to give it a try? reply with an email so that I will send you an access link
3. any other thoughts to make it useful?

Thanks for your feedback in advance!",True,False,False,dataengineering,t5_36en4,48257,public,self,Will this idea be useful to develop further?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rt5npg/will_this_idea_be_useful_to_develop_further/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,awsconsultant,,,[],,,,text,t2_5ggm5svc,False,False,False,[],False,False,1640975768,google.com,https://www.reddit.com/r/dataengineering/comments/rt09iu/coursera_plus_subscription_100_off_new_year_offer/,{},rt09iu,False,True,False,False,False,True,False,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rt09iu/coursera_plus_subscription_100_off_new_year_offer/,False,link,"{'enabled': False, 'images': [{'id': 'RwQXU4m0BFeGTD9_Oi2s0dR5VE73MSppvLad6DWub_o', 'resolutions': [{'height': 60, 'url': 'https://external-preview.redd.it/mQFl61rJ9G_G7aqyKaw-s5J_mC4lqHQEBI9V0SJODM8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=63262bd27bc381b6ec367286cbe71716550313e8', 'width': 108}, {'height': 120, 'url': 'https://external-preview.redd.it/mQFl61rJ9G_G7aqyKaw-s5J_mC4lqHQEBI9V0SJODM8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f2b76faeb4100fbae9f837156347bb2bd0c28f80', 'width': 216}, {'height': 178, 'url': 'https://external-preview.redd.it/mQFl61rJ9G_G7aqyKaw-s5J_mC4lqHQEBI9V0SJODM8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5d6a90072d68a81a9a25cf1d9be7ab680720ac05', 'width': 320}, {'height': 356, 'url': 'https://external-preview.redd.it/mQFl61rJ9G_G7aqyKaw-s5J_mC4lqHQEBI9V0SJODM8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ba689321bb8446b9f7d348fc2a6dd754dfadfc84', 'width': 640}], 'source': {'height': 445, 'url': 'https://external-preview.redd.it/mQFl61rJ9G_G7aqyKaw-s5J_mC4lqHQEBI9V0SJODM8.jpg?auto=webp&amp;s=3ffeea885b9d336d12edeb2ebbff96e8e013ac2a', 'width': 800}, 'variants': {}}]}",6,1640975779,1,,True,False,False,dataengineering,t5_36en4,48247,public,https://b.thumbs.redditmedia.com/XaHL2ox5DOVtiScmSrBe5MT8M3usyR5q4QmZ7Dg3Dog.jpg,Coursera Plus Subscription $100 OFF New Year Offer,0,[],1.0,https://www.google.com/amp/s/onlinecoursesgalore.com/coursera-plus/amp/,all_ads,6,,,,,,77.0,140.0,https://www.google.com/amp/s/onlinecoursesgalore.com/coursera-plus/amp/,,,,,,,,,,
[],False,DarkestKnight_23,,,[],,,,text,t2_d1v0zccf,False,False,False,[],False,False,1640940424,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rspknt/wanted_to_set_up_alerts_for_my_kpis_if_they_cross/,{},rspknt,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rspknt/wanted_to_set_up_alerts_for_my_kpis_if_they_cross/,False,,,6,1640940434,1,Any help would be appreciated,True,False,False,dataengineering,t5_36en4,48222,public,self,Wanted to set up alerts for my KPIs if they cross a certain threshold but not sure if it's possible. Any ideas on how to implement this in bigquery? Also is it possible to mail people about job statuses without using external services like functions?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rspknt/wanted_to_set_up_alerts_for_my_kpis_if_they_cross/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,dechire20,,,[],,,,text,t2_16wk2t,False,False,False,[],False,False,1640921453,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rskar0/how_did_you_get_started_with_data_engineering/,{},rskar0,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rskar0/how_did_you_get_started_with_data_engineering/,False,,,6,1640921463,1,"Hello everyone, comp sci grad that ended up transitioning from coding to analyst work and decided I wanted to get into data engineering instead. Was wondering how y'all got started, were you a software engineer that did more data work ,just straight out of college, etc? How difficult did you find it to get into the industry? I am currently doing the Springboard DE program (yes, I've seen mixed reviews about it) and wanted to gain some more insight. 

I do have some professional background coding in Python/VBA and using SQL. Is there anything specific I should focus on and should I be aiming for entry level/junior roles or higher if all I will have is a portfolio of personal/capstone projects?

Thank you and Happy New Year :)",True,False,False,dataengineering,t5_36en4,48220,public,self,How did you get started with Data Engineering?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rskar0/how_did_you_get_started_with_data_engineering/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Badtechstuff,,,[],,,,text,t2_v6tz0ci,False,False,False,[],False,False,1640910011,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rsgi3m/books_on_lakefs_and_related_technologies/,{},rsgi3m,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rsgi3m/books_on_lakefs_and_related_technologies/,False,,,6,1640910022,1,"This is a bit embarrassing, but I will probably soon be working on or with this stream (Lakehouse, time travels, delta lake, S3) of technologies, but I'm a complete novice on some of the core pieces (with exception to S3).

I want to do my due diligence so I can have a fighting chance to not stuff it up once I get cracking. I seem to do better through examples and a mix of theory to doing something new. 

Can anyone help me out with some references and or books to get up to speed?",True,False,False,dataengineering,t5_36en4,48213,public,self,Books on LakeFS and related technologies,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rsgi3m/books_on_lakefs_and_related_technologies/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,kebabplz,,,[],,,,text,t2_kzv31,False,False,False,[],False,False,1640905310,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rseu67/big_data_pipelines/,{},rseu67,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rseu67/big_data_pipelines/,False,,,6,1640905320,1,"Hi,

Can anyone recommend me some materials regarding big data pipelines?
It has to be like full pipelines from begging to the end examples. 

Thanks 
Cheers",True,False,False,dataengineering,t5_36en4,48207,public,self,Big data pipelines,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rseu67/big_data_pipelines/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,stackedhats,,,[],,,,text,t2_asezmvm8,False,False,False,[],False,False,1640894693,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rsas79/building_self_service_tools_for_non_des/,{},rsas79,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,1,0,False,all_ads,/r/dataengineering/comments/rsas79/building_self_service_tools_for_non_des/,False,,,6,1640894704,1,"Solo DE here.

There's some sentiment in my workplace (which I don't really disagree with in principle) that we'd like to build self-service tools to surface data for some departments/purposes.

We're an investment firm so most queries would need to support arbitrary sets of securities, dates and times (and I really mean arbitrary when I say it).

So, obviously I can't give people a live python or SQL script and tell them to edit particular variables and not touch anything else. That's just a bad idea on so many levels.

Really, all I need is a way for someone to interact with it and set like two variables (tickers and date) safely, without being able to break anything.

I suppose I could ask for input into the console, except input prompts are evil and half my code would probably be fatfinger/sanity checks/ formatting of whatever they typed in.

&amp;#x200B;

Does anyone have suggestions on how they've built similar tools, or alternatively, what existing libraries/tools you used?

And of course, anecdotes about how something like this can go wrong are welcome as well.

As an aside, this firm is small, but quite open to change... they just haven't quite realized the extent of changes that are possible with proper access to data.",True,False,False,dataengineering,t5_36en4,48200,public,self,Building Self Service Tools for non DEs,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rsas79/building_self_service_tools_for_non_des/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,prutwo,,,[],,,,text,t2_gjrwx,False,False,False,[],False,False,1640882024,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rs5tyb/how_much_of_your_time_is_spent_on_adhoc_or_one/,{},rs5tyb,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rs5tyb/how_much_of_your_time_is_spent_on_adhoc_or_one/,False,,,6,1640882034,1,"I've been reading recently about **Data as a Product** \- basically the advantages of data teams working like product teams, opposed to working as consulting teams.

It got me thinking ...   
How much time do data engineers (and other members of data teams) spend working on one-off requests, vs working on improving data infrastructure and ongoing processes?

Would love to hear any specific feedback in the comments.

[View Poll](https://www.reddit.com/poll/rs5tyb)",True,False,False,dataengineering,t5_36en4,48186,public,self,How much of your time is spent on ad-hoc or one time requests?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rs5tyb/how_much_of_your_time_is_spent_on_adhoc_or_one/,all_ads,6,,,,,,,,,,,,,"{'is_prediction': False, 'options': [{'id': '12839853', 'text': 'less than 20%'}, {'id': '12839854', 'text': '21% to 40%'}, {'id': '12839855', 'text': '41% to 60%'}, {'id': '12839856', 'text': '61% to 80%'}, {'id': '12839857', 'text': 'more than 80%'}], 'prediction_status': None, 'resolved_option_id': None, 'total_stake_amount': None, 'total_vote_count': 0, 'tournament_id': None, 'user_selection': None, 'user_won_amount': None, 'vote_updates_remained': None, 'voting_end_timestamp': 1641486824068}",,,,,
[],False,Tobsyas,,,[],,,,text,t2_14w15t,False,False,False,[],False,False,1640881865,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rs5rre/how_to_validate_data_in_bigquery/,{},rs5rre,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rs5rre/how_to_validate_data_in_bigquery/,False,,,6,1640881876,1,"Hey Folks ! 

We aim to set up a proper data validation pipeline for our BigQuery Warehouse. Spanning multiple DataSets and many more tables. 

We aimed for validations like provided with the Great Expectations Framework but I personally find it a nightmare to set up and it seems to be quite buggy with BigQuery.

What’s you approach here ?",True,False,False,dataengineering,t5_36en4,48186,public,self,How to validate data in BigQuery ?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rs5rre/how_to_validate_data_in_bigquery/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Tharparkar1565,,,[],,,,text,t2_ea14eezz,False,False,False,[],False,False,1640866106,technologiesinindustry4.com,https://www.reddit.com/r/dataengineering/comments/rs0cad/pooling_in_convolutional_neural_network/,{},rs0cad,False,False,False,False,False,False,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rs0cad/pooling_in_convolutional_neural_network/,False,link,"{'enabled': False, 'images': [{'id': '7vsIatEXLHKaqn9V17PgNoLZnOmz9l2jnAaxrbU6180', 'resolutions': [{'height': 71, 'url': 'https://external-preview.redd.it/YENcHTsnIlzP4rla-OWicTctv7W9wSP40qc5trGS_oI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3cd1227c02ca3e7f33877c4d4d1c2dd7fd7a5714', 'width': 108}, {'height': 143, 'url': 'https://external-preview.redd.it/YENcHTsnIlzP4rla-OWicTctv7W9wSP40qc5trGS_oI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fe87717ab80ddc25a73818f02d881df3afebc720', 'width': 216}, {'height': 213, 'url': 'https://external-preview.redd.it/YENcHTsnIlzP4rla-OWicTctv7W9wSP40qc5trGS_oI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6703fe67b873a43a85fd2bd29710629293244e16', 'width': 320}], 'source': {'height': 339, 'url': 'https://external-preview.redd.it/YENcHTsnIlzP4rla-OWicTctv7W9wSP40qc5trGS_oI.jpg?auto=webp&amp;s=0cca7a49556c13db6245ba9b5ffbf9c3080b8f5a', 'width': 509}, 'variants': {}}]}",6,1640866117,1,,True,False,False,dataengineering,t5_36en4,48172,public,https://b.thumbs.redditmedia.com/ncuTyy1UDIB4P5pQ27za2QfNbI8B4RxvZAr2IL0jveU.jpg,Pooling in convolutional neural network,0,[],1.0,https://www.technologiesinindustry4.com/2021/12/pooling-in-convolutional-neural-network.html,all_ads,6,,,reddit,,,93.0,140.0,https://www.technologiesinindustry4.com/2021/12/pooling-in-convolutional-neural-network.html,,,,,,,,,,
[],False,EntropyRX,,,[],,,,text,t2_3fyu9j5o,False,False,False,[],False,False,1640861717,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rrz417/the_best_approach_to_build_apis_for_billions_of/,{},rrz417,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rrz417/the_best_approach_to_build_apis_for_billions_of/,False,,,6,1640861728,1,"Let's say you have billions of data points on BigQuery (or comparable) and you want to build an API to give users access to the data the closer you can get to real-time.

What would be some feasible solution to achieve the above? And where would you cache the data if necessary? 

Thanks!",True,False,False,dataengineering,t5_36en4,48165,public,self,The best approach to build APIs for billions of data points?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rrz417/the_best_approach_to_build_apis_for_billions_of/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,killer_unkill,,,[],,,,text,t2_18oazxf4,False,False,False,[],False,False,1640854430,ottertune.com,https://www.reddit.com/r/dataengineering/comments/rrx4ow/databases_in_2021_a_year_in_review/,{},rrx4ow,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rrx4ow/databases_in_2021_a_year_in_review/,False,link,"{'enabled': False, 'images': [{'id': 'o_7EselILFSn7OnumusyUkLyOdfSbLsVE3b3PDdfzE8', 'resolutions': [{'height': 54, 'url': 'https://external-preview.redd.it/rlrDhkF4yJBuAWsvHW80c7ThocGapwh0sGZMXUIsbbw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=11a8920df24ed569923ea02762d5b15e94c9183a', 'width': 108}, {'height': 108, 'url': 'https://external-preview.redd.it/rlrDhkF4yJBuAWsvHW80c7ThocGapwh0sGZMXUIsbbw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8c6092b5a575db3d6ca2d8fa19cc8aa100cc8176', 'width': 216}, {'height': 160, 'url': 'https://external-preview.redd.it/rlrDhkF4yJBuAWsvHW80c7ThocGapwh0sGZMXUIsbbw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=89aeac484da07c7137a04f1fedf7f04d7a5e3f0e', 'width': 320}, {'height': 320, 'url': 'https://external-preview.redd.it/rlrDhkF4yJBuAWsvHW80c7ThocGapwh0sGZMXUIsbbw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bafdf6cc448c88ed5e74252551ab9af70d9989a5', 'width': 640}, {'height': 480, 'url': 'https://external-preview.redd.it/rlrDhkF4yJBuAWsvHW80c7ThocGapwh0sGZMXUIsbbw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=635ecab5924a8987b60ac62337befd57506c76ec', 'width': 960}, {'height': 540, 'url': 'https://external-preview.redd.it/rlrDhkF4yJBuAWsvHW80c7ThocGapwh0sGZMXUIsbbw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a1df39de10c8889894cfd798e79447b9b4c98b3a', 'width': 1080}], 'source': {'height': 744, 'url': 'https://external-preview.redd.it/rlrDhkF4yJBuAWsvHW80c7ThocGapwh0sGZMXUIsbbw.jpg?auto=webp&amp;s=a70809d628dd7e695195b5878933a7f001e5c659', 'width': 1488}, 'variants': {}}]}",6,1640854441,1,,True,False,False,dataengineering,t5_36en4,48162,public,https://b.thumbs.redditmedia.com/PupGOsqYXkGv_U8FkKH0_cgsG-z-F4P9CHUrbaCXt_U.jpg,Databases in 2021: A Year in Review,0,[],1.0,https://ottertune.com/blog/2021-databases-retrospective/,all_ads,6,,,,,,70.0,140.0,https://ottertune.com/blog/2021-databases-retrospective/,,,,,,,,,,
[],False,KipT800,,,[],,,,text,t2_4j1l7eu,False,False,False,[],False,False,1640853522,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rrww28/best_way_to_sink_multiple_kafka_topics_to_s3/,{},rrww28,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rrww28/best_way_to_sink_multiple_kafka_topics_to_s3/,False,,,6,1640853533,1,"We currently have a handful of Kafka topics that we write to S3 in 30 min batch Windows using spark steaming with EMR for analytic events. One of the advantages of the 30 min window is that we don’t end up with multiple parquet files per day which goes some way to helping Redshift performance (we use external tables). 

There are several disadvantages though so we want to break the Kafka topics down into a topic per analytics event, so will likely end up with 500 topics. What I’m not so sure on is the best way to:

1. Sink the data to S3 in parquet whilst still maintaining a 30 min window (thinking about maintaining Redshift performance here). 
2. Configure something that will cover all 500 topics. Are we talking a container per topic? Or is there a toolset that can do it quite easily. 

Has anyone got any ideas or experience they can share?",True,False,False,dataengineering,t5_36en4,48161,public,self,Best way to sink multiple Kafka topics to S3,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rrww28/best_way_to_sink_multiple_kafka_topics_to_s3/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,tmanipra,,,[],,,,text,t2_4cullil,False,False,False,[],False,False,1640851023,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rrw85p/airflow_framework_for_validation/,{},rrw85p,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rrw85p/airflow_framework_for_validation/,False,,,6,1640851033,1,"I'm working on File level validation(File Pattern, Empty File etc) module which will populate RDBMS Table with Flags before the Actual Job runs. So that, if any FLAG is Negative, Actual Job will be put on hold and will alert Process Teams. 

I have around 50 Jobs which needs to be validated on daily basis and schedule vary from every 5 mins to Daily. In future, there could be addition of new jobs coming in. 

I would like to create a Framework using Airflow which should use only one DAG to validate each and every application using Configuration passed as Json files for each Application. 

Whats your thought on this?",True,False,False,dataengineering,t5_36en4,48153,public,self,Airflow Framework for Validation,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rrw85p/airflow_framework_for_validation/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,DigitalMarkone,,,[],,,,text,t2_huk5qxji,False,False,False,[],False,False,1640849712,youtube.com,https://www.reddit.com/r/dataengineering/comments/rrvvpv/how_to_use_hasty_annotation_tool_part_1/,{},rrvvpv,False,False,False,False,False,False,False,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rrvvpv/how_to_use_hasty_annotation_tool_part_1/,False,rich:video,"{'enabled': False, 'images': [{'id': '4Z1pR4Ev009X4OBe7y8tOeE2Ym6gS6KV83vWt62KGqc', 'resolutions': [{'height': 81, 'url': 'https://external-preview.redd.it/DJKjcja0X46wpYxIx2X2MNoLLjwE5PxWno43d-lMEVQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4dbb52591cac5a2572db0b314c498c895071d490', 'width': 108}, {'height': 162, 'url': 'https://external-preview.redd.it/DJKjcja0X46wpYxIx2X2MNoLLjwE5PxWno43d-lMEVQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6b8fc20f5397995687cd9d98acc66d48e682772a', 'width': 216}, {'height': 240, 'url': 'https://external-preview.redd.it/DJKjcja0X46wpYxIx2X2MNoLLjwE5PxWno43d-lMEVQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=777fa87b01aaadc80727b4bd7805291556d6239e', 'width': 320}], 'source': {'height': 360, 'url': 'https://external-preview.redd.it/DJKjcja0X46wpYxIx2X2MNoLLjwE5PxWno43d-lMEVQ.jpg?auto=webp&amp;s=8992957ad4fad0d46212dead60e2121cd4b22309', 'width': 480}, 'variants': {}}]}",6,1640849723,1,,True,False,False,dataengineering,t5_36en4,48152,public,https://b.thumbs.redditmedia.com/Da3Ki9Zq5yyjA7lJVNObmoqGFeJQY1zMh6Z_tGGlcPo.jpg,How to use Hasty Annotation tool part 1,0,[],1.0,https://www.youtube.com/watch?v=-9yWPvbUhr4&amp;t=16s,all_ads,6,"{'oembed': {'author_name': 'Inabia Solutions &amp; Consulting Inc', 'author_url': 'https://www.youtube.com/channel/UCeAWO4dFHVd5wKZ0SVt--jw', 'height': 200, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/-9yWPvbUhr4?start=16&amp;feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/-9yWPvbUhr4/hqdefault.jpg', 'thumbnail_width': 480, 'title': 'How to use Hasty Annotation tool part 1', 'type': 'video', 'version': '1.0', 'width': 356}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/-9yWPvbUhr4?start=16&amp;feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'scrolling': False, 'width': 356}",automod_filtered,"{'oembed': {'author_name': 'Inabia Solutions &amp; Consulting Inc', 'author_url': 'https://www.youtube.com/channel/UCeAWO4dFHVd5wKZ0SVt--jw', 'height': 200, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/-9yWPvbUhr4?start=16&amp;feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/-9yWPvbUhr4/hqdefault.jpg', 'thumbnail_width': 480, 'title': 'How to use Hasty Annotation tool part 1', 'type': 'video', 'version': '1.0', 'width': 356}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/-9yWPvbUhr4?start=16&amp;feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/rrvvpv', 'scrolling': False, 'width': 356}",105.0,140.0,https://www.youtube.com/watch?v=-9yWPvbUhr4&amp;t=16s,,,,,,,,,,
[],False,BadGuyBadGuy,,,[],,,,text,t2_535yg,False,False,False,[],False,False,1640847980,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rrve6q/where_do_i_learn_about_best_practices_for_storing/,{},rrve6q,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rrve6q/where_do_i_learn_about_best_practices_for_storing/,False,,,6,1640847990,1,"I'm 80% that this is a naive question.  Been searching the internet off and on about a month and it seems like it's always a workaround or a jury-rigged solution.  

Here's a few sketchy-looking solutions I've seen:

* Store them in your environment variables (Windows)
* Store them in clear text on your user profile
* (better-looking solution, but ...)  Seen a password vault suggested, but this seems to always be followed by someone saying ""it's kind of pointless from a security perspective"" ... 

Like ... how is this done in the professional world?  Here's my scenario:

I have a service account with a vendor to connect to an Oracle database and send ad-hoc queries there and download my results.

I wrote up some Python with SQLalchemy and cx_Oracle to extract and save my data.  I can schedule this with Windows Task scheduler, but I don't want to save the credentials in some random text file on my desktop.  Am I worrying too much?  It feels so janky.  

What can I do with this username &amp; password to secure it somewhere and run a script that can access it without feeling like I'm opening a gaping security hole?  

**More importantly, where do I learn this sort of thing?  If you know the answer, how did you get the answer?**  I'm frustrated that I can't seem to figure this out.  

Reddit posts like this are usually my last resort.  I've searched this subreddit, I've searched stack overflow and anything else on Google.  I don't know where I'm supposed to be looking for these types of best practices.  

I'm just some stupid DA hoping to be a DE one day, but stuff like this makes me feel like ""if you have to ask, you'll never know"" lol.  I don't see other people asking this question on here, so that's why I'm beating myself up over it.",True,False,False,dataengineering,t5_36en4,48151,public,self,Where do I learn about best practices for storing usernames &amp; passwords for ETL?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rrve6q/where_do_i_learn_about_best_practices_for_storing/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,sriny4c,,,[],,,,text,t2_2t744gxb,False,False,False,[],False,False,1640842862,i.redd.it,https://www.reddit.com/r/dataengineering/comments/rrtutd/what_are_your_opinions_friends_will_this_change/,{},rrtutd,False,True,False,False,True,True,False,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rrtutd/what_are_your_opinions_friends_will_this_change/,False,image,"{'enabled': True, 'images': [{'id': 'FXJi1JpDmEOIZuVrGyU4D4WxClrpG4TY8hoEpX9yoBg', 'resolutions': [{'height': 85, 'url': 'https://preview.redd.it/k8ktbeby9m881.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=43937898b75e9e33f315846caf9fcd9a9390c5a1', 'width': 108}, {'height': 171, 'url': 'https://preview.redd.it/k8ktbeby9m881.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=30469f8d21bd57151f80fd3399dcc7f6eff09cba', 'width': 216}, {'height': 254, 'url': 'https://preview.redd.it/k8ktbeby9m881.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6c4bba349a5cc1bd16c633099e72681546ba677c', 'width': 320}, {'height': 508, 'url': 'https://preview.redd.it/k8ktbeby9m881.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6814c7547d86b2c65b505f17abe49f78f6662eed', 'width': 640}, {'height': 762, 'url': 'https://preview.redd.it/k8ktbeby9m881.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=498571d6913f83dce7857e4409a4e69328799d37', 'width': 960}, {'height': 858, 'url': 'https://preview.redd.it/k8ktbeby9m881.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=451bad654841c2d118c20f02538998535262fe21', 'width': 1080}], 'source': {'height': 1160, 'url': 'https://preview.redd.it/k8ktbeby9m881.png?auto=webp&amp;s=5aaee2bf5c4aa388455d890455c92bd9beb498a3', 'width': 1460}, 'variants': {}}]}",6,1640842872,1,,True,False,False,dataengineering,t5_36en4,48149,public,https://b.thumbs.redditmedia.com/vu1kNIYarCeNXOxUi48IuAK4mmdhrJOY7Sg5JmJ2taU.jpg,What are your opinions friends? Will this change or Impact the Data Engineering landscape? Gartner expects these 12 technology trends to act as force multipliers of digital business and innovation over the next three to five years.,0,[],1.0,https://i.redd.it/k8ktbeby9m881.png,all_ads,6,,,,,,111.0,140.0,https://i.redd.it/k8ktbeby9m881.png,,,,,,,,,,
[],False,davik2001,,,[],,,,text,t2_5gpe0,False,False,False,[],False,False,1640842524,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rrtr4t/best_ide_for_databricks/,{},rrtr4t,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rrtr4t/best_ide_for_databricks/,False,,,6,1640842535,1,"I know it’s a broad question but I’m new to the Databricks world, all of our engineering in the past was done in Visual Studio with SQL server but it’s just not cutting it anymore. Our thoughts are to start with PySpark and go from there. Any pointers would be greatly appreciated.",True,False,False,dataengineering,t5_36en4,48149,public,self,Best IDE for Databricks,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rrtr4t/best_ide_for_databricks/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,dragonachu117,,,[],,,,text,t2_a7vbf8fe,False,False,False,[],False,False,1640842057,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rrtlxu/adf_trigger_pipe_2_on_pipe_1_successful_completion/,{},rrtlxu,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rrtlxu/adf_trigger_pipe_2_on_pipe_1_successful_completion/,False,,,6,1640842067,1,"I want to chain 2 ADF pipelines together. If pipeline 1 runs successfully, I want to trigger pipeline 2.

Please note that pipeline 1 has a schedule based trigger and I dont want to create another pipeline where I combine pipeline 1 and 2.

In AWS we have something called cloudwatch events/rules that takes care of this requirement. Is there something similar in Azure? If not, what are my options?",True,False,False,dataengineering,t5_36en4,48149,public,self,ADF - trigger pipe 2 on pipe 1 successful completion,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rrtlxu/adf_trigger_pipe_2_on_pipe_1_successful_completion/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,py_root,,,[],,,,text,t2_hq2rigr0,False,False,False,[],False,False,1640835137,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rrrbka/need_help_with_rpython_pandas_job_scheduling_tools/,{},rrrbka,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rrrbka/need_help_with_rpython_pandas_job_scheduling_tools/,False,,,6,1640835148,1,"My team is working on R previously and they are using rshiny for dashboards and cron jobs for scheduling rscripts to run daily using bash scripts.

Now we have started using python and snowflake for data warehousing. So, we are now looking for an alternative to cron jobs where we can schedule all kind of scripts or notebook if possible. So that it can be managed and tracked easily.",True,False,False,dataengineering,t5_36en4,48144,public,self,Need help with R/Python pandas job scheduling tools,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rrrbka/need_help_with_rpython_pandas_job_scheduling_tools/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,surverse,,,[],,,,text,t2_m3qkptg,False,False,False,[],False,False,1640833159,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rrqmzy/aws_glue_or_emr_for_data_transformations/,{},rrqmzy,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rrqmzy/aws_glue_or_emr_for_data_transformations/,False,,,6,1640833170,1,"I'm trying to train myself on AWS and DE tools through a personal project, but I'm stuck with whether to use Glue or EMR to perform some data calculations.

Basically, I'm just trying to get some practice using pyspark to perform data transformations. I managed to get data into S3, and load it into staging tables in Redshift. Now, I want to perform some calculations on that data (technical analysis for stocks/crypto) via pyspark. 

I figured my options are to learn how to set up and us AWS Glue or AWS EMR, but which one might be best for this situation? Also, is using the provided notebooks in Glue/EMR the best way to practice writing some pyspark code? Appreciate any insight!",True,False,False,dataengineering,t5_36en4,48140,public,self,AWS Glue or EMR for data transformations,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rrqmzy/aws_glue_or_emr_for_data_transformations/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,BoiElroy,,,[],,,,text,t2_r8dyi,False,False,True,[],False,False,1640812262,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rriuil/is_anyone_using_apache_pulsar_in_prod/,{},rriuil,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rriuil/is_anyone_using_apache_pulsar_in_prod/,False,,,6,1640812273,1,"Curious about the communities experiences with Apache Pulsar

Additionally, if anyone is well versed in streaming technologies, what are the overlaps / distinctions between Pulsar / Kafka / Flink / Spark Structured Streaming ?

If I had to pick a streaming stack now, and lets say we have far more Python skills and very limited Java skills, what would you recommend?",True,False,False,dataengineering,t5_36en4,48129,public,self,Is anyone using Apache Pulsar in prod?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rriuil/is_anyone_using_apache_pulsar_in_prod/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,dataengineerdude,,,[],,,,text,t2_4wrevs7q,False,False,False,[],False,False,1640798249,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rrd8eg/dataframes_vs_sparksql_what_to_use_and_why/,{},rrd8eg,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rrd8eg/dataframes_vs_sparksql_what_to_use_and_why/,False,,,6,1640798259,1,"I use Dataframes in Spark instead of SparkSQL. I feel like SparkSQL is easier, but leads to bad habits like too much logic lumped together, harder to unit test etc, just generally worse coding practices. Writing functions that use bits of the Dataframe API seems to make the code more reusable and extensible as well. Curious what most other people use when writing Spark, and why.",True,False,False,dataengineering,t5_36en4,48119,public,self,Dataframes vs SparkSQL - What To Use and Why?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rrd8eg/dataframes_vs_sparksql_what_to_use_and_why/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,iamgeoknight,,,[],,,,text,t2_avt84u4i,False,False,False,[],False,False,1640795720,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rrc87c/implement_advance_snapping_in_openlayers/,{},rrc87c,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rrc87c/implement_advance_snapping_in_openlayers/,False,,,6,1640795730,1,"&amp;#x200B;

[OpenLayers Snapping](https://i.redd.it/vq0bmzdiei881.gif)

[Implement Advance Snapping in OpenLayers](https://spatial-dev.guru/2021/12/29/implement-advance-snapping-in-openlayers/)",True,False,False,dataengineering,t5_36en4,48115,public,https://b.thumbs.redditmedia.com/VwLEwX5OMKw-i2YlFhCV9fJxouNNMJuszJ0qoaLE77c.jpg,Implement Advance Snapping in OpenLayers,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rrc87c/implement_advance_snapping_in_openlayers/,all_ads,6,,,,,,97.0,140.0,,"{'vq0bmzdiei881': {'e': 'AnimatedImage', 'id': 'vq0bmzdiei881', 'm': 'image/gif', 'p': [{'u': 'https://preview.redd.it/vq0bmzdiei881.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=1f1d5f389178dfb21be754889dad10671e93f7d9', 'x': 108, 'y': 75}, {'u': 'https://preview.redd.it/vq0bmzdiei881.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=6281118d4facecdccaf193f5f4588d9808d04010', 'x': 216, 'y': 150}, {'u': 'https://preview.redd.it/vq0bmzdiei881.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=289a2eb3bab8c256391a9210992574142b7e8151', 'x': 320, 'y': 222}, {'u': 'https://preview.redd.it/vq0bmzdiei881.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=1d24cae3b5a0ead512809388cca65b08717b904a', 'x': 640, 'y': 445}, {'u': 'https://preview.redd.it/vq0bmzdiei881.gif?width=960&amp;crop=smart&amp;format=png8&amp;s=2d26478ffd44d9fabdc76b70da9f725915b2ca53', 'x': 960, 'y': 667}, {'u': 'https://preview.redd.it/vq0bmzdiei881.gif?width=1080&amp;crop=smart&amp;format=png8&amp;s=0414f9a95513ec57067b347912a2b3c1a2f13e5d', 'x': 1080, 'y': 751}], 's': {'gif': 'https://i.redd.it/vq0bmzdiei881.gif', 'mp4': 'https://preview.redd.it/vq0bmzdiei881.gif?format=mp4&amp;s=441298712478d8236d01267c1230fcc34980485a', 'x': 1186, 'y': 825}, 'status': 'valid'}}",,,,,,,,,
[],False,TrainquilOasis1423,,,[],,,,text,t2_3wxhxt1i,False,False,False,[],False,False,1640794293,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rrbor4/starting_my_job_search_critique_my_resume/,{},rrbor4,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rrbor4/starting_my_job_search_critique_my_resume/,False,,,6,1640794304,1,"Problem I have always had making a resume is I don't have a degree or anything that feels able to fill out a resume with. I know I can do the work, but I have a hard time selling that on paper. Any help would be greatly appreciated. 

Anon

480-555-1234 ● [anon@gmail.com](mailto:anon@gmail.com)

Data Engineer

Ambitious, Data enthusiast seeking opportunities to further my experience in the world of data engineering and analytics. I am Interested in using skills in computer programming and data engineering to improve productivity by automating ETL Pipelines, and providing data driven business insight.

**Technical Skill Set**

**Snowflake:**  Experience developing and implementing data warehouse solutions and data pipelines for large data sets and business analytics. Comfortability managing relational data models to provide efficient access to business intelligence.

* Snowflake Zero to Hero Certificate (2020)

**Python:** On the job experience using the python programming language to automate ELT processes for large datasets.

* Short list of library knowledge
   * Pandas
   * Apache Spark
   * Numpy / Scipy
   * Selenium
   * Pyautogui
   * Matplotlib 

**SQL:**  Robust knowledge of database querying language SQL. Daily use of this language to process data in preparation for visualization.

* Ultimate MySQL Bootcamp Certification (2022)

&amp;#x200B;

**Work Experience**

**Endurance International Group ●** Phoenix, AZ (November 2017 - Current)

**Sales Data Analyst** (August 2019 - Current)**:** Provide Business intelligence to the stake holders through data Analytics. 

**Achievements**: 

* Revolutionized the daily reporting process.
   * As the key man on the team I used my knowledge of VBA, and Python programming to automate the process of reporting out daily analytics to the stake holders. Saving on average 2.5 to 3 hours of work each morning. 
* Lead the transition from Excel to Snowflake.
   * I took a leading role in transitioning my departments data environment from Excel sheets to a relational data model in the Snowflake warehouse. 
   * Enabled the scaling of data ETL from 100 agents locally to tracking the sales of 1500 agents in multiple calls centers globally. 
* Detailed analytics of data
   * Using my experience in data analytics and data processing it was my responsibility to identify irregularities and opportunities in enterprise data.",True,False,False,dataengineering,t5_36en4,48113,public,self,Starting my job search. Critique my Resume?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rrbor4/starting_my_job_search_critique_my_resume/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,thedowcast,,,[],,,,text,t2_5sd4k01y,False,False,False,[],False,False,1640793369,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rrbbkh/hypothesis_that_the_federal_reserve_can_set/,{},rrbbkh,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rrbbkh/hypothesis_that_the_federal_reserve_can_set/,False,self,"{'enabled': False, 'images': [{'id': '2jmnaRdDu4WByyTKwoOtl0KyPN0udIeUJSJFinr0mgU', 'resolutions': [{'height': 162, 'url': 'https://external-preview.redd.it/rd9RnDSUfDuFbPey39gIqOq9wcZ55Uwv8O-bSFIj-RY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ba55251dd8ae09a565894a8cf991687cee043e7a', 'width': 108}], 'source': {'height': 192, 'url': 'https://external-preview.redd.it/rd9RnDSUfDuFbPey39gIqOq9wcZ55Uwv8O-bSFIj-RY.jpg?auto=webp&amp;s=36f2ab8b3e8e955971c33cf135e3ea300bcb167b', 'width': 128}, 'variants': {}}]}",6,1640793380,1," Hypothesis that the Federal Reserve can set interest rates based on the movements of the planet Mars. Here are the daily percentage changes in the Dow Jones going back to 1896. The ""introduction"" section of the book, which can be read for free on Amazon, proposes a currency that could overpower the value of both cryptocurrency and the dollar during a major financial meltdown [https://books.google.com/books?id=Ke91zgEACAAJ&amp;newbks=1&amp;newbks\_redir=0&amp;hl=en](https://books.google.com/books?id=Ke91zgEACAAJ&amp;newbks=1&amp;newbks_redir=0&amp;hl=en)",True,False,False,dataengineering,t5_36en4,48114,public,self,Hypothesis that the Federal Reserve can set interest rates based on the movements of the planet Mars. Here are the daily percentage changes in the Dow Jones going back to 1896,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rrbbkh/hypothesis_that_the_federal_reserve_can_set/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Earl_grey_is_bae,,,[],,,,text,t2_67qagg45,False,False,False,[],False,False,1640783964,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rr80r2/junior_data_engineer_interview_advice/,{},rr80r2,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rr80r2/junior_data_engineer_interview_advice/,False,self,"{'enabled': False, 'images': [{'id': 'VED2KxmyajfMgSpVBB2YScURz8cQeM2y6pzOmVORfBg', 'resolutions': [{'height': 60, 'url': 'https://external-preview.redd.it/Q3zjHQmemjqZti0FwsXnWOvn9HKUvjyvdFVMiQqL3xM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5c2c2bd096a508f0ee13522527e6ac04b1ed853f', 'width': 108}, {'height': 121, 'url': 'https://external-preview.redd.it/Q3zjHQmemjqZti0FwsXnWOvn9HKUvjyvdFVMiQqL3xM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=372b2bcd3678f7fbbae6955eb81f00940de60fc7', 'width': 216}, {'height': 180, 'url': 'https://external-preview.redd.it/Q3zjHQmemjqZti0FwsXnWOvn9HKUvjyvdFVMiQqL3xM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=40ee2e179d1e8aa356ec20cf8c582d7e5509d3b5', 'width': 320}, {'height': 360, 'url': 'https://external-preview.redd.it/Q3zjHQmemjqZti0FwsXnWOvn9HKUvjyvdFVMiQqL3xM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1fb5db2f68dd8d823f19a1339b9e99757f240b57', 'width': 640}, {'height': 540, 'url': 'https://external-preview.redd.it/Q3zjHQmemjqZti0FwsXnWOvn9HKUvjyvdFVMiQqL3xM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5911458dd3b2c0762ef0bb932770662964e7151e', 'width': 960}, {'height': 607, 'url': 'https://external-preview.redd.it/Q3zjHQmemjqZti0FwsXnWOvn9HKUvjyvdFVMiQqL3xM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=eebc94e40af2aa50ae234e3c4ebda23eb2fc8578', 'width': 1080}], 'source': {'height': 1080, 'url': 'https://external-preview.redd.it/Q3zjHQmemjqZti0FwsXnWOvn9HKUvjyvdFVMiQqL3xM.jpg?auto=webp&amp;s=879377ab85735612c0133c8ce97549709d661618', 'width': 1920}, 'variants': {}}]}",6,1640783975,1,"Hello all,

I have an interview coming up for a Junior data engineer at IKEA. They have a data engineer accelerator program (see link below) and I have an interview coming up with their head of data. My background is quantitative finance (in the process of finishing my masters) and they know this, but I have decent python skills. I have an interview coming up and they asked me to show a project I worked on (I will be showing a script I wrote for my current internship which automated the requesting, via excel, and retrieval of data via ftp from a large financial data provider). What should I do to prepare for the technical questions? What kind of questions can I expect. Should I build something else from scratch to show them? If so, what?

Your assistance would be much appreciated! This is a big opportunity for me and I want to nail it!

[data engineer accelerator program](https://join.ingka.com/accelerator-de)",True,False,False,dataengineering,t5_36en4,48111,public,self,Junior data engineer interview advice,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rr80r2/junior_data_engineer_interview_advice/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,therealtibblesnbits,,,[],,,,text,t2_42gtu2c3,False,False,False,[],False,False,1640782928,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rr7paf/im_leaving_faang_after_only_4_months/,{},rr7paf,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rr7paf/im_leaving_faang_after_only_4_months/,False,,,6,1640782939,1,"I apologize for the clickbaity title, but I wanted to make a post that hopefully provides some insight for anyone looking to become a DE in a FAANG-like company. I know for many people that's the dream, and for good reason. Meta was a fantastic company to work for; it just wasn't for me. I've attempted to explain why below.

## It's Just Metrics
I'm a person that really enjoys working with data early in its lifecycle, closer to the collection, processing, and storage phases. However, DEs at Meta (and from what I've heard all FAANG-like companies) are involved much later in that lifecycle, in the analysis and visualization stages. In my opinion, DEs at FAANG are actually Analytics Engineers, and a lot of the work you'll do will involve building dashboards, tweaking metrics, and maintaining pipelines that have already been built. Because the company's data infra is so mature, there's not a lot of pioneering work to be done, so if you're looking to _build_ something, you might have better luck at a smaller company.

## It's All Tables
A lot of the data at Meta is generated in-house, by the products that they've developed. This means that any data generated or collected is made available through the logs, which are then parsed and stored in tables. There are no APIs to connect to, CSVs to ingest, or tools that need to be connected so they can share data. It's just tables. The pipelines that parse the logs have, for the most part, already been built, and thus your job as a DE is to work with the tables that are created every night. I found this incredibly boring because I get more joy/satisfaction out of working with really dirty, raw data. That's where I feel I can add value. But data at Meta is already pretty clean just due to the nature of how it's generated and collected. If your joy/satisfaction comes from helping Data Scientists make the most of the data that's available, then FAANG is definitely for you. But if you get your satisfaction from making unusable data usable, then this likely isn't what you're looking for.

## It's the Wrong Kind of Scale
I think one of the appeals to working as a DE in FAANG is that there is just so much data! The idea of working with petabytes of data brings thoughts of how to work at such a large scale, and it all sounds really exciting. That was certainly the case for me. The problem, though, is that this has all pretty much been solved in FAANG, and it's being solved by SWEs, not DEs. Distributed computing, hyper-efficient query engines, load balancing, etc are all implemented by SWEs, and so ""working at scale"" means implementing basic common sense in your SQL queries so that you're not going over the 5GB memory limit on any given node. I much prefer ""breadth"" over ""depth"" when it comes to scale. I'd much rather work with a large variety of data types, solving a large variety of problems. FAANG doesn't provide this. At least not in my experience.

## I Can't Feel the Impact
A lot of the work you do as a Data Engineer is related to metrics and dashboards with the goal of helping the Data Scientists use the data more effectively. For me, this resulted in all of my impact being along the lines of ""I put a number on a dashboard to facilitate tracking of the metric"". This doesn't resonate with me. It doesn't motivate me. I can certainly understand how some people would enjoy that, and it's definitely important work. It's just not what gets me out of bed in the morning, and a result I was struggling to stay focused or get tasks done. 

In the end, Meta (and I imagine all of FAANG) was a great company to work at, with a lot of really important and interesting work being done. But for me, as a Data Engineer, it just wasn't my thing. I wanted to put this all out there for those who might be considering pursuing a role in FAANG so that they can make a more informed decision. I think it's also helpful to provide some contrast to all of the hype around and FAANG and acknowledge that it's not for everyone and that's okay. 

## tl;dr
I thought being a DE in FAANG would be the ultimate data experience, but it was far too analytical for my taste, and I wasn't able to feel the impact I was making. So I left.",True,False,False,dataengineering,t5_36en4,48109,public,self,I'm Leaving FAANG After Only 4 Months,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rr7paf/im_leaving_faang_after_only_4_months/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,hkanything,,,[],,,,text,t2_gwpc5de,False,False,False,[],False,False,1640771971,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rr4nzt/if_i_were_you_system_design_diablo_2_ressurected/,{},rr4nzt,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rr4nzt/if_i_were_you_system_design_diablo_2_ressurected/,False,self,"{'enabled': False, 'images': [{'id': 'e5krQTDcKGBnidOV7jhdznQHZ5daGLNPxiDpOtzbM6s', 'resolutions': [{'height': 108, 'url': 'https://external-preview.redd.it/nb1ciHolwqQedGkFmHJl7jD54C9i5MVHq2FNYTaPH7s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ad3e7f605cb6239fdc9005f3b284f79a76a48095', 'width': 108}, {'height': 216, 'url': 'https://external-preview.redd.it/nb1ciHolwqQedGkFmHJl7jD54C9i5MVHq2FNYTaPH7s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b8d3955e7d38082bc7072345358b398ad559328e', 'width': 216}], 'source': {'height': 275, 'url': 'https://external-preview.redd.it/nb1ciHolwqQedGkFmHJl7jD54C9i5MVHq2FNYTaPH7s.jpg?auto=webp&amp;s=363ec2218772a7a1adc48a5394abcfe028278850', 'width': 275}, 'variants': {}}]}",6,1640771981,1,"Diablo 2 Ressurected [experienced outage](https://us.forums.blizzard.com/en/d2r/t/diablo-ii-resurrected-outages-an-explanation-how-we%E2%80%99ve-been-working-on-it-and-how-we%E2%80%99re-moving-forward/28164) at launch (it is a good read for data engineering):

&gt;The problem(s) with the servers:  
&gt;  
&gt;... So to **alleviate load and latency on our global database, each region–NA, EU, and Asia–has individual databases that also store your character’s information and progress, and your region’s database will periodically write to the global one.** Most of your in-game actions are performed against this regional database because it’s faster, and your character is “locked” there to maintain the individual character record integrity. The global database also has a back-up in case the main fails.  
&gt;  
&gt;...  
&gt;  
&gt;In staying true to the original game, we kept a lot of legacy code. However, one legacy service in particular is struggling to keep up with modern player behavior.  
&gt;  
&gt;  
&gt;  
&gt;This service, with some upgrades from the original, handles critical pieces of game functionality, namely game creation/joining, updating/reading/filtering game lists, verifying game server health, and **reading characters from the database to ensure your character can participate in whatever it is you’re filtering for. Importantly, this service is a singleton, which means we can only run one instance of it in order to ensure all players are seeing the most up-to-date and correct game list at all times.**   
&gt;  
&gt;...  
&gt;  
&gt;Additionally, overall, we were **saving too often to the global database: There is no need to do this as often as we were. We should really be saving you to the regional database**, and only saving you to the global database when we need to unlock you–this is one of the mitigations we have put in place. Right now we are writing code to change how we do this entirely, so we will almost never be saving to the global database, 

My shower thought of treating it like a system desgin interview question. If I design the system of distributed database while leaving legacy code alone, my immediate thought is that the replication to global database can be replaced by a queue of update request like Cassadra (Kafka sound ideal), so you can replay lost progress in case of disaster. Presumably this is faster than a convensional global database.

My second thought is that to ensure the legecy code have the latest view of character updates. It is probably simple to keep a cached view of players but stored in a lightweight structure that can be sent across in a blink no matter how heavily loaded the queue is. It can be a bloomfilter of a list of players that can see your created game. Or even a list of players has updated (dirty page) that require to fetch from the queue. Sending a bloomfilter across regions in nanoseconds should be relatively lightweight.

Now discuss!",True,False,False,dataengineering,t5_36en4,48101,public,self,"""If I were you"" system design - Diablo 2 Ressurected database replication",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rr4nzt/if_i_were_you_system_design_diablo_2_ressurected/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,ukpfqa2020,,,[],,,,text,t2_gkcueyuh,False,False,False,[],False,False,1640767725,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rr3lhn/expected_uk_data_engineer_salary_ranges/,{},rr3lhn,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rr3lhn/expected_uk_data_engineer_salary_ranges/,False,,,6,1640767736,1,"What would you expect the salary ranges to be for the UK (London and non-London)?

7 years of experience in software, but 1 as a Data Engineer. Last year I was on £45k, two of my colleagues told me that my salary was way too low. But sites like PayScale and Glassdoor say that's on track for a Manchester salary. 6 months later, I got promoted at the same company to Senior Data Engineer to £50k. Left after a year for a Data Engineer role at £60k (still Manchester).

Is £60k what you'd expect for a Manchester Data engineer salary? What would the difference be for a Senior Data Engineer?

I also may have to move down to London for my partner. What are the salary expectations there?

If relevant, I've got experience in Azure, AWS, GCP, Python, SQL, Airflow and CICD.",True,False,False,dataengineering,t5_36en4,48097,public,self,Expected UK Data Engineer salary ranges,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rr3lhn/expected_uk_data_engineer_salary_ranges/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,InfinitePermutations,,,[],,,,text,t2_tu011,False,False,False,[],False,False,1640765256,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rr2ys5/serving_layer_for_delta_lake/,{},rr2ys5,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rr2ys5/serving_layer_for_delta_lake/,False,,,6,1640765266,1,"Hi all, we are currently building out a delta Lake and using the bronze silver and gold logic and are working out how to best serve the tables to end users.
We have a mix of skill sets so some will use databricks notebooks while others want a traditional sql ui such as ssms. We plan to expose the delta tables to serverless sql as external tables and will give access to these tables to users.

Our challenge is where to store view logic. We want any view logic to be in one place but If we build views in synapse they are locked to that platform so we could build these as silver /gold tables and expose them as external tables unless there is a better option?

I thought the view could be stored in the hive metastore but views need a spark compute to work so won't be picked up by serveless sql.",True,False,False,dataengineering,t5_36en4,48096,public,self,Serving layer for delta Lake,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rr2ys5/serving_layer_for_delta_lake/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,c_cannon18,,,[],,,,text,t2_brexsg9h,False,False,False,[],False,False,1640762793,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rr2be4/data_lakewarehouse_and_permissions/,{},rr2be4,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rr2be4/data_lakewarehouse_and_permissions/,False,,,6,1640762804,1,"Hey everyone! 

I work in a smaller company and we are starting to take data more seriously. I have a design question that I’m stumped on.

What’s the best way to manage permissions with the least amount of overhead?

We’re a smaller company, so our current setup is pretty brutal for data analysts and we want to bring in some new technologies to help them out. Currently our setup looks like this: 

Prod -&gt; Kafka Connect -&gt; MySQL Analytics DB

Currently, analysts have full permissions to the analytics database and its in real time thanks to Kafka. Sensitive tables are removed through Kafka, but this is hands on as new tables are created frequently. 

I have a clean slate as we kill MySQL which is awesome so any input would be appreciated. I need to get away from managing tables in Kafka and just have Kafka dump everything somewhere, and then worry about permissions and where the data goes next",True,False,False,dataengineering,t5_36en4,48094,public,self,Data Lake/Warehouse and Permissions,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rr2be4/data_lakewarehouse_and_permissions/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,statistical_engineer,,,[],,,,text,t2_3plcyhxk,False,False,False,[],False,False,1640733997,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rqsu8n/is_data_science_experience_valued_if_you/,{},rqsu8n,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rqsu8n/is_data_science_experience_valued_if_you/,False,,,6,1640734007,1,"One of my first jobs had the official title of ""data scientist"" although I functioned as a data engineer.  Since then, I have worked other jobs where I held the title ""data engineer"" officially.





Would my experience as a ""data scientist"" count towards my years of experience as a data engineer?",True,False,False,dataengineering,t5_36en4,48086,public,self,Is data science experience valued if you functioned as a data engineer?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rqsu8n/is_data_science_experience_valued_if_you/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,chiefbeef300kg,,,[],,,,text,t2_8f0cpva,False,False,False,[],False,False,1640733728,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rqsqp1/da_to_de_vs_swe/,{},rqsqp1,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rqsqp1/da_to_de_vs_swe/,False,,,6,1640733739,1,"DA to DE or SWE?

Hi Reddit,

I'm at a bit of a crossroads and was hoping for some advice.

**About me:** I've been working as a DA at a large Bank for the past 2.5 years. I've become the technical lead on my team. The majority of my work has been designing a data product with Python used for marketing segmentation and QA and writing/running many ETL pipelines with Python or PySpark + DataBricks. I'm relatively strong with Python and SQL.  I'm hoping to transition to SWE or DE at a FAANG or similar company in the next several months.  I enjoy technically challenging work and want to focus on improving my technical skills throughout my career. I generally enjoy working with tech partners over business, but have gotten lots of positive feedback on my ability to work with business.  I enjoy working with business, but not extensively. I've been told by an ex-coworker at FB my responsibilities seem similar to what  their DE coworkers do.  I find Python more interesting than SQL and hate building dashboards,

**Why I'm considering SWE:**

* SWEs appear to solve more diverse problems.  It seems as though many DE jobs involve lots of ETL work, which is beginning to get stale.
* Generally, it seems as though SWEs build products while DEs run processes. This impression might be wrong
* Higher pay? It's likely I'd start as a junior so it'd probably be lower at first.

**Why I'm not:**

* It's daunting. I'm concerned even with extensive interview prep, getting a job might be a stretch. And if I get a job, imposter syndrome will be real.

**Why I'm considering DE:**

* It more closely matches my experience. I think I'll be much more prepared than for SWE. More likely to get a job.
* At worst, it's a step in the direction I want to go, while SWE is a leap.
* I enjoy my work currently, but I have so much room for improvement. I feel like I've gotten very good at what I do in my environment, but I know I'd have so much to learn from peers on a new, more technical team.
* Multiple DE contacts at target companies.

**Why I'm not:**

* I'm concerned the work will become stale eventually.  It appears DE has less breadth, but I could be wrong.
* I'm concerned if I want to switch to SWE, my DE experience won't be super relevant. I'm sure this is team dependent.

For those who made it this far, any advice? I've started interview prepping, but the material is diverging and I need to make a decision. If I get bored, might it be realistic to switch to SWE after 1-2 years of DE experience? Thanks all",True,False,False,dataengineering,t5_36en4,48086,public,self,DA to DE vs. SWE,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rqsqp1/da_to_de_vs_swe/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,de2022,,,[],,,,text,t2_i0ugtwds,False,False,False,[],False,False,1640733256,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rqskc8/expected_uk_data_engineer_salary_ranges/,{},rqskc8,False,False,False,False,False,False,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rqskc8/expected_uk_data_engineer_salary_ranges/,False,,,6,1640733266,1,[removed],True,False,False,dataengineering,t5_36en4,48084,public,self,Expected UK Data Engineer salary ranges,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rqskc8/expected_uk_data_engineer_salary_ranges/,all_ads,6,,,automod_filtered,,,,,,,,,,,,,,,
[],False,Selection_Paralysis,,,[],,,,text,t2_g7kz2jl7,False,False,False,[],False,False,1640731613,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rqrxm9/da_to_de_or_swe/,{},rqrxm9,False,False,False,False,False,False,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rqrxm9/da_to_de_or_swe/,False,,,6,1640731624,1,[removed],True,False,False,dataengineering,t5_36en4,48080,public,self,DA to DE or SWE?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rqrxm9/da_to_de_or_swe/,all_ads,6,,,automod_filtered,,,,,,,,,,,,,,,
[],False,Ruuca,,,[],,,,text,t2_5q5xgt06,False,False,False,[],False,False,1640730051,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rqrd2h/can_someone_paint_a_picture_on_how_my_path_to_de/,{},rqrd2h,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rqrd2h/can_someone_paint_a_picture_on_how_my_path_to_de/,False,,,6,1640730062,1,"Currently I am studying economics in University and learning DE fundamentals from Dataquest on the side. 

I want to know what approach I can take to get DE as my first job after graduating in 4 years. Do I look for DE internship? Will interviewers even be interested in me? Or do look for DA opportunities and ""work"" my way into DE. Additionally, how else can I prepare myself for the role, ie examination?

I've been told to take many hard math modules in my university because they will interest interviewers, is that true?

Any advice is appreciated",True,False,False,dataengineering,t5_36en4,48078,public,self,Can someone paint a picture on how my path to DE looks like?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rqrd2h/can_someone_paint_a_picture_on_how_my_path_to_de/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,dataengineerdude,,,[],,,,text,t2_4wrevs7q,False,False,False,[],False,False,1640723741,confessionsofadataguy.com,https://www.reddit.com/r/dataengineering/comments/rqp0kb/2_useful_pyspark_functions/,{},rqp0kb,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rqp0kb/2_useful_pyspark_functions/,False,,,6,1640723751,1,,True,False,False,dataengineering,t5_36en4,48072,public,default,2 Useful PySpark Functions,0,[],1.0,https://www.confessionsofadataguy.com/2-useful-pyspark-functions/,all_ads,6,,,,,,,,https://www.confessionsofadataguy.com/2-useful-pyspark-functions/,,,,,,,,,,
[],False,zelenadinja,,,[],,,,text,t2_garpgtut,False,False,False,[],False,False,1640714211,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rqljet/uploading_files_to_s3/,{},rqljet,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rqljet/uploading_files_to_s3/,False,,,6,1640714221,1,"Hi, i have about 4M images on EC2 instance, and i have a file.txt where each line is a path to a image that i want to upload. Out of 4M images there is about 300k t hat i want to upload to my S3 Bucket.

What is the best way to do it?",True,False,False,dataengineering,t5_36en4,48065,public,self,uploading files to s3,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rqljet/uploading_files_to_s3/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,BlancBryn,,,[],,,,text,t2_1i178g26,False,False,False,[],False,False,1640704889,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rqi6d1/airflow_best_practice_to_transfer_data_between/,{},rqi6d1,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rqi6d1/airflow_best_practice_to_transfer_data_between/,False,,,6,1640704900,1,"Hi all,

I am relatively new to Airflow. 

I have already written smaller DAGs in which in each task data is fetched from an API and written to Azure Blob.

I would now like to fetch data from a MSSQL database (or further CSV file or Azure Blob), then transform it with Python and finally write the result to the data warehouse (MSSQL).

I am coming from a drag n drop ETL background like Pentaho.

My approach would be to run a SQL script in the first task to fetch the data (MSSQLOperator), in the second task to transform the data with e.g. Pandas and a PythonOperator and in the last step to write the result to the DWH (MSSQLOperator).

But I don't know how to transfer the data between the tasks. Usually it should be possible to pass the result of one step as a pandas dataframe to a next step, right?

I think this is possible somehow with XCOM, but I didn't understand it yet and I was advised not to work with it.

What would be your approach to build such data pipelines?

It's important to me that I don't throw everything into one big script. I want to store the SQL scripts separately and call them from the DAG for better version control and uniqueness of scripts.

&amp;#x200B;

Thanks a lot in advance!",True,False,False,dataengineering,t5_36en4,48059,public,self,Airflow: Best practice to transfer data between tasks,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rqi6d1/airflow_best_practice_to_transfer_data_between/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Yoav212,,,[],,,,text,t2_gdbapkpf,False,False,False,[],False,False,1640704531,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rqi1qa/which_nosql_database_do_you_use/,{},rqi1qa,False,False,False,False,False,False,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rqi1qa/which_nosql_database_do_you_use/,False,,,6,1640704541,1,"[removed]

[View Poll](https://www.reddit.com/poll/rqi1qa)",True,False,False,dataengineering,t5_36en4,48058,public,self,Which NOSQL Database do you use?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rqi1qa/which_nosql_database_do_you_use/,all_ads,6,,,automod_filtered,,,,,,,,,,,,,,,
[],False,matavelhos,,,[],,,,text,t2_tk2n7,False,False,False,[],False,False,1640703015,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rqhih5/interview_preparation/,{},rqhih5,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rqhih5/interview_preparation/,False,,,6,1640703025,1,"Hello everyone,

I applied to a D.E. position in a big retailer company and I will be interviewed in the next few days. 

It will be my first interview for this position. I have some knowledge in cloudera stack (on premisses). I think that I know the basics about D.E. but I want be prepared for the interview. 

For what I know they use AWS. Do you have any advice to prepare for the interview?

Thank you.",True,False,False,dataengineering,t5_36en4,48054,public,self,Interview preparation,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rqhih5/interview_preparation/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,NaN_Loss,,,[],,,,text,t2_cx5ueh84,False,False,False,[],False,False,1640701420,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rqgzbe/system_design_interview_at_datadog/,{},rqgzbe,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rqgzbe/system_design_interview_at_datadog/,False,,,6,1640701430,1,"Hi reddit,

I  have an upcoming system design interview at datadog, where I'm told  I'll have to think about a scalable product (something like twitter or  spotify). I'll have to draft the global architecture, then dive in a  specific component of my choice. I'll have to show knowledge about  things like distributed systems, queues (consumer, producer) , load  balancers, sql/no sql, ...

I have decent programming experience (python + SQL + DS) but not with large distributed systems.

Any advice on how to prepare for the interview?

I've seen leetcode premium mentioned in similar threads. Is it worth it?

Thank you for the help",True,False,False,dataengineering,t5_36en4,48051,public,self,System design interview at Datadog,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rqgzbe/system_design_interview_at_datadog/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,VitalYin,,,[],,,,text,t2_h5gp0l6,False,False,False,[],False,False,1640697149,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rqfl4r/resources_to_learn_about_running_spark_on_emr/,{},rqfl4r,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,1,0,False,all_ads,/r/dataengineering/comments/rqfl4r/resources_to_learn_about_running_spark_on_emr/,False,,,6,1640697159,1,I am having some trouble with my spark job running out of space. I want to find some good resources that can help me learn more about spark and tunning it. Can you guys please help!,True,False,False,dataengineering,t5_36en4,48048,public,self,Resources to learn about running spark on EMR?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rqfl4r/resources_to_learn_about_running_spark_on_emr/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Relative-Addition672,,,[],,,,text,t2_a97fnnv4,False,False,False,[],False,False,1640691950,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rqe1xz/i_need_help_to_come_up_with_an_idea_usecase_for/,{},rqe1xz,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rqe1xz/i_need_help_to_come_up_with_an_idea_usecase_for/,False,,,6,1640691960,1,"Hey everyone,

I am Data Engineer who is finishing his master's in Statistics / Data Analytics and ML and I am really struggling to find a good idea/use-case for my graduation thesis/project so bear with me... 

On a daily basis, I am working as a Data Engineer with lots of DE technologies (Spark with Scala/Python/SQL, Kafka, RabbitMQ, Databricks, SparkML...) but I started my Master in ML because I was really interested in Machine Learning and I have solid experience in it (with Tensorflow/SparkML/Sklearn...) but recently I've gained a lot of interest in web3.0 too. I've started learning a lot about web3.0 technologies like Crypto, NFTs, and Smart Contracts and I was able to implement them in React app. 

Is there some use-case where I can do some data analysis on web3.0 data, or where I can implement some ML model in order to predict something? I know that the scope of technologies is very large but If you have any ideas please write them in the comments.",True,False,False,dataengineering,t5_36en4,48036,public,self,"I need help to come up with an IDEA (use-case) for my Master's thesis project (BIG DATA, ML, WEB3.0)",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rqe1xz/i_need_help_to_come_up_with_an_idea_usecase_for/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,rip_pop_smoke_,,,[],,,,text,t2_83v4r5f5,False,False,False,[],False,False,1640688357,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rqd5dt/new_grad_data_engineer/,{},rqd5dt,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rqd5dt/new_grad_data_engineer/,False,,,6,1640688368,1,"I’m an undergrad(junior) CS major who has done DE for projects and internships, and really want to work in a DE role right out of college. However I’ve been hearing most DE roles do not hire new grads and instead want candidates with 2+ YOE. Is there a chance for me, or should I go for SWE or analyst roles then transition?
Thanks",True,False,False,dataengineering,t5_36en4,48030,public,self,New grad data engineer?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rqd5dt/new_grad_data_engineer/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,mowaptpop,,,[],,,,text,t2_h5f0hf69,False,False,False,[],False,False,1640683454,selectfrom.dev,https://www.reddit.com/r/dataengineering/comments/rqbw6o/how_to_scale_out_your_clickhouse_cluster/,{},rqbw6o,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rqbw6o/how_to_scale_out_your_clickhouse_cluster/,False,link,"{'enabled': False, 'images': [{'id': '188BSKloOqNyQFUyp7UZIl_X763_t0xHQg4KsQ9AwRs', 'resolutions': [{'height': 71, 'url': 'https://external-preview.redd.it/k781zrGreFxSPv7dlCwcd31OcmQz2QqlKegBaz6_rnY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=de99365f4e591035bc3efc991f7edd81aa32dd9d', 'width': 108}, {'height': 143, 'url': 'https://external-preview.redd.it/k781zrGreFxSPv7dlCwcd31OcmQz2QqlKegBaz6_rnY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8b29b013c2eb451abb6e26f0f12d96c1971032b3', 'width': 216}, {'height': 213, 'url': 'https://external-preview.redd.it/k781zrGreFxSPv7dlCwcd31OcmQz2QqlKegBaz6_rnY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=86c6ac708e72fb3842be227388eefaba34fb0983', 'width': 320}, {'height': 426, 'url': 'https://external-preview.redd.it/k781zrGreFxSPv7dlCwcd31OcmQz2QqlKegBaz6_rnY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2fac5f99a486c77f6762ae1ecc29fd57980e6f0e', 'width': 640}, {'height': 639, 'url': 'https://external-preview.redd.it/k781zrGreFxSPv7dlCwcd31OcmQz2QqlKegBaz6_rnY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=71a136a9e59a423f20b5b09eae527e913b7a04f7', 'width': 960}], 'source': {'height': 666, 'url': 'https://external-preview.redd.it/k781zrGreFxSPv7dlCwcd31OcmQz2QqlKegBaz6_rnY.jpg?auto=webp&amp;s=65ab63f770322e74cfb2690339cd9bf1bdd9b7d6', 'width': 1000}, 'variants': {}}]}",6,1640683465,1,,True,False,False,dataengineering,t5_36en4,48022,public,https://b.thumbs.redditmedia.com/61D7_2oHx8H_i4Obxghh2g3ew4MVObn6wJIpEJa_8eU.jpg,How to Scale Out Your ClickHouse Cluster,0,[],1.0,https://selectfrom.dev/how-to-scale-out-your-clickhouse-cluster-aaccfd111a37,all_ads,6,,,,,,93.0,140.0,https://selectfrom.dev/how-to-scale-out-your-clickhouse-cluster-aaccfd111a37,,,,,,,,,,
[],False,Own_Archer3356,,,[],,,,text,t2_7qs0ir3r,False,False,False,[],False,False,1640682136,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rqbjgu/tech_stack_with_snowflake_tool/,{},rqbjgu,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rqbjgu/tech_stack_with_snowflake_tool/,False,,,6,1640682147,1,"What will be the best and minimum tech stack to learn with the Snowflake tool?

Currently, I know Python and SQL.

Could you please guide me on this?",True,False,False,dataengineering,t5_36en4,48021,public,self,Tech Stack with Snowflake Tool,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rqbjgu/tech_stack_with_snowflake_tool/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Xyrku000,,,[],,,,text,t2_5ofbuhev,False,False,False,[],False,False,1640666215,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rq6v3g/getting_an_internship/,{},rq6v3g,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rq6v3g/getting_an_internship/,False,,,6,1640666225,1,"I am enrolled for a computer science bachelor's (junior at the moment) and going on LinkedIn to apply for remote DE internships to replace my grocery store job. I have a little experience with Python and SQL—just one class on each. Should a scattershot approach work? I am just applying for anything with ""data intern"" as keywords. I am hungry for any experience in the field. Should I be looking for a more entry-level position, since I've had no experience at all yet? Am I on the right track? (Obviously I'm not asking you guys for an internship—just thoughts!). Also will remote options work with my 2011 Macbook? I live quite outside Chattanooga and will get a car in the next few months hopefully so I can get an in-person internship in Chattanooga.",True,False,False,dataengineering,t5_36en4,48018,public,self,Getting an internship(?),0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rq6v3g/getting_an_internship/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,nexcorp,,,[],,,,text,t2_yssz7,False,False,False,[],False,False,1640663934,asiaposts.com,https://www.reddit.com/r/dataengineering/comments/rq63gx/how_do_you_retrieve_data_from_a_data_warehouse/,{},rq63gx,False,True,False,False,False,True,False,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rq63gx/how_do_you_retrieve_data_from_a_data_warehouse/,False,link,"{'enabled': False, 'images': [{'id': 'jfXk90R4ZRXVLKybYtpq-ywCft9Rc3JcaiHy6WDJsl4', 'resolutions': [{'height': 54, 'url': 'https://external-preview.redd.it/FREk4BolcoEhQI3SjQ6X7PMCMEf1jqb4GDARuqRGDfw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=728fbc75bf45fed4c36184d6a61fc24bf62ac866', 'width': 108}, {'height': 108, 'url': 'https://external-preview.redd.it/FREk4BolcoEhQI3SjQ6X7PMCMEf1jqb4GDARuqRGDfw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1e3a653e7ba0ee3b5c932e532b9503b059e3cfb0', 'width': 216}, {'height': 160, 'url': 'https://external-preview.redd.it/FREk4BolcoEhQI3SjQ6X7PMCMEf1jqb4GDARuqRGDfw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cce4d1d61a3f6e0c008819c17db35d8b961c03d2', 'width': 320}, {'height': 320, 'url': 'https://external-preview.redd.it/FREk4BolcoEhQI3SjQ6X7PMCMEf1jqb4GDARuqRGDfw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f08addfc03ce29c431a172605292806c09eecb13', 'width': 640}], 'source': {'height': 400, 'url': 'https://external-preview.redd.it/FREk4BolcoEhQI3SjQ6X7PMCMEf1jqb4GDARuqRGDfw.jpg?auto=webp&amp;s=deb97be648608b318c4ab9ad1a604b5523f3eaa0', 'width': 800}, 'variants': {}}]}",6,1640663945,1,,True,False,False,dataengineering,t5_36en4,48017,public,https://b.thumbs.redditmedia.com/VXXjUEEZStAtSlUOVgduEOwjSY2CINfRWXPGk29hdpg.jpg,How do you retrieve data from a data warehouse?,0,[],1.0,https://asiaposts.com/data-warehouse-leaders-need-to-stop-search-time-for-productivity/,all_ads,6,,,,,,70.0,140.0,https://asiaposts.com/data-warehouse-leaders-need-to-stop-search-time-for-productivity/,,,,,,,,,,
[],False,Yamii_theBlack_bull,,,[],,,,text,t2_fhz9x2a0,False,False,False,[],False,False,1640657290,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rq3sh1/upload_csv_to_s3_with_aws_lambda/,{},rq3sh1,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rq3sh1/upload_csv_to_s3_with_aws_lambda/,False,,,6,1640657300,1,"hey,

I am very new to AWS. I am working on a DE project. in this project, I am getting data from an online static link that spits out JSON.

I want to get that data, convert it to CSV, and upload it to the AWS S3 bucket. I also want to run this lambda function every 15 days.

I am trying to do this by following [https://www.youtube.com/watch?v=vXiZO1c5Sk0](https://www.youtube.com/watch?v=vXiZO1c5Sk0) for some reason had no luck

here is my code;-[https://github.com/Mandeepsingh666/lambda\_aws/blob/main/lambda.py](https://github.com/Mandeepsingh666/lambda_aws/blob/main/lambda.py)

I have attached the IAM role that has [AmazonS3FullAccess](https://console.aws.amazon.com/iam/home#/policies/arn%3Aaws%3Aiam%3A%3Aaws%3Apolicy%2FAmazonS3FullAccess), [AWSLambdaBasicExecutionRole](https://console.aws.amazon.com/iam/home#/policies/arn%3Aaws%3Aiam%3A%3Aaws%3Apolicy%2Fservice-role%2FAWSLambdaBasicExecutionRole) policies

I have attached some screenshots of the output.

can some help me?

[execution results ](https://preview.redd.it/jet0v5bjy6881.png?width=2072&amp;format=png&amp;auto=webp&amp;s=ec14dab71f7c5f06dec385e59edac9f88550b867)

[python function](https://preview.redd.it/wyolpwajy6881.png?width=1554&amp;format=png&amp;auto=webp&amp;s=c0bb8c8c1a3c6a80524e8fc4eca2f4edbb99b0ab)

[policies attached to the role](https://preview.redd.it/w4jnnpajy6881.png?width=2146&amp;format=png&amp;auto=webp&amp;s=c0cca775e930442d0031fd9abf0c0d9d8ee9ff62)",True,False,False,dataengineering,t5_36en4,48010,public,https://b.thumbs.redditmedia.com/33JmiOa6C_6OzCfXNlIJ8PaCeyoqeR1PKiOE6yr0rJE.jpg,Upload CSV to s3 with AWS lambda.,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rq3sh1/upload_csv_to_s3_with_aws_lambda/,all_ads,6,,,,,,44.0,140.0,,"{'jet0v5bjy6881': {'e': 'Image', 'id': 'jet0v5bjy6881', 'm': 'image/png', 'p': [{'u': 'https://preview.redd.it/jet0v5bjy6881.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e641676c9c9946bbe24814a1823a6f6601683df2', 'x': 108, 'y': 33}, {'u': 'https://preview.redd.it/jet0v5bjy6881.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5b3c9089a576aa84eb69bbe43888f50877048fb2', 'x': 216, 'y': 67}, {'u': 'https://preview.redd.it/jet0v5bjy6881.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e55d9efad8f09abf501aef3318c1be21258d2b0a', 'x': 320, 'y': 100}, {'u': 'https://preview.redd.it/jet0v5bjy6881.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2cd558aebdcb916616bb8bfad967008f0a784efb', 'x': 640, 'y': 201}, {'u': 'https://preview.redd.it/jet0v5bjy6881.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=62e34afe9319372bdd2ada2c5876b215ba6e625d', 'x': 960, 'y': 302}, {'u': 'https://preview.redd.it/jet0v5bjy6881.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b186a00588e46bb77da1ccb19bd6d085fff1d9cf', 'x': 1080, 'y': 339}], 's': {'u': 'https://preview.redd.it/jet0v5bjy6881.png?width=2072&amp;format=png&amp;auto=webp&amp;s=ec14dab71f7c5f06dec385e59edac9f88550b867', 'x': 2072, 'y': 652}, 'status': 'valid'}, 'w4jnnpajy6881': {'e': 'Image', 'id': 'w4jnnpajy6881', 'm': 'image/png', 'p': [{'u': 'https://preview.redd.it/w4jnnpajy6881.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0f63d69c6f22d61c9389d9879e9f30899ad64a39', 'x': 108, 'y': 12}, {'u': 'https://preview.redd.it/w4jnnpajy6881.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5959ad13f0c6126e50a60c74e71c9eb63e6c354e', 'x': 216, 'y': 25}, {'u': 'https://preview.redd.it/w4jnnpajy6881.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=761d91800ad13d510489ee1bf760ec9df9ef610c', 'x': 320, 'y': 37}, {'u': 'https://preview.redd.it/w4jnnpajy6881.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=278b6c3716e36cbcef33a75255cc3a1844c814dd', 'x': 640, 'y': 74}, {'u': 'https://preview.redd.it/w4jnnpajy6881.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=282722dca78392d81ce7a4d102583ec2aa9b1e73', 'x': 960, 'y': 111}, {'u': 'https://preview.redd.it/w4jnnpajy6881.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ad109d2d97fed9c46caf87d393e3008600825183', 'x': 1080, 'y': 125}], 's': {'u': 'https://preview.redd.it/w4jnnpajy6881.png?width=2146&amp;format=png&amp;auto=webp&amp;s=c0cca775e930442d0031fd9abf0c0d9d8ee9ff62', 'x': 2146, 'y': 250}, 'status': 'valid'}, 'wyolpwajy6881': {'e': 'Image', 'id': 'wyolpwajy6881', 'm': 'image/png', 'p': [{'u': 'https://preview.redd.it/wyolpwajy6881.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=524d11666718aeda1279b6f98faa4e6e7f79de7d', 'x': 108, 'y': 57}, {'u': 'https://preview.redd.it/wyolpwajy6881.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2852022feafac8dbb1f7df33f5bb246c9553a4c5', 'x': 216, 'y': 114}, {'u': 'https://preview.redd.it/wyolpwajy6881.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ff892111fe069180e8891160dca66155f45ca6b3', 'x': 320, 'y': 169}, {'u': 'https://preview.redd.it/wyolpwajy6881.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0ae4fdac825ff5e59c77662890c7ecfb41f0150b', 'x': 640, 'y': 338}, {'u': 'https://preview.redd.it/wyolpwajy6881.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b7654298b2d02caaf16007c0fae4566800493b32', 'x': 960, 'y': 507}, {'u': 'https://preview.redd.it/wyolpwajy6881.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=faefc79a72b4a4d8c25b68072a5c460ba725a08d', 'x': 1080, 'y': 571}], 's': {'u': 'https://preview.redd.it/wyolpwajy6881.png?width=1554&amp;format=png&amp;auto=webp&amp;s=c0bb8c8c1a3c6a80524e8fc4eca2f4edbb99b0ab', 'x': 1554, 'y': 822}, 'status': 'valid'}}",,,,,,,,,
[],False,twadftw10,,,[],,,,text,t2_991xsmvf,False,False,False,[],False,False,1640650102,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rq1c0i/kafka_best_practices_for_de/,{},rq1c0i,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rq1c0i/kafka_best_practices_for_de/,False,,,6,1640650112,1,"How is everyone utilizing Kafka in DE? I am wondering what the most popular use cases for Kafka besides CDC data streaming/processing. Is it common to have reports query Kafka with some type of middleware consumer API layer that can query it? 

How are your teams building/maintaining Kafka infrastructure?   
1. Fully managed with Confluent?  
2. Fully managed with AWS MSK?  
3. Self-managed with a Kubernetes cluster?

Lastly, what language(s) are your teams using to build Kafka apps? Scala or Java?",True,False,False,dataengineering,t5_36en4,48005,public,self,Kafka best practices for DE,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rq1c0i/kafka_best_practices_for_de/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Username_917,,,[],,,,text,t2_95o0tunh,False,False,False,[],False,False,1640631717,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rpukeg/how_to_make_a_career_switch_from_hr_to_data/,{},rpukeg,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rpukeg/how_to_make_a_career_switch_from_hr_to_data/,False,,,6,1640631727,1,"Hi. As the title says, I started my career a little over two years ago in HR. However, it’s not the typical HR career you have in mind. I got my masters degree in industrial and organizational psychology and was hired into an analytical role within HR. My job focused on analyzing people data. I regularly use Excel to run analyses, but also learned SQL (and Oracle specific syntax) to run or optimize data model queries for output reporting which I then use for analysis. 

Over the two years in my role I began to project lead Oracle module implementations, and became really good with the software. I began performing ETLs to load large volume of data into specific business object areas, and began specializing in specific functional areas of the software. In addition to that, I also lead the initiative to develop dashboard visualization (became Tableau desktop verified) and developed new pipeline data workflows for data refreshes.

I am now starting a new role as a senior Oracle functional analyst for a multinational company. As of now, I am working towards Oracle SaaS cloud certification. 

I would like to pivot my career towards data engineering. I looked into various areas, data analyst, data scientist, software engineering, but realized data engineering is most aligned with my interest. I enjoy the building aspect more than the strategic data analysis and storytelling piece to managers.

In terms of training, I was looking into the IBM data engineering certification or the MIT program. Can anyone recommend which program, or others, that could help me prepare for this career switch?

Thanks.",True,False,False,dataengineering,t5_36en4,47996,public,self,How to make a career switch from HR to data engineering?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rpukeg/how_to_make_a_career_switch_from_hr_to_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,TrainquilOasis1423,,,[],,,,text,t2_3wxhxt1i,False,False,False,[],False,False,1640630321,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rpu1p0/anyone_know_of_any_job_fairs_going_on_near_west/,{},rpu1p0,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rpu1p0/anyone_know_of_any_job_fairs_going_on_near_west/,False,,,6,1640630332,1,"On paper my resume looks like shit. No degree, few online certificates, and only 2 years exp as an analyst. Been getting a lot of auto rejection letters lately. However I know I can sell myself if I can just sit down with someone. Anyone know of any job fairs going on so I can meet someone face to face before they auto reject my resume?",True,False,False,dataengineering,t5_36en4,47996,public,self,"Anyone know of any job fairs going on near west cost USA, virtually?",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rpu1p0/anyone_know_of_any_job_fairs_going_on_near_west/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,tea_horse,,,[],,,,text,t2_1w1o79i7,False,False,False,[],False,False,1640627144,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rpsucx/apache_hive_best_practice_advice/,{},rpsucx,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rpsucx/apache_hive_best_practice_advice/,False,,,6,1640627154,1,"I'm working on a project which is using data split into 4 categories (all related to socio-economic metrics: crime, health care, food availability, government expenditure) 

Each category (e.g. health care) will have several files (e.g. access to health care) with different metrics in a single file (e.g. access to healthcare by household income, % of rural population with access to healthcare, etc) -  the metrics names are on a row by row basis with the measure for every recorded year in the columns.

All the data is at global level, so the rows in each csv will essentially contain all the metrics for every country (on average about 10 rows per country in each file)

Most csv files in the same category have the same columnar format

If this was a relational DB, I'd be looking to normalize this data and break it all down into several tables. But I've been researching Hive and apparently normalisation is not the best option in most cases

As I'm new to Hive, I'd much appreciate some advice on the best practices. 

Based on what I've been reading online. My plan is to reshape the files so there is a single year column and a column for each metric (simply bringing the metrics from rows to columns and vice versa for the years). Once done, concatenate the files within the same category into one ORC table (so 4 tables in total)

If there are any good resources on Hive best practices that you can recommend, please also suggest some. I'm slightly overwhelmed with the sheer amount of resources

Thanks!",True,False,False,dataengineering,t5_36en4,47992,public,self,Apache Hive - best practice advice,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rpsucx/apache_hive_best_practice_advice/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,datameshlearning,,,[],,,,text,t2_a926z8al,False,False,False,[],False,False,1640624807,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rprzoh/what_are_people_doingusing_for_data_contracts/,{},rprzoh,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rprzoh/what_are_people_doingusing_for_data_contracts/,False,,,6,1640624818,1,"So, I have been chatting with a few folks in the data mesh and broader data community re data contracts - essentially extending the idea of API/schema contracts to data where there is also a guarantee to not change the semantic meaning of the data (prevents the unbroken pipeline but broken data issue of only using schema contracts).

If you aren't familiar with API/schema contracts, [here's a brief](https://jools.dev/what-is-an-api-contract) on API contracts. I think of schema contracts as basically the same (a consumer ""agrees"" to the contract and if things change, they get alerted). 

But searching around the web, I am finding VERY little on data contracts or even schema contracts. I will drop the links I have found re data contracts in a comment but what are you using (tools) or doing (processes?) to do data contracts? Is it all roll-your-own? Or is anyone even doing data contracts?

I am launching a podcast soon so we can dig into specifics of ""how tos"" re data mesh and related topics and data contracts is my first deep dive. So if there are people that also want to chat or potentially be interviewed (doing 3-5 interviews a week as I'm a crazy person), hit me up. At 3 scheduled and \~4 potentials that have said maybe, so hopefully it's a good start on solving this.",True,False,False,dataengineering,t5_36en4,47987,public,self,What are people doing/using for data contracts?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rprzoh/what_are_people_doingusing_for_data_contracts/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,MaxoLP,,,[],,,,text,t2_a24apeie,False,False,False,[],False,False,1640622727,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rpr8hc/downloading_websites_using_a_nas/,{},rpr8hc,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rpr8hc/downloading_websites_using_a_nas/,False,,,6,1640622738,1,"

Good day, 
is it possible to download web pages automatically with my QNAP T419+ or any QNAP?
So I would like whenever a website uploads a new article or changes it, that my QNAP automatically downloads this article/website.
I can then evaluate these records.",True,False,False,dataengineering,t5_36en4,47986,public,self,Downloading websites using a NAS,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rpr8hc/downloading_websites_using_a_nas/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,sneakerhead1310,,,[],,,,text,t2_dhkzk5h,False,False,False,[],False,False,1640617169,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rppa4w/seeking_advice_on_data_collection_for_multiple/,{},rppa4w,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rppa4w/seeking_advice_on_data_collection_for_multiple/,False,,,6,1640617179,1,"I am part of a project that consists of multiple organizations wanting to share data with each other on an ongoing basis. It would consist of the same data elements from each company, and it would be refreshed at a set interval. Ideally, we would like to join all of the collected data together while providing access to the data for each of the companies involved. What would be the best approach for accomplishing this that would also have a high level of security?",True,False,False,dataengineering,t5_36en4,47984,public,self,"Seeking advice on data collection for multiple organizations into a centralized location. Would I utilize a data lake, data warehouse, or something else entirely?",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rppa4w/seeking_advice_on_data_collection_for_multiple/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Nekoshido,,,[],,,,text,t2_174l45,False,False,False,[],False,False,1640601126,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rpkond/api_integration_saas_with_hubspot_compatibility/,{},rpkond,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rpkond/api_integration_saas_with_hubspot_compatibility/,False,self,"{'enabled': False, 'images': [{'id': 'MjfGOJmIC97_sJiNRwzgq1oJRu_xJV1IVBcRy7jCJRE', 'resolutions': [{'height': 60, 'url': 'https://external-preview.redd.it/MhzyibtP6PjDXTLmN75Rz_BOQZmHEQxhsCEpZqD8IR4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=50cc1c803a01234d40ebc9aa249acd7b27f0930f', 'width': 108}, {'height': 121, 'url': 'https://external-preview.redd.it/MhzyibtP6PjDXTLmN75Rz_BOQZmHEQxhsCEpZqD8IR4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=58bac6735307e6f7d0bea3d690e56311c697eeff', 'width': 216}, {'height': 180, 'url': 'https://external-preview.redd.it/MhzyibtP6PjDXTLmN75Rz_BOQZmHEQxhsCEpZqD8IR4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f47eb83a4a96f4c90d1a278beda2bd57186baebd', 'width': 320}, {'height': 360, 'url': 'https://external-preview.redd.it/MhzyibtP6PjDXTLmN75Rz_BOQZmHEQxhsCEpZqD8IR4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=262ce9c4c188a95a637bac7e9eb790b33476a879', 'width': 640}, {'height': 540, 'url': 'https://external-preview.redd.it/MhzyibtP6PjDXTLmN75Rz_BOQZmHEQxhsCEpZqD8IR4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b7f2385bbed4716fb09adb92c2d341030897f4f8', 'width': 960}, {'height': 607, 'url': 'https://external-preview.redd.it/MhzyibtP6PjDXTLmN75Rz_BOQZmHEQxhsCEpZqD8IR4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0b789a5f1046f1311d0a03e2d1f6b514811e613d', 'width': 1080}], 'source': {'height': 675, 'url': 'https://external-preview.redd.it/MhzyibtP6PjDXTLmN75Rz_BOQZmHEQxhsCEpZqD8IR4.jpg?auto=webp&amp;s=32022a3d0c6a9af391df7ed0f6ead69f84d52d12', 'width': 1200}, 'variants': {}}]}",6,1640601137,1,"I would like to ask if you know of any SAAS that offers complete Hubspot compatibility, especially with this API:

[https://developers.hubspot.com/docs/api/crm/feedback-submissions](https://developers.hubspot.com/docs/api/crm/feedback-submissions)

I've tried Trials with:

* Fivetran
* Segment
* Rudderstack
* Stitch
* Hevo Data 
* Panoply
* Dataddo

But any of them have complete integration with Hubspot and is difficult to request new features.

Any help is welcomed. Thank you in advance",True,False,False,dataengineering,t5_36en4,47970,public,self,API Integration SAAS with Hubspot compatibility,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rpkond/api_integration_saas_with_hubspot_compatibility/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Xyrku000,,,[],,,,text,t2_5ofbuhev,False,False,False,[],False,False,1640573317,i.redd.it,https://www.reddit.com/r/dataengineering/comments/rpcqe2/which_one_of_these_might_correspond_to/,{},rpcqe2,False,True,False,False,True,True,False,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rpcqe2/which_one_of_these_might_correspond_to/,False,image,"{'enabled': True, 'images': [{'id': '4gzxNSGNr92ee2JSM2Z-YHecPXThdKmc0lz9rtitVvs', 'resolutions': [{'height': 192, 'url': 'https://preview.redd.it/q1u9y6t710881.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8fe45e2ef7474252b9cc20485e87961a083df12a', 'width': 108}, {'height': 384, 'url': 'https://preview.redd.it/q1u9y6t710881.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6adbdf24eb2707a327e23bece29ed1de5ebf7b05', 'width': 216}, {'height': 568, 'url': 'https://preview.redd.it/q1u9y6t710881.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=998af68474383cc4cf82e836133a400ae2dc9b22', 'width': 320}, {'height': 1137, 'url': 'https://preview.redd.it/q1u9y6t710881.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d11cef0934452e017c6f162710911e58d020256a', 'width': 640}], 'source': {'height': 1536, 'url': 'https://preview.redd.it/q1u9y6t710881.png?auto=webp&amp;s=69146cff31e939c2378988d55132d376594a372b', 'width': 864}, 'variants': {}}]}",6,1640573327,1,,True,False,False,dataengineering,t5_36en4,47956,public,https://b.thumbs.redditmedia.com/xgrU1vQ8GJiB469lbQGh8dFJSCCmn9l30PGBnSh4hTA.jpg,Which one of these might correspond to software/data engineering? I'm on O*NET.,0,[],1.0,https://i.redd.it/q1u9y6t710881.png,all_ads,6,,,,,,140.0,140.0,https://i.redd.it/q1u9y6t710881.png,,,,,,,,,,
[],False,creatstar,,,[],,,,text,t2_7opejf1s,False,False,False,[],False,False,1640563567,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rp9isl/have_you_tried_starrocks/,{},rp9isl,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rp9isl/have_you_tried_starrocks/,False,self,"{'enabled': False, 'images': [{'id': 'XNIj0uGdn8A_wAY3fSnFSVMisTXytTN4nEqpgM3GNHo', 'resolutions': [{'height': 54, 'url': 'https://external-preview.redd.it/Jg9Im04lFQOJvBFeSdg2N716pHAhbFkFzisX_xjnBvc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ef6046a126eac816a42b92749aa94b6af95bf027', 'width': 108}, {'height': 108, 'url': 'https://external-preview.redd.it/Jg9Im04lFQOJvBFeSdg2N716pHAhbFkFzisX_xjnBvc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=501ab3ece8f1cfab5dac92fcdd30334b421c1ad1', 'width': 216}, {'height': 160, 'url': 'https://external-preview.redd.it/Jg9Im04lFQOJvBFeSdg2N716pHAhbFkFzisX_xjnBvc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f4da7349b3e5d4ea047d5fd1bff0dfb5faec2609', 'width': 320}, {'height': 320, 'url': 'https://external-preview.redd.it/Jg9Im04lFQOJvBFeSdg2N716pHAhbFkFzisX_xjnBvc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=43009a3979382c3892452d0b005151beb0225cf8', 'width': 640}, {'height': 480, 'url': 'https://external-preview.redd.it/Jg9Im04lFQOJvBFeSdg2N716pHAhbFkFzisX_xjnBvc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=005e518695f7abd8ae9b7c8b75162d230aa3a519', 'width': 960}, {'height': 540, 'url': 'https://external-preview.redd.it/Jg9Im04lFQOJvBFeSdg2N716pHAhbFkFzisX_xjnBvc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=18da8d8773ed38c7dab087bfb0c3c4c55d408ea0', 'width': 1080}], 'source': {'height': 600, 'url': 'https://external-preview.redd.it/Jg9Im04lFQOJvBFeSdg2N716pHAhbFkFzisX_xjnBvc.jpg?auto=webp&amp;s=8a97f0e7cd843543cab93d9786e1fb4e1cb5171a', 'width': 1200}, 'variants': {}}]}",6,1640563577,1,"Here is a interesting project, [https://github.com/StarRocks/starrocks](https://github.com/StarRocks/starrocks) 

It seems that StarRocks runs blazing fast, especially when it comes to multi-table join queries. I also find a comparison to ClickHouse [https://medium.com/@wangtianyi\_86442/clickhouse-or-starrocks-here-is-a-detailed-comparison-c743a0b7b95f](https://medium.com/@wangtianyi_86442/clickhouse-or-starrocks-here-is-a-detailed-comparison-c743a0b7b95f)

Can anyone give some insight about this MPP database?",True,False,False,dataengineering,t5_36en4,47951,public,self,Have you tried StarRocks?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rp9isl/have_you_tried_starrocks/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,novato78,,,[],,,,text,t2_om83x98,False,False,False,[],False,False,1640552339,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rp5jqp/google_cloud_data_engineer/,{},rp5jqp,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rp5jqp/google_cloud_data_engineer/,False,,,6,1640552350,1,"Hi Group
I have a upcoming phone screen for google cloud data engineer . There is not much information what is expected like it’s  leetcode hard or medium or mostly SQL since it’s on data side 

Does any body given a idea what to expect . Recruiter just says 45 min screen and no additional information",True,False,False,dataengineering,t5_36en4,47944,public,self,Google cloud data engineer,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rp5jqp/google_cloud_data_engineer/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Meriu,,,[],,,,text,t2_dbb2y,False,False,False,[],False,False,1640535435,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rp0068/managing_data_lake_clean_what_type_of_data_should/,{},rp0068,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rp0068/managing_data_lake_clean_what_type_of_data_should/,False,,,6,1640535446,1,"Hello Everyone!

I'm working on some personal project scraping data from friend's API and would like to ask you for advice as not I'm trying to figure out what data should be stored on each layer of my Data Lake. I've come up with idea to build 3 layers of my Data Lake: Bronze, Silver and Gold. I'm trying to build Kimball style Delta Lake on top of those data, at the moment I'm using Databricks for it. In mid to long term I'm thinking about moving to Postgres to simulate Data Warehouse loading jobs and test writing scripts as python files and executing them via HD Insight instead of running it inside Databricks environment. 

My main concern is if it's worth to store archive data on Staging layer or should I only maintain only one file for each normalized endpoint. For this layer, maintaining schema that creates new source directory with each job is a little pain in the ass for me personally, but I'm not sure if it's important yet. 

Also I'm a little concerned about Gold layer schema - is it enough  to maintain sufficient  access control if it was business use or should I redesign it? If so, what layer should be used and when?

\-----------------

&amp;#x200B;

My architecture concept looks like described below:

Bronze Layer:

Purpose: reads application API and stores it in distinct files. Each GET job yields new file directory named of job date. Files are saved as .json

Schema:

    bronze:
    |- General
        |- Application 1
            |- Endpoint 1
                |- ts_XX-XX-XX_xx:xx:xx
                |- ts_XX-XX-XX_xx:xx:xx
            |- Endpoint 2
        |- Application 2
            |- Endpoint 1
                |- Schema 1
                |- Schema 2
    |- Sensitive
        |- Application 1 
    ...

\-----------------

Silver Layer:

Purpose:  normalizes data and applies correct data formats to schemas. Files saved as .parquet

Schema:

    silver:
    |- General
        |-Master:
            |- Application 1
                |- Endpoint 1
                    |- ts_XX-XX-XX_xx:xx:xx
                    |- ts_XX-XX-XX_xx:xx:xx
                |- Endpoint 2
            |- Application 2
                |- Endpoint 1
                    |- Schema 1
                    |- Schema 2
        |- Delta:
            |- Application 1
                |- Endpoint 1
                    | file.parquet
                |- Endpoint 2
                    | file.parquet
            |- Application 2
                |- Endpoint 1
                    |- Schema 1
                        | file.parquet
                    |- Schema 2
                        | file.parquet
    |- Sensitive
        |- Application 1
    ...

\-----------------

Gold layer:

Purpose:  Provides analytics-grade datasets. Files saved as .csv as I'm reading them later using functions. Each next job moves previous result\_file to Archive directory so in case of any ETL error I'm able to easily revert data to reflect previous cycle's data

Schema:

    bronze:
    |- General
        |- Application 1
            |- Dataset 1
                |- Archive
                | result_file.csv
            |- Dataset 2
                |- Archive
                | result_file.csv
        |- Application 2
            |- Dataset 1
    ...

Thanks for taking your time and Happy Holidays!",True,False,False,dataengineering,t5_36en4,47940,public,self,Managing Data lake clean - what type of data should be stored on each layer,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rp0068/managing_data_lake_clean_what_type_of_data_should/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Professional-Bee-937,,,[],,,,text,t2_8cevbrvg,False,False,False,[],False,False,1640534801,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rozsik/data_engineers_salaries_in_canada/,{},rozsik,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rozsik/data_engineers_salaries_in_canada/,False,,,6,1640534811,1,"Hello,
I'm 26yo and paid 6k€ net after taxes in Paris.
I have about 3 years of full time work experience.
I would love to go to Canada (Toronto or Quebec), what are the salaries there ?
Thanks",True,False,False,dataengineering,t5_36en4,47940,public,self,Data engineers salaries in Canada?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rozsik/data_engineers_salaries_in_canada/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,shittyfuckdick,,,[],,,,text,t2_1kset4fg,False,False,False,[],False,False,1640531765,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/roytrv/what_to_learn_to_become_more_of_a_full_stack/,{},roytrv,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/roytrv/what_to_learn_to_become_more_of_a_full_stack/,False,,,6,1640531775,1,"I want to branch out in my free time to learn more front end stuff. As a DE my knowledge is all backend like Linux, Python, DBs, etc. 

I’m thinking of just learning some HTML and JavaScript so I can do some basic front end stuff and build webapps. This isn’t necessarily for my career just for own personal development. I’m not sure I want to be a DE my whole life and would like to round out my skills to be more Full Stack than just back end.",True,False,False,dataengineering,t5_36en4,47934,public,self,What to Learn to Become More of a Full Stack Engineer?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/roytrv/what_to_learn_to_become_more_of_a_full_stack/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Own_Archer3356,,,[],,,,text,t2_7qs0ir3r,False,False,False,[],False,False,1640526266,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rox78g/technology_stack_with_snowflake/,{},rox78g,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rox78g/technology_stack_with_snowflake/,False,,,6,1640526277,1,"Hello,

So currently, I am learning tools used in DE, and I know Snowflake, python, and SQL(intermediate),

I want to know what I should learn to make my tech stack good as a DE.

Would you please let me know some more essential tools used more frequently with the above skills?

Thanks",True,False,False,dataengineering,t5_36en4,47931,public,self,Technology Stack with Snowflake,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rox78g/technology_stack_with_snowflake/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Right-Philosopher810,,,[],,,,text,t2_becac2mj,False,False,False,[],False,False,1640513278,coblob.com,https://www.reddit.com/r/dataengineering/comments/rou5l5/considerations_for_system_design_to_solve/,{},rou5l5,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rou5l5/considerations_for_system_design_to_solve/,False,link,"{'enabled': False, 'images': [{'id': 'fnbw2gAqiMricRgENVgF1mUfeOYtZ2Jwv2X0TbemM8Y', 'resolutions': [{'height': 97, 'url': 'https://external-preview.redd.it/HHVNWqmBuK2RxvHcRYVzLoaR3ngg1yTSTJDCDLCEP_c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1abce75a63e13231e272ad8da39e86e23f7b80d1', 'width': 108}, {'height': 195, 'url': 'https://external-preview.redd.it/HHVNWqmBuK2RxvHcRYVzLoaR3ngg1yTSTJDCDLCEP_c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fb392d8dd9f130dc39bd8da666746494b2ea21f1', 'width': 216}, {'height': 289, 'url': 'https://external-preview.redd.it/HHVNWqmBuK2RxvHcRYVzLoaR3ngg1yTSTJDCDLCEP_c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=22afb68d9d54e8cbcf24ea518fdf26037ea8c038', 'width': 320}, {'height': 578, 'url': 'https://external-preview.redd.it/HHVNWqmBuK2RxvHcRYVzLoaR3ngg1yTSTJDCDLCEP_c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=30c2e5ad27c2f361dbd49934722306543791c0f9', 'width': 640}], 'source': {'height': 768, 'url': 'https://external-preview.redd.it/HHVNWqmBuK2RxvHcRYVzLoaR3ngg1yTSTJDCDLCEP_c.jpg?auto=webp&amp;s=e044febdd70379c7ceccc8e37bff0a3e6f8ee931', 'width': 850}, 'variants': {}}]}",6,1640513288,1,,True,False,False,dataengineering,t5_36en4,47929,public,https://a.thumbs.redditmedia.com/StT7WWnkItgKOO57kaKUZXUfkjSSKwwJBO3t68oH3t4.jpg,"Considerations for System Design to solve Scalability, Consistency, Partition, and Availability in Distributed and Non-Distributed environments",0,[],0.99,https://coblob.com/blogs/Considerations-for-System-Design-to-solve-Scalability-Consistency-Partition-and-Availability-5b21ac3bbb7f6faaac473688,all_ads,6,,,,,,126.0,140.0,https://coblob.com/blogs/Considerations-for-System-Design-to-solve-Scalability-Consistency-Partition-and-Availability-5b21ac3bbb7f6faaac473688,,,,,,,,,,
[],False,mannu_11,,,[],,,,text,t2_3dsjwu75,False,False,False,[],False,False,1640498147,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/roqnd3/any_advice_for_2030_years_commitment_to_data/,{},roqnd3,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/roqnd3/any_advice_for_2030_years_commitment_to_data/,False,,,6,1640498158,1,"I’m just starting out in Data Engineering World. 
I want to know What would it be like after 30years and should I be committed to this field, Given that i’m interested in it. 
Will it be fruitful?",True,False,False,dataengineering,t5_36en4,47925,public,self,Any advice for 20-30 years commitment to Data engineering world?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/roqnd3/any_advice_for_2030_years_commitment_to_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,GypsyElder,,,[],,,,text,t2_43jihhnz,False,False,False,[],False,False,1640496348,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/roq6gw/is_data_engineering_stressful/,{},roq6gw,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/roq6gw/is_data_engineering_stressful/,False,,,6,1640496358,1,"Hi everyone!

I’m a senior data analyst thinking about  transitioning into something new, and data engineering is of interest. 

I currently work in biotech. My work is very fast paced and demanding. 

Generally, how is work-life balance given the industry you work in?",True,False,False,dataengineering,t5_36en4,47925,public,self,Is data engineering stressful?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/roq6gw/is_data_engineering_stressful/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,rrpelgrim,,,[],,,,text,t2_dklnu36k,False,False,False,[],False,False,1640465443,crunchcrunchhuman.com,https://www.reddit.com/r/dataengineering/comments/roh6z4/save_numpy_arrays_to_csv_files/,{},roh6z4,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/roh6z4/save_numpy_arrays_to_csv_files/,False,link,"{'enabled': False, 'images': [{'id': 'sAYY86zgdT-GUuuQxQ55XhcVr4wy0uMIajd_UOBdg3Y', 'resolutions': [{'height': 108, 'url': 'https://external-preview.redd.it/F6AGihc6sjMmnVLm-XWIe0jcP2vPZpBwjOqH2PhZX-c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0563ade4652d2408b81c0d4fd5f4ad1c26458007', 'width': 108}], 'source': {'height': 200, 'url': 'https://external-preview.redd.it/F6AGihc6sjMmnVLm-XWIe0jcP2vPZpBwjOqH2PhZX-c.jpg?auto=webp&amp;s=13353f39561ed11ccd79ec9880d3b36c19dcc81c', 'width': 200}, 'variants': {}}]}",6,1640465454,1,,True,False,False,dataengineering,t5_36en4,47899,public,https://b.thumbs.redditmedia.com/ji8q-3PQJV0akt1Z2MxlNwzq90CKdVyJ9QbalLDjbuw.jpg,Save NumPy Arrays to CSV Files,0,[],1.0,https://crunchcrunchhuman.com/2021/12/25/numpy-save-csv-write/,all_ads,6,,,,,,140.0,140.0,https://crunchcrunchhuman.com/2021/12/25/numpy-save-csv-write/,,,,,,,,,,
[],False,SeaworthinessFit7893,,,[],,,,text,t2_4bor0kj8,False,False,False,[],False,False,1640460094,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rofnm0/is_being_a_data_engineer_just_a_specialised/,{},rofnm0,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rofnm0/is_being_a_data_engineer_just_a_specialised/,False,,,6,1640460105,1,Ive been thinking about how similar both jobs are and what not and how alot of data engineers had backrounds in designing websites. So am I right or wrong with this analogy.,True,False,False,dataengineering,t5_36en4,47897,public,self,Is being a data engineer just a specialised software engineer?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rofnm0/is_being_a_data_engineer_just_a_specialised/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Born-Comment3359,,,[],,,,text,t2_5t56uq7x,False,False,False,[],False,False,1640447617,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/roc3ci/need_help_to_prepare_for_faang_data_engineer/,{},roc3ci,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/roc3ci/need_help_to_prepare_for_faang_data_engineer/,False,,,6,1640447628,1,"Hi All. First of all I wanna wish everyone Merry X-Mas and Happy New Year. May 2022 be the most glorious year for you and your loved ones.

I am preparing for a technical/design interview for one of the FAANG DE role interview and would like to know your experience of those interviews. What are the most common SQL/Python/Design questions they ask and what are the best resources to learn them?",True,False,False,dataengineering,t5_36en4,47885,public,self,Need help to prepare for FAANG data engineer interview question.,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/roc3ci/need_help_to_prepare_for_faang_data_engineer/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,B1TB1T,,,[],,,,text,t2_ad1j37ea,False,False,False,[],False,False,1640445611,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/robk4g/anyone_using_python_rayio_framework_in_production/,{},robk4g,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/robk4g/anyone_using_python_rayio_framework_in_production/,False,,,6,1640445621,1,"What's your experience with ray? Whats the advantage over spark or is it used in totally different scenarios? Some article says it's faster than spark, is this true?",True,False,False,dataengineering,t5_36en4,47884,public,self,Anyone using python ray.io framework in production?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/robk4g/anyone_using_python_rayio_framework_in_production/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,OneLong4395,,,[],,,,text,t2_c9sllfcr,False,False,False,[],False,False,1640444675,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/robb70/onedrive_storage_problem/,{},robb70,False,False,False,False,False,False,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/robb70/onedrive_storage_problem/,False,,,6,1640444686,1,[removed],True,False,False,dataengineering,t5_36en4,47883,public,self,Onedrive storage problem,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/robb70/onedrive_storage_problem/,all_ads,6,,,automod_filtered,,,,,,,,,,,,,,,
[],False,rosenloev,,,[],,,,text,t2_8ay2350q,False,False,False,[],False,False,1640438080,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ro9m5i/comments_on_using_wsl2_or_linux_for_data/,{},ro9m5i,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/ro9m5i/comments_on_using_wsl2_or_linux_for_data/,False,,,6,1640438091,1,"So I recently restarted my career as a DE, coming from a non-SWE/data background, so I don't have that much experience of what is ""best practice"" or industry standard when developing/working as a DE. I've only ever used Windows as an OS and never wanted to switch to Mac because it seems horribly overpriced. 

So far I've been using VSCode on WSL2 (Ubuntu) exclusively because I found it easier to combine with Airflow. I like the idea of Linux so much that I'm thinking of switching fully, but don't know if I'll be missing something from the combination of Windows and Linux using WSL2? Like applications that are not available for Linux OSs perhaps?
One other concern I have is that, as I work as a consultant, I'll probably work with the Azure ecosystem a lot (gaining popularity where I'm located). Would that be a hindrance i some way?

TLDR; Using WSL2 now, thinking of going full Linux. Pros/cons of doing this in DE work?

Merry Christmas 🎄",True,False,False,dataengineering,t5_36en4,47880,public,self,Comments on using WSL2 or Linux for data engineering? Pros and cons?,0,[],0.99,https://www.reddit.com/r/dataengineering/comments/ro9m5i/comments_on_using_wsl2_or_linux_for_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Surikatos,,,[],,,,text,t2_furkl6zq,False,False,False,[],False,False,1640429005,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ro7kza/small_company_looking_for_a_data_engineer/,{},ro7kza,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/ro7kza/small_company_looking_for_a_data_engineer/,False,,,6,1640429016,1,"Featured job adverts in online job portals can be expensive, so I'm wondering if anyone has recommendations for the best places to post. Advice from someone who has hired Data Engineers would be really helpful.

And if anyone is curious, here's the [job description](https://gormanconsulting.files.wordpress.com/2021/12/data-engineer-consultant-job-description-gorman-consulting.pdf).",True,False,False,dataengineering,t5_36en4,47874,public,self,Small company looking for a Data Engineer consultant: Where should we post?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ro7kza/small_company_looking_for_a_data_engineer/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Ok_Advance_2714,,,[],,,,text,t2_cwrfl9jz,False,False,False,[],False,False,1640427849,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ro7cgn/do_oracle_hire_data_engineers_and_data_scientist/,{},ro7cgn,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/ro7cgn/do_oracle_hire_data_engineers_and_data_scientist/,False,,,6,1640427859,1,"When I gave an interview as a fresher at Oracle, the interviewer from Oracle told that they don't have a lot of projects in data science as I told that I am really interested in it. I got rejected in the final round because of it though I did very well in all the other rounds. 

Now I work as a data engineer in a Service based company but really want get my foot into Oracle again after two years. Anyone here work in Oracle as data engineer? How to apply and give some interview experience",True,False,False,dataengineering,t5_36en4,47874,public,self,Do Oracle hire data engineers and data scientist?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ro7cgn/do_oracle_hire_data_engineers_and_data_scientist/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,saaaalut,,,[],,,,text,t2_cd04mw52,False,False,False,[],False,False,1640415409,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ro4kc2/as_per_the_college_syllabus_i_must_do_a/,{},ro4kc2,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/ro4kc2/as_per_the_college_syllabus_i_must_do_a/,False,,,6,1640415420,1,I am thinking backend would make more sense as I ll get exp dealing with server db and stuff.,True,False,False,dataengineering,t5_36en4,47862,public,self,"As per the college syllabus I must do a internship in web dev, I should choose backend over frontend right? I want to go in DE and aws later",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ro4kc2/as_per_the_college_syllabus_i_must_do_a/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,maximus_deUX,,,[],,,,text,t2_5fc55oje,False,False,False,[],False,False,1640408531,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ro2ve3/system_design_interview_for_data_engineers/,{},ro2ve3,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/ro2ve3/system_design_interview_for_data_engineers/,False,self,"{'enabled': False, 'images': [{'id': 'PP9m09PYBKp-0p3fChCJ8UOWJKR_Vf1w7VQek4Y3pqU', 'resolutions': [{'height': 60, 'url': 'https://external-preview.redd.it/w44guMKjJA3SxkdYACchEVHIi2HOqK-YF_6cokOzvoM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2abaefd6600eb4b468a6369339ac253d5fbc48e8', 'width': 108}, {'height': 121, 'url': 'https://external-preview.redd.it/w44guMKjJA3SxkdYACchEVHIi2HOqK-YF_6cokOzvoM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c79f23c60aec77a2b8e974dc656c96121b1a35b9', 'width': 216}, {'height': 180, 'url': 'https://external-preview.redd.it/w44guMKjJA3SxkdYACchEVHIi2HOqK-YF_6cokOzvoM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b39513262a4a7b7c102b4afe5fea006b7db4741f', 'width': 320}, {'height': 360, 'url': 'https://external-preview.redd.it/w44guMKjJA3SxkdYACchEVHIi2HOqK-YF_6cokOzvoM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=548371bb73e0d7c9695c206fec9fcc2596312c0a', 'width': 640}, {'height': 540, 'url': 'https://external-preview.redd.it/w44guMKjJA3SxkdYACchEVHIi2HOqK-YF_6cokOzvoM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2a23baff76291ff2c8b8d878b9387aa4ebc6f5bd', 'width': 960}, {'height': 607, 'url': 'https://external-preview.redd.it/w44guMKjJA3SxkdYACchEVHIi2HOqK-YF_6cokOzvoM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d7b9c21519b0e32af8a3756819d3ce9d9cee3a05', 'width': 1080}], 'source': {'height': 1080, 'url': 'https://external-preview.redd.it/w44guMKjJA3SxkdYACchEVHIi2HOqK-YF_6cokOzvoM.jpg?auto=webp&amp;s=ef480dd21d70774921c89cb6a202632bc9e9a92c', 'width': 1920}, 'variants': {}}]}",6,1640408542,1,I am planning to take up  interviews fir DE roles in 2022 and was wondering how i should prepare for the system design round ? I found that grokking the system design interview from [educative.io](https://educative.io) to cater towards SWE as a whole. So was wondering if the same holds good for data engineer of if there was anything else that I need to focus on ?,True,False,False,dataengineering,t5_36en4,47853,public,self,System Design Interview for Data Engineers,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ro2ve3/system_design_interview_for_data_engineers/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,surverse,,,[],,,,text,t2_m3qkptg,False,False,False,[],False,False,1640397561,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ro004g/resources_for_breaking_into_blockchain_field_as_a/,{},ro004g,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,False,1,0,False,all_ads,/r/dataengineering/comments/ro004g/resources_for_breaking_into_blockchain_field_as_a/,False,,,6,1640397572,1,"Does anyone have any good resources or recommendations on getting a job in a crypto/blockchain company as a data engineer/analyst?

This is the question I have at the moment, and I'm wondering if there are any groups/telegrams/discords that might be focused on data engineers in Blockchain. As the crypto industry matures is seem like they have a strong need for data engineers, so I'm interested if anyone has any experience getting into crypto, or any tips for data-engineering projects to better understand blockchain data. Thanks y'all.",True,False,False,dataengineering,t5_36en4,47849,public,self,Resources for breaking into blockchain field as a data engineer/analyst,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ro004g/resources_for_breaking_into_blockchain_field_as_a/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,LilDudeFromStreet,,,[],,,,text,t2_w8htd,False,False,False,[],False,False,1640387048,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rnx09s/restapi_to_s3_bucket_using_python/,{},rnx09s,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rnx09s/restapi_to_s3_bucket_using_python/,False,,,6,1640387058,1,"TLDR - Could somebody advice/ send me a URL/ refer me to a course that goes into step by step detail on how to extract data from API and dump it into S3 using Python?

I've been in ""traditional"" ETL/BI space for almost a decade now (SSIS/Informatica/Azure Data Factory etc.) and recently transitioned to a modern tech stack (Fivetran/DBT/Airflow etc.) I'm learning basics of Python everyday but to accomplish this project I've started, I'll need to find out how to use Python to get data from REST -&gt; S3.",True,False,False,dataengineering,t5_36en4,47839,public,self,REST(API) to S3 bucket using Python,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rnx09s/restapi_to_s3_bucket_using_python/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Own_Archer3356,,,[],,,,text,t2_7qs0ir3r,False,False,False,[],False,False,1640372815,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rnsnuh/personal_project_ideas_with_snowflake/,{},rnsnuh,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rnsnuh/personal_project_ideas_with_snowflake/,False,,,6,1640372826,1,"Hello,

Could you guys please help me with some of the personal project ideas that I can create with Snowflake?

Currently, I have knowledge of Python, SQL, Snowflake, and ETL.

Will be helpful if you can provide some guidance on this.",True,False,False,dataengineering,t5_36en4,47827,public,self,Personal Project ideas with Snowflake,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rnsnuh/personal_project_ideas_with_snowflake/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,TrainquilOasis1423,,,[],,,,text,t2_3wxhxt1i,False,False,False,[],False,False,1640370066,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rnrtbz/is_it_unrealistic_to_expect_a_higher_salary_for/,{},rnrtbz,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rnrtbz/is_it_unrealistic_to_expect_a_higher_salary_for/,False,,,6,1640370077,1,"TLDR: Data Analyst working to transition to Data Engineer looking for career advice.

I am a Data Analyst for the sales team at a web hosting company, and I'm making $61k/year. I do not have a degree or much in the way of a portfolio to show right now. However I have been teaching myself everything I can about data engineering and I am a leader in my team in our transition from Excel to an Oracle database. I am self taught with python, SQL, VBA, and snowflake. In January I am starting a free community data engineering course that will give a certificate, but I don't really have any other credentials to put on a resume.

Recently I have lost faith that I will receive anything more than a 3% cost of living raise, from my company. I have started putting my resume out to see what bites, but have only gotten automatic rejection letters so far. I'm honestly feeling like I don't qualify for anything labeled Data Engineer, or anything with a higher starting salary than I am currently making. However with a wife and two kids I can't really accept a pay cut right now.

Am I fooling myself by thinking I am qualified for anything $75k and above? Is that took much to ask for effectively a junior or starting role? I'm very hard working and know I could be a good asset to a DE team, but hard to get to the part where I can show that. Any advice on the job search?",True,False,False,dataengineering,t5_36en4,47825,public,self,Is it unrealistic to expect a higher salary for my skill level?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rnrtbz/is_it_unrealistic_to_expect_a_higher_salary_for/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Kokubo-ubo,,,[],,,,text,t2_9gx56cnc,False,False,False,[],False,False,1640367906,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rnr4ru/implementing_dwh_for_a_startup_without_any_data/,{},rnr4ru,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rnr4ru/implementing_dwh_for_a_startup_without_any_data/,False,,,6,1640367917,1,"Working for a medium SaaS company. We are finally building a data warehouse (chose Snowflake) and it is time to ""put data"" into it. Set up of Snowflake was done by the Dev Team so far.

Management and CTO believe we should just connect our data source and start creating the tables we need. I believe we should follow a certain philosophy (kimball, etc...) and that someone with experience should work on it. Am I over complicating stuff or am I the only one that has any grasp on reality? :)",True,False,False,dataengineering,t5_36en4,47825,public,self,Implementing DWH for a startup without any data architect or data engineer.,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rnr4ru/implementing_dwh_for_a_startup_without_any_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,ali_azg,,,[],,,,text,t2_177iae,False,False,False,[],False,False,1640359985,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rnolfk/how_do_you_handle_schema_changes_in_tables_and/,{},rnolfk,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rnolfk/how_do_you_handle_schema_changes_in_tables_and/,False,,,6,1640359996,1,"Hello everyone,

I work in a mid size company with a lot of tables and databases(MongoDB and PostgreSQL mostly) that need to get synced and store their data as parquet format files on HDFS.

The problem is everytime the backend teams launch a new feature, one or multiple columns are added to the tables, and when we want to sync their data, we face an error that schema is changed and some new columns are added.

Our solution for now, is to find out what columns are added (names and their types), and then we read all previous parquet files and manually add these columns to data and again store it on HDFS, and now we can sync the tables without an error.

But this process takes a lot of time and effort. And as the company is growing, a lot of features and these kinds of changes are going to happen and it's very hard and time consuming to handle this issue manually.

I really wonder to know how bigger companies and professional Data Engineers can overcome this very common problem?

How can I somehow automate this whole process when syncing data?

P.S: the technologies we are using are Spark, ZooKeeper, HDFS, Hive, Scala, Python.

Thank you in advance for your help and advices.",True,False,False,dataengineering,t5_36en4,47820,public,self,How do you handle schema changes in tables and data files?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rnolfk/how_do_you_handle_schema_changes_in_tables_and/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,happysunshinekidd,,,[],,,,text,t2_dltif81,False,False,False,[],False,False,1640359838,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rnojts/anyone_here_ever_use_the_huawei_cloud_stack/,{},rnojts,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rnojts/anyone_here_ever_use_the_huawei_cloud_stack/,False,,,6,1640359849,1,"Starting working as an ML/DE eng at a startup. They have some super weird setup with dvc that just isn't working. No shade on dvc I think they just set it up incorrectly. 

Anyway I'm redoing their architecture mostly from scratch and had a good s3 storage system with a Django api servicing access requests thing going when all of a sudden our CEO got a ton of Huawei credits for Christmas (don't know, don't wana know) and now all of a sudden I'm on the Huawei stack. 

TBH it basically looks like an AWS rebrand for the Chinese market (again, no shade, the tech world was built on that kind of thing), so I'm hoping it won't be too bad to switch up the API calls, but the SDK looks... not good. 

Anyone have experience working with:
Huawei ECS 
Object Block Storage (OBS) (Definitely not S3)
Huawei Postgres RDS?

Again I'm hopin its straightforward but there's always gotchas and tricks and my mandarin isn't so good.

ty and merry xmas eve!",True,False,False,dataengineering,t5_36en4,47820,public,self,Anyone here ever use the Huawei Cloud Stack?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rnojts/anyone_here_ever_use_the_huawei_cloud_stack/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,dawarravi,,,[],,,,text,t2_3i3gcd05,False,False,False,[],False,False,1640354243,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rnmumx/kimball_vs_inmon_vs_vault/,{},rnmumx,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rnmumx/kimball_vs_inmon_vs_vault/,False,,,6,1640354253,1,"This post does a good job explaining the nuances of the three methodologies

https://link.medium.com/Arq8VTkzfmb

I would love to hear your experience with using any of the three.",True,False,False,dataengineering,t5_36en4,47815,public,self,Kimball vs. Inmon vs. Vault,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rnmumx/kimball_vs_inmon_vs_vault/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Kooky-Ad8131,,,[],,,,text,t2_5rsz77et,False,False,False,[],False,False,1640346069,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rnkntv/can_you_tell_if_am_i_a_data_engineer/,{},rnkntv,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rnkntv/can_you_tell_if_am_i_a_data_engineer/,False,,,6,1640346079,1,"Hi all !

I'm working for a startup in north Africa (Morocco), and in the last 2 years my work was about:

Building a Datalake in a 360-degree customer view for our client: 

\- Developing ETL using Pyspark, Airflow, Bash, SQL, Hadoop, Sqoop, MongoDB

\- Implementing Lambda Architecture using Spark Structured Streaming, Kafka, Pyspark, Airflow, Bash, SQL, Hadoop, MongoDB.

\- Implementing CDC (proof of concept) using docker, docker-compose...

  
Extending the Datalake for bank nano-loan:

\- ETL

\- Computing client KPIs

\- Developing a REST API for the end-client (the bank)

&amp;#x200B;

 I always keep in mind ""Build Data Framework **not** Data Pipeline"", and about the above mentioned techs (I know only what I need to know).

&amp;#x200B;

Opinions are always welcome and constructive criticism and debate are always enriching !",True,False,False,dataengineering,t5_36en4,47810,public,self,Can you tell If am I a Data Engineer ?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rnkntv/can_you_tell_if_am_i_a_data_engineer/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,francesco1093,,,[],,,,text,t2_zlyww,False,False,False,[],False,False,1640345924,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rnkmkr/you_need_to_ingest_a_new_dataset_how_do_you/,{},rnkmkr,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rnkmkr/you_need_to_ingest_a_new_dataset_how_do_you/,False,,,6,1640345934,1,"Hi,

Got asked this question in an interview and I am curious about how you would reply. How do you approach the ingestion of new data in your database/DWH?",True,False,False,dataengineering,t5_36en4,47809,public,self,You need to ingest a new dataset: how do you proceed?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rnkmkr/you_need_to_ingest_a_new_dataset_how_do_you/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Georgehwp,,,[],,,,text,t2_5okt4lzw,False,False,False,[],False,False,1640343828,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rnk47h/looking_for_data_engineering_mentor_willing_to_pay/,{},rnk47h,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rnk47h/looking_for_data_engineering_mentor_willing_to_pay/,False,,,6,1640343838,1,"Basically I've found myself in a start-up that I love, but I'm permanently out of my depth setting up data systems I'm not sure I can maintain. Never sure if I'm breaking some industry wide 'best practice' that no one's ever got round to telling me about.

I'd take a pay cut to remove some of this stress, which is maybe equivalent to just paying someone to teach me / guide me through some of my problems. Don't know where to go to get this or how to move forwards.",True,False,False,dataengineering,t5_36en4,47807,public,self,Looking for Data Engineering 'mentor' willing to pay,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rnk47h/looking_for_data_engineering_mentor_willing_to_pay/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,mildbyte,,,[],,,,text,t2_50049,False,False,False,[],False,False,1640329505,splitgraph.com,https://www.reddit.com/r/dataengineering/comments/rngn3o/airbyte_dbt_splitgraph_how_we_built_our_modern/,{},rngn3o,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rngn3o/airbyte_dbt_splitgraph_how_we_built_our_modern/,False,link,"{'enabled': False, 'images': [{'id': 'FPJHNpL4FR215R4VeScILD5AakR7L1G9jqz-9OzGDWI', 'resolutions': [{'height': 56, 'url': 'https://external-preview.redd.it/TBMxY7mcJ9EOoldiBV9KuDfMJYSYh7juoSYmgKApC48.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4093462c461ed60faad59548c19a77ba94b2ebee', 'width': 108}, {'height': 113, 'url': 'https://external-preview.redd.it/TBMxY7mcJ9EOoldiBV9KuDfMJYSYh7juoSYmgKApC48.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=719c20b8f52fadb4aa92a3c18f4671e471120fa9', 'width': 216}, {'height': 168, 'url': 'https://external-preview.redd.it/TBMxY7mcJ9EOoldiBV9KuDfMJYSYh7juoSYmgKApC48.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b3dd6981051ee30183f97f31c02a1367b5c72a58', 'width': 320}, {'height': 336, 'url': 'https://external-preview.redd.it/TBMxY7mcJ9EOoldiBV9KuDfMJYSYh7juoSYmgKApC48.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b3714a62f57028c9781bbcdb00121572b13d9e3e', 'width': 640}, {'height': 504, 'url': 'https://external-preview.redd.it/TBMxY7mcJ9EOoldiBV9KuDfMJYSYh7juoSYmgKApC48.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c16b97ee4bdd4fcdfedb9d93063ba973c707bea5', 'width': 960}, {'height': 567, 'url': 'https://external-preview.redd.it/TBMxY7mcJ9EOoldiBV9KuDfMJYSYh7juoSYmgKApC48.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fc41731e0da4c2da0b4ab78c4563f9bc60312db9', 'width': 1080}], 'source': {'height': 630, 'url': 'https://external-preview.redd.it/TBMxY7mcJ9EOoldiBV9KuDfMJYSYh7juoSYmgKApC48.jpg?auto=webp&amp;s=02800a28756b2a6b2482c65dd1f3324177a3dff9', 'width': 1200}, 'variants': {}}]}",6,1640329515,1,,True,False,False,dataengineering,t5_36en4,47800,public,https://b.thumbs.redditmedia.com/efDKl9HWnZfG8cuvfQPOo7qRSZxMNyiGL5uBDPdSATQ.jpg,"Airbyte, dbt, Splitgraph: how we built our modern data stack",0,[],1.0,https://www.splitgraph.com/blog/airbyte-dbt-splitgraph,all_ads,6,,,,,,73.0,140.0,https://www.splitgraph.com/blog/airbyte-dbt-splitgraph,,,,,,,,,,
[],False,DontMuck9866,,,[],,,,text,t2_fw437la6,False,False,False,[],False,False,1640323529,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rnezsy/how_do_i_write_pyspark_jobs/,{},rnezsy,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rnezsy/how_do_i_write_pyspark_jobs/,False,,,6,1640323540,1,"Hi everyone,

I’m very new to Data Engineering and I’d like to know how to write pyspark jobs I’ve read some tutorials over net but couldn’t find any help.

PS: I’m aware of pyspark",True,False,False,dataengineering,t5_36en4,47790,public,self,How do I write pyspark jobs ?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rnezsy/how_do_i_write_pyspark_jobs/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,american-roast,,,[],,,,text,t2_c9apwds3,False,False,False,[],False,False,1640321542,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rneefk/how_to_find_mature_data_orgs_how_to_get_in/,{},rneefk,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rneefk/how_to_find_mature_data_orgs_how_to_get_in/,False,,,6,1640321552,1,"What do you look for when searching for a good organization to work for?  What are some red flags?

For example:

Good sign:  They have a blog that is regularly updated with new articles.  This shows me that they take pride in their engineering culture and want to show it off.

Bad sign:  The position listed is part of a one to three-person data team.  This may be explainable (maybe the company itself is very small), but this is a red flag to me.  It's a signal that they don't invest in their data team and that I will be swamped with trying to support every business unit's data needs.",True,False,False,dataengineering,t5_36en4,47789,public,self,How to find mature data orgs? How to get in?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rneefk/how_to_find_mature_data_orgs_how_to_get_in/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Cold-Chard-6933,,,[],,,,text,t2_ei8ignjl,False,False,False,[],False,False,1640301390,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rn846k/best_practice_for_a_trading_startup/,{},rn846k,False,False,False,False,False,False,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rn846k/best_practice_for_a_trading_startup/,False,,,6,1640301400,1,[removed],True,False,False,dataengineering,t5_36en4,47770,public,self,Best practice for a trading startup,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rn846k/best_practice_for_a_trading_startup/,all_ads,6,,,automod_filtered,,,,,,,,,,,,,,,
[],False,secretly_not_a_dog,,,[],,,,text,t2_i0nkr,False,False,False,[],False,False,1640282988,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rn1kpg/am_i_crazy_to_think_im_being_underpaid/,{},rn1kpg,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rn1kpg/am_i_crazy_to_think_im_being_underpaid/,False,,,6,1640282998,1,"I’m currently a cloud data engineer with 2 years of professional experience, a computer science bachelors degree, and recently attained Google’s Associate Cloud Engineer certification. I’m working in a major Canadian city for a multinational organization that has only just really started developing their cloud infrastructure in the past year. Right now I’m making ~$55,000 CAD annually, am I crazy to think this is below average? What salary ranges do people in similar positions find themselves in?",True,False,False,dataengineering,t5_36en4,47750,public,self,Am I crazy to think I’m being underpaid?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rn1kpg/am_i_crazy_to_think_im_being_underpaid/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,phmark19,,,[],,,,text,t2_33924v0v,False,False,False,[],False,False,1640280227,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rn0jx8/good_data_related_questions_to_ask_on_a_product/,{},rn0jx8,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rn0jx8/good_data_related_questions_to_ask_on_a_product/,False,,,6,1640280238,1,"I am currently working for a startup internet company as a Data Engineer. We have a plan to buy a customer success platform or CSP the next coming months. This is also my first time to integrate this kind of software on a data pipeline (newbie in the data engineering realm).

What will happen is that, we have a snowflake dw which will upload the data to the CSP, then using the data, in the CSP, I can create metrics/dashboards, customer segments, trigger another software and create customer success workflows, etc.

In my mind, these are the questions that I want to ask:

1. What is the expected shape of the data to be synched for the CSP to understand it? Do I need to clean / massage the data?

2. Where will the CSP store the data, can we access it via an API? Rate limits?

3. Do you have a retention period for the data? How long?

I have limited background to this kind of integration, which is why I can't generate more critical/rich questions.

Hope someone who had the same experience (even those other type of platform but the same concept) before can give some useful advice.",True,False,False,dataengineering,t5_36en4,47742,public,self,Good Data Related Questions to Ask on a Product Demo Before Buying a Software,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rn0jx8/good_data_related_questions_to_ask_on_a_product/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,akthero1,,,[],,,,text,t2_rx452,False,False,False,[],False,False,1640274470,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rmygiw/data_pipelines_in_aws_is_this_normal/,{},rmygiw,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rmygiw/data_pipelines_in_aws_is_this_normal/,False,,,6,1640274481,1,"I've recently joined an early stage startup as a data engineer where I've been tasked mostly with working on/improving the companies ETL pipelines.

Our daily pipelines involve extracting data from some external data vendors, cleaning/amalgamating/transforming all of this data in several steps, and finally feeding the results into various ML models.

I haven't worked in an AWS environment before and have only had one data engineering job before this so not sure if this is normal but the following things have struck me as not very good practice.

1. We don't use databases at all - each daily process reads from/writes to flat files (.json/.csv/.h5) in S3 buckets. Most of our data is either structured or semi-structured. This makes historical data analysis very difficult. Also means the entire file needs to be read into memory even if just one row is needed.
2. Absence of monitoring/scheduling tools - if something breaks in the pipeline, there is no way of knowing other than trawling through log files (again stored in S3). Then each pipeline step has to be rerun manually (in my previous role we used Ansible to chain tasks together so entire pipelines could be rerun easily on failure).

I'm trying to decide whether to stick with this company long term - I've brought these issues up with the team before but no one seems to think it's that big of a deal. Is this kind of thing common in the industry? I suspect there are much more advanced tools offered by AWS to solve these problems that we could use - but not sure what those tools are.",True,False,False,dataengineering,t5_36en4,47736,public,self,Data pipelines in AWS - is this normal?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rmygiw/data_pipelines_in_aws_is_this_normal/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,DataStackAcademy,,,[],,,,text,t2_gsfor44i,False,False,False,[],False,False,1640273921,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rmy9ov/free_docker_webinar_1223_at_1030am_pst/,{},rmy9ov,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rmy9ov/free_docker_webinar_1223_at_1030am_pst/,False,,,6,1640273931,1,Hi all - We are hosting [Intro to Docker &amp; using Docker in Cloud Data Engineering](https://www.eventbrite.com/myevent?eid=222626892027) today (12/23) at 10:30AM PST. Our webinars are free and we answer Data Engineering related questions during the webinar. Thanks!,True,False,False,dataengineering,t5_36en4,47735,public,self,Free Docker Webinar (12/23 at 10:30AM PST),0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rmy9ov/free_docker_webinar_1223_at_1030am_pst/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,chaos87johnito,,,[],,,,text,t2_2rziq7e7,False,False,False,[],False,False,1640273026,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rmxyhv/how_to_integrate_dbt_and_lookml_datawarehouse_on/,{},rmxyhv,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rmxyhv/how_to_integrate_dbt_and_lookml_datawarehouse_on/,False,,,6,1640273037,1,"Hey there,

At our company we use BQ and are changing the way we model data by doing a PoC with dbt. Before that the analysts would create in silos their tables/views.

We schedule the queries with Airflow.

The team also wants to change the viz tool from using DataStudio/Gsheets to Looker.

Have you guys implemented both dbt and lookML? How do you make so that both complement each other?

My fear is that there could be some data governance issues as in should we trust that dbt model or the query generated with lookML etc...",True,False,False,dataengineering,t5_36en4,47735,public,self,How to integrate dbt and lookml (datawarehouse on BigQuery)?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rmxyhv/how_to_integrate_dbt_and_lookml_datawarehouse_on/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,chaos87johnito,,,[],,,,text,t2_2rziq7e7,False,False,False,[],False,False,1640272935,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rmxxct/data_warehousing_on_bigquery_tools_and_processes/,{},rmxxct,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rmxxct/data_warehousing_on_bigquery_tools_and_processes/,False,,,6,1640272946,1,"Hey there,

At our company we use BQ and are changing the way we model data by doing a PoC with dbt. Before that the analysts would create in silos their tables/views.

We schedule the queries with Airflow.

The team also wants to change the viz tool from using DataStudio/Gsheets to Looker. 

Have you guys implemented both dbt and lookML? How do you make so that both complement each other? 

My fear is that there could be some data governance issues as in should we trust that dbt model or the query generated with lookML etc...",True,False,False,dataengineering,t5_36en4,47735,public,self,Data Warehousing on BigQuery: tools and processes for data modelling?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rmxxct/data_warehousing_on_bigquery_tools_and_processes/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,zelenadinja,,,[],,,,text,t2_garpgtut,False,False,False,[],False,False,1640271504,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rmxf18/follow_progress_of_reading_from_s3/,{},rmxf18,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rmxf18/follow_progress_of_reading_from_s3/,False,,,6,1640271515,1,"Hey, if i read a objecy from S3 Bucket,

obj = s3.Object(bucket, key)

stream = obj.get()\['Body'\].read()

Is there a way to track progress of reading with tqdm?",True,False,False,dataengineering,t5_36en4,47734,public,self,Follow progress of reading from s3,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rmxf18/follow_progress_of_reading_from_s3/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,russianjohn,,,[],,,,text,t2_2700b6mx,False,False,False,[],False,False,1640271413,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rmxdyq/options_for_building_a_scalable_database/,{},rmxdyq,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rmxdyq/options_for_building_a_scalable_database/,False,,,6,1640271424,1,"Hi everyone,

I'm a data engineering newbie. I recently joined a company that uses Airtable as its database. Its paired with a research tool that gathers the data, the data is then uploaded via an API thru Zapier automations. 

What would be the best way/option to replace this current setup? It also seems that my supervisors are more keen to buying a product rather than building one, so ideally an all in one scalable solution would be great. Any suggestions?",True,False,False,dataengineering,t5_36en4,47733,public,self,Options for Building a Scalable Database,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rmxdyq/options_for_building_a_scalable_database/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,sixtiethtry,,,[],,,,text,t2_4480di2k,False,False,False,[],False,False,1640271226,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rmxboy/transition_to_cloud_engineering/,{},rmxboy,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rmxboy/transition_to_cloud_engineering/,False,,,6,1640271236,1,"Hey all, happy holidays. 

I’ve been presented with an interesting series of moves that would essentially mean sidelining about 50% of my work to spend 2021 training in GCP to take a job as a Cloud Engineer in my current department. This (and a raise) are in writing. 

If I decline, I can continue as a DE and shouldn’t have issues progressing, although that won’t be in writing. I’ve been with the company a while and I’m not too worried about whether the next level comes on Jan 1, 2022 if I decline the cloud opportunity. 

Some pros:
- pay, obviously
- a fun opportunity to learn some new, valuable skills on company time and dollar
- a chance to get in near the ground level of a fairly new field 
- I've enjoyed the GCP work I've done so far bur it’s a new addition to my tool belt (and brand new to my company)

Some cons:
- I’m fairly fresh in the DE field (3 years analytics engineer, 6 months DE) so I’d be “specializing” earlier than I expected to 
- Nobody in my current department has a strong knowledge in cloud computing or GCP so I’d be THE guy. In some ways, this is a pro too

have any of you made a transition to full time cloud developers? Any thoughts?

Thanks!",True,False,False,dataengineering,t5_36en4,47733,public,self,Transition to cloud engineering?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rmxboy/transition_to_cloud_engineering/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,an_tonova,,,[],,,,text,t2_d834g,False,False,False,[],False,False,1640267743,dataengineering.academy,https://www.reddit.com/r/dataengineering/comments/rmw6ya/oh_thats_fun_2021_awards_for_data_engineering/,{},rmw6ya,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rmw6ya/oh_thats_fun_2021_awards_for_data_engineering/,False,link,"{'enabled': False, 'images': [{'id': 'eP246VXM-21r4A4IRg5V9jHZtkjMcjiZhlpZ-Blghak', 'resolutions': [{'height': 53, 'url': 'https://external-preview.redd.it/0CamUJbfYNiUXW0zIh_I2a3ldwhGPkLuQgDikIqwevU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=375039a910ae1f03dda031bef7af438b1139bda1', 'width': 108}, {'height': 107, 'url': 'https://external-preview.redd.it/0CamUJbfYNiUXW0zIh_I2a3ldwhGPkLuQgDikIqwevU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f029731bda0ca8a2ddccb0b5ba0bf452d6e7526d', 'width': 216}, {'height': 159, 'url': 'https://external-preview.redd.it/0CamUJbfYNiUXW0zIh_I2a3ldwhGPkLuQgDikIqwevU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=15bdcc7db9682fc32a1407524a49659830edbfc3', 'width': 320}, {'height': 319, 'url': 'https://external-preview.redd.it/0CamUJbfYNiUXW0zIh_I2a3ldwhGPkLuQgDikIqwevU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0da4143b9e44a8d75d540978225a0594792da912', 'width': 640}], 'source': {'height': 362, 'url': 'https://external-preview.redd.it/0CamUJbfYNiUXW0zIh_I2a3ldwhGPkLuQgDikIqwevU.jpg?auto=webp&amp;s=a2c33e961657152054d335316d8f61937734da05', 'width': 726}, 'variants': {}}]}",6,1640267753,1,,True,False,False,dataengineering,t5_36en4,47728,public,https://a.thumbs.redditmedia.com/O0qj_WV67XeBJ9eiYIid22FeAjt2sIMN23o2JBRLDR0.jpg,"Oh, that's fun: 2021 awards for data engineering tooling",0,[],1.0,https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/the-pipeline-academy-awards-2021-pipies,all_ads,6,,,,,,69.0,140.0,https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/the-pipeline-academy-awards-2021-pipies,,,,,,,,,,
[],False,pavlik_enemy,,,[],,,,text,t2_71ru7,False,False,False,[],False,False,1640266328,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rmvrg4/how_to_you_release_pyspark_jobs/,{},rmvrg4,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rmvrg4/how_to_you_release_pyspark_jobs/,False,,,6,1640266338,1,"Suppose I have a PySpark job that looks like this

    import pyspark
    import pandas
    import pytorch
    import my_helpers # some package from corporate PyPi

    # Do Spark stuff

    # Do Pandas stuff

    # Do ML stuff

I'm really struggling to understand how can I create a .tar.gz that could be passed to `spark-submit`. My idea is this: create Conda env with build tools and build a Conda package containing our code. Install this package into new minimal environment. Use `conda-pack` to create a tarball with that environment and upload it to S3 or wherever.

I'm really struggling with understanding what is a proper way to build Conda packages, specifically `build.sh` and `build` section in `meta.yaml`",True,False,False,dataengineering,t5_36en4,47725,public,self,How to you release PySpark jobs?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rmvrg4/how_to_you_release_pyspark_jobs/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Valkyrja-Kara,,,[],,,,text,t2_hcejskpo,False,False,False,[],False,False,1640260062,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rmu09l/twoway_unidirectional_sync_platform_also_help/,{},rmu09l,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rmu09l/twoway_unidirectional_sync_platform_also_help/,False,,,6,1640260073,1,"Disclaimer: I'm a John Snow. I know nothing. Where do I start .

According to the interwebs there are 4 data integration patterns: Migration, broadcast (one to many), Bi-directional sync (target system can override fields in source), correlation (bi-directional sync on common data objects between two systems), aggregation (consolidate multiple sources into one),  Two-way unidirectional sync (target field cannot override fields that belong to source). 

Employer has salesforce as main source, and wants bi-directional integration with targets. Salesforce is the main source/master, we have applications like accounting systems, CSM, PSA, ERP systems.

For example prospect conversion in salesforce. From the time a lead is created, up until a customer places an order, Salesforce owns the Prospect record. After an order is placed, the prospect is converted into a customer and the ERP takes ownership of the record.

 Or, Salesforce owns info in an object, but data from another system needs to be surfaced in Salesforce, so a designated set of fields is updated an a regular basis from that other system. The ERP system owns and pushes updates for billing addresses  read-only in Salesforce), but mailing addresses are owned and maintained in Salesforce and get pushed to the ERP (read-only in ERP).

This calls for two-way unidirectional sync if you ask me, bi-directional sync will cause chaos. How can I achieve this? 

My scope is to build a warehouse for application integration, DQ and reporting. I decided on a delta lake infrastructure. I'm overwhelmed in terms of the integration - would a messaging broker be the best solution to orchestrate updates and integration between salesforce and a long list of other applications? 

We are talking 10GB very poorly integrated salesforce data with the scope of rapid expansion over the next 6 months. 

I'm at a loss - every conversation loops back to kafka but surely that is overkill. The data engineering team consist of me, entry-level and inexperienced. 

Sorry for the long post, I would rather take advice than set myself up for failure.",True,False,False,dataengineering,t5_36en4,47718,public,self,Two-way Unidirectional Sync Platform. Also - help.,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rmu09l/twoway_unidirectional_sync_platform_also_help/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,randomusicjunkie,,,[],,,,text,t2_3tzpeuhd,False,False,False,[],False,False,1640258697,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rmtnk6/did_you_have_to_do_leetcode_during_your_interviews/,{},rmtnk6,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rmtnk6/did_you_have_to_do_leetcode_during_your_interviews/,False,,,6,1640258708,1,"if not, what was the main focus of the interview?",True,False,False,dataengineering,t5_36en4,47718,public,self,Did you have to do Leetcode during your interviews?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rmtnk6/did_you_have_to_do_leetcode_during_your_interviews/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Data_Cog,,,[],,,,text,t2_9edta7qq,False,False,False,[],False,False,1640253320,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rmsbd1/dummy_data_for_testing/,{},rmsbd1,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rmsbd1/dummy_data_for_testing/,False,,,6,1640253330,1,"Generally using production data in a test environment is not allowed at clients. How do you guys get over this? Any specific tools you use to create dummy/fake data for your ETL/pipeline testing?

Thank you!",True,False,False,dataengineering,t5_36en4,47714,public,self,Dummy Data for Testing,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rmsbd1/dummy_data_for_testing/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Valkyrja-Kara,,,[],,,,text,t2_hcejskpo,False,False,False,[],False,False,1640249379,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rmrdih/which_language_should_i_learn_to_work_in_azure/,{},rmrdih,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rmrdih/which_language_should_i_learn_to_work_in_azure/,False,,,6,1640249390,1,"Entry-level data engineer, only know SQL. Need to pick the first language to learn.

[View Poll](https://www.reddit.com/poll/rmrdih)",True,False,False,dataengineering,t5_36en4,47709,public,self,"Which language should I learn to work in Azure Databricks, Delta Lake and Kafka?",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rmrdih/which_language_should_i_learn_to_work_in_azure/,all_ads,6,,,,,,,,,,,,,"{'is_prediction': False, 'options': [{'id': '12676772', 'text': 'Scala First'}, {'id': '12676773', 'text': 'Python First'}], 'prediction_status': None, 'resolved_option_id': None, 'total_stake_amount': None, 'total_vote_count': 0, 'tournament_id': None, 'user_selection': None, 'user_won_amount': None, 'vote_updates_remained': None, 'voting_end_timestamp': 1640681379884}",,,,,
[],False,cokeapm,,,[],,,,text,t2_12halb,False,False,False,[],False,False,1640248439,dev.to,https://www.reddit.com/r/dataengineering/comments/rmr5ot/i_just_wrote_ain_introductory_post_on_how_to_use/,{},rmr5ot,False,False,False,False,False,False,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rmr5ot/i_just_wrote_ain_introductory_post_on_how_to_use/,False,link,"{'enabled': False, 'images': [{'id': 'gE5_M_4E7CPOOHInIIE9t00H-UrDvInAFVwl2VTNAWY', 'resolutions': [{'height': 57, 'url': 'https://external-preview.redd.it/2Sien0ZupeEswZyyUlEqv_FYUOLMmk9nbSQJCgm8cEE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1f1dede90a3b22da92e21be63aaedc19af7e3bce', 'width': 108}, {'height': 114, 'url': 'https://external-preview.redd.it/2Sien0ZupeEswZyyUlEqv_FYUOLMmk9nbSQJCgm8cEE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6c85bc2302e7bf3bbf2e5e3f0ccbb4779edbb474', 'width': 216}, {'height': 170, 'url': 'https://external-preview.redd.it/2Sien0ZupeEswZyyUlEqv_FYUOLMmk9nbSQJCgm8cEE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=95ddeee5efc5f8bcb072863c45e9b241054192ac', 'width': 320}, {'height': 340, 'url': 'https://external-preview.redd.it/2Sien0ZupeEswZyyUlEqv_FYUOLMmk9nbSQJCgm8cEE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2f7e1c04d49563a6445086d63bca531f2a17ae60', 'width': 640}, {'height': 510, 'url': 'https://external-preview.redd.it/2Sien0ZupeEswZyyUlEqv_FYUOLMmk9nbSQJCgm8cEE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=cf4750191f0db6fe8b9513f7625057a601f12a95', 'width': 960}, {'height': 574, 'url': 'https://external-preview.redd.it/2Sien0ZupeEswZyyUlEqv_FYUOLMmk9nbSQJCgm8cEE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=35733b3b1d68bed276d101a1be1ad0c8d8911da6', 'width': 1080}], 'source': {'height': 600, 'url': 'https://external-preview.redd.it/2Sien0ZupeEswZyyUlEqv_FYUOLMmk9nbSQJCgm8cEE.jpg?auto=webp&amp;s=83c2f8396f3fcce4b18d36de1f6ecdadd8921507', 'width': 1128}, 'variants': {}}]}",6,1640248450,1,,True,False,False,dataengineering,t5_36en4,47709,public,https://b.thumbs.redditmedia.com/Mzz-0o2074rLAlfpDZUy5geFRABbrt498REHx-2RsHo.jpg,I just wrote ain introductory post on how to use meltano open source ELT to move data from csvs to Postgres SQL,0,[],1.0,https://dev.to/zompro/extract-csv-data-and-load-it-to-postgresql-using-meltano-elt-4ipf,all_ads,6,,,reddit,,,74.0,140.0,https://dev.to/zompro/extract-csv-data-and-load-it-to-postgresql-using-meltano-elt-4ipf,,,,,,,,,,
[],False,Greg_Z_,,,[],,,,text,t2_1z5jdh5h,False,False,False,[],False,False,1640248327,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rmr4qj/tools_for_ad_hoc_analysis/,{},rmr4qj,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rmr4qj/tools_for_ad_hoc_analysis/,False,,,6,1640248337,1,"When you think about an ad-hoc analysis, what tools pop up in your mind? Which are the best? Why?",True,False,False,dataengineering,t5_36en4,47709,public,self,Tools for ad hoc analysis?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rmr4qj/tools_for_ad_hoc_analysis/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,CingKan,,,[],,,,text,t2_x0wpn,False,False,False,[],False,False,1640229143,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rmm1xc/when_you_want_all_that_juicy_ai_and_ml_but_you/,{},rmm1xc,False,True,False,False,False,True,True,False,#ff66ac,[],dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1,Meme,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rmm1xc/when_you_want_all_that_juicy_ai_and_ml_but_you/,False,,,6,1640229153,1,"&amp;#x200B;

https://preview.redd.it/pcb5ycbtl7781.png?width=1170&amp;format=png&amp;auto=webp&amp;s=d2887f537a04d7a5231f797fc0a3c1bd2cc8f8b8",True,False,False,dataengineering,t5_36en4,47682,public,https://b.thumbs.redditmedia.com/n_OCJ6zE6myE4HPDXxhAK9D-J1JUGy4qys9kApurk8Q.jpg,"When you want all that juicy AI and ML but you have no time for the boring DWH, data cleaning or maths malarkey",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rmm1xc/when_you_want_all_that_juicy_ai_and_ml_but_you/,all_ads,6,,,,,,140.0,140.0,,"{'pcb5ycbtl7781': {'e': 'Image', 'id': 'pcb5ycbtl7781', 'm': 'image/png', 'p': [{'u': 'https://preview.redd.it/pcb5ycbtl7781.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0685c211fe9a9e45dcd7b84d626a74296790a849', 'x': 108, 'y': 119}, {'u': 'https://preview.redd.it/pcb5ycbtl7781.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7eb2664eea1e04025b2aa886b5cafed67b4c61f6', 'x': 216, 'y': 238}, {'u': 'https://preview.redd.it/pcb5ycbtl7781.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cd647c2639f1fc40af18d5de3e7407baaa42f0b7', 'x': 320, 'y': 353}, {'u': 'https://preview.redd.it/pcb5ycbtl7781.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3f231a5b2e2b03130085c574fba1d873621427da', 'x': 640, 'y': 707}, {'u': 'https://preview.redd.it/pcb5ycbtl7781.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=49985af33d44714767e3b72f42a817bb8cbb0402', 'x': 960, 'y': 1060}, {'u': 'https://preview.redd.it/pcb5ycbtl7781.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5cddf4a8bac5e0aee2f240855910025b8ececc9f', 'x': 1080, 'y': 1193}], 's': {'u': 'https://preview.redd.it/pcb5ycbtl7781.png?width=1170&amp;format=png&amp;auto=webp&amp;s=d2887f537a04d7a5231f797fc0a3c1bd2cc8f8b8', 'x': 1170, 'y': 1293}, 'status': 'valid'}}",,,,,,,,,
[],False,CingKan,,,[],,,,text,t2_x0wpn,False,False,False,[],False,False,1640229030,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rmm0lw/when_you_want_all_that_juicy_ai_and_ml_without/,{},rmm0lw,False,True,False,False,False,True,True,False,#ff66ac,[],dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1,Meme,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rmm0lw/when_you_want_all_that_juicy_ai_and_ml_without/,False,,,6,1640229042,1,"&amp;#x200B;

https://preview.redd.it/n2kbw6h4l7781.png?width=1170&amp;format=png&amp;auto=webp&amp;s=8bcaf0f1ce58f84af782e8c3beab35770b494447",True,False,False,dataengineering,t5_36en4,47682,public,https://a.thumbs.redditmedia.com/drPNwMPalQq7Q_4OkxZFWLCgyd75iIJU9StcWeR-WP8.jpg,"When you want all that juicy AI and ML without all that bring DWH, data cleaning malarkey or Maths",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rmm0lw/when_you_want_all_that_juicy_ai_and_ml_without/,all_ads,6,,,,,,140.0,140.0,,"{'n2kbw6h4l7781': {'e': 'Image', 'id': 'n2kbw6h4l7781', 'm': 'image/png', 'p': [{'u': 'https://preview.redd.it/n2kbw6h4l7781.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b852d82418a34170dca5a5d7149804972dab1ccc', 'x': 108, 'y': 119}, {'u': 'https://preview.redd.it/n2kbw6h4l7781.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=96964aae27f2e964e902a5b6fd64c42651240d3f', 'x': 216, 'y': 238}, {'u': 'https://preview.redd.it/n2kbw6h4l7781.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c9d0e89f6a873e8316b4533b39ecbae013330818', 'x': 320, 'y': 353}, {'u': 'https://preview.redd.it/n2kbw6h4l7781.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ce61968bd78977a9d30c450bed0a35f78cca496f', 'x': 640, 'y': 707}, {'u': 'https://preview.redd.it/n2kbw6h4l7781.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=680974801e67aa2e48ff118646c90a1331051cb9', 'x': 960, 'y': 1060}, {'u': 'https://preview.redd.it/n2kbw6h4l7781.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5cba2507a9c2a59c8c5b68deb98b32c8782848f6', 'x': 1080, 'y': 1193}], 's': {'u': 'https://preview.redd.it/n2kbw6h4l7781.png?width=1170&amp;format=png&amp;auto=webp&amp;s=8bcaf0f1ce58f84af782e8c3beab35770b494447', 'x': 1170, 'y': 1293}, 'status': 'valid'}}",,,,,,,,,
[],False,Minimum-Membership-8,,,[],,,,text,t2_a0qsnkph,False,False,False,[],False,False,1640219568,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rmj08e/data_engineer_or_product_owner/,{},rmj08e,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rmj08e/data_engineer_or_product_owner/,False,,,6,1640219578,1,I’m a data engineer on a scrum team and I’m being asked to take the product owner role. Do you think this is a good career move?,True,False,False,dataengineering,t5_36en4,47676,public,self,Data Engineer or Product Owner,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rmj08e/data_engineer_or_product_owner/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,robertinoc,,,[],,,,text,t2_6f4ptj3o,False,False,False,[],False,False,1640202647,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rmd7j5/building_an_edge_api_gateway_with_fauna_and/,{},rmd7j5,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rmd7j5/building_an_edge_api_gateway_with_fauna_and/,False,self,"{'enabled': False, 'images': [{'id': 'YRp0CSeEabo_msjA3Hq53qD4cMBbAQ5EsP04fyv1G3k', 'resolutions': [{'height': 96, 'url': 'https://external-preview.redd.it/JucCcIbU3jp4c74gUfpaCNrKrjq_8jBpm6Q1LWAA6dA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=893561b051d2144bf3a6f22d87be354f96a39de1', 'width': 108}, {'height': 193, 'url': 'https://external-preview.redd.it/JucCcIbU3jp4c74gUfpaCNrKrjq_8jBpm6Q1LWAA6dA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e38c2de5df3d07042442b0ee43a1230582db9afc', 'width': 216}, {'height': 287, 'url': 'https://external-preview.redd.it/JucCcIbU3jp4c74gUfpaCNrKrjq_8jBpm6Q1LWAA6dA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e41254db6eb3abab53f8ddc30794a8c9645206c8', 'width': 320}, {'height': 574, 'url': 'https://external-preview.redd.it/JucCcIbU3jp4c74gUfpaCNrKrjq_8jBpm6Q1LWAA6dA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=42da2a27abab3fb540c785625286fce321dba86c', 'width': 640}, {'height': 862, 'url': 'https://external-preview.redd.it/JucCcIbU3jp4c74gUfpaCNrKrjq_8jBpm6Q1LWAA6dA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0199bf282810fc56da9d0fb6b6e4d4cf3fae54a5', 'width': 960}, {'height': 969, 'url': 'https://external-preview.redd.it/JucCcIbU3jp4c74gUfpaCNrKrjq_8jBpm6Q1LWAA6dA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a4b35d127003774c203d1d45f40f9f56d13e455c', 'width': 1080}], 'source': {'height': 1056, 'url': 'https://external-preview.redd.it/JucCcIbU3jp4c74gUfpaCNrKrjq_8jBpm6Q1LWAA6dA.jpg?auto=webp&amp;s=3e591458c2d319368833ae3623b14eb23a05a623', 'width': 1176}, 'variants': {}}]}",6,1640202658,1,"In this tutorial, we’ll explore architecting REST APIs in a fully serverless manner by leveraging Fastly’s Compute@Edge, Fauna, and Auth0.

[Read more…](https://auth0.com/blog/building-an-edge-api-gateway-with-fauna-and-securing-it-with-auth0/?utm_source=reddit&amp;utm_medium=sc&amp;utm_campaign=fauna)",True,False,False,dataengineering,t5_36en4,47662,public,self,Building an Edge API Gateway with Fauna and Securing It with Auth0,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rmd7j5/building_an_edge_api_gateway_with_fauna_and/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,mynameisfuk,transparent,,[],fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b,Data Engineer,dark,text,t2_9ndrjc58,False,False,False,[],False,False,1640200991,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rmcmv8/sparkglue_performance_help_needed/,{},rmcmv8,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rmcmv8/sparkglue_performance_help_needed/,False,,,6,1640201002,1,"Hi all, 

I am a beginner in the field. I am trying to run a simple ETL process using AWS Glue.

The process is simple: use a JDBC connector to read from 20+ tables from a Database, and then sink them in S3. Everything works fine, the only issue is the amount of time it is required to run the job (2+ hours).

The main bottleneck is caused by some very large tables (16 to 20 millions records), and by the fact that I have to extract number of rows and fields list.

The glue job uses Python 3, Spark 3, 2 workers (of which 1 driver).  


I first read the table: 

    df = sparkSession.read.format(""jdbc"").option(""url"", connection_url).option(""dbtable"", table).option(""driver"", DRIVER).load()

Then I convert it to a GlueDynamicFrame (as it is easier for me to run operations on it):

    df = DynamicFrame.fromDF(df, glueContext, ""df"")

Then I proceed to calculate number of rows: 

    n_rows = df.count()

Which starts the pain: for some tables it takes 10 to 20 minutes to just return this value. I have researched and (I think) understand the concept of lazy-evaluations and computations in Spark, but it seems to me that this operation should take way less anyway and I am surely doing something wrong. Anyway, then I proceed to generate a field list: 

    fields = [df.schema().fields[x].name for x in range(0, len(df.schema().fields))]

Which again, 10 to 20 minutes to run. 

Eventually, I sink the dataframe: 

    glueContext.write_dynamic_frame.\
            from_options(frame = df,
 connection_type = ""s3"",
 connection_options = {""path"": f's3://{BUCKET_NAME}/{ZONE}/{TIER}/{SOURCE}/{extraction}/',
 ""partitionKeys"": [DUMPDATE]},
 format = ""parquet"")

Which again, it takes a long time for these large tables. 

It is worth mentioning that I extract from db tables that contain few rows as well. I mention this as I have read to repartition as soon as I read the table, but it would make zero senso to repartition a DataFrame of 3 rows. The only way of doing it sistematically would be to count rows first, and then base on n\_rows repartition, but it takes already forever.   
Also, I have read that the number of partitions should be somewhat related to the number of workers. I have 1 worker, so 1 partition seems logical to me.

My question would be: what am I doing wrong? Should I just increase number of workers and repartition accordingly at the moment of reading? Or what other solutions are available?   


Thanks a lot for any advice!",True,False,False,dataengineering,t5_36en4,47658,public,self,"Spark/Glue Performance, help needed",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rmcmv8/sparkglue_performance_help_needed/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,marshr9523,transparent,,[],19bba012-ac9d-11eb-b77b-0eec37c01719,Data Analyst,dark,text,t2_y3x7xvk,False,False,False,[],False,False,1640198619,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rmbs4e/how_to_reach_out_to_recruiters_as_someone_who_is/,{},rmbs4e,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rmbs4e/how_to_reach_out_to_recruiters_as_someone_who_is/,False,self,"{'enabled': False, 'images': [{'id': '_BUIjF4g8RS3XNlJw7X812vzXra70O_QDIpCS-n-EHs', 'resolutions': [{'height': 56, 'url': 'https://external-preview.redd.it/Ep7naw3b8WiYg97q_SuoMTq3wPxaem5FmuwC7524Apg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=191a45892445913b8133721d8896f511b5e41117', 'width': 108}, {'height': 113, 'url': 'https://external-preview.redd.it/Ep7naw3b8WiYg97q_SuoMTq3wPxaem5FmuwC7524Apg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=eec82c80d79a478c1505f9974f41533f8e7252b0', 'width': 216}, {'height': 168, 'url': 'https://external-preview.redd.it/Ep7naw3b8WiYg97q_SuoMTq3wPxaem5FmuwC7524Apg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0c6ea4a339e365f225754022fad3da793ce91ce6', 'width': 320}, {'height': 336, 'url': 'https://external-preview.redd.it/Ep7naw3b8WiYg97q_SuoMTq3wPxaem5FmuwC7524Apg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=26a211ad30ce00eb697854e29f46c0d46babec8d', 'width': 640}, {'height': 504, 'url': 'https://external-preview.redd.it/Ep7naw3b8WiYg97q_SuoMTq3wPxaem5FmuwC7524Apg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e0a3b84de723cb829cc9a65545eee72628d80173', 'width': 960}, {'height': 567, 'url': 'https://external-preview.redd.it/Ep7naw3b8WiYg97q_SuoMTq3wPxaem5FmuwC7524Apg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f2df363aa8cc892a122e0535c74ae0a16a8e4059', 'width': 1080}], 'source': {'height': 630, 'url': 'https://external-preview.redd.it/Ep7naw3b8WiYg97q_SuoMTq3wPxaem5FmuwC7524Apg.jpg?auto=webp&amp;s=9cebe84f92b349a6b12c84b5d55adb1b2a12b3d8', 'width': 1200}, 'variants': {}}]}",6,1640198629,1,"I am a Data Analyst transitioning to DE, and I don't have any prior experience in a Data Engineer role. Looking at the DE ob descriptions gives me severe stress, as every opportunity lists multiple years of DE experience as the first criteria (at least in India). I don't know how to reach out to recruiters to discuss my application for these, as well as other positions, as I already feel that I am ineligible for such positions even though I've got a good grasp of SQL and Python, and continuously learning more DE specific stuff.

I want to know how to reach out to recruiters to at least discuss my resume/skills, and not get outright rejected because I don't have prior DE experience. I also don't want to lie and state that I do have prior experience, just to pass the job posting criteria. Any tips would be welcome, especially from the Indian crowd here.

Thanks!

PS: In case anyone wants to check my resume to understand where I am coming from - [here's a copy](https://drive.google.com/file/d/1XsCv3MelTcbvb1eUbJiRVpY-F4hg2JIv/view?usp=sharing)",True,False,False,dataengineering,t5_36en4,47653,public,self,How to reach out to recruiters as someone who is transitioning to DE?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rmbs4e/how_to_reach_out_to_recruiters_as_someone_who_is/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,tylerjaywood,,,[],,,,text,t2_wxoh8,False,False,False,[],False,False,1640194028,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rma3fv/optimizing_rdss3_ingest_help_understanding_pg/,{},rma3fv,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rma3fv/optimizing_rdss3_ingest_help_understanding_pg/,False,,,6,1640194038,1,"I'm working on an RDS-&gt;S3 process using AWS MWAA. 
I'm using MWAA medium containers that have 4GB RAM. 

To ingest around this constraint I'm using psycopg2 and server side cursors. 

roughly 

```
cursor.itersize(X)
cursor.execute(sql)
rows = cursor.fetchmany(X)
write_rows_to_s3(rows)
```

How can I optimize the size of X? 

X is a number of rows which is variable between tables. 
I can calculate avg row size in Postgresql with pg_stat tables as table_size/reltuples.

Then I feel like I should be able to calculate

mem_limit (4gb) / avg_tuple_size = max itersize 

But in practice my job starts to fail from memory constraints at roughly half of what I would expect and the resultant CSVs are in the range of ~150MB -- not close to the 4GB max I would expect. 

How much of this is from the Postgresql disk size being significantly different from the in-memory-psycopg2 representation of the data, and then the on-disk-csv representation of the data?

Is the psycopg2.cursor.fetchmany() result set significantly larger in memory than the CSV/Postgresql native storage of that same info?

Any ideas on how you would optimize for an optimal chunksize/itersize/X value across 600+ tables?",True,False,False,dataengineering,t5_36en4,47648,public,self,"Optimizing RDS-&gt;S3 ingest. Help understanding pg_stat table sizes, container memory constraints, and resultant CSV disk size",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rma3fv/optimizing_rdss3_ingest_help_understanding_pg/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,nudgekennels,,,[],,,,text,t2_epti8mt9,False,False,False,[],False,False,1640190623,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rm8uos/bootcamp/,{},rm8uos,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rm8uos/bootcamp/,False,,,6,1640190634,1,"Does anyone have experience with either WeCloudData or Pipeline Academy? They’re bootcamps specifically for data engineering. I’m hoping to do one of the two. I know a lot of ppl recommend learning on your own via projects—I’m sure I can do that, but I feel like I’ll have a lot of gaps in my knowledge and won’t have a good grasp of the fundamentals. 

A little about myself: I’m coming from a SQL analyst (ish) background in the finance and software consulting sector.",True,False,False,dataengineering,t5_36en4,47644,public,self,Bootcamp…,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rm8uos/bootcamp/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,signacaste,,,[],,,,text,t2_fadc9ofm,False,False,False,[],False,False,1640190193,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rm8p7t/how_to_prepare_for_data_bricks_apache_30/,{},rm8p7t,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rm8p7t/how_to_prepare_for_data_bricks_apache_30/,False,,,6,1640190203,1,"Hi,
As the title says, would you guys share some tips? I've tried googling but didn't learn much.
Thanks",True,False,False,dataengineering,t5_36en4,47644,public,self,How to prepare for data bricks Apache 3.0?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rm8p7t/how_to_prepare_for_data_bricks_apache_30/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Paulythress,,,[],,,,text,t2_l1vnoo,False,False,False,[],False,False,1640188913,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rm88jk/anyone_ever_work_for_a_sports_franchise/,{},rm88jk,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rm88jk/anyone_ever_work_for_a_sports_franchise/,False,,,6,1640188924,1,"Found some job openings for data engineers in the MLB, but none really for NFL or NBA. I do understand the MLB though was the first pro franchise to take up making data-driven decision making",True,False,False,dataengineering,t5_36en4,47642,public,self,Anyone ever work for a sports franchise?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rm88jk/anyone_ever_work_for_a_sports_franchise/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,myworkaccnthrowaua,,,[],,,,text,t2_cmxaffgh,False,False,False,[],False,False,1640175167,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rm4001/what_is_etl_really/,{},rm4001,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rm4001/what_is_etl_really/,False,,,6,1640175178,1,"Sorry for the stupid question. Is this just a fancy term for extracting data from sources, doing some processing and then loading the data to the target system? 

In my last project I wrote SQL-queries that got me 60% of the way there but I had to do some processing in powershell because of the difference in datamodel of the target system. The processing included remapping of data based on another source. Is this ETL? The reason i'm asking is because there's a client who wants me to help them with an ETL pipeline. I'm not sure I can with good confidence say yes even though they are willing to let me learn on the job.",True,False,False,dataengineering,t5_36en4,47629,public,self,What is ETL really?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rm4001/what_is_etl_really/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Ambitious-Bedroom847,,,[],,,,text,t2_b33tudsy,False,False,False,[],False,False,1640174454,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rm3t2j/senior_data_engineer_interview/,{},rm3t2j,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rm3t2j/senior_data_engineer_interview/,False,,,6,1640174465,1,"Currently interviewing for a bank in the UK for a Senior Python Data Engineering role.

Got through the first round which was mostly Q/A around previous projects and other Spark/Python

related topics.

The next stage will be the technical stage and the heads up I got from the recruiter was it will involve working  on incomplete data science problems publicly available on GitHub on Google Colab.

I honestly wasn't expecting this but I immediately thought of polishing on Pandas and Numpy since I dont use the data science libraries very often in my day to day. But I was curious if anyone here has had a similar interview experience in the past and can provide any other insight I would need to prepare for this.

Thanks!",True,False,False,dataengineering,t5_36en4,47629,public,self,Senior Data Engineer Interview,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rm3t2j/senior_data_engineer_interview/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Ok_Advance_2714,,,[],,,,text,t2_cwrfl9jz,False,False,False,[],False,False,1640158397,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rlzyss/amazon_sde_1_more_superior_than_de_1/,{},rlzyss,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rlzyss/amazon_sde_1_more_superior_than_de_1/,False,,,6,1640158407,1,"I have been getting interview calls for Amazon SDE-1 role very frequently and none to data engineer (DE-1) positions. Why?? 

Is there any one her who got an offer for DE roles from Amazon recently? Please share your interview experience and how you got a call from recruiter!",True,False,False,dataengineering,t5_36en4,47619,public,self,Amazon SDE 1 more superior than DE 1?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rlzyss/amazon_sde_1_more_superior_than_de_1/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,honestly_tho_00,,,[],,,,text,t2_45oraucn,False,False,False,[],False,False,1640153169,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rlym5p/anybody_want_to_lend_me_a_hand_with_options_data/,{},rlym5p,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rlym5p/anybody_want_to_lend_me_a_hand_with_options_data/,False,,,6,1640153179,1,DM for details,True,False,False,dataengineering,t5_36en4,47618,public,self,Anybody want to lend me a hand with options data analysis for 2 hours? Pay is $50,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rlym5p/anybody_want_to_lend_me_a_hand_with_options_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,BoiElroy,,,[],,,,text,t2_r8dyi,False,False,True,[],False,False,1640138112,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rlu26c/have_any_of_you_actually_run_the_tpcds_v3/,{},rlu26c,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rlu26c/have_any_of_you_actually_run_the_tpcds_v3/,False,self,"{'enabled': False, 'images': [{'id': 'yVHu3pQaresXQpSX7pOrKQSeXSoccISdN2Yx451v9eY', 'resolutions': [{'height': 54, 'url': 'https://external-preview.redd.it/eLbB6p2xixOSGyG4aRmmdy8JIT4RQny5iYewhnqh9-Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ab00737a358f0c7fcbe9f56cc6db83f67b5e1132', 'width': 108}, {'height': 108, 'url': 'https://external-preview.redd.it/eLbB6p2xixOSGyG4aRmmdy8JIT4RQny5iYewhnqh9-Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fef1d0b9e219cb1fbdc34a54de18629ae7828929', 'width': 216}, {'height': 160, 'url': 'https://external-preview.redd.it/eLbB6p2xixOSGyG4aRmmdy8JIT4RQny5iYewhnqh9-Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=42ce5085c99de365b297c30e11e94fda9152a834', 'width': 320}, {'height': 320, 'url': 'https://external-preview.redd.it/eLbB6p2xixOSGyG4aRmmdy8JIT4RQny5iYewhnqh9-Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=297c1a2e39b94fca3ec88d9d6cb938daf5aa90c9', 'width': 640}, {'height': 480, 'url': 'https://external-preview.redd.it/eLbB6p2xixOSGyG4aRmmdy8JIT4RQny5iYewhnqh9-Y.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3bbe750446e62126b5660e851354b719cce0952d', 'width': 960}, {'height': 540, 'url': 'https://external-preview.redd.it/eLbB6p2xixOSGyG4aRmmdy8JIT4RQny5iYewhnqh9-Y.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6b793e25385d8a1a26a18122004459f90c11141b', 'width': 1080}], 'source': {'height': 600, 'url': 'https://external-preview.redd.it/eLbB6p2xixOSGyG4aRmmdy8JIT4RQny5iYewhnqh9-Y.jpg?auto=webp&amp;s=0d816be6a71b35eb740892844b6c0c95b60a27be', 'width': 1200}, 'variants': {}}]}",6,1640138123,1,"I'm finding it such a PIA to do this.

The tool from the official website is a shitshow. Doesn't build correctly, the documentation is mixed across versions and refers to executables that don't exist. Some things build with a certain version of visual studio, other things build under a *different* version of visual studio. Trying to get it to work is just...oof.

I want to try recreate Databricks benchmark but the two repositories they point me to, tpcds-kit and spark-sql-perf are both out of date and refer to V2. They also don't seem to work out of the box.

I ended up cloning from this repo: [https://github.com/Agirish/tpcds](https://github.com/Agirish/tpcds) because it was the closest thing to usable queries I could find. I then read them in as a string one by one and then run \`spark.sql('query')\` which only creates the plan, to actually time it I did \`df =spark.sql('query'); df.collect()\` but just the first query has been running for close to an hour on a 4 node cluster which doesn't seem right...How would you even time a query in this situation? 

Anyone actually run this benchmark successfully or have anything reproducible?",True,False,False,dataengineering,t5_36en4,47609,public,self,Have any of you actually run the TPCDS V3 benchmark?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rlu26c/have_any_of_you_actually_run_the_tpcds_v3/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,AspData_engineer,,,[],,,,text,t2_5w6go4dp,False,False,False,[],False,False,1640136692,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rltm02/need_help_with_my_resume_for_a_data_engineer_open/,{},rltm02,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rltm02/need_help_with_my_resume_for_a_data_engineer_open/,False,,,6,1640136703,1,"Hi everyone,

This my first official reddit post and I'm really looking to tap into the power of this wonderful reddit community that has helped me a great deal in my DE journey thus far. I'll get right to the point. I am applying for a DE position at a company that interests me. I connected to an alumni of my current company who introduced me to the director of DE at company X and he wants to review my resume. I think i'm getting anxious and I'm having episodes of imposter syndrome as it is my first DE resume review made by a hiring manager. I am new in the DE space and I would appreciate your help in reviewing my resume against the job description. Your suggestions will be greatly appreciated.  Thank you.

[job description](https://www.linkedin.com/jobs/view/2839321636)

[DE resume](https://docs.google.com/document/d/1T3mThNBGo8knTVMa7yVzOtDYrL4WRM46/edit?usp=drivesdk&amp;ouid=114605175099935502357&amp;rtpof=true&amp;sd=true)",True,False,False,dataengineering,t5_36en4,47610,public,self,Need help with my resume for a Data Engineer open position,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rltm02/need_help_with_my_resume_for_a_data_engineer_open/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,rectalrectifier,,,[],,,,text,t2_4v7au,False,False,False,[],False,False,1640136479,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rltjeu/productionizing_warehouse_data/,{},rltjeu,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rltjeu/productionizing_warehouse_data/,False,,,6,1640136489,1,"Hey everyone, just looking for a little guidance on some issues that are coming up with how my org works with data. I’ll preface all of this by saying I’m a full stack engineer with minimal data engineering experience so I apologize if this is a noob question. We have a variety of data coming in through different sources (airflow jobs, airbyte, fivetran) and it ends up in our (snowflake) warehouse. I’m finding that I’m more often wanting to pull data out of the warehouse to reintegrate into existing api/service databases. Is this a smell? Should I use the warehouse directly instead? I’ve come across the concept of reverse ETL but that seems more like services that sync data into third party services.",True,False,False,dataengineering,t5_36en4,47610,public,self,Productionizing Warehouse Data,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rltjeu/productionizing_warehouse_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,BayerVelt,,,[],,,,text,t2_14vhpn,False,False,False,[],False,False,1640132804,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rlsc5e/learning_how_to_schedule_jobs_in_azkaban/,{},rlsc5e,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rlsc5e/learning_how_to_schedule_jobs_in_azkaban/,False,,,6,1640132814,1,"Hey everyone, I’m currently working as an analyst but have been asked to step up and help with a process that formerly belonged to one of our data scientists. This mostly involves maintaining existing scheduled processes in Azkaban, which I know how to do, however this person left the company without teaching me how to create a new scheduled process as I was OOO. 

I’ve looked around for tutorials but am not yet as familiar with this side of things - I’ve only had one course in Big Data where we learned about Hadoop, the rest of my masters was largely focused on statistical analytics and how to use R/Python. Is there a good example of how to create a .job file, and how to schedule a query? I appreciate any advice, thanks.",True,False,False,dataengineering,t5_36en4,47608,public,self,Learning how to schedule jobs in Azkaban,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rlsc5e/learning_how_to_schedule_jobs_in_azkaban/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Chiefjack98,,,[],,,,text,t2_acc1u0w7,False,False,False,[],False,False,1640130053,tyny.dev,https://www.reddit.com/r/dataengineering/comments/rlreks/tynydev_mock_data_creation_using_rapid/,{},rlreks,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rlreks/tynydev_mock_data_creation_using_rapid/,False,link,"{'enabled': False, 'images': [{'id': 'UFJnwE2a5WVPcDm3PCSMCtKJZEO-dKN3KWXw_itEnyE', 'resolutions': [{'height': 72, 'url': 'https://external-preview.redd.it/NQDxdUvNvcQPRknJjjtbZr1Ma90NckIPGZoMej9ATvA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4dcd3669c04d9726f8cb8a23636c3b85c51d7b4c', 'width': 108}, {'height': 144, 'url': 'https://external-preview.redd.it/NQDxdUvNvcQPRknJjjtbZr1Ma90NckIPGZoMej9ATvA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=04a775e611dc80dc146c5eb515edaba5a0696feb', 'width': 216}, {'height': 213, 'url': 'https://external-preview.redd.it/NQDxdUvNvcQPRknJjjtbZr1Ma90NckIPGZoMej9ATvA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8b7b678e06ecd219d429c686722f5f12e7ed64a6', 'width': 320}, {'height': 426, 'url': 'https://external-preview.redd.it/NQDxdUvNvcQPRknJjjtbZr1Ma90NckIPGZoMej9ATvA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5083ceda833c1a5f17428886527cd2eb18e58ed0', 'width': 640}, {'height': 640, 'url': 'https://external-preview.redd.it/NQDxdUvNvcQPRknJjjtbZr1Ma90NckIPGZoMej9ATvA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9d27ac4e4f852649e8edc9f1089837b4273aee4f', 'width': 960}, {'height': 720, 'url': 'https://external-preview.redd.it/NQDxdUvNvcQPRknJjjtbZr1Ma90NckIPGZoMej9ATvA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ac39461b6608468993d998a65bd108ab5e8f2339', 'width': 1080}], 'source': {'height': 736, 'url': 'https://external-preview.redd.it/NQDxdUvNvcQPRknJjjtbZr1Ma90NckIPGZoMej9ATvA.jpg?auto=webp&amp;s=cc015d49fb3f67a60f7bc24ef738c6e0eefab637', 'width': 1104}, 'variants': {}}]}",6,1640130063,1,,True,False,False,dataengineering,t5_36en4,47607,public,https://b.thumbs.redditmedia.com/GOhj_mcQxicr2NHLAKTVRcEKxMGiMK9rdPwkNZMCn4k.jpg,tyny.dev | Mock data Creation: Using Rapid Prototyping to Speed up Development time | Blog,0,[],1.0,https://tyny.dev/blog/mock-data-creation-using-rapid-prototyping-to-speed-up-development-time,all_ads,6,,,,,,93.0,140.0,https://tyny.dev/blog/mock-data-creation-using-rapid-prototyping-to-speed-up-development-time,,,,,,,,,,
[],False,pagenotdisplayed,,,[],,,,text,t2_69ezk,False,False,False,[],False,False,1640128213,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rlqroa/elt_database_in_aws_leaning_towards_aurora/,{},rlqroa,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rlqroa/elt_database_in_aws_leaning_towards_aurora/,False,,,6,1640128223,1,"Sorry for the long post - just trying to share all the relevant info for our issue at hand. 

.

**Overview:** We are using python to ingest music data from across the web. This includes a handful of web scrapers, as well as data ingestion from 2 different data APIs. Our data pipelines are batch, and we are orchestrating things using self-managed Apache Airflow run with docker on an EC2 instance. The data ingested will be displayed in the front end of our MERN stack app.

.

Some additional info:

* Relative to how big data gets, our data is not too big. We anticipate the entire database to be 10 - 50GB in size over the next few years.

* Our data is relational. In music data, we have tables for **artists**, **songs**, **albums**, etc. Songs belong to albums and artists, and albums belong to artists.

* We are following ELT framework rather than ETL, and so there may be some transformations to the data after loading into the database. All in all, we do not have too many transformations, and the ones we do have are fairly basic (linear combinations of the data).

.

We are working within AWS ecosystem and I need to determine which production database to use. Here is where my mind is currently:

* We need a relational database, so not any of the non-relational databases (so no to DynamoDB, no to ElastiCache) 

* We need to prioritize fast reads by our Node API, not fast analytical transformations. It seems we need an application database, not an analytical database (so no to Redshift)

* It seems our remaining options are RDS PostgreSQL, RDS MySql, RDS Aurora (PostgreSQL or MySQL), RDS Aurora Serverless (Postgres or RDS).


The decision between PostgreSQL vs MySQL seems like a matter of preference, and I am fairly neutral on the two of these options. Assuming we go with PostgreSQL, I am then stuck between (a) RDS PostgreSQL, (b) RDS Aurora PostgreSQL, and (c) RDS Aurora Serverless PostgreSQL. The main full-stack dev on our team is encouraging us to use (c) RDS Aurora Serverless PostgreSQL.

.

My main question(s) then are:

* Is there anything to beware with Aurora Serverless PostgreSQL for ELT?
* Is it challenging or doable to build data pipelines with Python &amp; Airflow that insert data into Aurora Serverless PostgreSQL?
* Can we easily load data from an S3 bucket into Aurora Serverless PostgreSQL? 
* How challenging is it to incorporate Aurora Serverless PostgreSQL into our Apache Airflow project generally?

It seems like Aurora Serverless PostgreSQL will be cheaper than the alternatives, and I am simply trying to figure out if the lower cost comes with additional complexities for building our data pipelines. Let me know if I can share any additional info on this.",True,False,False,dataengineering,t5_36en4,47606,public,self,"ELT database in AWS? Leaning towards Aurora Serverless PostgreSQL, anything to beware?",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rlqroa/elt_database_in_aws_leaning_towards_aurora/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,csukcl,,,[],,,,text,t2_4q6esisa,False,False,False,[],False,False,1640127169,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rlqez1/has_anyone_compiled_all_the_data_in_the_quarterly/,{},rlqez1,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rlqez1/has_anyone_compiled_all_the_data_in_the_quarterly/,False,,,6,1640127179,1,Would be interesting to see trends in the data and also be able to filter on locations.,True,False,False,dataengineering,t5_36en4,47604,public,self,Has anyone compiled all the data in the quarterly salary discussions?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rlqez1/has_anyone_compiled_all_the_data_in_the_quarterly/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,TheFragan,,,[],,,,text,t2_a6b9szlc,False,False,False,[],False,False,1640124853,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rlpme4/spark_execution_planning/,{},rlpme4,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rlpme4/spark_execution_planning/,False,,,6,1640124863,1,"Hello,

I'm a Junior DE and I've been working with Spark for some time, but there's one topic i can't get too information on and it is Spark Execution Planning.

So, I'm trying to better understand the Spark Execution Planning, and I find the articles on the internet quite imprecise.

I think we're all familiar with this diagram: 

&amp;#x200B;

[Spark Logical Planning Diagram](https://preview.redd.it/k2hp54mkxy681.png?width=1024&amp;format=png&amp;auto=webp&amp;s=c1b4df312c600a3cc0cf408c6edc2303ac702cca)

I was wondering which components are involved in each of those phase :

1. Parsing : What component is responsible of parsing the Dataframe/Dataset transformations into Trees ? Is it Spark's CatalystParser? 
2. Analyzing  to resolve the logical Plan:  I think the component involved here is the Analyzer and it uses Catalyst rules along with Catalog to resolve the logical plan. But i want to know how is the Catalog used here ? Is it possible to interfere with the Catalog and the Analyzer (I mean Inject some code to do further verification in order to resolve the Logical Plan) ?
3. Optimization: I guess that here we only uses Catalyst along with Rule Executors and Sets to predefined rules (like Projection pruning, etc.) to optimize the logical plan. I'm i wrong ? Is it possible to exclude some rules from the Logical Planning Optimizations or Write new sets of rules ?
4. \- Physical Planning &amp; Cost Model: the information I found on some Databricks blogs says that here, we use Catalyst rules and Strategies to convert the Logical Plan into one or more physical plan. How is that actually done ? Why does the logical plan translated into \*\*multiple\*\* physical plan ? For Example, if the logical plan's tree has a Join node, will that node be translated into a physical plan with a Sort-Merge Join, another one with a Broadcast Join and so on, and then the Cost Model will chose the most economic way to actually do that join ?
5. Code Generation: I've read somewhere that Tungsten is used here to compile parts of queries to java bytecode, but I can't really find how this is done, can you explain me this part too ? 

I would really appreciate your answers as this topic is kinda getting on my nerves haha, every article I found on the Internet isn't precise enough...

Regards,",True,False,False,dataengineering,t5_36en4,47602,public,https://b.thumbs.redditmedia.com/KOi8uylb_j2qM-hbxcnMoMM1H8KRc_5Q15UkpOF5InA.jpg,Spark Execution Planning,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rlpme4/spark_execution_planning/,all_ads,6,,,,,,32.0,140.0,,"{'k2hp54mkxy681': {'e': 'Image', 'id': 'k2hp54mkxy681', 'm': 'image/png', 'p': [{'u': 'https://preview.redd.it/k2hp54mkxy681.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=024ee5c63223708ca9982538be62b98ca2054753', 'x': 108, 'y': 24}, {'u': 'https://preview.redd.it/k2hp54mkxy681.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=df45e0b16415647cca953a58d4f52493c51d1035', 'x': 216, 'y': 49}, {'u': 'https://preview.redd.it/k2hp54mkxy681.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dc9930390ea4ae3418ecfbf4521cee878a0b2730', 'x': 320, 'y': 73}, {'u': 'https://preview.redd.it/k2hp54mkxy681.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=887e47961c9343027c82742df75bfa3519850f4f', 'x': 640, 'y': 146}, {'u': 'https://preview.redd.it/k2hp54mkxy681.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=18f3a5b1ae7190bece759d3e58a51394da54e0aa', 'x': 960, 'y': 220}], 's': {'u': 'https://preview.redd.it/k2hp54mkxy681.png?width=1024&amp;format=png&amp;auto=webp&amp;s=c1b4df312c600a3cc0cf408c6edc2303ac702cca', 'x': 1024, 'y': 235}, 'status': 'valid'}}",,,,,,,,,
[],False,steelThunderMountain,,,[],,,,text,t2_8ptcyqqn,False,False,False,[],False,False,1640123560,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rlp5pj/is_a_data_warehouse_the_correct_place_for/,{},rlp5pj,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rlp5pj/is_a_data_warehouse_the_correct_place_for/,False,self,"{'enabled': False, 'images': [{'id': 'FbOp4DrVQeBUnEkTmklEJNRZgTMygpIzaI0F1Wsr7hY', 'resolutions': [{'height': 101, 'url': 'https://external-preview.redd.it/py8U_jqhUDzjwxuYXZ8ocZnGxcuqs5ihzvt85jr1YGQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dea49d96022fdcd6646152dd7ffd18c47b00ec60', 'width': 108}, {'height': 202, 'url': 'https://external-preview.redd.it/py8U_jqhUDzjwxuYXZ8ocZnGxcuqs5ihzvt85jr1YGQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f8ea3e8b1dc50f0f022016309547aae200ef68f2', 'width': 216}, {'height': 300, 'url': 'https://external-preview.redd.it/py8U_jqhUDzjwxuYXZ8ocZnGxcuqs5ihzvt85jr1YGQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=40bc21d83ef6c96bd39dcca5e66e3e9f1d0d2987', 'width': 320}, {'height': 600, 'url': 'https://external-preview.redd.it/py8U_jqhUDzjwxuYXZ8ocZnGxcuqs5ihzvt85jr1YGQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=83e6bcea24f53c315657e3f2de04e6a0d05c577d', 'width': 640}, {'height': 901, 'url': 'https://external-preview.redd.it/py8U_jqhUDzjwxuYXZ8ocZnGxcuqs5ihzvt85jr1YGQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=178630cd05bdd881f1e9516f3ba56d9155df9723', 'width': 960}], 'source': {'height': 902, 'url': 'https://external-preview.redd.it/py8U_jqhUDzjwxuYXZ8ocZnGxcuqs5ihzvt85jr1YGQ.jpg?auto=webp&amp;s=ef8f5e9b990ba012b9b028dcd52456b3eaacd295', 'width': 961}, 'variants': {}}]}",6,1640123571,1,"I consult for a number of companies all owned at least in part by the same person. The data (income statement, balance sheet, etc.) is stored in excel files and various accounting softwares for each. Recently, the discussion came up that it would be great to bring together all this information in a single place, even if just the key line items are available at first. 

At first, I thought a SQLite database with a few dimensions (like [this](https://analyzingalpha.com/assets/images/posts/2020-02-28-financial-statement-schema.jpg)) would do the trick, but I can't help but wonder if I'm: 1. Using the right tool for the job 2. Thinking about this project the right way. 

This would be my first experience with schema design/creation. Has anyone here had experience with a project like this?",True,False,False,dataengineering,t5_36en4,47602,public,self,Is a Data Warehouse the correct place for combining/storing private company financial statement data?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rlp5pj/is_a_data_warehouse_the_correct_place_for/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,theporterhaus,#46d160,mod,[],fd5b074e-239e-11e8-a28b-0e0f8d9eda5a,mod | Sr. Data Engineer,light,text,t2_2tv9i42n,False,False,False,[],False,False,1640115915,businessinsider.com,https://www.reddit.com/r/dataengineering/comments/rlmhvx/facebooks_reputation_is_so_bad_the_company_must/,{},rlmhvx,False,True,False,False,False,True,False,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rlmhvx/facebooks_reputation_is_so_bad_the_company_must/,False,link,"{'enabled': False, 'images': [{'id': 'AsSl2RkeSmYAWi3t4AZvxTGP1IYi8_vl9IJ0mC4zQiA', 'resolutions': [{'height': 54, 'url': 'https://external-preview.redd.it/wcRVnsILXP7cYMwkynOFdak8Uq01Q2o9bTw4QXkkOcM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=50252d8865c391eda59ebd24508ba7fb29b01e2a', 'width': 108}, {'height': 108, 'url': 'https://external-preview.redd.it/wcRVnsILXP7cYMwkynOFdak8Uq01Q2o9bTw4QXkkOcM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3c22c948096a16e589ec9eb529f30d6ade2d4ec0', 'width': 216}, {'height': 160, 'url': 'https://external-preview.redd.it/wcRVnsILXP7cYMwkynOFdak8Uq01Q2o9bTw4QXkkOcM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=784a8c3c2885a98b30a6a8f32254ad00594af193', 'width': 320}, {'height': 320, 'url': 'https://external-preview.redd.it/wcRVnsILXP7cYMwkynOFdak8Uq01Q2o9bTw4QXkkOcM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b2e564a7fed711632015045ac315e9eb331489bc', 'width': 640}, {'height': 480, 'url': 'https://external-preview.redd.it/wcRVnsILXP7cYMwkynOFdak8Uq01Q2o9bTw4QXkkOcM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b07d73cb9adce6a1f9ffe1dc8cdd265293b6321e', 'width': 960}, {'height': 540, 'url': 'https://external-preview.redd.it/wcRVnsILXP7cYMwkynOFdak8Uq01Q2o9bTw4QXkkOcM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1d7ed1431ecea5270d41223e12c8f831a6cbdf79', 'width': 1080}], 'source': {'height': 600, 'url': 'https://external-preview.redd.it/wcRVnsILXP7cYMwkynOFdak8Uq01Q2o9bTw4QXkkOcM.jpg?auto=webp&amp;s=4d6cc6aa545ca430093a4dbfd37a8af3beaad5a8', 'width': 1200}, 'variants': {}}]}",6,1640115926,1,,True,False,False,dataengineering,t5_36en4,47594,public,https://b.thumbs.redditmedia.com/PMfllIczoQPxx2L5cDLBWkUQ1TD5pQ9-iQ-1yBP6pDQ.jpg,"Facebook's reputation is so bad, the company must pay even more now to hire and retain talent. Some are calling it a 'brand tax' as tech workers fear a 'black mark' on their careers.",0,[],1.0,https://www.businessinsider.com/facebook-pays-brand-tax-hire-talent-fears-career-black-mark-2021-12,all_ads,6,,,,,,70.0,140.0,https://www.businessinsider.com/facebook-pays-brand-tax-hire-talent-fears-career-black-mark-2021-12,,,,True,,t3_rlfxa8,"[{'all_awardings': [{'award_sub_type': 'GLOBAL', 'award_type': 'global', 'awardings_required_to_grant_benefits': None, 'coin_price': 150, 'coin_reward': 0, 'count': 3, 'days_of_drip_extension': 0, 'days_of_premium': 0, 'description': 'Thank you stranger. Shows the award.', 'end_date': None, 'giver_coin_reward': None, 'icon_format': None, 'icon_height': 2048, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'icon_width': 2048, 'id': 'award_f44611f1-b89e-46dc-97fe-892280b13b82', 'is_enabled': True, 'is_new': False, 'name': 'Helpful', 'penny_donate': None, 'penny_price': None, 'resized_icons': [{'height': 16, 'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16}, {'height': 32, 'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32}, {'height': 48, 'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48}, {'height': 64, 'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64}, {'height': 128, 'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128}], 'resized_static_icons': [{'height': 16, 'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45', 'width': 16}, {'height': 32, 'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1', 'width': 32}, {'height': 48, 'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d', 'width': 48}, {'height': 64, 'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11', 'width': 64}, {'height': 128, 'url': 'https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878', 'width': 128}], 'start_date': None, 'static_icon_height': 2048, 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png', 'static_icon_width': 2048, 'subreddit_coin_reward': 0, 'subreddit_id': None, 'tiers_by_required_awardings': None}], 'allow_live_comments': True, 'approved_at_utc': None, 'approved_by': None, 'archived': False, 'author': 'Dull_Tonight', 'author_flair_background_color': None, 'author_flair_css_class': None, 'author_flair_richtext': [], 'author_flair_template_id': None, 'author_flair_text': None, 'author_flair_text_color': None, 'author_flair_type': 'text', 'author_fullname': 't2_8jafqacl', 'author_is_blocked': False, 'author_patreon_flair': False, 'author_premium': True, 'awarders': [], 'banned_at_utc': None, 'banned_by': None, 'can_gild': True, 'can_mod_post': False, 'category': None, 'clicked': False, 'content_categories': None, 'contest_mode': False, 'created': 1640097253.0, 'created_utc': 1640097253.0, 'discussion_type': None, 'distinguished': None, 'domain': 'businessinsider.com', 'downs': 0, 'edited': False, 'gilded': 1, 'gildings': {}, 'hidden': False, 'hide_score': False, 'id': 'rlfxa8', 'is_created_from_ads_ui': False, 'is_crosspostable': True, 'is_meta': False, 'is_original_content': False, 'is_reddit_media_domain': False, 'is_robot_indexable': True, 'is_self': False, 'is_video': False, 'likes': None, 'link_flair_background_color': '', 'link_flair_css_class': 'general', 'link_flair_richtext': [], 'link_flair_template_id': '49cac61c-a816-11e9-be34-0ebbab5890a0', 'link_flair_text': 'Business', 'link_flair_text_color': 'dark', 'link_flair_type': 'text', 'locked': False, 'media': None, 'media_embed': {}, 'media_only': False, 'mod_note': None, 'mod_reason_by': None, 'mod_reason_title': None, 'mod_reports': [], 'name': 't3_rlfxa8', 'no_follow': False, 'num_comments': 626, 'num_crossposts': 5, 'num_reports': None, 'over_18': False, 'parent_whitelist_status': 'all_ads', 'permalink': '/r/technology/comments/rlfxa8/facebooks_reputation_is_so_bad_the_company_must/', 'pinned': False, 'post_hint': 'link', 'preview': {'enabled': False, 'images': [{'id': 'AsSl2RkeSmYAWi3t4AZvxTGP1IYi8_vl9IJ0mC4zQiA', 'resolutions': [{'height': 54, 'url': 'https://external-preview.redd.it/wcRVnsILXP7cYMwkynOFdak8Uq01Q2o9bTw4QXkkOcM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=50252d8865c391eda59ebd24508ba7fb29b01e2a', 'width': 108}, {'height': 108, 'url': 'https://external-preview.redd.it/wcRVnsILXP7cYMwkynOFdak8Uq01Q2o9bTw4QXkkOcM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3c22c948096a16e589ec9eb529f30d6ade2d4ec0', 'width': 216}, {'height': 160, 'url': 'https://external-preview.redd.it/wcRVnsILXP7cYMwkynOFdak8Uq01Q2o9bTw4QXkkOcM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=784a8c3c2885a98b30a6a8f32254ad00594af193', 'width': 320}, {'height': 320, 'url': 'https://external-preview.redd.it/wcRVnsILXP7cYMwkynOFdak8Uq01Q2o9bTw4QXkkOcM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b2e564a7fed711632015045ac315e9eb331489bc', 'width': 640}, {'height': 480, 'url': 'https://external-preview.redd.it/wcRVnsILXP7cYMwkynOFdak8Uq01Q2o9bTw4QXkkOcM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b07d73cb9adce6a1f9ffe1dc8cdd265293b6321e', 'width': 960}, {'height': 540, 'url': 'https://external-preview.redd.it/wcRVnsILXP7cYMwkynOFdak8Uq01Q2o9bTw4QXkkOcM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1d7ed1431ecea5270d41223e12c8f831a6cbdf79', 'width': 1080}], 'source': {'height': 600, 'url': 'https://external-preview.redd.it/wcRVnsILXP7cYMwkynOFdak8Uq01Q2o9bTw4QXkkOcM.jpg?auto=webp&amp;s=4d6cc6aa545ca430093a4dbfd37a8af3beaad5a8', 'width': 1200}, 'variants': {}}]}, 'pwls': 6, 'quarantine': False, 'removal_reason': None, 'removed_by': None, 'removed_by_category': None, 'report_reasons': None, 'saved': False, 'score': 9481, 'secure_media': None, 'secure_media_embed': {}, 'selftext': '', 'selftext_html': None, 'send_replies': False, 'spoiler': False, 'stickied': False, 'subreddit': 'technology', 'subreddit_id': 't5_2qh16', 'subreddit_name_prefixed': 'r/technology', 'subreddit_subscribers': 11182739, 'subreddit_type': 'public', 'suggested_sort': None, 'thumbnail': 'https://b.thumbs.redditmedia.com/PMfllIczoQPxx2L5cDLBWkUQ1TD5pQ9-iQ-1yBP6pDQ.jpg', 'thumbnail_height': 70, 'thumbnail_width': 140, 'title': ""Facebook's reputation is so bad, the company must pay even more now to hire and retain talent. Some are calling it a 'brand tax' as tech workers fear a 'black mark' on their careers."", 'top_awarded_type': None, 'total_awards_received': 3, 'treatment_tags': [], 'ups': 9481, 'upvote_ratio': 0.97, 'url': 'https://www.businessinsider.com/facebook-pays-brand-tax-hire-talent-fears-career-black-mark-2021-12', 'url_overridden_by_dest': 'https://www.businessinsider.com/facebook-pays-brand-tax-hire-talent-fears-career-black-mark-2021-12', 'user_reports': [], 'view_count': None, 'visited': False, 'whitelist_status': 'all_ads', 'wls': 6}]",,,
[],False,BoiElroy,,,[],,,,text,t2_r8dyi,False,False,True,[],False,False,1640113846,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rllrnj/database_client_for_delta_lake/,{},rllrnj,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rllrnj/database_client_for_delta_lake/,False,,,6,1640113857,1,"Are there any database clients that would work with delta lake? I'd like to just be able to view Databases and tables, I don't necessarily need the SQL editor, but could be nice. 

I know databaricks has a 'data' tab, and has the new SQL interface, but I have a couple of different databases, and wanted to just use one database client for high-level viewing stuff table names etc. I wanted to use something that would behave like DB Visualizer, Squirrel DB, or Jetbrain's Datagrip (haven't tried yet).

Has anyone done this? Or have any suggestions?",True,False,False,dataengineering,t5_36en4,47593,public,self,Database client for Delta Lake?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rllrnj/database_client_for_delta_lake/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,odahat,transparent,,[],fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b,Data Engineer,dark,text,t2_5e33q9sc,False,False,False,[],False,False,1640106798,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rlj9sl/bigquery_add_column_from_column_using_python_and/,{},rlj9sl,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rlj9sl/bigquery_add_column_from_column_using_python_and/,False,,,6,1640106809,1,"Hey,

I have a BigQuery data warehouse containing all the data from a mongodb database, those data are sync once a day.

I would like to add a column to one of my table, that column is a cleaned + lemmatized version of another column (the type is string). I can't do that with DBT because I need to use the python library Spacy. How could I run such a transformation on my table without having to get all the data locally and sending 10M UPDATE on bigquery ? Is there some GCP tools to run python function against bigquery like dataflow or something like that ?

Thanks for your help !",True,False,False,dataengineering,t5_36en4,47586,public,self,BigQuery - Add column from column using Python and Spacy,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rlj9sl/bigquery_add_column_from_column_using_python_and/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Status-Cloud-6136,,,[],,,,text,t2_bzca0cx3,False,False,False,[],False,False,1640106225,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rlj234/how_to_connect_a_central_fact_table_to_datasets/,{},rlj234,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rlj234/how_to_connect_a_central_fact_table_to_datasets/,False,,,6,1640106236,1," 

Hi Question about database design. I am creating a database that stores many datasets and I have a central ""fact table"" that just has the unique names of the datasets. The problem is I cannot find a way to connect the actual datasets to the ""fact table"". For example I have a dataset **unistudents** that has the name *Imaginary\_Unistudents\_2020*, but the actual **unistudents** dataset has

    columns student_id, student_name and program. 

Is there a logical way to connect them( the central ""fact table "" and unistudents table). I could create a fictional column in the unistudents dataset so it would be a table unistudent with columns

    student_id, student_name, program and a dataset_name 

with value *Imaginary\_Unistudents\_2020*. But that would create a rather redundant column (for example dataset\_name with only one value *Imaginary\_Unistudents\_2020* everywhere). Is there a better, more logical way to do it?",True,False,False,dataengineering,t5_36en4,47586,public,self,"How to connect a central ""fact table"" to datasets?",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rlj234/how_to_connect_a_central_fact_table_to_datasets/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,apple_albatross,,,[],,,,text,t2_hf95bszx,False,False,False,[],False,False,1640104419,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rliei5/actuarial_analyst_interested_in_switching_to_de/,{},rliei5,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rliei5/actuarial_analyst_interested_in_switching_to_de/,False,,,6,1640104430,1,"Hi, 

I'm an actuarial analyst looking to switch out of the actuarial field. I'm currently interested in data analyst positions, but I have this suspicion that data engineering what I'm ultimately interested in and would to have the door to that career path open. 

My skills include cleaning data and setting up simple ML models in R and Python. I can write advanced SQL queries and have a bit of experience with Tableau. 

What kind of skills/responsibilities/team dynamic should I be looking for in a data analyst position that will help me learn more and possibly transition into a DE role in the future? Are there any supplemental skills I can pick up right now to help me land such data analyst position? Any tips would be appreciated! 

&amp;#x200B;

Thanks!",True,False,False,dataengineering,t5_36en4,47583,public,self,Actuarial analyst interested in switching to DE,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rliei5/actuarial_analyst_interested_in_switching_to_de/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,PaulSandwich,,,[],,,,text,t2_x2wqm,False,False,False,[],False,False,1640097581,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rlg0qa/best_practices_for_nested_json_with_pyspark/,{},rlg0qa,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rlg0qa/best_practices_for_nested_json_with_pyspark/,False,,,6,1640097591,1,"Like the title says, I'm doing a super common task of pulling log data from an API in JSON format and I'm looking to transform it into parquet files. Essentially: One complex object in, `n` relational objects out.   
   
I've got a great process that takes any JSON you throw at it, identifies arrays, and flattens the data. I'd like to enhance it with an option to flatten *or explode*. Explode would take each child array and put them in their own dataframe, but I'm going 'round and 'round on the best way to implement keys.   
   
Surely this is an old problem; anyone know any blogposts or boilerplate that deals with this model? All the search results are heavily skewed towards recursive flattening so I thought I'd ask here.",True,False,False,dataengineering,t5_36en4,47574,public,self,Best practices for nested JSON with PySpark? Specifically dynamic ways to create relational tables from nested arrays.,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rlg0qa/best_practices_for_nested_json_with_pyspark/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,growth_man,,,[],,,,text,t2_baajg5kk,False,False,False,[],False,False,1640094267,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rlezg1/modern_data_stack_is_live_on_product_hunt/,{},rlezg1,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,1,0,False,all_ads,/r/dataengineering/comments/rlezg1/modern_data_stack_is_live_on_product_hunt/,False,self,"{'enabled': False, 'images': [{'id': 'VMkkVfmIJQVZHfDWgysbw7eEcd7NkkTQ65xuOHPDkTM', 'resolutions': [{'height': 54, 'url': 'https://external-preview.redd.it/rLv30Mo5wBBOxpNHgvjM5PqTV78x5Kty3-6wlgsGDtA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cadf1e6852af32bc89987d21df9fae308f43bd09', 'width': 108}, {'height': 108, 'url': 'https://external-preview.redd.it/rLv30Mo5wBBOxpNHgvjM5PqTV78x5Kty3-6wlgsGDtA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ce228041af7d83334ea7337bc02ac69e2a3aff93', 'width': 216}, {'height': 160, 'url': 'https://external-preview.redd.it/rLv30Mo5wBBOxpNHgvjM5PqTV78x5Kty3-6wlgsGDtA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8c2251e412ac628ee9f4d221c04983d707ba9842', 'width': 320}, {'height': 320, 'url': 'https://external-preview.redd.it/rLv30Mo5wBBOxpNHgvjM5PqTV78x5Kty3-6wlgsGDtA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=730b38a6609227776147a10c995c90ce15980c6c', 'width': 640}, {'height': 480, 'url': 'https://external-preview.redd.it/rLv30Mo5wBBOxpNHgvjM5PqTV78x5Kty3-6wlgsGDtA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a831ec35419c35916a32cf9633ec9fa1610bd234', 'width': 960}], 'source': {'height': 512, 'url': 'https://external-preview.redd.it/rLv30Mo5wBBOxpNHgvjM5PqTV78x5Kty3-6wlgsGDtA.jpg?auto=webp&amp;s=048a6cd288cd87ef391d5ed923df816b610d5bba', 'width': 1024}, 'variants': {}}]}",6,1640094278,1,"🚀MDS is live on Product Hunt🚀  
It's a big day for us! We'd love to get your support and hear your feedback in the comments.  


Modern Data Stack- A platform for everything you need to know about the Modern Data Stack.  


✨Companies &amp; Categories shaping the Modern Data Stack  
📚Data stacks of the world's top companies  
📖Resources to get updates on the latest in this space  
🧰Jobs in data engineering &amp; more!  


Say hello to MDS on Product Hunt 👋 : [https://www.producthunt.com/posts/modern-data-stack](https://www.producthunt.com/posts/modern-data-stack)",True,False,False,dataengineering,t5_36en4,47572,public,self,Modern Data Stack is live on Product Hunt!,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rlezg1/modern_data_stack_is_live_on_product_hunt/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,jxys1723,,,[],,,,text,t2_9gsa8rcc,False,False,False,[],False,False,1640093911,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rlevbq/need_help_with_eer_diagram_if_this_isnt_allowed/,{},rlevbq,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rlevbq/need_help_with_eer_diagram_if_this_isnt_allowed/,False,,,6,1640093921,1,"    This database is used to be able to persist train stations. A train-
    station is uniquely identified by a name (e.g. 'Ypres'). In addition,
    for each station also a unique coordinate pair is stored, consisting of the
    latitude and longitude. A distinction is made between internal
    national and foreign stations, in the sense that for foreign stations the national
    code of the country in which the station is located is stored (e.g. 'fr' for
    France) and not for domestic stations. For domestic stations,
    the unique name is always displayed in Dutch, but may be
    several translations of this name. This does not apply to foreign
    stations. A translation of a station name logically consists of the translation
    as well as from the country code of the language to which the translation belongs (e.g. 'Ypres' is the
    ‘fr’ (English) translation of ‘Ypres’). For each country code, a maximum of
    times 1 translation.
    
    The NMBS also has different types of trains available with a unique
    generic name (e.g. 'IC', 'P', 'BUS', . . . ) each of which refers to exactly one category of connection.
    hear (e.g. “high speed”, “regional”,…). These train types can be
    placed on several routes. Each trajectory has a unique name and is linked
    to all train types that travel this route. For example, the trajectory with
    name 'Blankenberge -- Gent-Sint-Pieters' linked to the train types with name
    'IC' and 'IT'.
    A trajectory is carried out one or more times a day. The implementation of
    a route in which a train of a certain type stops at a fixed sequence
    5
    stations (stops) at a fixed sequence of times is called a trip. every
    first of all, trip has a unique code, a maximum capacity of travelers
    that can take place on this trip and a collection of dates on which these
    trip is performed. In addition, it is necessary to save before every trip
    at what time (regardless of the dates) a train arrives and departs
    a specific stop (with the exception of the first stop which has no arrival information
    matie and the last stop that has no departure information). Of course you can
    trains only depart from a stop when they have arrived there first. Additionally
    In this case, it is necessary to indicate whether this is an arrival and departure
    time falls on the day of the original departure of the train (before midnight),
    or on the day after the original departure of the train (after midnight). like a train
    arrives at a stop after midnight, it also departs after midnight.
    Conversely, a train will certainly arrive at a stop before midnight if it is there too
    leaves before midnight.
    Because the departure and arrival times are not necessarily unique for a trip, but
    the order of the stations must be known, every stop where a train is
    stops a unique stop number per trip (with the first stop being numbered 1).
    The order of the stop numbers per trip should of course be consistent with the
    arrival and departure times of the same trip. This means that the arrival
    time of a train at a stop must be (same or) later than the departure time
    at stops with a lower stop number and the reverse should also apply. for ie-
    other stop on a trip for which departure information is known (i.e. all stops except
    the latter), an expected passenger occupancy should also be stored.
    Of course, this expected occupation may never exceed the maximum capacity of the trip
    exceed.

&amp;#x200B;

[my third iteration of my eer diagram](https://preview.redd.it/nc2c070gfw681.png?width=320&amp;format=png&amp;auto=webp&amp;s=48dce5faf0a9492924c8398483928dd3ff605fa2)",True,False,False,dataengineering,t5_36en4,47572,public,https://b.thumbs.redditmedia.com/zV-1tsw2Y3k7xGfLNiiiROlRVUfzmU4CXIA7IA3QfdI.jpg,"need help with eer diagram, if this isn't allowed Ill delete.",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rlevbq/need_help_with_eer_diagram_if_this_isnt_allowed/,all_ads,6,,,,,,140.0,140.0,,"{'nc2c070gfw681': {'e': 'Image', 'id': 'nc2c070gfw681', 'm': 'image/png', 'p': [{'u': 'https://preview.redd.it/nc2c070gfw681.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=eb5894a45f73cac6fe741c0b8855209f8c34a00a', 'x': 108, 'y': 150}, {'u': 'https://preview.redd.it/nc2c070gfw681.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=275027a4b13d636cb4d48937c249542b03716f3c', 'x': 216, 'y': 301}, {'u': 'https://preview.redd.it/nc2c070gfw681.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ffd1512a7fecb81a3ad75f398eeb064b1a9e31a4', 'x': 320, 'y': 447}], 's': {'u': 'https://preview.redd.it/nc2c070gfw681.png?width=320&amp;format=png&amp;auto=webp&amp;s=48dce5faf0a9492924c8398483928dd3ff605fa2', 'x': 320, 'y': 447}, 'status': 'valid'}}",,,,,,,,,
[],False,vietlinh12hoa,,,[],,,,text,t2_5czjyjhi,False,False,False,[],False,False,1640092207,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rled8a/way_to_practice_sql_to_be_aligned_with_real_work/,{},rled8a,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rled8a/way_to_practice_sql_to_be_aligned_with_real_work/,False,,,6,1640092217,1,"Cheer all,

I will start an analyst job next month. The job is expected to do lots of SQL. I would love to practice advanced SQL. I found lots of SQL resources, but I'm afraid it's like the exercies/homework which is not really close to real work scenario. I was thinking also to pay LeetCode premium to practice SQL. Not sure it's really worthy.

Anyone knows any resources to practce real-work level of SQL?

Thanks.",True,False,False,dataengineering,t5_36en4,47570,public,self,Way to practice SQL to be aligned with real work,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rled8a/way_to_practice_sql_to_be_aligned_with_real_work/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,neheughk,,,[],,,,text,t2_t8ub2,False,False,False,[],False,False,1640090024,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rldqjk/any_data_engineers_here_who_work_in/,{},rldqjk,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rldqjk/any_data_engineers_here_who_work_in/,False,,,6,1640090034,1,"I know there are lots of software engineers and data analysts in government but what about data engineers? If you are a data engineer in government or public sector, what has your experience been like?",True,False,False,dataengineering,t5_36en4,47568,public,self,Any data engineers here who work in government/public sector?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rldqjk/any_data_engineers_here_who_work_in/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,jah_reddit,,,[],,,,text,t2_b2ibr,False,False,False,[],False,False,1640080650,i.redd.it,https://www.reddit.com/r/dataengineering/comments/rlbenz/weve_all_done_it_at_some_point/,{},rlbenz,False,True,False,False,True,True,False,False,#ff66ac,[],dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1,Meme,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rlbenz/weve_all_done_it_at_some_point/,False,image,"{'enabled': True, 'images': [{'id': 'OWY6S57QeavwhB83YZq0PVD6QwZM5TWvAuLRr8T4Qvw', 'resolutions': [{'height': 69, 'url': 'https://preview.redd.it/md4nyic8cv681.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=46b701e07c74a9f32be12e8a9af818e91ca6425b', 'width': 108}, {'height': 139, 'url': 'https://preview.redd.it/md4nyic8cv681.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ceda833c7c78d70333da361619ec713887975abf', 'width': 216}, {'height': 207, 'url': 'https://preview.redd.it/md4nyic8cv681.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4a5fd41d9bafa78515e1bccae5e96f04fef5f3e6', 'width': 320}, {'height': 414, 'url': 'https://preview.redd.it/md4nyic8cv681.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4a8fdb2b73f3ffdb9c16d676518b2652161cd086', 'width': 640}], 'source': {'height': 474, 'url': 'https://preview.redd.it/md4nyic8cv681.png?auto=webp&amp;s=b06ca1fd58bea0d25c62ec90e2aed4b5885a1cd2', 'width': 732}, 'variants': {}}]}",6,1640080661,1,,True,False,False,dataengineering,t5_36en4,47562,public,https://b.thumbs.redditmedia.com/i4BdGocXMB5O8iBbEjdvU6l21NdxT8ZibBgsctswAtw.jpg,We've all done it at some point...,0,[],1.0,https://i.redd.it/md4nyic8cv681.png,all_ads,6,,,,,,90.0,140.0,https://i.redd.it/md4nyic8cv681.png,,,,,,,,,,
[],False,srajeevan89,,,[],,,,text,t2_25nt9ty6,False,False,False,[],False,False,1640068054,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rl88g4/de_course/,{},rl88g4,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,1,0,False,all_ads,/r/dataengineering/comments/rl88g4/de_course/,False,self,"{'enabled': False, 'images': [{'id': 'ybJhJYT4aFhXzoXKl_okztxOsuJpvKYceXDQxEk7qcE', 'resolutions': [{'height': 58, 'url': 'https://external-preview.redd.it/0mL3Gmb0yU6pKUIr71HaqQswf-uAC9Ja-3ya0g-UcEk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=35d5da0ed415a74782b05c00574fe8621c1351bc', 'width': 108}, {'height': 117, 'url': 'https://external-preview.redd.it/0mL3Gmb0yU6pKUIr71HaqQswf-uAC9Ja-3ya0g-UcEk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=42e3a5988bff1da5889f09463cf343e9cd64b165', 'width': 216}, {'height': 174, 'url': 'https://external-preview.redd.it/0mL3Gmb0yU6pKUIr71HaqQswf-uAC9Ja-3ya0g-UcEk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5f8781a0d3352e3cf38db87dad02c32cbec05597', 'width': 320}], 'source': {'height': 273, 'url': 'https://external-preview.redd.it/0mL3Gmb0yU6pKUIr71HaqQswf-uAC9Ja-3ya0g-UcEk.jpg?auto=webp&amp;s=0cc2c72301cb4065d63cb26b09f1e32bd0295238', 'width': 500}, 'variants': {}}]}",6,1640068065,1,"Any one tried andreas kretz data engineering courses from https://www.learndataengineering.com.

Any reviews on that ?",True,False,False,dataengineering,t5_36en4,47547,public,self,DE course,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rl88g4/de_course/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Different_Two7298,,,[],,,,text,t2_7go4t932,False,False,False,[],False,False,1640064780,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rl7a59/download_schemas_and_tables_from_teradata_using/,{},rl7a59,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,1,0,False,all_ads,/r/dataengineering/comments/rl7a59/download_schemas_and_tables_from_teradata_using/,False,self,"{'enabled': False, 'images': [{'id': 'QqSY3F9i2BgB-OdT_JpQr1vBqr2oq4spYNzkghHXwCM', 'resolutions': [], 'source': {'height': 64, 'url': 'https://external-preview.redd.it/5ZI7oL3JTPPt59G0vTfOaQMHvka17QCAdFnF87leUeA.jpg?auto=webp&amp;s=52cc36e047bdca039326e84d3b7ce7aabaf12be6', 'width': 64}, 'variants': {}}]}",6,1640064790,1," Hi guys,

I'm facing a challenge and would be great if you can share your input for this. I want to export schema or tables (DDL/DML SQL files) from Teradata to a local system using python. What should be the best approach for this process? 

Is there any scripter available for Teradata, unlike mssql-scripter? If so can you please share the link for the same?

Also, is there any website/course/blog anything for learning Teradata using python? If so please share the link it would be of great help.

[#python](https://www.linkedin.com/feed/hashtag/?keywords=python&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6878192997729734656) [#python3](https://www.linkedin.com/feed/hashtag/?keywords=python3&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6878192997729734656) [#teradata](https://www.linkedin.com/feed/hashtag/?keywords=teradata&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6878192997729734656) [#datamigration](https://www.linkedin.com/feed/hashtag/?keywords=datamigration&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6878192997729734656)",True,False,False,dataengineering,t5_36en4,47545,public,self,Download Schemas and tables from Teradata using python,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rl7a59/download_schemas_and_tables_from_teradata_using/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,librocubicularist69,,,[],,,,text,t2_4i0hl7r6,False,False,False,[],False,False,1640047420,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rl1rio/100_operational_data_in_lake_for_data_governance/,{},rl1rio,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rl1rio/100_operational_data_in_lake_for_data_governance/,False,,,6,1640047430,1,"New data in data lake are commonly incorporated after analyst request. Like to ask opinion here what are the implications if we turn the tables and set that all operational data must land a copy to lake at limited frequency a year for data governance purpose?

Extension to that is all business glossary must be updated here onwards",True,False,False,dataengineering,t5_36en4,47532,public,self,100% operational data in lake for data governance purpose. How common?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rl1rio/100_operational_data_in_lake_for_data_governance/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,SuperduperApe21,,,[],,,,text,t2_9e9gyb9p,False,False,False,[],False,False,1640040013,youtu.be,https://www.reddit.com/r/dataengineering/comments/rkz7b7/ex_boston_consulting_group_google_manager_reveals/,{},rkz7b7,False,True,False,False,False,True,False,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rkz7b7/ex_boston_consulting_group_google_manager_reveals/,False,rich:video,"{'enabled': False, 'images': [{'id': 'LkM1Qqf2uhzUyaMSBTTxeUrdkgxYkEfBoi55coThu0s', 'resolutions': [{'height': 81, 'url': 'https://external-preview.redd.it/nAXGUGQYASAk__Mb1XFr8p1sCNIan9kyWVdeDsHscr4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bfd7275e2fdd6e46c0376a27b592b4f011feb1e8', 'width': 108}, {'height': 162, 'url': 'https://external-preview.redd.it/nAXGUGQYASAk__Mb1XFr8p1sCNIan9kyWVdeDsHscr4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=24423526a7262d2c47815c5ecc281d63afdded2c', 'width': 216}, {'height': 240, 'url': 'https://external-preview.redd.it/nAXGUGQYASAk__Mb1XFr8p1sCNIan9kyWVdeDsHscr4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a97f623d436290e051e4059bf74a8e0698375640', 'width': 320}], 'source': {'height': 360, 'url': 'https://external-preview.redd.it/nAXGUGQYASAk__Mb1XFr8p1sCNIan9kyWVdeDsHscr4.jpg?auto=webp&amp;s=446866f5ee060fab86254f76166b8e0037b83f3f', 'width': 480}, 'variants': {}}]}",6,1640040023,1,,True,False,False,dataengineering,t5_36en4,47523,public,https://a.thumbs.redditmedia.com/zoOR7UsKIOx8tsOODJS9m6zgdFd89B-clzhyTJTzkw4.jpg,"Ex Boston Consulting Group &amp; Google Manager reveals Hiring Processes, Required Skills &amp; Experiences",0,[],1.0,https://youtu.be/9KSm_Zd37hE,all_ads,6,"{'oembed': {'author_name': 'HUSTLE HUB', 'author_url': 'https://www.youtube.com/c/HUSTLEHUB-CHANNEL', 'height': 200, 'html': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/9KSm_Zd37hE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/9KSm_Zd37hE/hqdefault.jpg', 'thumbnail_width': 480, 'title': 'Ex Boston Consulting Group &amp; Google Manager reveals Hiring Processes, Required Skills &amp; Experiences', 'type': 'video', 'version': '1.0', 'width': 267}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/9KSm_Zd37hE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'scrolling': False, 'width': 267}",,"{'oembed': {'author_name': 'HUSTLE HUB', 'author_url': 'https://www.youtube.com/c/HUSTLEHUB-CHANNEL', 'height': 200, 'html': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/9KSm_Zd37hE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/9KSm_Zd37hE/hqdefault.jpg', 'thumbnail_width': 480, 'title': 'Ex Boston Consulting Group &amp; Google Manager reveals Hiring Processes, Required Skills &amp; Experiences', 'type': 'video', 'version': '1.0', 'width': 267}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/9KSm_Zd37hE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/rkz7b7', 'scrolling': False, 'width': 267}",105.0,140.0,https://youtu.be/9KSm_Zd37hE,,,,,,,,,,
[],False,dfragnito,,,[],,,,text,t2_1s8y,False,False,False,[],False,False,1640033176,schemafreesql.com,https://www.reddit.com/r/dataengineering/comments/rkwqin/a_schemaless_data_store_within_your_sql_database/,{},rkwqin,False,True,False,False,False,True,False,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rkwqin/a_schemaless_data_store_within_your_sql_database/,False,,,6,1640033187,1,,True,False,False,dataengineering,t5_36en4,47514,public,default,A Schemaless Data Store within YOUR SQL Database,0,[],1.0,https://schemafreesql.com/,all_ads,6,,,,,,,,https://schemafreesql.com/,,,,,,,,,,
[],False,BadGuyBadGuy,,,[],,,,text,t2_535yg,False,False,False,[],False,False,1640023877,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rktdy4/have_you_ever_had_a_predecessor_who_impressed_you/,{},rktdy4,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rktdy4/have_you_ever_had_a_predecessor_who_impressed_you/,False,,,6,1640023888,1,"I'm asking because it seems common to hear the story where a guy takes a solo gig and has to fix a mess.  A fire.

Wondering if anyone here has taken over a role and actually been impressed by a system the previous person built.

If so, what did your predecessor do that you appreciated?",True,False,False,dataengineering,t5_36en4,47509,public,self,"Have you ever had a predecessor who impressed you, or at least did a decent job in your eyes?",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rktdy4/have_you_ever_had_a_predecessor_who_impressed_you/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,bindalert,,,[],,,,text,t2_1ju7sm8v,False,False,False,[],False,False,1640018781,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rkrhz0/looking_for_career_advice_data_engineer_to/,{},rkrhz0,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rkrhz0/looking_for_career_advice_data_engineer_to/,False,,,6,1640018792,1,"I’m currently a data engineer at a bank in Australia. Got 5yrs of experience and primarily work on a mssql stack, with the end goal of producing automated reports (systems performance data set, not related to the business). 5 years exp includes 2 years of working as a data analyst and 3 as a data engineer, which comprised of quite a bit of managing a migration project. 

I try to learn and implement different things out of my own interest, for eg - implement python scripts instead of SSIS for ETL, tableau for reporting, etc. I recently realised that there is not much opportunity for growth, technical or otherwise, in my current role. 

Have been eyeing an financial insight analyst role, and wondering if that is a good career move from the level of data engineering I’m at? Or should I be looking at  other data engineering roles which utilise spark, python, etc.",True,False,False,dataengineering,t5_36en4,47506,public,self,Looking for career advice - data engineer to finance insight analyst?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rkrhz0/looking_for_career_advice_data_engineer_to/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,forgot_to_floss,,,[],,,,text,t2_8uavj69z,False,False,False,[],False,False,1640018135,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rkr9tv/looking_for_career_advice/,{},rkr9tv,False,False,False,False,False,False,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rkr9tv/looking_for_career_advice/,False,,,6,1640018146,1,[removed],True,False,False,dataengineering,t5_36en4,47506,public,self,Looking for career advice,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rkr9tv/looking_for_career_advice/,all_ads,6,,,automod_filtered,,,,,,,,,,,,,,,
[],False,mannu_11,,,[],,,,text,t2_3dsjwu75,False,False,False,[],False,False,1640016825,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rkqsey/secret_society/,{},rkqsey,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rkqsey/secret_society/,False,,,6,1640016835,1,What if there is a secret society for Data engineers which can allow its members to share how their companies are tackling their data problems which can help other members to use those approaches in their careers! Will you join and trust other members with confidential information?,True,False,False,dataengineering,t5_36en4,47506,public,self,— Secret Society —,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rkqsey/secret_society/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Minimum-Membership-8,,,[],,,,text,t2_a0qsnkph,False,False,False,[],False,False,1640010474,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rkohmi/thoughts_on_talend/,{},rkohmi,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rkohmi/thoughts_on_talend/,False,,,6,1640010485,1,"Any Talend users that can comment on the capabilities around data governance, dictionary, ETL, etc? Does it work well with large datasets? Does it support CDC? How much does it cost?",True,False,False,dataengineering,t5_36en4,47495,public,self,Thoughts on Talend,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rkohmi/thoughts_on_talend/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,jakebuilds,,,[],,,,text,t2_4h9exy7k,False,False,False,[],False,False,1640009755,bostata.com,https://www.reddit.com/r/dataengineering/comments/rko8rs/staying_fresh_with_freshness_tables/,{},rko8rs,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rko8rs/staying_fresh_with_freshness_tables/,False,link,"{'enabled': False, 'images': [{'id': 'dqAyWe9AvRtCSJ6stHO8qQujUHYokKPFxjlA8WAf0wI', 'resolutions': [{'height': 81, 'url': 'https://external-preview.redd.it/WsXiNuv6VmRiN8jxiCxhMaJUehtcPOUVrOX7CeN4gSw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a47227e56f6ed9102a0f7b43ff624ed8fe8f9020', 'width': 108}, {'height': 163, 'url': 'https://external-preview.redd.it/WsXiNuv6VmRiN8jxiCxhMaJUehtcPOUVrOX7CeN4gSw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=312a432eb596a400d479c2063aca69d936a2e17a', 'width': 216}, {'height': 241, 'url': 'https://external-preview.redd.it/WsXiNuv6VmRiN8jxiCxhMaJUehtcPOUVrOX7CeN4gSw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=188a14230329e1b35834e42b579e942ef9d0577f', 'width': 320}, {'height': 483, 'url': 'https://external-preview.redd.it/WsXiNuv6VmRiN8jxiCxhMaJUehtcPOUVrOX7CeN4gSw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4e874879c841cec9ed4b20ed917dedfdbdbfddac', 'width': 640}, {'height': 725, 'url': 'https://external-preview.redd.it/WsXiNuv6VmRiN8jxiCxhMaJUehtcPOUVrOX7CeN4gSw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4eae44281f122013ea1d6497fbc1fd74bc8e7d36', 'width': 960}, {'height': 816, 'url': 'https://external-preview.redd.it/WsXiNuv6VmRiN8jxiCxhMaJUehtcPOUVrOX7CeN4gSw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4e72608b23a044b40379793db500a3442b5d0dc9', 'width': 1080}], 'source': {'height': 1042, 'url': 'https://external-preview.redd.it/WsXiNuv6VmRiN8jxiCxhMaJUehtcPOUVrOX7CeN4gSw.jpg?auto=webp&amp;s=0512acd69234e1719f716bfc3df2e8e1a6f2158e', 'width': 1378}, 'variants': {}}]}",6,1640009766,1,,False,False,False,dataengineering,t5_36en4,47496,public,https://b.thumbs.redditmedia.com/WidhgvjSeRfFjcFGxEwr3dWoabbG-kZu60otNMLcJbc.jpg,Staying Fresh with Freshness Tables,0,[],1.0,https://www.bostata.com/staying-fresh-with-freshness-tables,all_ads,6,,,,,,105.0,140.0,https://www.bostata.com/staying-fresh-with-freshness-tables,,,,,,,,,,
[],False,Cool_Telephone,,,[],,,,text,t2_87zfi59a,False,False,False,[],False,False,1640001642,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rklqy0/joining_two_snapshot_tables/,{},rklqy0,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rklqy0/joining_two_snapshot_tables/,False,,,6,1640001653,1,"Hi,

Hoping for some guidance as this has me puzzled. We have two (or more) tables that need to be joined. There is a primary key. Easy enough. However both or snapshots (dbt snapshot if it helps) with valid_from and valid_to timestamps. They update at different frequencies.

How do I join these into one wider table making a unified snapshot table taking into account the valid dates from each of the tables?

I hope that makes sense? Dbt guides say to snapshot first, rather than downstream at the dimension table level. Happy to go with that, but they fail to mention *how* to actually do it.",True,False,False,dataengineering,t5_36en4,47491,public,self,Joining two snapshot tables,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rklqy0/joining_two_snapshot_tables/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Chiefjack98,,,[],,,,text,t2_acc1u0w7,False,False,False,[],False,False,1639997385,tyny.dev,https://www.reddit.com/r/dataengineering/comments/rkkm52/tynydev_tracking_ui_why_tynydev_has_the_best/,{},rkkm52,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rkkm52/tynydev_tracking_ui_why_tynydev_has_the_best/,False,link,"{'enabled': False, 'images': [{'id': '5WjbgCO9D5a78bz8nA7AFMp6gjJFIatyCThndSoklB8', 'resolutions': [{'height': 72, 'url': 'https://external-preview.redd.it/GTL8knJD7WumRREXRKepQplHhMQLUFtxH4X0LJPIfNI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a1156675aa4606b16a4d28916954c2a986cfd1f2', 'width': 108}, {'height': 144, 'url': 'https://external-preview.redd.it/GTL8knJD7WumRREXRKepQplHhMQLUFtxH4X0LJPIfNI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f912670c7c1368dff986f086a5807d9116b50fa8', 'width': 216}, {'height': 213, 'url': 'https://external-preview.redd.it/GTL8knJD7WumRREXRKepQplHhMQLUFtxH4X0LJPIfNI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ce10866bf57222d2ebc5116bb846ea7c3486327b', 'width': 320}, {'height': 426, 'url': 'https://external-preview.redd.it/GTL8knJD7WumRREXRKepQplHhMQLUFtxH4X0LJPIfNI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=dd9d2a01d22bc0d55b442186949bf99d409cb67f', 'width': 640}, {'height': 640, 'url': 'https://external-preview.redd.it/GTL8knJD7WumRREXRKepQplHhMQLUFtxH4X0LJPIfNI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a3342a87d9e479cc6f20698386595a4b5a8fc3da', 'width': 960}, {'height': 720, 'url': 'https://external-preview.redd.it/GTL8knJD7WumRREXRKepQplHhMQLUFtxH4X0LJPIfNI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dc048e525be7f5bd5ea1337352b2b72665db715e', 'width': 1080}], 'source': {'height': 736, 'url': 'https://external-preview.redd.it/GTL8knJD7WumRREXRKepQplHhMQLUFtxH4X0LJPIfNI.jpg?auto=webp&amp;s=eff06a0631c9e825e19ee12cc3aac77cde8424f0', 'width': 1104}, 'variants': {}}]}",6,1639997395,1,,True,False,False,dataengineering,t5_36en4,47488,public,https://b.thumbs.redditmedia.com/DBr7N3jobxh7JlVWIoMomRb40OWiJuIbefX5hzu3XUY.jpg,tyny.dev | Tracking UI: Why tyny.dev Has The Best Command of UI On The Market | Blog,0,[],1.0,https://tyny.dev/blog/tracking-ui-why-tyny-dev-has-the-best-command-of-ui-on-the-market,all_ads,6,,,,,,93.0,140.0,https://tyny.dev/blog/tracking-ui-why-tyny-dev-has-the-best-command-of-ui-on-the-market,,,,,,,,,,
[],False,randomusicjunkie,,,[],,,,text,t2_3tzpeuhd,False,False,False,[],False,False,1639996758,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rkkgpr/what_is_the_best_way_to_practice_transformations/,{},rkkgpr,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rkkgpr/what_is_the_best_way_to_practice_transformations/,False,,,6,1639996768,1,"Hi,

I am looking for some practice material/practice projects and the likes to practice transformations (eTl) with Spark, SQL, and Pandas. I am looking for something to practice the data engineering/analytics side of things. How did you practice transformations?",True,False,False,dataengineering,t5_36en4,47488,public,self,What is the best way to practice transformations?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rkkgpr/what_is_the_best_way_to_practice_transformations/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,zer0crash,,,[],,,,text,t2_9p5e14ud,False,False,False,[],False,False,1639984654,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rkhevl/what_did_you_guys_wish_you_knew_before/,{},rkhevl,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rkhevl/what_did_you_guys_wish_you_knew_before/,False,,,6,1639984665,1,So I'm in charge of the dev/dataengineering/devops for our startup and it's daunting how many things need to be done. What did you wish you knew before building data pipelines and choosing datawarehouses/data lakes/delta lakes etc. Thanks guys.,True,False,False,dataengineering,t5_36en4,47480,public,self,What did you guys wish you knew before implementing everything for your company?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rkhevl/what_did_you_guys_wish_you_knew_before/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,dawarravi,,,[],,,,text,t2_3i3gcd05,False,False,False,[],False,False,1639975136,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rkesjx/airflow_orchestration_only_or_transformation_as/,{},rkesjx,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rkesjx/airflow_orchestration_only_or_transformation_as/,False,,,6,1639975147,1,"Hey peeps, reading a lot about how cool Airflow is to manage your pipeline dependencies and scheduling. 

Can someone please clarify if Airflow can be used as a full-fledged transformation tool as well (the T in ELT)? E.g. if my flow is Postgres - S3 - Snowflake, how best can I use Airflow?",True,False,False,dataengineering,t5_36en4,47473,public,self,Airflow - orchestration only or transformation as well?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rkesjx/airflow_orchestration_only_or_transformation_as/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,infl1ct1on,,,[],,,,text,t2_5g4le,False,False,False,[],False,False,1639967440,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rkcg4s/epidemiologist_seeking_advice_on_where/,{},rkcg4s,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rkcg4s/epidemiologist_seeking_advice_on_where/,False,,,6,1639967450,1,"As a disclaimer, I am in a hybrid role where I am an epidemiologist, programmer, and work on several projects that I would describe as data engineering. I am reaching out because I think there is a lot of room for potential process improvement or technologies our team should be taking advantage of, but I would like your advice on which to prioritize.

Our company runs clinical trials and the current workflow requires the generation of dozens of sas7bdat datasets for each study that are required to be sent to the FDA. So let's say we have 100 ongoing trials. We may notice that there are a handful of common lab parameters of interest between these studies, and we want to build one application that can visualize these lab parameters for these studies, then scale up so future studies and teams can also take advantage of this application and reduce the duplication of efforts.

Our company primarily uses SAS and R, but we have the freedom to use python too. For the project as described above, we will work with study teams to develop a common data model that can be read in our visualization app (typically developed in RShiny). We develop a program which can read in all the SAS datasets for a particular study, and transform the input data into the common data model we agreed upon. This updated dataset can automatically be read by the visualization app. However, one major bottle neck is the process from SAS dataset to common data model is not automated. We have an R program that can transform the data quickly, but the study teams email us when their source data is refreshed and we manually go and run the program when we have time. Can you please recommend something to explore here, or a workflow we should consider?

Due to the current state of the business, for the foreseeable future we will always have sas7bdat datasets as the source data. Our job will be to transform this for various purposes, develop common data models, and overall develop pipelines from source data from different sources to either a dataset or app (I provided an easy example, we have much more complex work which involves pulling data from electronic health records of insurance claims data that is raw/messy). We will then either send this transformed data to data sciences teams for analysis, or we will develop a dashboard/app to visualize the information ourselves. I think our team is behind the curve and our processes are overly time consuming and crude. If you have any suggestions whatsoever please let me know! Thank you!",True,False,False,dataengineering,t5_36en4,47470,public,self,Epidemiologist seeking advice on where improvements can be made in workflow,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rkcg4s/epidemiologist_seeking_advice_on_where/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Mr_BeardedOne,,,[],,,,text,t2_hegif73x,False,False,False,[],False,False,1639962249,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rkaqu2/is_anyone_willing_to_have_a_conversation_or_help/,{},rkaqu2,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rkaqu2/is_anyone_willing_to_have_a_conversation_or_help/,False,,,6,1639962260,1,I work as an IT support tech and I want to make a transition. I have the basics of SQL and Python down. Would greatly appreciate it!,True,False,False,dataengineering,t5_36en4,47466,public,self,Is anyone willing to have a conversation or help me get started on a path to become a data engineer?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rkaqu2/is_anyone_willing_to_have_a_conversation_or_help/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,roohitavaf,,,[],,,,text,t2_2rtceaie,False,False,False,[],False,False,1639959297,mydistributed.systems,https://www.reddit.com/r/dataengineering/comments/rk9qdk/seven_reading_suggestions_for_the_holidays_on/,{},rk9qdk,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rk9qdk/seven_reading_suggestions_for_the_holidays_on/,False,link,"{'enabled': False, 'images': [{'id': 'eRT1HjX950vUOnwyDUGD2RL22nvkiUupkG3fAjcDszI', 'resolutions': [{'height': 73, 'url': 'https://external-preview.redd.it/cw5a1gSOIaNH9X2Ifkc1nLzoPi1Ol_ieowgaYdGlt_g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bf7e326d0eaf2a69e47cd6bf531d07cd98281627', 'width': 108}, {'height': 147, 'url': 'https://external-preview.redd.it/cw5a1gSOIaNH9X2Ifkc1nLzoPi1Ol_ieowgaYdGlt_g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=40d55904d694b6d58da91430f942a859bd633d44', 'width': 216}, {'height': 218, 'url': 'https://external-preview.redd.it/cw5a1gSOIaNH9X2Ifkc1nLzoPi1Ol_ieowgaYdGlt_g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=dbebecb0afde7768234323f5b4e407e6a1ee7433', 'width': 320}, {'height': 436, 'url': 'https://external-preview.redd.it/cw5a1gSOIaNH9X2Ifkc1nLzoPi1Ol_ieowgaYdGlt_g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=08e1d3ac33aaa7ffdf916bba169f92db0202a060', 'width': 640}], 'source': {'height': 548, 'url': 'https://external-preview.redd.it/cw5a1gSOIaNH9X2Ifkc1nLzoPi1Ol_ieowgaYdGlt_g.jpg?auto=webp&amp;s=5d9f980befaf088e08cb9f7cc9869dd374a4fb05', 'width': 803}, 'variants': {}}]}",6,1639959307,1,,True,False,False,dataengineering,t5_36en4,47465,public,https://b.thumbs.redditmedia.com/lrnMJyg7Xod0KPOpWjygT49Vu-mPjFXDz6SJIu9wVsU.jpg,Seven Reading Suggestions for the Holidays (on Distributed Systems/Databases),0,[],1.0,https://www.mydistributed.systems/2021/12/holiday-reading-list-2021.html,all_ads,6,,,,,,95.0,140.0,https://www.mydistributed.systems/2021/12/holiday-reading-list-2021.html,,,,,,,,,,
[],False,twopairisgood,,,[],,,,text,t2_d50wl,False,False,False,[],False,False,1639954635,medium.com,https://www.reddit.com/r/dataengineering/comments/rk84l6/the_guide_to_data_versioning/,{},rk84l6,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rk84l6/the_guide_to_data_versioning/,False,link,"{'enabled': False, 'images': [{'id': 'EuKCS1NewqFZ6h8MazKTxG3gY8NnUAO1TfkaryrOmK0', 'resolutions': [{'height': 60, 'url': 'https://external-preview.redd.it/h78mH9dTY5BB9BSMNYe2-u9K437wgFqgGhzqNLQFodQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=efa0fef965338e33c519339e51cadc2e3c646b95', 'width': 108}, {'height': 121, 'url': 'https://external-preview.redd.it/h78mH9dTY5BB9BSMNYe2-u9K437wgFqgGhzqNLQFodQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=43e000e945f7dec3c7925d7a5d903130f02adb13', 'width': 216}, {'height': 180, 'url': 'https://external-preview.redd.it/h78mH9dTY5BB9BSMNYe2-u9K437wgFqgGhzqNLQFodQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=97b6e352244dfd33d66949a61ccd9da3be4896dd', 'width': 320}, {'height': 360, 'url': 'https://external-preview.redd.it/h78mH9dTY5BB9BSMNYe2-u9K437wgFqgGhzqNLQFodQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b01d6d18a8b5e6a25cd1cb1795f5dc74e8a75c81', 'width': 640}, {'height': 540, 'url': 'https://external-preview.redd.it/h78mH9dTY5BB9BSMNYe2-u9K437wgFqgGhzqNLQFodQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=847ca39151eb2e13de44bc24805a408c2b5c1a36', 'width': 960}, {'height': 607, 'url': 'https://external-preview.redd.it/h78mH9dTY5BB9BSMNYe2-u9K437wgFqgGhzqNLQFodQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=59d820fcdd3276f30e1b1a9d5a6562cb83911b69', 'width': 1080}], 'source': {'height': 675, 'url': 'https://external-preview.redd.it/h78mH9dTY5BB9BSMNYe2-u9K437wgFqgGhzqNLQFodQ.jpg?auto=webp&amp;s=71b3f10f3959687a975dc99fd30fed12ca9868d1', 'width': 1200}, 'variants': {}}]}",6,1639954645,1,,True,False,False,dataengineering,t5_36en4,47464,public,https://a.thumbs.redditmedia.com/StlLRVxa0G3LbxCIDieHTx-KKX73PVgwZDLBgl4xzW0.jpg,The Guide to Data Versioning,0,[],1.0,https://medium.com/whispering-data/the-guide-to-data-versioning-d4315a146456,all_ads,6,,,,,,78.0,140.0,https://medium.com/whispering-data/the-guide-to-data-versioning-d4315a146456,,,,,,,,,,
[],False,diegolujan1,,,[],,,,text,t2_2tclli02,False,False,False,[],False,False,1639954269,youtu.be,https://www.reddit.com/r/dataengineering/comments/rk8050/homomorphic_encryption_is_a_cryptographic_method/,{},rk8050,False,True,False,False,False,True,False,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rk8050/homomorphic_encryption_is_a_cryptographic_method/,False,rich:video,"{'enabled': False, 'images': [{'id': 'sTMZfHpyC55xqCrsTCA09Z-Dou7bc9PKYLbhekwH9X0', 'resolutions': [{'height': 81, 'url': 'https://external-preview.redd.it/YaDCHjNheEPPzX95kWwwiL3A-ES9IGxtXvnNlJk_Qgc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2f29712014bba43d3c699543a374843d7b8323d7', 'width': 108}, {'height': 162, 'url': 'https://external-preview.redd.it/YaDCHjNheEPPzX95kWwwiL3A-ES9IGxtXvnNlJk_Qgc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ccc8b796935d1d655d63323020f54a3c644825ea', 'width': 216}, {'height': 240, 'url': 'https://external-preview.redd.it/YaDCHjNheEPPzX95kWwwiL3A-ES9IGxtXvnNlJk_Qgc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=22941164f23069ae205af5c222e93c5091486d9a', 'width': 320}], 'source': {'height': 360, 'url': 'https://external-preview.redd.it/YaDCHjNheEPPzX95kWwwiL3A-ES9IGxtXvnNlJk_Qgc.jpg?auto=webp&amp;s=58bd3e4138e62559ec2d0e03464d149319885531', 'width': 480}, 'variants': {}}]}",6,1639954279,1,,True,False,False,dataengineering,t5_36en4,47464,public,https://a.thumbs.redditmedia.com/64mA2gK3TwaAPxbRlKmSf1EtE0eqZBlsmQSPkNQpKu8.jpg,"Homomorphic encryption is a cryptographic method that returns an encrypted result to the data owner. Essentially, this enables third parties to process encrypted data while having no knowledge about the data or the results.",0,[],1.0,https://youtu.be/QEDf_US04aA,all_ads,6,"{'oembed': {'author_name': 'Toy_Virtual_Structures', 'author_url': 'https://www.youtube.com/channel/UCr5Akn6LhGDin7coWM7dfUg', 'height': 200, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/QEDf_US04aA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/QEDf_US04aA/hqdefault.jpg', 'thumbnail_width': 480, 'title': 'Homomorphic Encryption Explained', 'type': 'video', 'version': '1.0', 'width': 356}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/QEDf_US04aA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'scrolling': False, 'width': 356}",,"{'oembed': {'author_name': 'Toy_Virtual_Structures', 'author_url': 'https://www.youtube.com/channel/UCr5Akn6LhGDin7coWM7dfUg', 'height': 200, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/QEDf_US04aA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/QEDf_US04aA/hqdefault.jpg', 'thumbnail_width': 480, 'title': 'Homomorphic Encryption Explained', 'type': 'video', 'version': '1.0', 'width': 356}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/QEDf_US04aA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/rk8050', 'scrolling': False, 'width': 356}",105.0,140.0,https://youtu.be/QEDf_US04aA,,,,,,,,,,
[],False,jamestop00,,,[],,,,text,t2_d2xngvz8,False,False,False,[],False,False,1639938669,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rk2m56/branching_into_data_centre_engineering/,{},rk2m56,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rk2m56/branching_into_data_centre_engineering/,False,,,6,1639938680,1,"This might be a silly post but I'm looking into trying for a career in data centre engineering, preferably through an apprenticeship or on-the-job training as I learn best hands on. Does anybody have any company or course recommendations? I'm coming from a background of no formal degree and no formal experience in tech beyond a tech support call centre job I held for a few months in 2017 while I was at technical school (I didn't end up following through with the cert so pretty useless lol).

If not, any advice on getting into the field helps! Thanks in advance :)",True,False,False,dataengineering,t5_36en4,47458,public,self,Branching into Data Centre Engineering?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rk2m56/branching_into_data_centre_engineering/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,DigAggressive2982,,,[],,,,text,t2_5p9fb9t6,False,False,False,[],False,False,1639936587,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rk1wzn/quantum_computers_in_data_engineering/,{},rk1wzn,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rk1wzn/quantum_computers_in_data_engineering/,False,,,6,1639936597,1,"Hi fellow data engineers 😀

Do you see opportunities to learn some new quantum computing technology/language that would be relevant for data engineering? 

This technology could be in it's infancy. My vision would be slowly build up a portfolio on the intersection of quantum tech/data engineering and be prepared for a potential boom that could happen in maybe 5 years.",True,False,False,dataengineering,t5_36en4,47454,public,self,Quantum computers in data engineering,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rk1wzn/quantum_computers_in_data_engineering/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,tw3akercc,,,[],,,,text,t2_85vbh,False,False,False,[],False,False,1639927123,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rjyrsc/data_warehousing_course_recommendation_needed/,{},rjyrsc,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rjyrsc/data_warehousing_course_recommendation_needed/,False,,,6,1639927133,1,Does anyone know of an online course that teaches all the steps of building a data warehouse using real examples? I've taken one on Udemy that gives a high level overview of the concepts but would like to find one that actually makes the students build one on their own.,True,False,False,dataengineering,t5_36en4,47449,public,self,Data warehousing course recommendation needed!,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rjyrsc/data_warehousing_course_recommendation_needed/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,RobotsMakingDubstep,,,[],,,,text,t2_462d4yf3,False,False,False,[],False,False,1639926307,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rjyikf/poll_de_vs_mle_careers/,{},rjyikf,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rjyikf/poll_de_vs_mle_careers/,False,,,6,1639926317,1,"Being a Data Engineer in a big team comprising of Data Engineers, Machine Learning Engineers &amp; Data Scientists, I can't help but wonder which profile will actually prevail in 5-7 years since all these profiles require similar if not same skillset.
So, if you were to choose one of these, which one do you think will stay in demand
A Data Engineer with complimentary MLE skills
Or
A Machine Learning Engineer with Data Engineering complimentary skills?",True,False,False,dataengineering,t5_36en4,47449,public,self,Poll: DE vs MLE careers,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rjyikf/poll_de_vs_mle_careers/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Jboynt13,,,[],,,,text,t2_5bhq08mz,False,False,False,[],False,False,1639915483,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rjvg96/just_got_onto_a_data_and_analytics_graduate/,{},rjvg96,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rjvg96/just_got_onto_a_data_and_analytics_graduate/,False,,,6,1639915493,1,"So I’ve just secured a grad job at JLR however I come from a background in Chemistry so don’t have much knowledge in data engineering which is a key part off the scheme. Obviously I’m not expected to have lots of prior knowledge and I’ll learn along the way, but I’d love to get a bit ahead before starting. If anyone has any good resources or tips on where to get started that would be great, there’s so much to learn so I don’t know where to start!",True,False,False,dataengineering,t5_36en4,47440,public,self,"Just got onto a data and analytics graduate scheme, any tips?",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rjvg96/just_got_onto_a_data_and_analytics_graduate/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,porcelainsmile,,,[],,,,text,t2_7txotlcm,False,False,False,[],False,False,1639912039,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rjum9q/contradicting_information_on_the_spark_catalyst/,{},rjum9q,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rjum9q/contradicting_information_on_the_spark_catalyst/,False,,,6,1639912049,1,"Hey All,

This is a question on how the Spark Catalyst optimizer works and which info to believe. 

The databricks blog and The Definitive guide say - One optimized logical plan is created and then multiple physical plans are created and then they are chosen based on the cost attached to it.

Learning Spark 2.0 says - there is one selected logical plan, which is chosen based on the costs attached by the cost-based optimizer. And then a single physical plan is created out of it.

Now I am not sure which one to consider the correct.

Thanks in advance.",True,False,False,dataengineering,t5_36en4,47438,public,self,Contradicting information on the Spark Catalyst Optimizer?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rjum9q/contradicting_information_on_the_spark_catalyst/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Martinez__,,,[],,,,text,t2_xgs1jrj,False,False,False,[],False,False,1639907801,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rjtnvz/how_should_a_good_data_processing_application_for/,{},rjtnvz,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rjtnvz/how_should_a_good_data_processing_application_for/,False,,,6,1639907811,1,How should be made?,True,False,False,dataengineering,t5_36en4,47436,public,self,How should a good data processing application for a Data Warehouse (based on Python and PySpark) be made?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rjtnvz/how_should_a_good_data_processing_application_for/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Engineeroo27,,,[],,,,text,t2_hon7e6n0,False,False,False,[],False,False,1639869931,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rjjd10/2021_eoy_engineering_job_survey/,{},rjjd10,False,False,False,False,False,False,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rjjd10/2021_eoy_engineering_job_survey/,False,self,"{'enabled': False, 'images': [{'id': 'qQWtCcRU6xrWc0gwCMeRUTtDNFEWZEP0VdzOLwMaThI', 'resolutions': [{'height': 56, 'url': 'https://external-preview.redd.it/T_MdiC8pPp9TuqIM1MDhLZ-JPAIlf-W4Zx1qbRctQT8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=157fc1ecb67cb81b02eecb4c932a89e41ff5765e', 'width': 108}, {'height': 113, 'url': 'https://external-preview.redd.it/T_MdiC8pPp9TuqIM1MDhLZ-JPAIlf-W4Zx1qbRctQT8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9e2d1065f2f89e4a0e045f414795e27f7e250963', 'width': 216}, {'height': 168, 'url': 'https://external-preview.redd.it/T_MdiC8pPp9TuqIM1MDhLZ-JPAIlf-W4Zx1qbRctQT8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=aef9d0330f6360b4094d2989b59412ec162453a3', 'width': 320}, {'height': 336, 'url': 'https://external-preview.redd.it/T_MdiC8pPp9TuqIM1MDhLZ-JPAIlf-W4Zx1qbRctQT8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4130c294c2c3d9678f834ca4a30f4cc5c9839d4b', 'width': 640}, {'height': 504, 'url': 'https://external-preview.redd.it/T_MdiC8pPp9TuqIM1MDhLZ-JPAIlf-W4Zx1qbRctQT8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=86855f16c3671eb6bfa9ee21cf7dfba94bf3f0b1', 'width': 960}, {'height': 567, 'url': 'https://external-preview.redd.it/T_MdiC8pPp9TuqIM1MDhLZ-JPAIlf-W4Zx1qbRctQT8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4cca781a8db4f7c20423580fe145dc07a2082716', 'width': 1080}], 'source': {'height': 630, 'url': 'https://external-preview.redd.it/T_MdiC8pPp9TuqIM1MDhLZ-JPAIlf-W4Zx1qbRctQT8.jpg?auto=webp&amp;s=470139b810768d321f631c1bba162614819ab92d', 'width': 1200}, 'variants': {}}]}",6,1639869942,1,[removed],True,False,False,dataengineering,t5_36en4,47418,public,self,2021 EOY Engineering Job Survey,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rjjd10/2021_eoy_engineering_job_survey/,all_ads,6,,,automod_filtered,,,,,,,,,,,,,,,
[],False,_BearHawk,,,[],,,,text,t2_ekkeg,False,False,False,[],False,False,1639868781,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rjizpx/will_learning_informatica_hurt_me_greatly/,{},rjizpx,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,False,1,0,False,all_ads,/r/dataengineering/comments/rjizpx/will_learning_informatica_hurt_me_greatly/,False,,,6,1639868791,1,"Currently in my first job out of college in a de/swe role. I’m at a large private university in my state, in my department team there are 6 developers and 1 data architect who I work with. We mainly do in-house work for other teams in the department, helping build pipelines for data warehouses from whatever data sources they have.

We do everything ETL related in Informatica. Sometimes when we work with APIs we use python mainly as a scripting language to format the API responses, but that’s it.

I’ve been here about 2 months and really enjoyed it, I just hear a lot about Airflow and writing pipelines in Python I feel like I might be stunting my growth?

The good news is that the entire data engineering/architecture team is me and 1 other guy who has been here about 2 years. I’m wondering if it might be feasible to switch or something down the line? Not sure.

Any advice is great.",True,False,False,dataengineering,t5_36en4,47415,public,self,Will learning Informatica hurt me greatly?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rjizpx/will_learning_informatica_hurt_me_greatly/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,dataengineerdude,,,[],,,,text,t2_4wrevs7q,False,False,False,[],False,False,1639867941,confessionsofadataguy.com,https://www.reddit.com/r/dataengineering/comments/rjiq0z/performance_testing_postgres_inserts_with_python/,{},rjiq0z,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rjiq0z/performance_testing_postgres_inserts_with_python/,False,link,"{'enabled': False, 'images': [{'id': 'LfOU-nVg4E99sw9nN1zRWlRCAC9cwCLXQdw7sKJx2OI', 'resolutions': [{'height': 67, 'url': 'https://external-preview.redd.it/kJGPRErvwZAPCHZlmMa5y_wwegjpBnJJBgt4oVeA4kY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4e4ba3bd7cacb259cf8857e66d32cdc9fb6f59f1', 'width': 108}, {'height': 135, 'url': 'https://external-preview.redd.it/kJGPRErvwZAPCHZlmMa5y_wwegjpBnJJBgt4oVeA4kY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c059b86a85e92a3d07c0cedeeef9501b01651c42', 'width': 216}, {'height': 201, 'url': 'https://external-preview.redd.it/kJGPRErvwZAPCHZlmMa5y_wwegjpBnJJBgt4oVeA4kY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bab0fa488a8fec70510523f3a6bd14cd0131275a', 'width': 320}, {'height': 402, 'url': 'https://external-preview.redd.it/kJGPRErvwZAPCHZlmMa5y_wwegjpBnJJBgt4oVeA4kY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5820e9fb4d659e0068ef3802f8a6e975e8985fd6', 'width': 640}, {'height': 603, 'url': 'https://external-preview.redd.it/kJGPRErvwZAPCHZlmMa5y_wwegjpBnJJBgt4oVeA4kY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0894615103f9d2b94fac9890b68eb72b4056edce', 'width': 960}], 'source': {'height': 648, 'url': 'https://external-preview.redd.it/kJGPRErvwZAPCHZlmMa5y_wwegjpBnJJBgt4oVeA4kY.jpg?auto=webp&amp;s=5275421ebddc991ae74012202760da01640569b4', 'width': 1030}, 'variants': {}}]}",6,1639867951,1,,True,False,False,dataengineering,t5_36en4,47414,public,https://b.thumbs.redditmedia.com/aS3fyifs-lSeVQnzwt5TKaGCKQd4XVsqhbMbvcT2WGg.jpg,Performance Testing Postgres Inserts with Python,0,[],1.0,https://www.confessionsofadataguy.com/performance-testing-postgres-inserts-with-python/,all_ads,6,,,,,,88.0,140.0,https://www.confessionsofadataguy.com/performance-testing-postgres-inserts-with-python/,,,,,,,,,,
[],False,BoiElroy,,,[],,,,text,t2_r8dyi,False,False,True,[],False,False,1639864977,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rjhqz9/yes_another_airflow_managed_vs_prefect_cloud_post/,{},rjhqz9,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rjhqz9/yes_another_airflow_managed_vs_prefect_cloud_post/,False,,,6,1639864988,1,"Used a small airflow deployment to orchestrate pipelines for a small subset of data science projects that had relatively simple DAG structures, i.e. no data needed to be passed from one task to another.

Building out new pipelines now at a different company, I'm torn because Airflow is by far the most popular and has way more native integrations, and way easier to troubleshoot by virtue of their just being a larger community that has already had similar issues. My choice is between managed Airflow like Astronomer, or Prefect Cloud.

Prefect, at least from skimming the docs, seems like its less intrusive. Airflow does sometimes feel like writing ""airflow code"", whereas prefect seems like simply adding some decorators and largely it stays out of the way.

Anyone have deep experience with both managed airflow and prefect cloud? Or anyone moving from airflow to prefect or vice versa?

Prefect sounds great, and has 80% of the integrations I need. But I don't want to pick the new and shiny thing and then see it not mature into the ecosystem well. Airflow feels like the default and is easier to hire for, but people on here in previous threads have said even at teams of 10 people it was a nightmare to manage. 

&amp;#x200B;

If you have used BOTH, or are migrating from one to the other. Please share your experiences. Or DM me. I can potentially pay a [sr.](https://sr.data) data engineer for consulting on this matter.",True,False,False,dataengineering,t5_36en4,47413,public,self,"Yes, another airflow (managed) vs prefect (cloud) post",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rjhqz9/yes_another_airflow_managed_vs_prefect_cloud_post/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,BlancBryn,,,[],,,,text,t2_1i178g26,False,False,False,[],False,False,1639854438,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rje71b/tools_or_frameworks_to_simply_trigger_python/,{},rje71b,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rje71b/tools_or_frameworks_to_simply_trigger_python/,False,,,6,1639854448,1,"Hi engineers,
I am looking for a way to trigger different python functions with a simple UI.

The background is that I have implemented an engine for the commission calculation for the employees in our company with Python. 
There are different functions like show the employee Revenue or KPI achievement, calculate payout for a single employee or for a group of employees. 

To hand over the execution part to HR I am looking for a simple way to trigger these functions from a UI / WebUI and show the results there. 

Can you help ne finding out which tools or frameworks I could use for that?

Currently the calculation takes place in a secured VM. To have Web UI on the localhost on the VM would be fine. 

Thank you!!",True,False,False,dataengineering,t5_36en4,47407,public,self,Tools or frameworks to simply trigger python functions and view data via UI,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rje71b/tools_or_frameworks_to_simply_trigger_python/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,odahat,transparent,,[],fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b,Data Engineer,dark,text,t2_5e33q9sc,False,False,False,[],False,False,1639853622,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rjdwww/why_use_segment_instead_of_just_doing_it_manually/,{},rjdwww,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rjdwww/why_use_segment_instead_of_just_doing_it_manually/,False,,,6,1639853633,1,"Hello, I am pretty junior in data engineering and I would like to use Twilio Segment within my company (early stage startup) to start collecting usage data. One of the PO told me the devs from the web team already created few functions to log some of the users actions and insert those info into the product db (unique table with timestamp, user id, and action).  For me it's not a good thing to mix usage data and data used to make the product work on the same db. 

But now I am wondering what are the reasons to use a solution like Segment instead of just creating some backend functions to log what the users are doing ? Thanks for your help !",True,False,False,dataengineering,t5_36en4,47406,public,self,Why use Segment instead of just doing it manually ?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rjdwww/why_use_segment_instead_of_just_doing_it_manually/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,TheLastKingofReddit,,,[],,,,text,t2_105nkj,False,False,False,[],False,False,1639853579,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rjdwfi/data_warehouse_normalized_vs_denormalized_fact/,{},rjdwfi,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rjdwfi/data_warehouse_normalized_vs_denormalized_fact/,False,,,6,1639853589,1,"I'm designing a data warehouse to hold survey data. It will hold various types of surveys each with different questions and answers. I'm trying to implement a conventional star schema but have some doubts regarding the fact table.

The two options are to go for what Kimball calls a measure type dimension (essentially the normalized version with a column containing the questions and another the answers) or fully denormalize the table and have a column for each question populated with respective answers. If I go for the latter approach I might eventually end up with up to 10 fact tables (was thinking one per survey type would be easier to manage in this case) each having 50-100 columns vs a single fact for the first.

The way I see it the first approach has the main advantage that it's easier to maintain and has a fixed schema even if new questions are added, while the second would be better for validating the data, connecting to olap and visualisation tools and general, and would make it a lot easier to aggregate since each measure would have its own column. Lastly, the first with be tall and dense while the second approach would be very wide and sparse (1 measure or value per row at the limit).

If you have any experience or have been burned by this in the past please share your thoughts. Which one would you go for and why?",True,False,False,dataengineering,t5_36en4,47406,public,self,Data warehouse - Normalized vs denormalized fact table (measure type dimension),0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rjdwfi/data_warehouse_normalized_vs_denormalized_fact/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,SeaworthinessFit7893,,,[],,,,text,t2_4bor0kj8,False,False,False,[],False,False,1639843443,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rjag9o/python_etl_tools_for_big_data/,{},rjag9o,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rjag9o/python_etl_tools_for_big_data/,False,,,6,1639843454,1,"From what I can infer you guys all like python. Simple, flexible, and a gentle learning curve. So what tools would you recommend to someone to use for a job as a data engineer.?",True,False,False,dataengineering,t5_36en4,47397,public,self,Python ETL tools for big data,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rjag9o/python_etl_tools_for_big_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,SeaworthinessFit7893,,,[],,,,text,t2_4bor0kj8,False,False,False,[],False,False,1639842843,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rja8s6/what_is_kubernetes_used_for_in_data_engineering/,{},rja8s6,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rja8s6/what_is_kubernetes_used_for_in_data_engineering/,False,,,6,1639842853,1,Im curious as do why people use kubernetes in this field.,True,False,False,dataengineering,t5_36en4,47397,public,self,What is Kubernetes used for in data engineering?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rja8s6/what_is_kubernetes_used_for_in_data_engineering/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Minimum-Membership-8,,,[],,,,text,t2_a0qsnkph,False,False,False,[],False,False,1639841775,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rj9w9l/data_architect_with_snowflake/,{},rj9w9l,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rj9w9l/data_architect_with_snowflake/,False,,,6,1639841785,1,How essential is a data architect in a snowflake environment with complex relationships? Does snowflake have data model deployment and management options?,True,False,False,dataengineering,t5_36en4,47397,public,self,Data architect with snowflake,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rj9w9l/data_architect_with_snowflake/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Suspicious-Use7032,,,[],,,,text,t2_ftacr407,False,False,False,[],False,False,1639840262,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rj9en6/working_as_an_etl_developer_using_informatica_and/,{},rj9en6,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rj9en6/working_as_an_etl_developer_using_informatica_and/,False,,,6,1639840272,1,"I am working as a data engineer (data integrations) for the last 1 year ,i am already familiar with data warehousing,etl,batch processing etc with fair bit of experience with SQL and python",True,False,False,dataengineering,t5_36en4,47394,public,self,"Working as an etl developer using informatica and teradata ,I want to move to big data ,can anyone help which courses should I take or any roadmap to follow ?",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rj9en6/working_as_an_etl_developer_using_informatica_and/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,barkbark23,,,[],,,,text,t2_3czda60d,False,False,False,[],False,False,1639839899,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rj9ame/mis_studentjunior_interested_in_a_data/,{},rj9ame,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rj9ame/mis_studentjunior_interested_in_a_data/,False,,,6,1639839910,1,What are the main skills I will need to land a position after graduation?,True,False,False,dataengineering,t5_36en4,47395,public,self,MIS student(junior) interested in a data engineering career.,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rj9ame/mis_studentjunior_interested_in_a_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,SpaceManSl0th,,,[],,,,text,t2_58px2aa3,False,False,False,[],False,False,1639839538,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rj95x3/resources_for_first_job/,{},rj95x3,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rj95x3/resources_for_first_job/,False,,,6,1639839548,1,"I’m starting my first salaried job in the new year, and I’d like some guidance on what resources I can use to help speed along the learning process.

For background, I’ll be the sole “data person” on a finance team at a mid-size company. Right now, the team is Excel only except for a small Python script to automate some calculations. Two Excel files arrive everyday in the morning, and from there they use those files for their financial trading. They want to build a more scalable system for their financial analysts. Their idea now is to build a database or database system in SQL Server to store those two Excel files and allow the financial analysts to query those.

I’m pretty well versed with Python, setting up small databases, and database theory (undergrad in CS, masters in data science), but I’m struggling to find a good resource that explains how the aforementioned system should be setup. I know a lot of requirements will come when I start talking to the team and the IT department, but I want some background knowledge before diving in. 

Any resources you think could be helpful for this project would be much appreciated!",True,False,False,dataengineering,t5_36en4,47394,public,self,Resources for First Job,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rj95x3/resources_for_first_job/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,_Marwan02,,,[],,,,text,t2_9iw7pa3b,False,False,False,[],False,False,1639788621,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rivc7i/am_i_a_data_engineer/,{},rivc7i,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rivc7i/am_i_a_data_engineer/,False,,,6,1639788632,1,"Hello, 

In my job I : 
- Code spark job in Scala (extract data,  transform data, load data)

- Code a back end for our application in Java/spring (app to let user query/extract data easily)

- Set up tool wich is like a Data Ware House (I juste modify conf file or sql creation table scripts when i have to add dimension/fact )

Am i a Data engineer or a software engineer ?",True,False,False,dataengineering,t5_36en4,47361,public,self,Am I a Data Engineer ?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rivc7i/am_i_a_data_engineer/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,rediturrox,,,[],,,,text,t2_44o80bg3,False,False,False,[],False,False,1639779533,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/risd17/recommendations_on_a_process_extract_data_from/,{},risd17,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/risd17/recommendations_on_a_process_extract_data_from/,False,,,6,1639779543,1,"Hi all! I am learning data engineering and I have the first interview with a technical challenge.

&amp;#x200B;

I have to extract the data from a CSV file from a URL. I have to skip the duplicate rows and add the updated date. And automate this process so that it runs every week.

&amp;#x200B;

The only restriction I have regarding programming languages ​​or software to use is to not use SQL Agent. I have a basic idea outlined in mind, I was planning to use Python for the whole process and Linux for automation.

&amp;#x200B;

Obviously, I am not looking for solutions to the problem but I was open to listening to recommendations, advice, where to look, examples of similar problems in terms of languages, libraries, frameworks, methods, and other types of considerations so that it is neat, efficient, and professional. Starting from what you tell me or mention, start investigating and research. I feel a bit lost in the generation of the logs of the process.

&amp;#x200B;

Any helpful comments or advice are welcome!",True,False,False,dataengineering,t5_36en4,47352,public,self,Recommendations on a process - Extract data from CSV file on a URL,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/risd17/recommendations_on_a_process_extract_data_from/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,IntellijentAspect,,,[],,,,text,t2_ee2f63jt,False,False,False,[],False,False,1639768855,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rioqtx/create_an_alert_on_table_change_for_postgres_in/,{},rioqtx,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rioqtx/create_an_alert_on_table_change_for_postgres_in/,False,,,6,1639768866,1,"Looking for recommendations on ways I could set up some kind of trigger function that will send me an alert when a table schema has changed in my database. I'm fairly limited in the extra tooling I can use since the database in question is hosted on Cloud SQL so as far as I know creating a function in PL/Python is out of the question. 

My ideal solution would be a trigger function that pings a PubSub topic which I can then create alerts from but really it could be as as simple as sending an email.",True,False,False,dataengineering,t5_36en4,47347,public,self,Create an alert on table change for Postgres in Cloud SQL,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rioqtx/create_an_alert_on_table_change_for_postgres_in/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Vaslo,,,[],,,,text,t2_e4wl2,False,False,False,[],False,False,1639767175,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rio5an/best_resources_to_learn_dbt_using_open_tools_like/,{},rio5an,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,1,0,False,all_ads,/r/dataengineering/comments/rio5an/best_resources_to_learn_dbt_using_open_tools_like/,False,,,6,1639767185,1,"I work for a company in a division that is not IT or Data management, but I do use SSIS and am pretty familiar with it.  I'd like to learn DBT but most training resources rely on DBT Cloud, Snowflake, etc.  I won't really have access (funds or IT approval) to those things until I can prove that DBT is somehow useful to my division.  Have some of you used tools like Postgres, Airflow, etc?  How did you learn to do this?  I'd have to stick to CLI. 

For background I have zero experience in dbt, Snowflake, Airflow, etc.  but I use SQL Server, SSIS, and Excel everyday at my job in one way or another. 

Thanks!",True,False,False,dataengineering,t5_36en4,47344,public,self,Best resources to learn dbt using open tools like Postgres and Airflow?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rio5an/best_resources_to_learn_dbt_using_open_tools_like/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,jah_reddit,,,[],,,,text,t2_b2ibr,False,False,False,[],False,False,1639765465,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rinkga/what_do_you_guys_think_about_etl_platforms_vs/,{},rinkga,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rinkga/what_do_you_guys_think_about_etl_platforms_vs/,False,,,6,1639765475,1,"Just trying to gauge general opinion on the pros and cons of using a dedicated ETL tool like Fivetran, Airbyte, etc.",True,False,False,dataengineering,t5_36en4,47341,public,self,What do you guys think about ETL platforms vs roll your own?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rinkga/what_do_you_guys_think_about_etl_platforms_vs/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,caksters,,,[],,,,text,t2_tux1p,False,False,False,[],False,False,1639764420,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rin780/do_you_unit_test_your_etl_pipelines/,{},rin780,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rin780/do_you_unit_test_your_etl_pipelines/,False,,,6,1639764431,1,"As title suggests. Do you write unit tests to sense check if units of your ETL jobs perform just as expected?

[View Poll](https://www.reddit.com/poll/rin780)",True,False,False,dataengineering,t5_36en4,47339,public,self,Do you unit test your ETL pipelines?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rin780/do_you_unit_test_your_etl_pipelines/,all_ads,6,,,,,,,,,,,,,"{'is_prediction': False, 'options': [{'id': '12551221', 'text': 'Yes'}, {'id': '12551222', 'text': 'No'}, {'id': '12551223', 'text': 'what are unit tests'}], 'prediction_status': None, 'resolved_option_id': None, 'total_stake_amount': None, 'total_vote_count': 0, 'tournament_id': None, 'user_selection': None, 'user_won_amount': None, 'vote_updates_remained': None, 'voting_end_timestamp': 1640196420247}",,,,,
[],False,tegridy_tony,,,[],,,,text,t2_ebl6mwhi,False,False,False,[],False,False,1639760161,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rilozg/analyst_to_engineer_project_and_skills_feedback/,{},rilozg,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rilozg/analyst_to_engineer_project_and_skills_feedback/,False,,,6,1639760172,1,"Currently working as a DA with 1 y/e with 1 y/e in another semi-related data job at NASA but I'm realizing I really enjoy the building and programming side of data as opposed to the analytical and business side. My current position consists of building dashboards, automating reporting, building very small scale (and crude as there aren't really quality standards on our BI team) data extraction and manipulation processes. All of this is done via R and SQL Server with rCron for scheduling.

Additional skills:
Basic Python, but I'm pretty confident I can pick it up pretty quickly with my R and Java experience.      
1 semester (just finished) of Java OOP.      
Also in grad school part-time for a Data Science MS.    

I'm looking to create a small-scale personal project to learn some basic DE skills and hopefully present it on a resume. With there being so many DE tools being thrown around, I'd like to start simple.  I'm thinking Docker + Airflow for scheduling, with Python + some sort of local SQL instance for processing and storing data.  Would this be feasible and/or enough to supplement my current resume?",True,False,False,dataengineering,t5_36en4,47336,public,self,Analyst to Engineer: project and skills feedback,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rilozg/analyst_to_engineer_project_and_skills_feedback/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Crypty,,,[],,,,text,t2_8otb0,False,False,False,[],False,False,1639759333,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rilebg/what_does_landing_some_data_or_process_mean/,{},rilebg,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rilebg/what_does_landing_some_data_or_process_mean/,False,,,6,1639759344,1,"Does that just mean it completed successfully? Never heard this jargon before and now its everywhere at work. Reminds me of how rock climbers are always saying ""I sent it""",True,False,False,dataengineering,t5_36en4,47336,public,self,"What does ""landing"" some data or process mean?",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rilebg/what_does_landing_some_data_or_process_mean/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,OkieDaddy,,,[],,,,text,t2_18spz3jk,False,False,False,[],False,False,1639753687,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rijf78/best_practices_for_header_level_measures_in_a/,{},rijf78,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rijf78/best_practices_for_header_level_measures_in_a/,False,,,6,1639753698,1,"While my case is a bit different, consider the following fact table:

&amp;#x200B;

Fact\_invoice\_lineitem:

    Invoice_key
    Product_key
    Customer_key
    Quantity_purchased
    Invoice_status
    Sale_date
    Invoice_payment_date
    Tax_amt

&amp;#x200B;

Now, with said fact table, it’s at its lowest granularity, but what is the best practice for answering invoice level questions such as:

How many invoices has customer A generated?

How many invoices does customer A have that are in status of “pending”

Something in a table report format, such as 

Cust\_id, month, invoice\_count, pending invoices

We have such a dilemma, albeit not with invoices, but our stack we're attempting to implement(PowerBI + Redshift) doesn’t seem jive very well together, unless PowerBI just ends up 100% as a presentation layer. Still haven't figured out how to get query folding to work/if reshift supports it. And, the count distincts and other filters required when building in PowerBI at the invoice header level just tank the performance to an unbelievibly bad level.   


I guess I have 2 questions. 

1. From a modeling perspective, what is the best approach here? I read Kimball, and he's all about not having a header fact table and detail fact table, but to have everything from header in detail. Yet implimentations and performance of said header level counts/measures...I just can't figure out yet how it would be performant, unless that's a known trade-off.
2. From a performance perspective, is there a way to get PowerBI to better perform when doing these types of queries? I know we can push the logic down to Redshift, in views, but then we lose some of the flexibility, and we have to manage the measures/aggregations in Redshift, rather than PowerBI(two different teams at play here. DE team\[my team\] has traditionally served data to redshift dw/data marts, and BI team builds the reports in PowerBI) Our BI team isn't very SQL Savvy, so it would require some decent learning and oversight so it doesn't turn into a plate of spaghetti in there.",True,False,False,dataengineering,t5_36en4,47330,public,self,Best practices for header level measures in a line-item fact table?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rijf78/best_practices_for_header_level_measures_in_a/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,SnowPlowOpenSource,,,[],,,,text,t2_enkpz7c3,False,False,False,[],False,False,1639749761,discourse.snowplowanalytics.com,https://www.reddit.com/r/dataengineering/comments/rii39k/snowplowweb_050_dbt_package_released_new_releases/,{},rii39k,False,True,False,False,False,True,False,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rii39k/snowplowweb_050_dbt_package_released_new_releases/,False,link,"{'enabled': False, 'images': [{'id': 'ftYYJgf5rDVH41k7NHfrg6wTD_SIxnRQFH8kuduRsjU', 'resolutions': [{'height': 108, 'url': 'https://external-preview.redd.it/muV8Q1ViFpFngP5RepJ4_o4Ln6ki02xEgflN5tJtsHg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3fdebf87dcec0fc14e9a3a7353951c17dfdd70ff', 'width': 108}, {'height': 216, 'url': 'https://external-preview.redd.it/muV8Q1ViFpFngP5RepJ4_o4Ln6ki02xEgflN5tJtsHg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c2f9d8dfa2bb1bccf7311b4d8f11022f715c2464', 'width': 216}, {'height': 320, 'url': 'https://external-preview.redd.it/muV8Q1ViFpFngP5RepJ4_o4Ln6ki02xEgflN5tJtsHg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c2d61f05cfee0af1cd79bd65db66266e2e295594', 'width': 320}], 'source': {'height': 512, 'url': 'https://external-preview.redd.it/muV8Q1ViFpFngP5RepJ4_o4Ln6ki02xEgflN5tJtsHg.jpg?auto=webp&amp;s=64158827ed510635bbdddc3a9335a690003a2d39', 'width': 512}, 'variants': {}}]}",6,1639749771,1,,True,False,False,dataengineering,t5_36en4,47325,public,https://b.thumbs.redditmedia.com/MQPSiDu3_n0mcrri4uuETwpRsIJJg2Dq24jm7-r6QUE.jpg,Snowplow-web 0.5.0 dbt package released - New releases - Discourse,0,[],1.0,https://discourse.snowplowanalytics.com/t/snowplow-web-0-5-0-dbt-package-released/6115,all_ads,6,,,,,,140.0,140.0,https://discourse.snowplowanalytics.com/t/snowplow-web-0-5-0-dbt-package-released/6115,,,,,,,,,,
[],False,gabbom_XCII,,,[],,,,text,t2_5fmit0v9,False,False,False,[],False,False,1639747377,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rihcgt/data_ingestion_api_responses_to_hivehdfs/,{},rihcgt,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rihcgt/data_ingestion_api_responses_to_hivehdfs/,False,,,6,1639747387,1,"So, i’m trying to ingest a fuck-ton of data (API responses) in our on-premisses Hive instance.

This need to be done in a daily manner. Every day I will be ingesting yesterday’s data.

The data is originally in nested json format, but my client need to use it in tables format.

Is it easier to parse the jsons to columns before loading to Hive or in post ingestion processing?

And by parsing into columns it should be denormalized or normalized? Which works best?",True,False,False,dataengineering,t5_36en4,47324,public,self,Data Ingestion API responses to Hive(hdfs),0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rihcgt/data_ingestion_api_responses_to_hivehdfs/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,morpho4444,,,[],,,,text,t2_5qg3y,False,False,False,[],False,False,1639746713,youtube.com,https://www.reddit.com/r/dataengineering/comments/rih5bd/this_isnt_a_new_subject_but_what_do_you_think/,{},rih5bd,False,True,False,False,False,True,False,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rih5bd/this_isnt_a_new_subject_but_what_do_you_think/,False,rich:video,"{'enabled': False, 'images': [{'id': 'oEhKWFwpGrv-ToIG_x7tkfak3sSUJtQqOFqKOykEx0A', 'resolutions': [{'height': 81, 'url': 'https://external-preview.redd.it/ExLN4bsZAmY5byZMlhJyvVuakPuEol9f-GV-s_CeSpc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=225a6f71950b969646907e1c386b647b798f3931', 'width': 108}, {'height': 162, 'url': 'https://external-preview.redd.it/ExLN4bsZAmY5byZMlhJyvVuakPuEol9f-GV-s_CeSpc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cdfefcda9ad80858dce5ebf75f5a9f5805f85207', 'width': 216}, {'height': 240, 'url': 'https://external-preview.redd.it/ExLN4bsZAmY5byZMlhJyvVuakPuEol9f-GV-s_CeSpc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=19f72786742d3b27d97dcb61089fc98b486191ce', 'width': 320}], 'source': {'height': 360, 'url': 'https://external-preview.redd.it/ExLN4bsZAmY5byZMlhJyvVuakPuEol9f-GV-s_CeSpc.jpg?auto=webp&amp;s=4e0625b4304b9be659d8cb735e11cf4aef146d59', 'width': 480}, 'variants': {}}]}",6,1639746723,1,,True,False,False,dataengineering,t5_36en4,47324,public,https://b.thumbs.redditmedia.com/-C3HDm7g44xTh5yNPg0-YQmjN7oC0Fpfa8M1-I76GAA.jpg,This isn't a new subject but what do you think? Should Dimensional model be replaced with wide tables?,0,[],1.0,https://www.youtube.com/watch?v=3OcS2TMXELU&amp;t=930s&amp;ab_channel=dbt,all_ads,6,"{'oembed': {'author_name': 'dbt', 'author_url': 'https://www.youtube.com/c/dbt-labs', 'height': 200, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/3OcS2TMXELU?start=930&amp;feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/3OcS2TMXELU/hqdefault.jpg', 'thumbnail_width': 480, 'title': ""Kimball in the context of the modern data warehouse: what's worth keeping, and what's not"", 'type': 'video', 'version': '1.0', 'width': 356}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/3OcS2TMXELU?start=930&amp;feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'scrolling': False, 'width': 356}",,"{'oembed': {'author_name': 'dbt', 'author_url': 'https://www.youtube.com/c/dbt-labs', 'height': 200, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/3OcS2TMXELU?start=930&amp;feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/3OcS2TMXELU/hqdefault.jpg', 'thumbnail_width': 480, 'title': ""Kimball in the context of the modern data warehouse: what's worth keeping, and what's not"", 'type': 'video', 'version': '1.0', 'width': 356}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/3OcS2TMXELU?start=930&amp;feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/rih5bd', 'scrolling': False, 'width': 356}",105.0,140.0,https://www.youtube.com/watch?v=3OcS2TMXELU&amp;t=930s&amp;ab_channel=dbt,,,,,,,,,,
[],False,Minimum-Membership-8,,,[],,,,text,t2_a0qsnkph,False,False,False,[],False,False,1639744995,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rign41/mdm_tools_and_architecture/,{},rign41,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rign41/mdm_tools_and_architecture/,False,,,6,1639745006,1,"I’m considering building a new data ecosystem and trying to figure out where I can add MDM. Right now I’m thinking about moving data from Postgres -&gt; Kafka -&gt; Snowflake -&gt; Tableau. If I need to merge two sources together and create master records, should I do it in Snowflake using code or is there a tool that is good for this? Some master records may need business logic to pick the right value.",True,False,False,dataengineering,t5_36en4,47323,public,self,MDM tools and architecture,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rign41/mdm_tools_and_architecture/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,owentrigueros,,,[],,,,text,t2_2aly4819,False,False,False,[],False,False,1639736556,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/riehcy/data_warehouse_driven_website/,{},riehcy,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/riehcy/data_warehouse_driven_website/,False,,,6,1639736566,1,"I have a relatively big dataset of public contracts and I want to build website that allows:

* Search by contract name (e.g.: ""Maintenance of equipment""), company... (a small search engine, basically)
* Visualize metrics like:
   * Costs of all contracts per year (bar chart: x-axis: year, y-axis: cost)
   * Number of contracts
   * Number of unique purchasers
* API calls

This webpage is a good reference for what I want to build: [tenders.guru](https://tenders.guru/es)

Could you recommend me a good way to achieve this? I was thinking about building a Data Warehouse with MySQL where contracts are facts and the rest of is contained in dimensions. Then, build and API on top of it for the webserver to make searches, but I don't know how to handle pre-calculated metrics for visualizations (maybe pre-calculate them in a separate table after new contracts are inserted and make the webserver retrieve the data from it?)

Is a good idea to have a ""data warehouse driven website"" or should I consider another approach?

Thanks in advance!",True,False,False,dataengineering,t5_36en4,47319,public,self,Data warehouse driven website,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/riehcy/data_warehouse_driven_website/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,AtmarAtma,,,[],,,,text,t2_c3abglhk,False,False,False,[],False,False,1639729325,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ricttm/kaggle_python_api_for_kaggle_stats/,{},ricttm,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/ricttm/kaggle_python_api_for_kaggle_stats/,False,,,6,1639729335,1,"Hi,

I’m looking for Python API for extracting some data for a learning assignment. Data that I would like to extract are following:

- Number of users for every week or month since 2010. Country wise data.
- Same for GMs.
- Competition and dataset tags and find top 25 tags, tag vs dataset association for those 25 tags over last few years etc.

I saw a Kaggle post but can’t find the dataset. Hence I am thinking if I can use Kaggle API for extracting such data.",True,False,False,dataengineering,t5_36en4,47316,public,self,Kaggle Python API for Kaggle Stats,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ricttm/kaggle_python_api_for_kaggle_stats/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Nishant_UIC,,,[],,,,text,t2_dvfuo3no,False,False,False,[],False,False,1639727896,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/richej/5_challenges_with_big_data_and_how_aws_can_help/,{},richej,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/richej/5_challenges_with_big_data_and_how_aws_can_help/,False,self,"{'enabled': False, 'images': [{'id': '2Hyaw3I6XACDHqELz0SMXTyIGZYAPNbaxAhW43Trpt0', 'resolutions': [{'height': 54, 'url': 'https://external-preview.redd.it/gSLpywWOl1ozPwSeqpZIy6KFOSx5A5HGnZ2150bNPF8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=674d2004036c492f3ae3e91a24f5214dacc76790', 'width': 108}, {'height': 109, 'url': 'https://external-preview.redd.it/gSLpywWOl1ozPwSeqpZIy6KFOSx5A5HGnZ2150bNPF8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e0e876180a477903d622fb8d8273cac436fbea4f', 'width': 216}, {'height': 161, 'url': 'https://external-preview.redd.it/gSLpywWOl1ozPwSeqpZIy6KFOSx5A5HGnZ2150bNPF8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=284d685916ffabcf011253564ad30083cc83724a', 'width': 320}, {'height': 323, 'url': 'https://external-preview.redd.it/gSLpywWOl1ozPwSeqpZIy6KFOSx5A5HGnZ2150bNPF8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=35cdc402d51b56d2ad91ed36d0490fc57c586071', 'width': 640}], 'source': {'height': 440, 'url': 'https://external-preview.redd.it/gSLpywWOl1ozPwSeqpZIy6KFOSx5A5HGnZ2150bNPF8.jpg?auto=webp&amp;s=a0ebd4f4da57a026ae8cfe02301e0ecbb52a5fd5', 'width': 870}, 'variants': {}}]}",6,1639727906,1,"Enterprises constantly grapple with the challenges of Big Data Analytics that range from volume and variety of data, on-prem costs, and security to the lack of purpose built tools.

Let’s take a look at 5 big challenges with Big Data and how AWS can help overcome these with a wide variety of tools and services.

[https://www.umbrellainfocare.com/blogs/5-challenges-with-big-data-and-how-aws-can-help-handle-them](https://www.umbrellainfocare.com/blogs/5-challenges-with-big-data-and-how-aws-can-help-handle-them)",True,False,False,dataengineering,t5_36en4,47315,public,self,5 Challenges with Big Data and How AWS Can Help Handle Them,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/richej/5_challenges_with_big_data_and_how_aws_can_help/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,fancy_fishbowl,,,[],,,,text,t2_fanei4wp,False,False,False,[],False,False,1639727335,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ricchb/data_mesh/,{},ricchb,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/ricchb/data_mesh/,False,self,"{'enabled': False, 'images': [{'id': 'rCCJyxtA4pKK8s_C8wl63iacGtd0_mNdsN7_PddYO7o', 'resolutions': [{'height': 57, 'url': 'https://external-preview.redd.it/AKLkJ7nmF6HiZ2c4l3LQjyDu2khAt1X8E7K5Q_RaIBw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b8e05c896b19f70e7d30be17213eb768c1fabae8', 'width': 108}, {'height': 115, 'url': 'https://external-preview.redd.it/AKLkJ7nmF6HiZ2c4l3LQjyDu2khAt1X8E7K5Q_RaIBw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=314884737f88a858ec9312a14b38a4c784595da1', 'width': 216}, {'height': 171, 'url': 'https://external-preview.redd.it/AKLkJ7nmF6HiZ2c4l3LQjyDu2khAt1X8E7K5Q_RaIBw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2f84599b610b0b81f3f870188007e841ce20fce2', 'width': 320}], 'source': {'height': 300, 'url': 'https://external-preview.redd.it/AKLkJ7nmF6HiZ2c4l3LQjyDu2khAt1X8E7K5Q_RaIBw.jpg?auto=webp&amp;s=d8b495b7209d72504c0331f0018a218f1c4b3b76', 'width': 560}, 'variants': {}}]}",6,1639727345,1,"I just read this wonderful article about Data Meshs:

https://martinfowler.com/articles/data-monolith-to-mesh.html

Hope this is interesting for someone else!",True,False,False,dataengineering,t5_36en4,47315,public,self,Data Mesh,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ricchb/data_mesh/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,RichKatz,,,[],,,,text,t2_32lec,False,False,False,[],False,False,1639713873,medium.com,https://www.reddit.com/r/dataengineering/comments/ri8jzg/etl_extract_transform_load_best_practices_etl/,{},ri8jzg,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/ri8jzg/etl_extract_transform_load_best_practices_etl/,False,link,"{'enabled': False, 'images': [{'id': 'bNlx8-adtHAHtlLeXwtuxkmadPtmlBT5fIJZT3ya-ag', 'resolutions': [{'height': 50, 'url': 'https://external-preview.redd.it/Of6ewUcQesJjRifTSWMNxPgQ1JxgS6PVaZ4OVuEffCU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d97ee253085f0ed0893f9de1ba19c43c8c247227', 'width': 108}, {'height': 100, 'url': 'https://external-preview.redd.it/Of6ewUcQesJjRifTSWMNxPgQ1JxgS6PVaZ4OVuEffCU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a98db7d5fbd33522a7803c5a156125d7885f25b7', 'width': 216}, {'height': 148, 'url': 'https://external-preview.redd.it/Of6ewUcQesJjRifTSWMNxPgQ1JxgS6PVaZ4OVuEffCU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=609607ed7732935ed689ebbb5fa09f0e7b473e4f', 'width': 320}, {'height': 297, 'url': 'https://external-preview.redd.it/Of6ewUcQesJjRifTSWMNxPgQ1JxgS6PVaZ4OVuEffCU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=86b1b8ce33b16dcea4b646ee5b27d916fd8de85e', 'width': 640}], 'source': {'height': 430, 'url': 'https://external-preview.redd.it/Of6ewUcQesJjRifTSWMNxPgQ1JxgS6PVaZ4OVuEffCU.jpg?auto=webp&amp;s=b717a4cbd19c0fa0d1253839ad4dd47c5e923c53', 'width': 925}, 'variants': {}}]}",6,1639713884,1,,True,False,False,dataengineering,t5_36en4,47305,public,https://b.thumbs.redditmedia.com/7PIRWWmcHFCFjKRe9sIJc91eixz7yTvzrOqn1slpFmc.jpg,"ETL (Extract, Transform, Load). Best Practices ETL Process And Lifehacks",0,[],1.0,https://medium.com/flatlogic/etl-extract-transform-load-best-practices-etl-process-and-lifehacks-553409238509,all_ads,6,,,,,,65.0,140.0,https://medium.com/flatlogic/etl-extract-transform-load-best-practices-etl-process-and-lifehacks-553409238509,,,,,,,,,,
[],False,RichKatz,,,[],,,,text,t2_32lec,False,False,False,[],False,False,1639710180,medium.com,https://www.reddit.com/r/dataengineering/comments/ri7eoz/etl_extract_transform_load_best_practices_etl/,{},ri7eoz,False,True,False,False,False,True,False,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/ri7eoz/etl_extract_transform_load_best_practices_etl/,False,,,6,1639710191,1,,True,False,False,dataengineering,t5_36en4,47304,public,default,"ETL (Extract, Transform, Load). Best Practices ETL Process And Lifehacks : Katarina Harbuzava (Medium)",0,[],1.0,https://medium.com/flatlogic/etl-extract-transform-load-best-practices-etl-process-and-lifehacks-553409238509https://medium.com/flatlogic/etl-extract-transform-load-best-practices-etl-process-and-lifehacks-553409238509,all_ads,6,,,,,,,,https://medium.com/flatlogic/etl-extract-transform-load-best-practices-etl-process-and-lifehacks-553409238509https://medium.com/flatlogic/etl-extract-transform-load-best-practices-etl-process-and-lifehacks-553409238509,,,,,,,,,,
[],False,The_Alpacas,,,[],,,,text,t2_3qmz09sb,False,False,False,[],False,False,1639701944,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ri4qzr/do_you_use_unit_testing_in_your_data_engineering/,{},ri4qzr,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/ri4qzr/do_you_use_unit_testing_in_your_data_engineering/,False,,,6,1639701955,1,"Doing a survey

[View Poll](https://www.reddit.com/poll/ri4qzr)",True,False,False,dataengineering,t5_36en4,47298,public,self,Do you use ‘unit testing’ in your data engineering job?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ri4qzr/do_you_use_unit_testing_in_your_data_engineering/,all_ads,6,,,,,,,,,,,,,"{'is_prediction': False, 'options': [{'id': '12536796', 'text': 'No'}, {'id': '12536797', 'text': 'Yes'}], 'prediction_status': None, 'resolved_option_id': None, 'total_stake_amount': None, 'total_vote_count': 0, 'tournament_id': None, 'user_selection': None, 'user_won_amount': None, 'vote_updates_remained': None, 'voting_end_timestamp': 1639961144573}",,,,,
[],False,BoiElroy,,,[],,,,text,t2_r8dyi,False,False,True,[],False,False,1639695400,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ri2k6b/can_someone_explain_the_big_deal_with_dbt/,{},ri2k6b,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/ri2k6b/can_someone_explain_the_big_deal_with_dbt/,False,,,6,1639695410,1,"I'm not sure I get what the craze is about? It just executes SQL using the already existing SQL engine in the database...

I get it's more observable than filling a database with a bunch of stored procedure scripts, but if I'm already orchestrating my SQL transforms with something like airflow or prefect anyways what do I need DBT for?",True,False,False,dataengineering,t5_36en4,47290,public,self,Can someone explain the big deal with dbt?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ri2k6b/can_someone_explain_the_big_deal_with_dbt/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,ferrywheel,,,[],,,,text,t2_vke8uo6,False,False,False,[],False,False,1639689498,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ri0hqz/testing_etl_pipelines/,{},ri0hqz,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/ri0hqz/testing_etl_pipelines/,False,,,6,1639689509,1,"Hello, I just started as a junior data engineer in my company and this is actually the first time i'm in touch with data engineering work.

We are using airflow to schedule our pipelines and running the etl scripts in python containers. I'm exploring all the code right now and I've never worked with docker before. I want to test and debug some of our scripts, to actually see what each step in the code does to the data we are working on.But I cant find an easy way to debug the code we have here. 

I already setup up docker and tried to run the container from my machine and its working fine. But everytime I change something in the etl code, i have to build and run the container again and its just not very practical. I commented all the code that saves the final data to azure as I'm just testing things and don't want to mess up witht the production data.  Looking up on the web, I found that I can mount my local directory in docker (so i won't need to keep building the container everytime I change something in the code) or I can develop some tests to be ran on docker.

So what are the best practice to test changes and modifications on etl code?",True,False,False,dataengineering,t5_36en4,47283,public,self,Testing ETL pipelines,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ri0hqz/testing_etl_pipelines/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,eganba,,,[],,,,text,t2_4dmg0zte,False,False,False,[],False,False,1639687097,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rhzmhf/relatively_new_to_my_job_and_having_serious/,{},rhzmhf,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rhzmhf/relatively_new_to_my_job_and_having_serious/,False,,,6,1639687107,1,"So I joined the data services team at my company. They kind of went about t backasswards and got a BI platform (Domo), a database (Redshift) and working on getting a data lake set up. I am working on getting the data collection piece set up and having a hell of a time finding a platform that fits nicely with our group of pieces. I keep hoping Azure Data Factory will work in this instance but it feels a lot like Azure plays best with its own tools and not when piecemeal like this. And I am unsure which AWS platform would do the things I am looking to do.

&amp;#x200B;

Anyone have experience in this realm? Any thing I should check out? My background has predominantly been in the dB management and analytics side and less the structure side so this is all very new to me.",True,False,False,dataengineering,t5_36en4,47280,public,self,Relatively new to my job and having serious issues in a data engineering solution,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rhzmhf/relatively_new_to_my_job_and_having_serious/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,arimbr,,,[],,,,text,t2_975og,False,False,False,[],False,False,1639685595,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rhz3m0/elt_pipelines_with_prefect_airbyte_and_dbt/,{},rhz3m0,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rhz3m0/elt_pipelines_with_prefect_airbyte_and_dbt/,False,,,6,1639685605,1,"Hello! We just released Airbyte's integration with Prefect. Now it's easy to connect Prefect with Airbyte and dbt to orchestrate ELT pipelines.

We wrote a tutorial to build an ELT pipeline to discover GitHub users that have contributed to the Prefect, Airbyte, and dbt Github repositories.

[https://airbyte.io/recipes/elt-pipeline-prefect-airbyte-dbt](https://airbyte.io/recipes/elt-pipeline-prefect-airbyte-dbt)  


Curious to know if you use one or multiple tools for your ETL/ELT pipelines?",True,False,False,dataengineering,t5_36en4,47279,public,self,"ELT pipelines with Prefect, Airbyte and dbt",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rhz3m0/elt_pipelines_with_prefect_airbyte_and_dbt/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,traveling_wilburys,,,[],,,,text,t2_56nxuw5r,False,False,False,[],False,False,1639673172,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rhunwu/spark_configs_for_datasets_eli5/,{},rhunwu,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rhunwu/spark_configs_for_datasets_eli5/,False,,,6,1639673182,1,"I've been working with Spark for over a year and still don't understand how to optimize spark jobs. Would anyone mind sharing how optimizations work for different datasets?

&amp;#x200B;

1. 1GB data set with   1000 Cols, 1 M rows
2. 10 GB dataset with 10000 cols, 2 M rows
3. 100 GB dataset with 10000 cols, 20 M rows
4. Left Joining 1 with 2
5. Left joining 2 with 1
6. left join 3 with 2/1 based on multiple primary keys
7. Left join 3 with 2/1 based on single primary keys (may return &gt; 20 M rows)
8. Creating 50 new columns for (3), with sql based transformations.

Some other questions:

1. How do I optimize driver memory/ executor memory / executor cores?
2. how do I know how much overhead/memory fraction do I need?
3. how do I allocate dynamicallocation.minexecutors/maxexecutors?
4. when to use a broadcast join for datasets above?
5. how to optimize sql.shuffle.partitions? 

&amp;#x200B;

feel free to comment on similar questions.",True,False,False,dataengineering,t5_36en4,47271,public,self,Spark configs for datasets ELI5,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rhunwu/spark_configs_for_datasets_eli5/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,fittytuckatron,,,[],,,,text,t2_7g7gnxs0,False,False,False,[],False,False,1639671168,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rhtyt1/i_took_a_ms_course_on_data_integration_and_it_was/,{},rhtyt1,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rhtyt1/i_took_a_ms_course_on_data_integration_and_it_was/,False,self,"{'enabled': False, 'images': [{'id': 'YApHHMbWpuH_VtbxYcfIqoEZcUYceWRq4U5PDmRKJVw', 'resolutions': [{'height': 56, 'url': 'https://external-preview.redd.it/a2ma3Lkie2EG-P-3gCgxSAWn7fKeMNOaVxbIoBBP908.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=37f7f44ffa34248c8ba16aec9cc214d9765998f0', 'width': 108}, {'height': 113, 'url': 'https://external-preview.redd.it/a2ma3Lkie2EG-P-3gCgxSAWn7fKeMNOaVxbIoBBP908.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=26f05f4d1f6d60cd5b5c27b29c2f25b6fad7da66', 'width': 216}, {'height': 167, 'url': 'https://external-preview.redd.it/a2ma3Lkie2EG-P-3gCgxSAWn7fKeMNOaVxbIoBBP908.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6c8afa81863199ac1a6833dc2bdccdb543194733', 'width': 320}, {'height': 334, 'url': 'https://external-preview.redd.it/a2ma3Lkie2EG-P-3gCgxSAWn7fKeMNOaVxbIoBBP908.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4e4bb337267b5e9acea6c092069a236f2c8df831', 'width': 640}], 'source': {'height': 428, 'url': 'https://external-preview.redd.it/a2ma3Lkie2EG-P-3gCgxSAWn7fKeMNOaVxbIoBBP908.jpg?auto=webp&amp;s=483e44ca7b0d86f177b188fbbd7371e4973bd0c4', 'width': 818}, 'variants': {}}]}",6,1639671179,1,"Hey guys, I'm in the first year of my masters degree in Computer Science (AI and Software Engineering majors), and last trimester I went off course and enrolled in a course on data analysis and integration because I've always been interested in the area. However, I thought it was pretty underwhelming stuff. We messed with databases (MySQL), implemented some ETL pipelines (Pentaho Data Integration), created a datawarehouse (we went over star/snowflake/starflake architectures etc), checked data statistics (with DataCleaner (?)), defined some OLAP cubes with Schema Workbench and played around with them with Saiko Analytics. Ah, we also fooled around with some sort of automatic report generating software.. stuff. I still ended up with nice grade, 18/20, highest of the class but felt kinda disappointed.

&amp;#x200B;

And.. that was it. I visit this sub sometimes and I feel like I didn't even scratch the surface of this field (we didn't even mention the *cloud* in the course, or data integration for that matter)

Also, I don't see anyone using the software that I used in the course. 

&amp;#x200B;

Where can I strengthen my knowledge of data engineering? I know it's very broad question but I'm struggling to understand where exactly I should start.. the best I've found was this: [https://awesomedataengineering.com/](https://awesomedataengineering.com/)

&amp;#x200B;

Thank you! ^(and sorry for my grammar)",True,False,False,dataengineering,t5_36en4,47266,public,self,I took a MS course on Data Integration and it was underwhelming - where can i learn more?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rhtyt1/i_took_a_ms_course_on_data_integration_and_it_was/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,LiquidSynopsis,,,[],,,,text,t2_8fuwkii9,False,False,False,[],False,False,1639669081,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rht9hr/front_end_for_data_lakes/,{},rht9hr,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rht9hr/front_end_for_data_lakes/,False,,,6,1639669092,1,"Has anyone ever had any experience with a front end user application for Data Lakes? 

Thinking of the equivalent of Microsoft Access connected to SQL Server. We have users who like creating their owns Access/VBA apps. As we move completely into Data Lakes and retire some of our SQL DBs I’m trying to see if their is a tool like that for them to adopt. FWIW our stack is in Azure.",True,False,False,dataengineering,t5_36en4,47264,public,self,Front End for Data Lakes,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rht9hr/front_end_for_data_lakes/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,unskilledexplorer,,,[],,,,text,t2_404pimvo,False,False,False,[],False,False,1639668943,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rht7xi/building_my_first_pipeline_with_lakehouse/,{},rht7xi,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rht7xi/building_my_first_pipeline_with_lakehouse/,False,self,"{'enabled': False, 'images': [{'id': '5eOU_FJWoZ-tTftVZjr63i8vaOIJxqXy6pT9qH-mA40', 'resolutions': [{'height': 74, 'url': 'https://external-preview.redd.it/LCc30aCX1KHzBzN2POZ_Ob6n45B4NnNo7Bv3PfG_1k0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b709d1e2a43f5cedc0c4f3baa98d8d02652c96df', 'width': 108}, {'height': 148, 'url': 'https://external-preview.redd.it/LCc30aCX1KHzBzN2POZ_Ob6n45B4NnNo7Bv3PfG_1k0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=16f82c2912d9b8add73f17b16e6ccf4583c90405', 'width': 216}, {'height': 219, 'url': 'https://external-preview.redd.it/LCc30aCX1KHzBzN2POZ_Ob6n45B4NnNo7Bv3PfG_1k0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=27b498e08810679672403c7ff79d208f7bc67514', 'width': 320}, {'height': 439, 'url': 'https://external-preview.redd.it/LCc30aCX1KHzBzN2POZ_Ob6n45B4NnNo7Bv3PfG_1k0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=40992443e073ac4b9997a5a631554e401905a3ec', 'width': 640}, {'height': 658, 'url': 'https://external-preview.redd.it/LCc30aCX1KHzBzN2POZ_Ob6n45B4NnNo7Bv3PfG_1k0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=738732ca48bf9fcd84b1276978a2f5a885009344', 'width': 960}, {'height': 741, 'url': 'https://external-preview.redd.it/LCc30aCX1KHzBzN2POZ_Ob6n45B4NnNo7Bv3PfG_1k0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=221dcab591f1a809b326cac158ae155021743fd5', 'width': 1080}], 'source': {'height': 1125, 'url': 'https://external-preview.redd.it/LCc30aCX1KHzBzN2POZ_Ob6n45B4NnNo7Bv3PfG_1k0.jpg?auto=webp&amp;s=1fd590312a250d47b0e8e7b39ba8e12c1c361222', 'width': 1639}, 'variants': {}}]}",6,1639668954,1,"I am about to build my first data pipeline ever. I have read about lakehouses as a most recent architecture for data pipelines. So why not to start there. I made some research on tools and approaches, and I would like to leverage [structured streaming](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#starting-streaming-queries) engine, delta format and dbt.

What I plan to achieve:

* I want ingest JSON documents (classic event tracking from web UIs) stream from kafka
* I want to do some transformations to silver and gold level via dbt
* I want to implement CI/CD for the transformations so I can automate my workflow
* expose the gold to any BI (JDBC?)

So far, my project is kinda purposeless, I want just to get in touch with it. However, if I am able to build it, I want to build on top of it later. My ultimate goal is to create an automated semantic layer which will analyze schema of the JSON documents and generate transformations for the purpose of analytics.

I have very little experience with devops and cloud solutions. Where to run it? is it possible run/develop it on my VPS and migrate to cloud later or do I need to use AWS, GCP, etc straight away?

So far, I created a dataframe from a csv file and created simple streamed SQL transformation using pyspark and [delta](https://delta.io/) locally. What is my next step?

Thanks for all advice.",True,False,False,dataengineering,t5_36en4,47263,public,self,"Building my first pipeline with lakehouse architecture, what is your advice?",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rht7xi/building_my_first_pipeline_with_lakehouse/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,PaulSandwich,,,[],,,,text,t2_x2wqm,False,False,False,[],False,False,1639662509,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rhr3sq/advice_on_encrypting_sensitive_ids_that_result_in/,{},rhr3sq,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rhr3sq/advice_on_encrypting_sensitive_ids_that_result_in/,False,,,6,1639662519,1,"We need to leverage account IDs in our reporting layer, but those IDs are sensitive so they need to be encrypted/tokenized. Our footprint is small, less than 100 million values.   
    
Token lookups seem like a lot of overhead, and I'd prefer to not have a lookup table with the keys to the kingdom in it.    
   
Encryption seems like the best option, especially since we can use different salts for different contexts (ex: if we need to share data with multiple vendors, the IDs wouldn't map back) and we have existing security infrastructure to manage that.   
   
**But** the guids we get back from secure encryption are long and not human friendly, and humans will be using these. Is there a best practice for masking IDs programmatically and globally?   
   
My initial thought is to double-dip the IDs; use a strong encryption initially, and then use a weak encryption to resolve that to something more readable. I would be fine including both values in our tables to account for data-loss in that process.   
   
Surely this is a thing people have dealt with before. Is there a Tried &amp; True solution?",True,False,False,dataengineering,t5_36en4,47252,public,self,Advice on encrypting sensitive IDs that result in values that are secure but still user friendly. Best Practice?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rhr3sq/advice_on_encrypting_sensitive_ids_that_result_in/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,AbdullahMohammadKhan,,,[],,,,text,t2_5b54gjt0,False,False,False,[],False,False,1639658821,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rhq1bt/what_do_i_need_to_learn_about_hive/,{},rhq1bt,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rhq1bt/what_do_i_need_to_learn_about_hive/,False,,,6,1639658832,1,"So, I am learning Hive right now. I don't know much about the industry trend at the moment and whether Hive is being used widely these days. I don't even know if people are using something else instead of that. So, if you could advise me on how much I need to learn about Hive and what are the topics that I need to focus on or, maybe which other tool(s) I should be learning instead because of the industry trend , that would be very very helpful.",True,False,False,dataengineering,t5_36en4,47239,public,self,What do I need to learn about Hive ?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rhq1bt/what_do_i_need_to_learn_about_hive/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,rudboi12,,,[],,,,text,t2_4s2dogl9,False,False,False,[],False,False,1639657638,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rhpqax/data_engineering_fun/,{},rhpqax,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rhpqax/data_engineering_fun/,False,,,6,1639657648,1,"I come from an Engineering background and ended up working as a Business Consultant for a Big 4 after college. I decided I wanted to change career path to data science and went to get a cheap (but good) masters in data science in a top europe university. Because I wanted some experience (and money), I ended up getting the only internship I could, as a Data Engineer. I had no idea about Data Engineering before starting, just the basics needed to start: sql and python. 

After 3 months in this internship and about 5 months in my masters, I realized what I do at work is 1000% more fun than what I do at school. I’m starting to get bored with all of my classes, specially the basics: ML and Linear Algebra. There is just too much math and stats that I think is useless in real life. Like do people actually use SVM and kernels in real life?. It is just too boring.

What I don’t know if what it is like to be a Data Scientist in real life. Are you doing stats and optimization and using kernels irl? Are they basically researchers? Or depends on the title?. I’m more of a doer so that why I think I’m starting to like more Data Engineering.

Can someone who has made the change from DS to DE (or viceversa) give me some insight on jobs as a DS.",True,False,False,dataengineering,t5_36en4,47234,public,self,Data Engineering fun?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rhpqax/data_engineering_fun/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,fancy_fishbowl,,,[],,,,text,t2_fanei4wp,False,False,False,[],False,False,1639646881,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rhn6d6/good_data_engineering_conferences/,{},rhn6d6,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rhn6d6/good_data_engineering_conferences/,False,,,6,1639646891,1,"Can anyone recommend conferences that are especially well suited for Data Engineers? Preferably in Europe but please also mention every other conference that could fit.

There are so many conferences that it's hard to keep track which ones will be helpful.",True,False,False,dataengineering,t5_36en4,47193,public,self,Good Data Engineering Conferences,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rhn6d6/good_data_engineering_conferences/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,snarkj,,,[],,,,text,t2_gnalnzfj,False,False,False,[],False,False,1639637723,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rhl0ty/architecture_suggestion_for_alerting_system_in/,{},rhl0ty,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rhl0ty/architecture_suggestion_for_alerting_system_in/,False,,,6,1639637733,1,"Hello All. I am currently working for a retail analytics company as a Data Engineer + Data Scientist. As of now I have built a simple alerting system. It checks for alert subscriber and alert type during change calculation in ETL ( for example price change) and sends email if match is found. Its done via python scripts with somewhat reconfigurable alert sink ( SQS, email etc) and alert data ( count of product with change or all products list,). However its time to make it more scalable ( upto 1000 alerts per minute) and separate it out from main ETL pipeline. I always prefer simple solution. Any suggestion on any aspect of the process is welcomed. Thank you",True,False,False,dataengineering,t5_36en4,47176,public,self,Architecture suggestion for Alerting System in retail data.,0,[],0.99,https://www.reddit.com/r/dataengineering/comments/rhl0ty/architecture_suggestion_for_alerting_system_in/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,noodlesoup89,,,[],,,,text,t2_6on5x,False,False,False,[],False,False,1639616823,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rhel9i/how_standardized_tooling_and_metadata_saved_our/,{},rhel9i,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rhel9i/how_standardized_tooling_and_metadata_saved_our/,False,self,"{'enabled': False, 'images': [{'id': 'wgnUNkuM3Ge_GPK9QDB7bk8uOsP6D7SiV86meH0za5g', 'resolutions': [{'height': 110, 'url': 'https://external-preview.redd.it/sROkYj3RIDlyx62Cz-qINy5RZ3_QFfZSmTeeFpI0F_E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=623ae205c47e06e45b0f963785fb3405883cfd3c', 'width': 108}, {'height': 221, 'url': 'https://external-preview.redd.it/sROkYj3RIDlyx62Cz-qINy5RZ3_QFfZSmTeeFpI0F_E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=07774559ca25d009fdd443d05dc31b1e3da62115', 'width': 216}, {'height': 327, 'url': 'https://external-preview.redd.it/sROkYj3RIDlyx62Cz-qINy5RZ3_QFfZSmTeeFpI0F_E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=204ec7c98599e0d104aa5a02b47d2293ce33f27c', 'width': 320}, {'height': 655, 'url': 'https://external-preview.redd.it/sROkYj3RIDlyx62Cz-qINy5RZ3_QFfZSmTeeFpI0F_E.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8bbd718c58ac477f43355f2b4465fd4ec8951513', 'width': 640}, {'height': 983, 'url': 'https://external-preview.redd.it/sROkYj3RIDlyx62Cz-qINy5RZ3_QFfZSmTeeFpI0F_E.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c906f314fc3fe32ca1885783629fb36e97a7a143', 'width': 960}, {'height': 1106, 'url': 'https://external-preview.redd.it/sROkYj3RIDlyx62Cz-qINy5RZ3_QFfZSmTeeFpI0F_E.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4580381ba9d81b2081ecc752dd83803fd58bb50b', 'width': 1080}], 'source': {'height': 1229, 'url': 'https://external-preview.redd.it/sROkYj3RIDlyx62Cz-qINy5RZ3_QFfZSmTeeFpI0F_E.jpg?auto=webp&amp;s=1e1fd50b69d4585dcb0cad064af48eba0cf8e156', 'width': 1200}, 'variants': {}}]}",6,1639616833,1,"I wrote about how we build some standard interfaces and tooling for ETL at our company, I'm curious if other people have found they needed to do similar at their companies as well? Or just in general, do a lot of people have issues standardizing their metadata / lineage or dealing with too many data pipelines / artifacts?

Also welcome any feedback on the blog. 

[How Standardized Tooling and Metadata Saved Our Data Organization](https://medium.com/keeptruckin-eng/how-metadata-saved-our-data-organization-cab3335eb4ae)",True,False,False,dataengineering,t5_36en4,47160,public,self,How Standardized Tooling and Metadata Saved Our Data Organization,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rhel9i/how_standardized_tooling_and_metadata_saved_our/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Money_Major,,,[],,,,text,t2_161c60,False,False,False,[],False,False,1639608007,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rhbi9v/advice_on_creating_a_data_warehouse_in_an_old/,{},rhbi9v,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rhbi9v/advice_on_creating_a_data_warehouse_in_an_old/,False,,,6,1639608017,1,"Hi everyone, 

I'd be interested to see what those more experienced than me would do in my situation.

I'm currently one of three analysts in a pretty old school business (logistics). They've only just created an analytics department over the past \~3 years, so there are no data engineers and I doubt management would believe that that's even a real job title. I've taught myself SQL and Python and am pretty confident with both (more-so Python than SQL). 

I spend most of my time creating reports in PowerBI. This is where I've started noticing issues, namely:

&amp;#x200B;

* We build the reports based on data straight from our OLTP databases (on-prem SQL Server). I know this isn't best practise and could cause performance issues down the track - I already sometimes crash the servers if my queries are too heavy. 
* There's data inconsistency between reports depending on who made it or how it's built. For example, I've built out our P&amp;L with a whole lot of tiny transformations (excluding this GL, including this department but not if it's from this site, etc.) which sits in a dataset for **that particular report**. If I ever want to reference back to say revenue for a particular period in another report, then I have to completely replicate the logic I've built into the P&amp;L report's dataset otherwise those figures will never match. This is causing those who rely on those numbers to lose confidence in the data. 
* I know that if anyone else ever tried to maintain the reports I've built, there's that many small and subtle transformations that take place for every single dataset that it'd be close to impossible for them to understand unless they knew the data inside and out, let alone make changes to without breaking the report. EG. if you forget to filter for *'customer\_field3 = 0'* in this particular table, you're actually included deleted transactions, which is incorrect and therefore won't match any of the other reports.
* We definitely have a use case for external data - I'm already scraping some data from the web that is really useful but I have no where to put it because I don't have write access to our SQL Server DBs. I also understand that chucking it into an OLTP DB probably isn't the best move anyway and that IT don't want people poking around the DBs we use for our applications, so am currently just keeping the data on a Postgres DB I've set up on my local machine..

I've come to the conclusion that considering these issues a data warehouse would probably be a step in the right direction. This will consolidate all of the data from all of the different DBs and there would be a single source of truth. As a bonus I'm really keen on moving my career towards data engineering if I can, so thought this would be a good (if not very ambitious) project to take on, assuming I can sell management on it.

I've done some reading and initially thought ""Okay, looks like the tools everyone's using is some combination of an orchestration tool (first thought is Airflow), data warehouse (Snowflake) and a staging/storage area for the data to initially land (S3)."". I've had a play around with this exact stack and while it's awesome to see how it all works, I have a feeling it's way overkill for what we need and that most of what I'm reading is from this sub, and most of you guys are probably working with a lot more data than we would be. For context, I've looked at our main DBs and the entire contents of them add up to &lt; 100GB. 

I'm also battling with the fact that I personally would probably prefer to use Airflow/cloud products simply to gain experience in these tools to open up further employment opportunities in the future in the data engineering space. 

I would really appreciate some advice or guidance on striking a balance between which tools I should be using that are appropriate for my situation but also tools that will provide me with experience that will help to push my career towards the data engineering space.",True,False,False,dataengineering,t5_36en4,47147,public,self,Advice on creating a data warehouse in an old school business that doesn't have much of a data culture yet.,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rhbi9v/advice_on_creating_a_data_warehouse_in_an_old/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,baikencordess,,,[],,,,text,t2_4j3n164m,False,False,False,[],False,False,1639606718,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rhb0g2/scrapping_reddit/,{},rhb0g2,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rhb0g2/scrapping_reddit/,False,,,6,1639606728,1,"Let me run an idea by you Data Engineers.  Let's say I wanted to pull data daily from Reddit, specifically from a subreddit.  For this example let's use Cats.  I want to pull data on the latest cat videos, specifically how to train cats, how to take care of cats, etc.

I would use the data for a website that tells people how to become better cat owners.  Basically a blog site with new cat ""tech"" everything day.

Is the data scrapping feasible by one person?  Could I include twitter too?

I just started a MS in data science, so I'm pretty new to APIs and data mining.",True,False,False,dataengineering,t5_36en4,47147,public,self,Scrapping Reddit?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rhb0g2/scrapping_reddit/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,m123av,,,[],,,,text,t2_c6m5h81g,False,False,False,[],False,False,1639593122,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rh5urx/springboard_data_engineering_bootcamp_learn/,{},rh5urx,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rh5urx/springboard_data_engineering_bootcamp_learn/,False,,,6,1639593132,1,"springboard Data Engineering Bootcamp: Learn online, on your own time. how is this course",True,False,False,dataengineering,t5_36en4,47136,public,self,"springboard Data Engineering Bootcamp: Learn online, on your own time. how is this course",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rh5urx/springboard_data_engineering_bootcamp_learn/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Apprehensive_Size_32,,,[],,,,text,t2_6lfogfln,False,False,False,[],False,False,1639593020,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rh5t6w/tips_for_dealing_with_data_transfers_via_dlls/,{},rh5t6w,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rh5t6w/tips_for_dealing_with_data_transfers_via_dlls/,False,,,6,1639593030,1,"I'm working on extracting data out of a legacy system w/o a backend rdbs. This is very simple time series data, two tables.

The only way to get to this data is via a set of DLL functions. I've done a bit of research, and these more or less seem to function like API calls.

It seems like the best way to do this will be to have my IT team set up a VM on a server where I can access the data via the dll. I could then write a program in c++ that calls the DLL functions I need, extracts the time series data, and sends batches of data over to my MongoDB existing on the same network. Here, that data could be used for analysis / BI / etc, and accessed remotely.

Any thoughts? I am not a DE, I am a DS working at an old company without DEs  :)

This seems like it would work to me but I don't know what I am missing, if there is a better way, how to orchestrate this, etc. The one thing I know is that the only way to get this data is via these DLL functions.",True,False,False,dataengineering,t5_36en4,47136,public,self,Tips for dealing with data transfers via DLLs?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rh5t6w/tips_for_dealing_with_data_transfers_via_dlls/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,GeneralMID1,,,[],,,,text,t2_xn855,False,False,False,[],False,False,1639589472,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rh4fd7/the_best_data_quality_framework_for_senior/,{},rh4fd7,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rh4fd7/the_best_data_quality_framework_for_senior/,False,self,"{'enabled': False, 'images': [{'id': 'rx8A3vJdaz0j5HH1tvnWSh0NipzvWg4Jc3t31w7INOo', 'resolutions': [{'height': 63, 'url': 'https://external-preview.redd.it/pVCN428hdI0dzgZE6AOIQS6JmsA8DQSq0eU3pOzyHg8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=82de1f0d4f4a43484716ffa3cc0bc8af2fed4144', 'width': 108}, {'height': 126, 'url': 'https://external-preview.redd.it/pVCN428hdI0dzgZE6AOIQS6JmsA8DQSq0eU3pOzyHg8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9180f04d0d7f48c00226dc2c88f45dc80e4bea15', 'width': 216}, {'height': 186, 'url': 'https://external-preview.redd.it/pVCN428hdI0dzgZE6AOIQS6JmsA8DQSq0eU3pOzyHg8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9f3cfe66fd012fc8d131f81dc60ad030a1c8c583', 'width': 320}, {'height': 373, 'url': 'https://external-preview.redd.it/pVCN428hdI0dzgZE6AOIQS6JmsA8DQSq0eU3pOzyHg8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6ac34be2afbbc7d8d94d1138a695e8e197725e58', 'width': 640}, {'height': 560, 'url': 'https://external-preview.redd.it/pVCN428hdI0dzgZE6AOIQS6JmsA8DQSq0eU3pOzyHg8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7045a8a0513cb21e906de6745a5fffe2cc76260f', 'width': 960}, {'height': 630, 'url': 'https://external-preview.redd.it/pVCN428hdI0dzgZE6AOIQS6JmsA8DQSq0eU3pOzyHg8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5f29709a66a2ef28061f5e275bc132ac449b31fc', 'width': 1080}], 'source': {'height': 1494, 'url': 'https://external-preview.redd.it/pVCN428hdI0dzgZE6AOIQS6JmsA8DQSq0eU3pOzyHg8.jpg?auto=webp&amp;s=d9246f2cd5cf8fbf11921bc5447279ebb0921f5c', 'width': 2560}, 'variants': {}}]}",6,1639589483,1,Hello friends! Does your data team have a data quality framework? A data quality framework is a tool that an organization can use to define relevant data quality attributes and provide guidance for a data quality management process of continuously ensuring data quality meets consumers’ expectations (SLAs). You can learn more about this from our blog post on building the best data quality framework for senior platform engineers: [https://databand.ai/blog/data-quality-framework/?utm\_source=forum&amp;utm\_medium=r&amp;utm\_group=de](https://databand.ai/blog/data-quality-framework/),True,False,False,dataengineering,t5_36en4,47132,public,self,The best data quality framework for senior platform engineers,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rh4fd7/the_best_data_quality_framework_for_senior/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Suspicious-Use7032,,,[],,,,text,t2_ftacr407,False,False,False,[],False,False,1639588786,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rh45ic/how_to_retrieve_3_words_from_a_sentence_using_sql/,{},rh45ic,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rh45ic/how_to_retrieve_3_words_from_a_sentence_using_sql/,False,,,6,1639588796,1,"Ex- I love data engineering
Expected result - I love data 
Use case -  I have a column which have whole sentence,I want to create a new column with only first 3 words ( kind of short description)",True,False,False,dataengineering,t5_36en4,47131,public,self,How to retrieve 3 words from a sentence using sql,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rh45ic/how_to_retrieve_3_words_from_a_sentence_using_sql/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Significant-Carob897,,,[],,,,text,t2_4x8s649h,False,False,False,[],False,False,1639582604,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rh1spn/dbt_for_biquery_how_are_you_hacking_the/,{},rh1spn,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rh1spn/dbt_for_biquery_how_are_you_hacking_the/,False,self,"{'enabled': False, 'images': [{'id': 'Vs7JkVrMKDGNHrJlQLg_GQnjTnXJTnqs-KW1ZjP17JI', 'resolutions': [{'height': 54, 'url': 'https://external-preview.redd.it/nO_HyhIctY_kgV1QQh93zCg4TwEwwa7IP5Fffb6eb9k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d63e5ef840fc5c6b4c1c9ccf5da1b2033f67a474', 'width': 108}, {'height': 108, 'url': 'https://external-preview.redd.it/nO_HyhIctY_kgV1QQh93zCg4TwEwwa7IP5Fffb6eb9k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4f490648001d440b4cd6c863dd148deec31b6806', 'width': 216}, {'height': 160, 'url': 'https://external-preview.redd.it/nO_HyhIctY_kgV1QQh93zCg4TwEwwa7IP5Fffb6eb9k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4dd16aac7ddb21f520b1baaf693e5c9a2548a689', 'width': 320}, {'height': 320, 'url': 'https://external-preview.redd.it/nO_HyhIctY_kgV1QQh93zCg4TwEwwa7IP5Fffb6eb9k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d52a9d55f9057caf619bfb151b9ddcb08fcea87d', 'width': 640}, {'height': 480, 'url': 'https://external-preview.redd.it/nO_HyhIctY_kgV1QQh93zCg4TwEwwa7IP5Fffb6eb9k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f0a306dc1861af185b53ad3b732dfebe47634ab5', 'width': 960}, {'height': 540, 'url': 'https://external-preview.redd.it/nO_HyhIctY_kgV1QQh93zCg4TwEwwa7IP5Fffb6eb9k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a9a1486c26a5f5a97411a3c39701e0e4e9ef2464', 'width': 1080}], 'source': {'height': 600, 'url': 'https://external-preview.redd.it/nO_HyhIctY_kgV1QQh93zCg4TwEwwa7IP5Fffb6eb9k.jpg?auto=webp&amp;s=bf6ca66d9e4cf140e81457fdf8eb78dca4130f67', 'width': 1200}, 'variants': {}}]}",6,1639582615,1,"So after readling post after post and comment after comment here I finally jumped on the DBT bandwagon.

Did first tutorial. Setup my profile. And tested dbt run for the first pipeline.

my_first_dbt_model as a table was smoothly crested in bigquery. my_second_dbt_model as a view was nice too.

And then I checked the docs for materialized view since it is first step after data ingestion that I am creating a materialized view on top of my raw layer to convert json into actual columns.

and viola it was an open issue on their [github repo](https://github.com/dbt-labs/dbt-bigquery/issues/18). :(
They have shared a [experimental version](https://github.com/dbt-labs/dbt-labs-experimental-features/tree/master/materialized-views/macros/bigquery) and it says to put the macros scripts in the macros folder of your dbt project. My question is whats next? how to use it?

Also the next step after materialized view is something equivalent to ""sql procedures in ms sql server orchestrated by ssms"". We were hoping bigquery routines + cloud composer might do it. Now I am not sure if dbt can version the routines part? We have 10s of procedures and 100s of lines of sql transformations in each of them that we have to replicate in bigquery.

I am so confused and doubtful now.",True,False,False,dataengineering,t5_36en4,47121,public,self,DBT for Biquery. How are you hacking the materialized view thing?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rh1spn/dbt_for_biquery_how_are_you_hacking_the/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Data_Cog,,,[],,,,text,t2_9edta7qq,False,False,False,[],False,False,1639573970,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rgyt7b/data_engineering_jargon_part_5_final/,{},rgyt7b,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rgyt7b/data_engineering_jargon_part_5_final/,False,,,6,1639573981,1,"Hi - here are the final 10 and some bonus ones as requested by the community.

1-10 is [here](https://www.reddit.com/r/dataengineering/comments/rdw3b3/data_engineering_jargon/?utm_source=share&amp;utm_medium=web2x&amp;context=3)

11-20 is [here](https://www.reddit.com/r/dataengineering/comments/rem26j/data_engineering_jargon_part_2/)

21-30 is [here](https://www.reddit.com/r/dataengineering/comments/rfbuu8/data_engineering_jargon_part_3/)

31-40 is [here](https://www.reddit.com/r/dataengineering/comments/rg5vr0/data_engineering_jargon_part_4/)

41-50 is below

**41. Sandbox**

Usually refers to an environment where extensive testing can be carried out without compromising the sanctity of the live platform.

*A sandbox to prove a concept of keyboard metric before getting this accepted in a live environment.*

**42. Subject Area**

A way of defining a data model by grouping the enterprise’s data according to known business directorates.

*A customer subject area containing all customer information that can be utilised across the business*

**43. Raw Data**

This is the data as it has been collected in its rawest format before it is processed, cleansed and loaded.

*Raw data of all the customer’s orders from the day of trading*

**44. Transactional Data**

This is data that describes an actual event.

*Order placed, a delivery arranged, or a delivery accepted.*

**45. Reference Data**

This is data that allows the classification of other data.

*Country code* *GB representing Great Britain.*

**46. Master Data**

This is data that is the best representation of a particular entity in the business. This gives you a 360 view of that data entity by generally consolidating multiple data sources.

*Best customer data representation from multiple sources of information.*

**47. Structured Data**

Data that is nicely organised in a table using rows and columns, allowing the user to easily interpret the data.

*Finance data in a database table, easily queryable using SQL.*

**48. Unstructured Data**

Data that cannot be nicely organised in a tabular format, like images, PDF files etc.

*An image stored on a data lake cannot be retrieved using common data query languages.*

**49. Data Quality**

A discipline of measuring the quality of the data to improve and cleanse it.

*Checking Customer data for completeness, accuracy and validity.*

**50. Data Management**

A discipline encompassing the end-to-end management of data lifecycle, including acquiring, transferring, securing and querying data.

*Combination of improving the quality of data, governing the data, enriching and cleansing the data.*

\-------------------------

**Bonus terms:**

**51. Metadata**

This is information about the data itself.

In DE this is likely to be table names, table size, data types and sizes, column names, even constraints like Foreign and Primary Keys.

*In a table with Country information. UK or US will be the data itself, whereas the Column name ""Country"" will be the metadata*

**52. Data Marketplace**

Concept of creating a marketplace where buyers and sellers come together to trade data. This has become even more popular due to IoT data that has rapidly increased over the past decade.

*Snowflake has a data marketplace where you can buy anonymised third party data to help with your use cases. Royalmail in the UK licenses PAF (Postcode Address File) to businesses, another way of buying data.*

*I expect with blockchain for this kind of use case to become more consumer-focused. A consumer maybe able to earn crypto tokens for willingly sharing their data, so instead of the ad revenue going to Google etc. it could come to you as an end consumer.*

**53. Data Product**

Solving a business problem using (mainly) data is defined as a data product. And using data could mean anything from simple dashboards to ML models helping with recommendations on products to buy on Amazon.

*A search function on Amazon is an example of a Data Product, without well-catalogued data, this function would be useless.*

**54. Scalability**

Scalability is generally defined as the ability of the application to scale in light of changing (increasing) demands.

In DE this could mean the datawarehouse or datalake platforms. Could also mean creating scalable data pipelines.

*There are many ways of scaling a database (datawarehouse): for example indexing a table would allow fast retrieval of information as your query would not need to search every row of data. This would be a simple change for significant benefits. Another scalability example is number 55.*

**55. Database Sharding / Partitioning**

A performance improvement technique of breaking up large data sets into small subsets. 

Sharding is when this break up of data set happens across multiple machines

Partitioning is when this break up of data set happens on the singular database

The point is, if the data is divided into smaller subsets and into different machines, you are not bottlenecking the same machine with your queries each time.

\-------------------------------------------------------------------

**Parting Thoughts**

This is the final post in the series, I hope you all enjoyed reading this as much as I enjoyed writing it. I also hope you at least learnt a few new terms, it certainly helped me clarify my thinking. Thanks also to everyone that took part in the comments and helped me improve some of these definitions.

Various people have posted the original Medium link of this article, where I usually write. So feel free to check that out.

Let me know what else you'd like to learn and I could write up a future series about it.

Thank You!

1-10 is [here](https://www.reddit.com/r/dataengineering/comments/rdw3b3/data_engineering_jargon/?utm_source=share&amp;utm_medium=web2x&amp;context=3)

11-20 is [here](https://www.reddit.com/r/dataengineering/comments/rem26j/data_engineering_jargon_part_2/)

21-30 is [here](https://www.reddit.com/r/dataengineering/comments/rfbuu8/data_engineering_jargon_part_3/)

31-40 is [here](https://www.reddit.com/r/dataengineering/comments/rg5vr0/data_engineering_jargon_part_4/)",True,False,False,dataengineering,t5_36en4,47111,public,self,Data Engineering Jargon - Part 5 (Final),0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rgyt7b/data_engineering_jargon_part_5_final/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,shadow-siin,,,[],,,,text,t2_fr2304gm,False,False,False,[],False,False,1639568713,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rgx99x/handling_dynamic_structure_in_during_data/,{},rgx99x,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rgx99x/handling_dynamic_structure_in_during_data/,False,,,6,1639568724,1,"Hello,

I've run into some interesting scenario where we have data coming from a certain source and have finished ingesting 2Billion records out of 4.8B

After 2 Billion records, It was observed one of the columns which didn't have any data during sampling came up with a list data type from the source. As a result the data ingested has to be exploded after the job is finished.

I was thinking if anyone here has implemented any type of intelligence within their code so that it when it sees a list type or nested data json it explodes based on the incoming data.

&amp;#x200B;

Thanks",True,False,False,dataengineering,t5_36en4,47106,public,self,Handling dynamic structure in during data ingestion (.py),0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rgx99x/handling_dynamic_structure_in_during_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,traderdrakor,,,[],,,,text,t2_2x32e84y,False,False,False,[],False,False,1639563122,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rgvved/am_i_doing_de_work/,{},rgvved,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rgvved/am_i_doing_de_work/,False,,,6,1639563132,1,I picked up a data analyst internship at a tiny startup about 2 months ago. Currently we are trying to create a reporting SQL database streaming data from a production MongoDB. So far I have designed and implemented majority of the reporting database and is currently writing a python script to clean up and insert data into the PostgreSQL database. I am super new to this type of work and I have been primarily learning about data analytics and dba things in my degree. What type of roles am I working as. Is this DE work?,True,False,False,dataengineering,t5_36en4,47104,public,self,Am I doing DE work?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rgvved/am_i_doing_de_work/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,soobrosa,,,[],,,,text,t2_4v8u9xg7,False,False,False,[],False,False,1639556060,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rgu7yx/the_2021_pipeline_academy_awards_the_pipies/,{},rgu7yx,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rgu7yx/the_2021_pipeline_academy_awards_the_pipies/,False,self,"{'enabled': False, 'images': [{'id': 'eP246VXM-21r4A4IRg5V9jHZtkjMcjiZhlpZ-Blghak', 'resolutions': [{'height': 53, 'url': 'https://external-preview.redd.it/0CamUJbfYNiUXW0zIh_I2a3ldwhGPkLuQgDikIqwevU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=375039a910ae1f03dda031bef7af438b1139bda1', 'width': 108}, {'height': 107, 'url': 'https://external-preview.redd.it/0CamUJbfYNiUXW0zIh_I2a3ldwhGPkLuQgDikIqwevU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f029731bda0ca8a2ddccb0b5ba0bf452d6e7526d', 'width': 216}, {'height': 159, 'url': 'https://external-preview.redd.it/0CamUJbfYNiUXW0zIh_I2a3ldwhGPkLuQgDikIqwevU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=15bdcc7db9682fc32a1407524a49659830edbfc3', 'width': 320}, {'height': 319, 'url': 'https://external-preview.redd.it/0CamUJbfYNiUXW0zIh_I2a3ldwhGPkLuQgDikIqwevU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0da4143b9e44a8d75d540978225a0594792da912', 'width': 640}], 'source': {'height': 362, 'url': 'https://external-preview.redd.it/0CamUJbfYNiUXW0zIh_I2a3ldwhGPkLuQgDikIqwevU.jpg?auto=webp&amp;s=a2c33e961657152054d335316d8f61937734da05', 'width': 726}, 'variants': {}}]}",6,1639556070,1,"The 2021 Pipeline Academy Awards (The Pipies) are brought to you by Pipeline Data Engineering Academy.

Want to get data on your premises? Use Airbyte, Quix and Prefect.  
Want to rule data already on your premises? Use dbt, ClickHouse and Great Expectations.  
Want to run data products? Use Pulumi, Fly and Streamlit.  
Honorable mentions go to Saturn Cloud, GitHub Actions and Datasette.

Praise the teams with meaningful solutions for real problems.

[https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/the-pipeline-academy-awards-2021-pipies](https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/the-pipeline-academy-awards-2021-pipies)",True,False,False,dataengineering,t5_36en4,47094,public,self,The 2021 Pipeline Academy Awards (The Pipies),0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rgu7yx/the_2021_pipeline_academy_awards_the_pipies/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,CapitalistZ,,,[],,,,text,t2_99stunfc,False,False,False,[],False,False,1639555083,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rgtz2f/what_is_data_streaming/,{},rgtz2f,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rgtz2f/what_is_data_streaming/,False,,,6,1639555094,1,Is it just webhooks from your API? What's the big deal?,True,False,False,dataengineering,t5_36en4,47090,public,self,What is data streaming?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rgtz2f/what_is_data_streaming/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,wetskydig,,,[],,,,text,t2_g3jo5zvq,False,False,False,[],False,False,1639554574,lakefs.io,https://www.reddit.com/r/dataengineering/comments/rgtun6/dbt_tests_create_staging_environments_for/,{},rgtun6,False,True,False,False,False,True,False,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rgtun6/dbt_tests_create_staging_environments_for/,False,link,"{'enabled': False, 'images': [{'id': '9cr10uN3q_VZn8WAvmIXdIz21XxDwpEcpOtN7noU4EQ', 'resolutions': [{'height': 51, 'url': 'https://external-preview.redd.it/6GNV51AJxDuJM0We3PWEFqzJ1nWjG1QbHQGO_J1ReV4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b858485f5ad6369239640bc69b243bc5fc98844e', 'width': 108}, {'height': 102, 'url': 'https://external-preview.redd.it/6GNV51AJxDuJM0We3PWEFqzJ1nWjG1QbHQGO_J1ReV4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=91a735e7a25d085221d62b90a229a3bbe118948b', 'width': 216}, {'height': 151, 'url': 'https://external-preview.redd.it/6GNV51AJxDuJM0We3PWEFqzJ1nWjG1QbHQGO_J1ReV4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3f4e34787eb4826ee63b9f36b8730db17f0ec974', 'width': 320}, {'height': 302, 'url': 'https://external-preview.redd.it/6GNV51AJxDuJM0We3PWEFqzJ1nWjG1QbHQGO_J1ReV4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2e0595f64f62e3df8867717c5b5a2a7302daa1a6', 'width': 640}], 'source': {'height': 336, 'url': 'https://external-preview.redd.it/6GNV51AJxDuJM0We3PWEFqzJ1nWjG1QbHQGO_J1ReV4.jpg?auto=webp&amp;s=4375f296d21dcd6cb268668c4ddcc90821b76202', 'width': 711}, 'variants': {}}]}",6,1639554585,1,,True,False,False,dataengineering,t5_36en4,47088,public,https://b.thumbs.redditmedia.com/Ak0kdHy_MFdes69H_9TB2plloAdjdywUwjSRBEFN8vA.jpg,dbt Tests – Create Staging Environments for Flawless Data CI/CD,0,[],1.0,https://lakefs.io/dbt-tests-create-staging-environments-for-flawless-data-ci-cd/,all_ads,6,,,,,,66.0,140.0,https://lakefs.io/dbt-tests-create-staging-environments-for-flawless-data-ci-cd/,,,,,,,,,,
[],False,ayearebay,,,[],,,,text,t2_dq86u3hy,False,False,False,[],False,False,1639554431,lakefs.io,https://www.reddit.com/r/dataengineering/comments/rgttfk/dbt_tests_create_staging_environments_for/,{},rgttfk,False,False,False,False,False,False,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rgttfk/dbt_tests_create_staging_environments_for/,False,link,"{'enabled': False, 'images': [{'id': '9cr10uN3q_VZn8WAvmIXdIz21XxDwpEcpOtN7noU4EQ', 'resolutions': [{'height': 51, 'url': 'https://external-preview.redd.it/6GNV51AJxDuJM0We3PWEFqzJ1nWjG1QbHQGO_J1ReV4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b858485f5ad6369239640bc69b243bc5fc98844e', 'width': 108}, {'height': 102, 'url': 'https://external-preview.redd.it/6GNV51AJxDuJM0We3PWEFqzJ1nWjG1QbHQGO_J1ReV4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=91a735e7a25d085221d62b90a229a3bbe118948b', 'width': 216}, {'height': 151, 'url': 'https://external-preview.redd.it/6GNV51AJxDuJM0We3PWEFqzJ1nWjG1QbHQGO_J1ReV4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3f4e34787eb4826ee63b9f36b8730db17f0ec974', 'width': 320}, {'height': 302, 'url': 'https://external-preview.redd.it/6GNV51AJxDuJM0We3PWEFqzJ1nWjG1QbHQGO_J1ReV4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2e0595f64f62e3df8867717c5b5a2a7302daa1a6', 'width': 640}], 'source': {'height': 336, 'url': 'https://external-preview.redd.it/6GNV51AJxDuJM0We3PWEFqzJ1nWjG1QbHQGO_J1ReV4.jpg?auto=webp&amp;s=4375f296d21dcd6cb268668c4ddcc90821b76202', 'width': 711}, 'variants': {}}]}",6,1639554441,1,,True,False,False,dataengineering,t5_36en4,47087,public,https://b.thumbs.redditmedia.com/Ak0kdHy_MFdes69H_9TB2plloAdjdywUwjSRBEFN8vA.jpg,dbt Tests – Create Staging Environments for Flawless Data CI/CD,0,[],1.0,https://lakefs.io/dbt-tests-create-staging-environments-for-flawless-data-ci-cd/,all_ads,6,,,reddit,,,66.0,140.0,https://lakefs.io/dbt-tests-create-staging-environments-for-flawless-data-ci-cd/,,,,,,,,,,
[],False,dawarravi,,,[],,,,text,t2_3i3gcd05,False,False,False,[],False,False,1639526938,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rgl98j/best_way_to_automate_files_upload_to_s3/,{},rgl98j,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rgl98j/best_way_to_automate_files_upload_to_s3/,False,,,6,1639526949,1,"We have our financial budget in excel files that we want to push to S3. these excel are first cleaned using power bi and then manually uploaded to S3. From S3, we have already built an automated ingest process Into snowflake. 

Is there a way to automate the push to s3?",True,False,False,dataengineering,t5_36en4,47069,public,self,Best way to automate files upload to S3,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rgl98j/best_way_to_automate_files_upload_to_s3/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,caksters,,,[],,,,text,t2_tux1p,False,False,False,[],False,False,1639526332,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rgl184/how_do_you_develop_and_test_your_etl_aws_glue/,{},rgl184,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rgl184/how_do_you_develop_and_test_your_etl_aws_glue/,False,,,6,1639526343,1,"Long story short, I am very new to DE and I am working on a project as a consultant with other consultant engineers. we are very new team and we definitely don’t have a set standards/agreement on the development process.

We are developing pipelines in glue studio using interactive UI where we are transforming external redshift table (so s3 source to s3 sink). The production code will sit in a repo where ci/cd pipeline will push the code the prod environment.
We are thinking of setting up a local aws environment so we don’t have to use glue ui because 
   1. this is expensive for the client as each code iteration is run manually
   2. takes ages to execute each glue job
   3. doesn’t feel right to use interractive UI tool which lacks many features what your typical ide would have -&gt; leads to inefficient development.
   4. unable to run unittes
   5. Overall is crap dev experience 

I want to set up a local environment and already found a docker image that lets you imitate AWS glue service with aws specific spark functions.

My question is how would you test your  etl code? 

In my mind I want to mock s3 and other aws service functionality so I can run the code completely locally. But there seems to be a disagreement how we should test our etl pipeline and people don’t seem to like my idea of unit testing and mocking services. Some guys want to manually test as you go where you actually read some chunk of data from s3 and then execute AWS glue job which runs locally on your machine using docker image. Their reasoning is that this way the code will remain exactly the same which I really dont understand. If you follow TDD then unittests and mocking services dont modify your actual code, they sit in a seperate file so no refactoring is needed for your code when you push it to production as long as all your tests pass.

Is it normal to develop etl code where you test and debug it by running the code manually where you actually call real services?? I want to understand if I am being too anal by pushing everyone to write unit tests where we mock the aws services (authentification, s3, spark) instead of manually doing this.",True,False,False,dataengineering,t5_36en4,47069,public,self,How do you develop and test your ETL aws glue pipelines?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rgl184/how_do_you_develop_and_test_your_etl_aws_glue/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,stickerdebt_bot,,,[],,,,text,t2_8bqtysxs,False,False,False,[],False,False,1639520084,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rgis16/need_a_reality_check_will_focusing_on_data/,{},rgis16,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rgis16/need_a_reality_check_will_focusing_on_data/,False,,,6,1639520094,1,"Background: Started my first Data Engineer job in the fintech space six months ago. This is my first DE job but worked in analytics before. Our stack is Python, SQL, Airflow, Snowflake, AWS, git

My responsibilities so far have been largely on data ops. My core tasks have involved process documentation and ensuring our weekly Airflow pipelines run successfully. I'll watch their progress at specified times of the week, dive into any bugs, look at logs, sometimes do manual work, and implement a fix (both in our codebase and any configs needed). This takes nearly all my time per week. There's also some dev work to automate things: I have an upcoming pipeline that involves automating our alerting system by auto-creating tickets on Jira when a pipeline fails at a specific step. This is fine and I'm learning a lot as a new DE. But my manager wants me to largely focus on this side of the business.

He fully intends this to be my main focus moving forward as he splits us into subteams. There are newer DEs being aligned to other sub-teams solely working on building out our data lake/warehouse and implementing our transformation processes.

I don't touch any of that nor the actual data directly. Just pipeline management, acting on bugs, and trying to automate peripheral processes. Our team is agile, but it looks like this separation of focus will be defacto moving forward.

I know there can be a gap between expectations vs. reality of being DE, so is my concern of being pigeon holed by doing operations justified? Is this in line with your DE work and I'm just a super new DE overreacting?

I'd really like to strike a balance between DE and SWE, but I'm worried this Ops focus might affect my career/job search in the future.",True,False,False,dataengineering,t5_36en4,47070,public,self,Need a reality check: will focusing on Data Operations hurt my career progression as a DE?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rgis16/need_a_reality_check_will_focusing_on_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,thxlog4js,,,[],,,,text,t2_hkg0yit7,False,False,False,[],False,False,1639517647,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rghx33/need_a_reality_check_will_focusing_on_data/,{},rghx33,False,False,False,False,False,False,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rghx33/need_a_reality_check_will_focusing_on_data/,False,,,6,1639517658,1,[removed],True,False,False,dataengineering,t5_36en4,47069,public,self,Need a reality check: will focusing on Data Operations hurt my career progression as a DE?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rghx33/need_a_reality_check_will_focusing_on_data/,all_ads,6,,,automod_filtered,,,,,,,,,,,,,,,
[],False,Culpgrant21,,,[],,,,text,t2_1n3qfa0v,False,False,False,[],False,False,1639517630,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rghwvi/question_about_snowpipe_architecture/,{},rghwvi,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rghwvi/question_about_snowpipe_architecture/,False,,,6,1639517641,1,"Architecture: Snowflake, Azure 

Hey we are hoping to get some data streaming setup at my company for some business requirements. 

I have a decent understanding of snowpipe and how it can automatically load in files from a stage (S3/Blob Storage). 

My main confusion is around how to get the data we need into files in Blob storage. The data we need is in a Microsoft SQL Database that is the transactional system. 

Would I use Kafka to capture the changes and new data in the source system and push them into a flat file in Blob Storage?


Any information on how you use data streaming/snowpipe in your organization would be helpful!",True,False,False,dataengineering,t5_36en4,47069,public,self,Question about Snowpipe architecture,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rghwvi/question_about_snowpipe_architecture/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,relentless_bull_,,,[],,,,text,t2_7ddbtrz1,False,False,False,[],False,False,1639516874,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rghmts/looking_for_snowflakedatabricks_crash_course_will/,{},rghmts,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rghmts/looking_for_snowflakedatabricks_crash_course_will/,False,,,6,1639516884,1,"I recently got hired as a data engineer. I Would like to get familiar with using databricks and snowflake together as I primarily came from just a sql background as an analyst. 

I would like to know what a cluster is conceptually and maybe a quick crash course on how a sample workflow between the two and sample data would look like? 

Will pay for an hour of this. Just enough to understand the workflow. Thanks",True,False,False,dataengineering,t5_36en4,47065,public,self,Looking for Snowflake/Databricks crash course (will pay),0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rghmts/looking_for_snowflakedatabricks_crash_course_will/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,unknownKira2697,,,[],,,,text,t2_94f0couy,False,False,False,[],False,False,1639514682,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rgguew/freelancing/,{},rgguew,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rgguew/freelancing/,False,,,6,1639514692,1,"Hi guys,

I am looking for freelancing. Please let me know if you guys are looking for DE work.

Skills : Pyspark, Azure Data Factory, Databricks, Azure Data lake, SQL

Thank you",True,False,False,dataengineering,t5_36en4,47062,public,self,Freelancing,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rgguew/freelancing/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,ArcticDreamz,,,[],,,,text,t2_e1usl,False,False,False,[],False,False,1639511568,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rgfp83/what_equipment_to_get_for_3000/,{},rgfp83,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rgfp83/what_equipment_to_get_for_3000/,False,,,6,1639511578,1,"Hello! I'm looking for your help today, I cross-posted this to a few other subreddits too.

I am going to join a new company as a data engineer.
I'll be programming in Python, a bit of TS, and will be working with Docker a lot.
Due to the field my company is working in, my disk will need to be encrypted.

My company will give me a 3000 € budget to equip myself, and they will provide me with a ultra wide curved screen as well.

Thus, for 3000 € I am looking for a laptop, mouse and keyboard combo, and whatever other equipment that you may deem essential to office work or programming productivity.

Most of the company is currently rocking MacBook Pro's with Intel processors and 32 GB of RAM.
I've been daily driving Linux for the past 5 years, and I would like to keep it like that if possible. I can install Linux after getting the laptop, no need for it to come pre-installed. I know that not all laptops are 100 % compatible with Linux, I have heard good things about Dell XPS 15 and Lenovo Laptops.

I think I'll need at least 16GB of RAM, a 512 GB SSD and I want a portable laptop, so max 15"".

Appreciate any help you can give me.",True,False,False,dataengineering,t5_36en4,47062,public,self,What equipment to get for 3000€?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rgfp83/what_equipment_to_get_for_3000/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Complete_Solution_83,,,[],,,,text,t2_7q0rla1e,False,False,False,[],False,False,1639509196,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rgeuav/have_you_taken_courses_from_andreas_kretz_learn/,{},rgeuav,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rgeuav/have_you_taken_courses_from_andreas_kretz_learn/,False,,,6,1639509207,1,"Link : 
https://www.linkedin.com/in/andreas-kretz",True,False,False,dataengineering,t5_36en4,47060,public,self,Have you taken courses from Andreas Kretz (Learn Data Engineering)? How was it? Do you recommend it to software engineers?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rgeuav/have_you_taken_courses_from_andreas_kretz_learn/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,jakeperalta777,,,[],,,,text,t2_f5bv0pr4,False,False,False,[],False,False,1639508866,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rgept3/hiring_for_data_engineers_in_india/,{},rgept3,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rgept3/hiring_for_data_engineers_in_india/,False,,,6,1639508877,1,"Looking for &lt;5 years of exp folks for Data Engineer role in GROUPON, Bangalore, India

Skills: SQL, Spark, Python, Airflow.

Let me know if you're interested",True,False,False,dataengineering,t5_36en4,47060,public,self,Hiring for Data Engineers in India,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rgept3/hiring_for_data_engineers_in_india/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Significant-Carob897,,,[],,,,text,t2_4x8s649h,False,False,False,[],False,False,1639503311,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rgcnfo/dozens_of_python_scripts_to_extract_data_and_load/,{},rgcnfo,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rgcnfo/dozens_of_python_scripts_to_extract_data_and_load/,False,,,6,1639503322,1,"Old setup was: on premise sql server with scripts residing in C drive. Run on schedule via SSMS.

New Setup: Anything on GCP,  that is faster, scaleable and modular (should be an improvement from old setup).

We were thinking Google Cloud Functions and then do scheduling via Google Cloud Composer (Airflow).

But while looking at Composer, I realized cloud composer can also have pythin scripts in its directory (or this is what I understood).

So my question is what is the best place to put dozens python scripts that extract data and push json to bigquery.

About these scripts:
- They have to fetch API keys from somewhere (we were using db tables in old setup)
- They would be running every hour at the minimum.
- All extractions are independant of each other.
- All extractions have number of things in common e.g. bigquery insertion function, bigquery insertion format, logging format, extraction function.


All the raw data will then be pushed from raw layer to history layer via bigquery routines. Can sql procedures be written and run airflow too? Or bigquery routines run from airflow too? So we can complete move to Airflow.


Thanks in advance for the help. Just trying to make the right decisions for atleast next 3-5 years in terms of code modularity, scalability and business deliverables.",True,False,False,dataengineering,t5_36en4,47055,public,self,Dozens of python scripts to extract data and load to bigquery,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rgcnfo/dozens_of_python_scripts_to_extract_data_and_load/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,randombetch,,,[],,,,text,t2_toovr,False,False,False,[],False,False,1639501134,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rgbui2/can_someone_explain_like_im_5_why_do_i_need_both/,{},rgbui2,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rgbui2/can_someone_explain_like_im_5_why_do_i_need_both/,False,,,6,1639501144,1,"MongoDB = noSQL data base

Data lake (take Delta Lake for example) = place to through all your data

Then you have a query processor on top of a data lake to bring your data to your BI/applications/etc.

&amp;#x200B;

I thought a data lake was just reselling compute power from AWS/GCP/Azure and helping folks organize and process data.  What does MongoDB do, and how does it fit in between a data lake and AWS/GCP/Azure?",True,False,False,dataengineering,t5_36en4,47051,public,self,Can someone explain like I'm 5 - why do I need both MongoDB and a data lake?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rgbui2/can_someone_explain_like_im_5_why_do_i_need_both/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Valkyrja-Kara,,,[],,,,text,t2_hcejskpo,False,False,False,[],False,False,1639497467,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rgailc/delta_lake_how_little_data_us_too_little/,{},rgailc,False,False,False,False,False,False,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rgailc/delta_lake_how_little_data_us_too_little/,False,,,6,1639497478,1,[removed],True,False,False,dataengineering,t5_36en4,47045,public,self,Delta Lake: how little data us too little?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rgailc/delta_lake_how_little_data_us_too_little/,all_ads,6,,,automod_filtered,,,,,,,,,,,,,,,
[],False,BlancBryn,,,[],,,,text,t2_1i178g26,False,False,False,[],False,False,1639493509,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rg93y8/question_about_airflow_data_pipelines/,{},rg93y8,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rg93y8/question_about_airflow_data_pipelines/,False,,,6,1639493520,1,"Hi all,

I am currently setting up Airflow on an Azure Linux VM.

The first I want to do is to execute some Python scripts which extract data from REST API.

By now the script is only running locally in Jupyter Notebook.

The result is by now stored in a pandas dataframe and saved as csv.

&amp;#x200B;

How would you recommend me to store the result by using Airflow?

Is it possible / recommended to store it on the VM in the preinstalled postgresDB?",True,False,False,dataengineering,t5_36en4,47043,public,self,Question about Airflow Data Pipelines,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rg93y8/question_about_airflow_data_pipelines/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,unsaltedrhino,,,[],,,,text,t2_9or07,False,False,False,[],False,False,1639490821,carto.com,https://www.reddit.com/r/dataengineering/comments/rg88sr/carto_raises_61m_to_lead_the_way_in_cloud_native/,{},rg88sr,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rg88sr/carto_raises_61m_to_lead_the_way_in_cloud_native/,False,link,"{'enabled': False, 'images': [{'id': 'gVqTLJtkiO9omD5n_RZMOa_mWj2okKoL6WCUPGV2tPU', 'resolutions': [{'height': 54, 'url': 'https://external-preview.redd.it/MxmuEKsVNAz1Wn8aEZ-tRlhoetsqDvXebIQ43gJNcBk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=afffceea12b4a101b6056c2849e9f4059b9ec4b6', 'width': 108}, {'height': 108, 'url': 'https://external-preview.redd.it/MxmuEKsVNAz1Wn8aEZ-tRlhoetsqDvXebIQ43gJNcBk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9c257f6fa9285a331b9ce9276861abbf3044de5a', 'width': 216}, {'height': 160, 'url': 'https://external-preview.redd.it/MxmuEKsVNAz1Wn8aEZ-tRlhoetsqDvXebIQ43gJNcBk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=51a0f365d6f849d5abb62661080184e442ef7758', 'width': 320}, {'height': 320, 'url': 'https://external-preview.redd.it/MxmuEKsVNAz1Wn8aEZ-tRlhoetsqDvXebIQ43gJNcBk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=17c556e4375041e391a7fd6acc7acd863e8f3f74', 'width': 640}, {'height': 480, 'url': 'https://external-preview.redd.it/MxmuEKsVNAz1Wn8aEZ-tRlhoetsqDvXebIQ43gJNcBk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e6f9cb60372546bee692fe9108916f27b65875fc', 'width': 960}, {'height': 540, 'url': 'https://external-preview.redd.it/MxmuEKsVNAz1Wn8aEZ-tRlhoetsqDvXebIQ43gJNcBk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1d508b7b316167ac8e29b076c49ac425e8d2dca9', 'width': 1080}], 'source': {'height': 1024, 'url': 'https://external-preview.redd.it/MxmuEKsVNAz1Wn8aEZ-tRlhoetsqDvXebIQ43gJNcBk.jpg?auto=webp&amp;s=4935035387de13b30bd4d001914b6978f5f6971e', 'width': 2048}, 'variants': {}}]}",6,1639490831,1,,True,False,False,dataengineering,t5_36en4,47041,public,https://b.thumbs.redditmedia.com/l6U2GaPCc0WPlCW4-RM2n6w6nigNGL6lUVo_74es6kE.jpg,CARTO raises $61M to lead the way in cloud native spatial analytics,0,[],1.0,https://carto.com/blog/carto-closes-series-c-round/,all_ads,6,,,,,,70.0,140.0,https://carto.com/blog/carto-closes-series-c-round/,,,,,,,,,,
[],False,e4ds,,,[],,,,text,t2_empiv8dh,False,False,False,[],False,False,1639488246,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rg7epg/event_driven_data_validation_with_google_cloud/,{},rg7epg,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rg7epg/event_driven_data_validation_with_google_cloud/,False,self,"{'enabled': False, 'images': [{'id': '-MhwPnbfhP1HjSg5yHNV88yYnnn7rd2eguVdzuZGZAo', 'resolutions': [{'height': 30, 'url': 'https://external-preview.redd.it/Um9pBJkO-nd7GZCEISlbrtzoaFmHtjoxwDkMDq1FQSs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=da16b3369d7804de325918ca9db3281a42b1cfeb', 'width': 108}, {'height': 61, 'url': 'https://external-preview.redd.it/Um9pBJkO-nd7GZCEISlbrtzoaFmHtjoxwDkMDq1FQSs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e85055793f48b08efa49b907dd6e47642ac14372', 'width': 216}, {'height': 90, 'url': 'https://external-preview.redd.it/Um9pBJkO-nd7GZCEISlbrtzoaFmHtjoxwDkMDq1FQSs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3ad8127bed79a2c284e1f3e10a7ceb1b2020458b', 'width': 320}, {'height': 181, 'url': 'https://external-preview.redd.it/Um9pBJkO-nd7GZCEISlbrtzoaFmHtjoxwDkMDq1FQSs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e0f132d9fca2613456d4e449ba5678b4ebfd8dc2', 'width': 640}], 'source': {'height': 261, 'url': 'https://external-preview.redd.it/Um9pBJkO-nd7GZCEISlbrtzoaFmHtjoxwDkMDq1FQSs.jpg?auto=webp&amp;s=1ac3c6f993acbefa8a08dfc479889698baeca0e4', 'width': 920}, 'variants': {}}]}",6,1639488257,1,"[https://engineeringfordatascience.com/posts/event\_driven\_data\_validation\_with\_google\_cloud\_functions\_and\_great\_expectations/](https://engineeringfordatascience.com/posts/event_driven_data_validation_with_google_cloud_functions_and_great_expectations/)

Using Google Cloud Functions and Great Expectations to validate data landing in a data lake on GCP.

One of my first times using Great Expectations. And, if I'm honest, I didn't find it the easiest library to work with. But I quite like this solution for automating testing of data landing in the data lake using serverless architecture.

Would appreciate any comments/feedback!",True,False,False,dataengineering,t5_36en4,47038,public,self,Event Driven Data Validation with Google Cloud Functions and Great Expectations,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rg7epg/event_driven_data_validation_with_google_cloud/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Data_Cog,,,[],,,,text,t2_9edta7qq,False,False,False,[],False,False,1639482765,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rg5vr0/data_engineering_jargon_part_4/,{},rg5vr0,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rg5vr0/data_engineering_jargon_part_4/,False,,,6,1639482775,1,"Hi - next 10

1-10 is [here](https://www.reddit.com/r/dataengineering/comments/rdw3b3/data_engineering_jargon/?utm_source=share&amp;utm_medium=web2x&amp;context=3)

11-20 is [here](https://www.reddit.com/r/dataengineering/comments/rem26j/data_engineering_jargon_part_2/)

21-30 is [here](https://www.reddit.com/r/dataengineering/comments/rfbuu8/data_engineering_jargon_part_3/)

31-40 is below

**31. Data Replication**

There are multiple ways to do this, but mainly it is a practice of copying data to multiple servers to protect an organisation against data loss.

*Replicating the customer information across two databases to ensure their core details are not lost.*

**32. Big Data**

A term coined for large amounts of data that cannot be processed using traditional databases. Refer to Data Lake.

*Hadoop Data Lake stores all the information received from sensors in a smart fridge.*

**33. Hive**

Apache Hive is a data warehouse open-source project that allows querying large amounts of data. Like SQL, it uses an easy-to-understand language called Hive QL.

*SELECT \* from tbl; returns all rows and columns from a data store like HDFS.*

**34. HDFS**

Hadoop Distributed File System is a data storage system used by Hadoop. It provides flexibility to manage structured or unstructured data.

*Storing large amounts of financial transactional data in an HDFS to query using Hive QL.*

**35. NiFi**

It is an open-source extract, transform and load tool (refer to ETL); this allows filtering, integrating and joining data.

*Moving postcode data from a .csv file to HDFS using NiFi.*

**36. Kafka**

It is more complex to work with than NiFi as it doesn’t have a user interface (UI), mainly used for real-time streaming data. It is a messaging system first created by LinkedIn engineers.

*Streaming real-time weather events using Kafka*

**37. Flat File**

Flat files are commonly used to transfer data due to their basic nature; flat files are a single table storing data in a plain text format.

*All customer order numbers stored in a comma-separated value (.csv) file*

**38. Latency**

The time it takes for a database or a web application to respond to a query or a click.

*Takes 30 seconds to query a database with 5 million records.*

**39. Caching**

This is when limited data is stored on the RAM to allow for quick retrieval of information.

*In-memory caching of data in a database returns results to query 100 times faster.*

**40. Staging**

The name of a storage area that is temporary in nature; to allow for processing of ETL jobs (refer to ETL).

*A staging area in an ETL routine allows data to be cleaned before loading into the final tables.*

1-10 is [here](https://www.reddit.com/r/dataengineering/comments/rdw3b3/data_engineering_jargon/?utm_source=share&amp;utm_medium=web2x&amp;context=3)

11-20 is [here](https://www.reddit.com/r/dataengineering/comments/rem26j/data_engineering_jargon_part_2/)

21-30 is [here](https://www.reddit.com/r/dataengineering/comments/rfbuu8/data_engineering_jargon_part_3/)",True,False,False,dataengineering,t5_36en4,47032,public,self,Data Engineering Jargon - Part 4,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rg5vr0/data_engineering_jargon_part_4/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,anyfactor,,,[],,,,text,t2_wkoxw,False,False,False,[],False,False,1639482298,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rg5rjp/just_got_fired/,{},rg5rjp,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rg5rjp/just_got_fired/,False,,,6,1639482308,1,"Entry level. Offshore. Remote. 25 bucks an hour. Contract.

I signed an NDA and I am not mentioning any name of the company or confidential or any specific information so I know I am good. If the client comes across this and is bitter and wants to have a fight about this he has my email.

So, I joined this 100% remote startup. They wanted to hire a fullstack developer but I begged and convinced my way into a python+data engineering role. They had me on probation for a 2 week period.

Day 0

During the onboarding interview I was given a ""simple"" task where I needed to generate two analytics table each time they made a request to saturate their database post analysis.

I was told that all I needed to run `main.py` and everything is ready to go. They have some helper documentation on the README.

Day 1

So, I get started and things start to go south. First they didn't assign me with access permissions to the cloud. I waste some time figuring that out. Then the docker file shows that I need to setup registry stuff so I can setup the machine learning models. That took some while to figure that out. I also needed to setup config dot files for that. That doesn't work.

At this moment I have spent a day but I logged around 4.5 half hours. The senior dev was helpful but he was super busy. The client comes back and gets mad as all I needed to run the `main.py` file and that was supposed to be it. I said we were trying to set up Docker for the python version, the machine learning libraries and python packages. He says run the requirement file. The machine learning libraries calls the database processes and sends it back. I needed to create an analytics log in the middle of that.

Day 2 (Weekend)

If you use multiple python versions in a project the requirements.txt installation using pip installing isn't exactly straightforward. After 2 hours of trying to figure this out I discover that they were using two versions of Python and for the requirements file they used one package that wasn't supported by the python version they recommended me. I change the version of that package and finally installed the packages.

This might sound like an easy fix but I urge to try this out.


Day 3

I run the file it doesn't work of course. Because this entire repo heavily depends on the Docker file and docker file sets up aliases for certain API calls the file will never work straight. I try my best to discover my way through the jungle that is that codebase. And even still I get permission issues. I get one permission issue which was sorted. And the entire file doesn't run anyway because I wasn't granted full permission of something.

I say I am not getting stuff done and the client gets mad. He says that, my skills aren't that strong as I have indicated in my interview.

How the heck am I supposed to react to that. I said please re-read our conversation. How am I supposed to solve this issue without permission and configuration issues ironed out.

In the begging of the project they have provided the schema for the analytics they needed, so it should be an easy task. I said, I can provide a code but I will be working be working blind. They said, I have all the necessary things for me to get the job done.

Based on a schema alone, I comment out the sections that doesn't work and I write a code that outputs a CSV. At this moment I let the client know let me know when I should leave. They indicate it is going to happen. And they said, what do you mean by re-reading our conversation.

Day 4 (Today)

I intentionally didn't reply to what I meant by re-reading thread. Yeah. Bad thing to say, but what am I supposed to say when the client thinks I am not getting things done while simultaneously locking me out from the very place I need to get things done. And moreover questioning my skill just felt bad to me.

I comment out sections of code to find what is going on. They are simultaneously fetching and merging data to aggregate data upon layers upon layers with multiple modules.

Then I discover they have another cloud service that they are using that I didn't knew I even needed access for. I said that I have found the problem for the issue and I need access to the service.

I get the message, ""Pack your things and go"".

---

Overall I logged about 10 hours for this but I set their on idle about double that time and obviously didn't get paid. Of the two task I did the first one took me 20 minutes to do. But the setup and configuration and code discovery took up the entire logged time.

The first day, I joined slack, I came across a few profile and all of them were deactivated. Scrolling up I saw guy quitting under 3 hours. 

I don't plan to work for another company that supposedly doesn't provide some level of mentorship. I don't want to work with someone who isn't technically competent and is not willing to guide me.

I understand I am supposed to be an expert because my rates are WAYYYYYYY too high as an offshore dev. But not again am I working without pair programming, routine video chats or supporting seniors.

I really despise freelancing.",False,False,False,dataengineering,t5_36en4,47032,public,self,Just got fired.,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rg5rjp/just_got_fired/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,nfrankel,,,[],,,,text,t2_ayl6m,False,False,False,[],False,False,1639481187,infoq.com,https://www.reddit.com/r/dataengineering/comments/rg5hro/hazelcast_announces_a_new_unified_platform_with/,{},rg5hro,False,True,False,False,False,True,False,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rg5hro/hazelcast_announces_a_new_unified_platform_with/,False,link,"{'enabled': False, 'images': [{'id': 'XVOjRZRG86C9H_jOtWz38bMQ_iA5fk4JHYZryjSUVI4', 'resolutions': [{'height': 56, 'url': 'https://external-preview.redd.it/g3BqlLesLwUxAMyhgRPDJ_gMb8zWXUtoccZCMN2cHlk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=63f4ac689d84db6fb74aa8fab8b03b36c880ae80', 'width': 108}, {'height': 113, 'url': 'https://external-preview.redd.it/g3BqlLesLwUxAMyhgRPDJ_gMb8zWXUtoccZCMN2cHlk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4c553b3067e9ebdb0c2dc70eb7ebdf9f31778681', 'width': 216}, {'height': 168, 'url': 'https://external-preview.redd.it/g3BqlLesLwUxAMyhgRPDJ_gMb8zWXUtoccZCMN2cHlk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a77f7129c7a28e2f4362304ba3fa18d89ecfbdb8', 'width': 320}, {'height': 336, 'url': 'https://external-preview.redd.it/g3BqlLesLwUxAMyhgRPDJ_gMb8zWXUtoccZCMN2cHlk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=86728446a1adf12877e97e2067f44940af2f505c', 'width': 640}, {'height': 504, 'url': 'https://external-preview.redd.it/g3BqlLesLwUxAMyhgRPDJ_gMb8zWXUtoccZCMN2cHlk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=32d8ced0f225108d1b49f350add11ac899db490f', 'width': 960}, {'height': 567, 'url': 'https://external-preview.redd.it/g3BqlLesLwUxAMyhgRPDJ_gMb8zWXUtoccZCMN2cHlk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=638733e11d3b11eccc6f83867bd846d05942dcb1', 'width': 1080}], 'source': {'height': 630, 'url': 'https://external-preview.redd.it/g3BqlLesLwUxAMyhgRPDJ_gMb8zWXUtoccZCMN2cHlk.jpg?auto=webp&amp;s=4850c9ed2e03b8469cc4dc0a64d2cdf37c5117e8', 'width': 1200}, 'variants': {}}]}",6,1639481197,1,,True,False,False,dataengineering,t5_36en4,47030,public,https://b.thumbs.redditmedia.com/M37mseBcMREjqPV2zP8M6J455Rc0ZwGC4YCTWeL_C2s.jpg,Hazelcast Announces a New Unified Platform with Version 5.0,0,[],1.0,https://www.infoq.com/news/2021/12/hazelcast-5-unified-platform/,all_ads,6,,,,,,73.0,140.0,https://www.infoq.com/news/2021/12/hazelcast-5-unified-platform/,,,,,,,,,,
[],False,Chiefjack98,,,[],,,,text,t2_acc1u0w7,False,False,False,[],False,False,1639480895,tyny.dev,https://www.reddit.com/r/dataengineering/comments/rg5far/using_types_of_data_why_tynydev_is_the_most/,{},rg5far,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rg5far/using_types_of_data_why_tynydev_is_the_most/,False,link,"{'enabled': False, 'images': [{'id': 'MxI1aKaPSptwQLzxn0DXaDB2CJxtoLN2z4NGegMEK6s', 'resolutions': [{'height': 72, 'url': 'https://external-preview.redd.it/k33kl6yGX4Okot2WgQGG62Ixdbkulvi8VzUqtD2BoOQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=96894604589e747d7929ca97e1ee6f80459e9c4a', 'width': 108}, {'height': 144, 'url': 'https://external-preview.redd.it/k33kl6yGX4Okot2WgQGG62Ixdbkulvi8VzUqtD2BoOQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=eb8a88341e58d68c9212c912bd592d4a6a19253a', 'width': 216}, {'height': 213, 'url': 'https://external-preview.redd.it/k33kl6yGX4Okot2WgQGG62Ixdbkulvi8VzUqtD2BoOQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1c657c995602425236ce88d555312542f817a242', 'width': 320}, {'height': 426, 'url': 'https://external-preview.redd.it/k33kl6yGX4Okot2WgQGG62Ixdbkulvi8VzUqtD2BoOQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=97ba762202293dcc1ec521d9bcf3de6e7bce6387', 'width': 640}, {'height': 640, 'url': 'https://external-preview.redd.it/k33kl6yGX4Okot2WgQGG62Ixdbkulvi8VzUqtD2BoOQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ad012a39c448046d60e1d3979d288e03ba66f34d', 'width': 960}, {'height': 720, 'url': 'https://external-preview.redd.it/k33kl6yGX4Okot2WgQGG62Ixdbkulvi8VzUqtD2BoOQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2d60c850eb542bb42f264c45f879fa4533dca136', 'width': 1080}], 'source': {'height': 736, 'url': 'https://external-preview.redd.it/k33kl6yGX4Okot2WgQGG62Ixdbkulvi8VzUqtD2BoOQ.jpg?auto=webp&amp;s=78af2e93b674793c00d4a6350a7c265582ee6e96', 'width': 1104}, 'variants': {}}]}",6,1639480906,1,,True,False,False,dataengineering,t5_36en4,47030,public,https://b.thumbs.redditmedia.com/SpVFdyRJxCRm_Ba2zPuspkXVhgUVDzU2NZd_wxsDyqo.jpg,Using Types of Data: Why tyny.dev is the Most Suitable API for Employing Unstructured Data,0,[],1.0,https://tyny.dev/blog/using-types-of-data-why-tyny-dev-is-the-most-suitable-api-for-employing-unstructured-data,all_ads,6,,,,,,93.0,140.0,https://tyny.dev/blog/using-types-of-data-why-tyny-dev-is-the-most-suitable-api-for-employing-unstructured-data,,,,,,,,,,
[],False,mannu_11,,,[],,,,text,t2_3dsjwu75,False,False,False,[],False,False,1639470245,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rg2xj0/s3_to_s3_transformation/,{},rg2xj0,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rg2xj0/s3_to_s3_transformation/,False,,,6,1639470255,1,"We are trying to build a scalable and generic framework for S3-S3 transformation. 
Currently we have AWS lambda and Databricks up for the competition!
Thoughts on this?",True,False,False,dataengineering,t5_36en4,47021,public,self,S3 to S3 transformation,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rg2xj0/s3_to_s3_transformation/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,BlancBryn,,,[],,,,text,t2_1i178g26,False,False,False,[],False,False,1639470011,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rg2vmb/how_to_build_a_commission_calculator_how_to/,{},rg2vmb,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rg2vmb/how_to_build_a_commission_calculator_how_to/,False,,,6,1639470021,1,"Hello all,

I need to automate commission payments in my company. For this I need to analyze the sales data and apply various KPIs to it. On the other hand, this must be historized in a traceable way and individual recalculations must be possible.

Currently, I work a lot with SQL and have the result for a certain period calculated per procedure for each department. For this, there is a lot of input data that I have to check for completeness beforehand. In addition, the calculation model changes often and there are many exceptions.

Last but not least I have to generate a PDF sheet with a summary for each employee.

1. Have you ever had experience with something like this? Do you have any tips and tricks on how to build such a framework? What would be your approach?

2. Do you know solutions how to automatically print a PDF sheet per employee? If yes, which ones?",True,False,False,dataengineering,t5_36en4,47021,public,self,How to build a commission calculator &amp; how to automate PDF sheet creation?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rg2vmb/how_to_build_a_commission_calculator_how_to/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,SeattleDataGuy,,,[],,,,text,t2_b003dzgv,False,False,False,[],False,False,1639458954,youtube.com,https://www.reddit.com/r/dataengineering/comments/rg00ld/my_favorite_books_for_data_engineers_from/,{},rg00ld,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rg00ld/my_favorite_books_for_data_engineers_from/,False,rich:video,"{'enabled': False, 'images': [{'id': 'OeIMTNq89Cnf9OzcBlrJm5J5XqkQvIPXkO6zuggkOFk', 'resolutions': [{'height': 81, 'url': 'https://external-preview.redd.it/mcDoMUij3KyAONgvs1AJjTFkBtWBsfb82wkaYB7rtBs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b8a4a1481e827b65e78c18dd6c8345d4103e99c8', 'width': 108}, {'height': 162, 'url': 'https://external-preview.redd.it/mcDoMUij3KyAONgvs1AJjTFkBtWBsfb82wkaYB7rtBs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d15b141569cc1447f5b6aba3568762cbb4e4ad43', 'width': 216}, {'height': 240, 'url': 'https://external-preview.redd.it/mcDoMUij3KyAONgvs1AJjTFkBtWBsfb82wkaYB7rtBs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9b6a5b93919a8b5e594da58bac895f2b5f3bf3e7', 'width': 320}], 'source': {'height': 360, 'url': 'https://external-preview.redd.it/mcDoMUij3KyAONgvs1AJjTFkBtWBsfb82wkaYB7rtBs.jpg?auto=webp&amp;s=d1a51ddacb9b13d5d37c4b96a5c665e6083b4dd0', 'width': 480}, 'variants': {}}]}",6,1639458964,1,,True,False,False,dataengineering,t5_36en4,47007,public,https://b.thumbs.redditmedia.com/WWWLNeF1Ju4JhzNiAhLlU2KIpmukuEhuJL7LLAhBiOI.jpg,My Favorite Books For Data Engineers - From Streaming To Software Engineering,0,[],1.0,https://www.youtube.com/watch?v=xtfuO7kGJeY,all_ads,6,"{'oembed': {'author_name': 'Seattle Data Guy', 'author_url': 'https://www.youtube.com/c/SeattleDataGuy', 'height': 200, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/xtfuO7kGJeY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/xtfuO7kGJeY/hqdefault.jpg', 'thumbnail_width': 480, 'title': 'My Favorite Books For Data Engineers - From Streaming To Software Engineering', 'type': 'video', 'version': '1.0', 'width': 356}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/xtfuO7kGJeY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'scrolling': False, 'width': 356}",,"{'oembed': {'author_name': 'Seattle Data Guy', 'author_url': 'https://www.youtube.com/c/SeattleDataGuy', 'height': 200, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/xtfuO7kGJeY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/xtfuO7kGJeY/hqdefault.jpg', 'thumbnail_width': 480, 'title': 'My Favorite Books For Data Engineers - From Streaming To Software Engineering', 'type': 'video', 'version': '1.0', 'width': 356}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/xtfuO7kGJeY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/rg00ld', 'scrolling': False, 'width': 356}",105.0,140.0,https://www.youtube.com/watch?v=xtfuO7kGJeY,,,,,,,,,,
[],False,hhn2505,,,[],,,,text,t2_43sambxt,False,False,False,[],False,False,1639448987,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rfwvea/getting_started_with_data_engineering/,{},rfwvea,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rfwvea/getting_started_with_data_engineering/,False,,,6,1639448998,1,"I want to get started with data engineering. When I looked on the internet for roadmaps and stuff, I have narrowed in on the following technologies that need to be learnt apart from SQL , Nosql and Linux scripting:
1) Hadoop 
2) Apache spark
3) spark streaming
4)  Kafka 
5) hive
I have the following three questions to ask:

1) have I skipped any of the technologies that I need to learn.

2) what is the best resource to learn these technologies so that I can start applying them to projects.

3) do projects that process/analyze large( 10+ GB) look ok on the resume even though streaming data isn't used?

Also, I know that aws or gcp must be learnt, but I feel that it's better to go 1 step at a time.",True,False,False,dataengineering,t5_36en4,46999,public,self,Getting started with data engineering,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rfwvea/getting_started_with_data_engineering/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,d3fmacro,,,[],,,,text,t2_4h8ymgx,False,False,False,[],False,False,1639431623,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rfqxhh/why_openmetadata_is_the_right_choice_for_you/,{},rfqxhh,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rfqxhh/why_openmetadata_is_the_right_choice_for_you/,False,self,"{'enabled': False, 'images': [{'id': 'dKiW5EN1_Veap673spqehNkfJC5-27g4VCX8Zjyv33c', 'resolutions': [{'height': 37, 'url': 'https://external-preview.redd.it/YE3bYmcZ0dXjn2hLSBJRJwRFYv3FGpwN5MERV-81hE4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=04aa59ac7c53cd2982e43984c701ae3af17add4a', 'width': 108}, {'height': 74, 'url': 'https://external-preview.redd.it/YE3bYmcZ0dXjn2hLSBJRJwRFYv3FGpwN5MERV-81hE4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5af7db5bddd5b1bbc8c6b027a95d4a7274ada8b6', 'width': 216}, {'height': 110, 'url': 'https://external-preview.redd.it/YE3bYmcZ0dXjn2hLSBJRJwRFYv3FGpwN5MERV-81hE4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d655575c052b68f7a3d0c41f8b2dcc62027605ac', 'width': 320}, {'height': 220, 'url': 'https://external-preview.redd.it/YE3bYmcZ0dXjn2hLSBJRJwRFYv3FGpwN5MERV-81hE4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fbffa71f423a1d392ab50896f8b3eee0beae2bff', 'width': 640}, {'height': 330, 'url': 'https://external-preview.redd.it/YE3bYmcZ0dXjn2hLSBJRJwRFYv3FGpwN5MERV-81hE4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d9e5264d5c7fad628fd8fa91a8c9b7456fb48088', 'width': 960}, {'height': 371, 'url': 'https://external-preview.redd.it/YE3bYmcZ0dXjn2hLSBJRJwRFYv3FGpwN5MERV-81hE4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=779f8b438fd49426c40614baab286a4e770604d5', 'width': 1080}], 'source': {'height': 413, 'url': 'https://external-preview.redd.it/YE3bYmcZ0dXjn2hLSBJRJwRFYv3FGpwN5MERV-81hE4.jpg?auto=webp&amp;s=ef39c23eb8bd429cccd329d826993eced6ee544e', 'width': 1200}, 'variants': {}}]}",6,1639431634,1,"If you are interested in metadata management and data discovery, quality and collaboration, we wrote a blog post [https://blog.open-metadata.org/why-openmetadata-is-the-right-choice-for-you-59e329163cac](https://blog.open-metadata.org/why-openmetadata-is-the-right-choice-for-you-59e329163cac) on why [https://github.com/open-metadata/OpenMetadata](https://github.com/open-metadata/OpenMetadata) is the right choice. Please take a look and if you are interested in learning more about the project or any feature requests etc.. please join our community [https://slack.open-metadata.org](https://slack.open-metadata.org)",True,False,False,dataengineering,t5_36en4,46990,public,self,Why OpenMetadata is the Right Choice for you,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rfqxhh/why_openmetadata_is_the_right_choice_for_you/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,w_savage,,,[],,,,text,t2_1afmkbx9,False,False,False,[],False,False,1639428869,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rfptu5/need_some_advice_for_year_end_salary_negotiations/,{},rfptu5,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rfptu5/need_some_advice_for_year_end_salary_negotiations/,False,,,6,1639428880,1,"Some context about me and my position:

I was hired as a Data Analyst making 80k yearly USD. About 6 - 7 months in I started helping out our data engineering team. They have given me an incredible opportunity to pivot to become a data engineer. I've mainly been using python to build data pipelines in AWS. Mainly using lambda. Also been helping with different bug fixes and error handling in addition to building pipelines and automating tasks. I don't have a Comp Sci background, 100% self taught in programming as of now. Also, a more senior QA Engineer has been mentoring me in all things DE. My new boss on the data engineering team has said they will hire and backfill my data analyst responsibilities relatively soon. I've been doing half and half analyst/engineering stuff. 

Recently I've asked them if I should change my title to better reflect my DE abilities, since I'm going more that direction now. They said I can change it to Jr. Data Engineer. 

My question for the community is what is a fair salary for a Jr. Data Engineer and how can I position myself to negotiate for it? My goal for next year is to eventually get the ""Jr"" dropped. I don't want to leave this company in search for a self promotion, love the company and culture so far. 

thanks!",True,False,False,dataengineering,t5_36en4,46986,public,self,Need some advice for year end salary negotiations,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rfptu5/need_some_advice_for_year_end_salary_negotiations/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,fireking_24,,,[],,,,text,t2_6amq2ewv,False,False,False,[],False,False,1639419591,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rfm91s/data_engineer_intern_interview_tips_adobe/,{},rfm91s,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rfm91s/data_engineer_intern_interview_tips_adobe/,False,,,6,1639419601,1,"Hello everyone!

I have an upcoming interview for data engineer intern with Adobe. Can anyone give some tips or suggestions for preparation. Thanks in advance",True,False,False,dataengineering,t5_36en4,46970,public,self,Data engineer intern interview tips - Adobe!,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rfm91s/data_engineer_intern_interview_tips_adobe/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Purple_Excitement_10,,,[],,,,text,t2_fdlhj0j5,False,False,False,[],False,False,1639417397,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rfldjv/show_your_love_for_sql/,{},rfldjv,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rfldjv/show_your_love_for_sql/,False,self,"{'enabled': False, 'images': [{'id': 'GSJAEusT718oYt8OFkYFdF-GM9YZkJb6mcpFm88tTLs', 'resolutions': [{'height': 56, 'url': 'https://external-preview.redd.it/iFnkFM6uLqjNGY5lTZa-0XClPZ0U8uQJZ1ASuPBuvy4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=882359743f0bf292a80f61d49144b4be0ced1aec', 'width': 108}, {'height': 112, 'url': 'https://external-preview.redd.it/iFnkFM6uLqjNGY5lTZa-0XClPZ0U8uQJZ1ASuPBuvy4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6e867ab0280d800be68789177eb93b1c9d039a01', 'width': 216}, {'height': 167, 'url': 'https://external-preview.redd.it/iFnkFM6uLqjNGY5lTZa-0XClPZ0U8uQJZ1ASuPBuvy4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=37435637269b7e3f56e319df0413c4f3e4555fde', 'width': 320}, {'height': 334, 'url': 'https://external-preview.redd.it/iFnkFM6uLqjNGY5lTZa-0XClPZ0U8uQJZ1ASuPBuvy4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ee245be2fd5ee5aeb8b2f491ac9991f142482023', 'width': 640}, {'height': 501, 'url': 'https://external-preview.redd.it/iFnkFM6uLqjNGY5lTZa-0XClPZ0U8uQJZ1ASuPBuvy4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e30b1d12ef8f82fde0ef78e03e3901a7dc1bab7d', 'width': 960}, {'height': 564, 'url': 'https://external-preview.redd.it/iFnkFM6uLqjNGY5lTZa-0XClPZ0U8uQJZ1ASuPBuvy4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6d7385266f63a9301ede1add99fbce6bb162c532', 'width': 1080}], 'source': {'height': 1254, 'url': 'https://external-preview.redd.it/iFnkFM6uLqjNGY5lTZa-0XClPZ0U8uQJZ1ASuPBuvy4.jpg?auto=webp&amp;s=9740fad028a5e957188c50ee59223253bf4fdc0e', 'width': 2400}, 'variants': {}}]}",6,1639417407,1,"Me and my team at [Census](https://www.getcensus.com/) have build a free feature called [sync-a-swag](https://blog.getcensus.com/sync-a-swag-use-operational-analytics-to-receive-free-census-clothing-gifts-and-more/). In a nutshell, you can write a SQL query that will send a free t-shirt right to your doorstep.   


No gimmicks. This is totally legitimate. Please give it a try!    


This [link](https://www.getcensus.com/sync-a-swag) provides step-by-step video tutorials. If you prefer documentation, you can complete sync-a-swag [here](https://docs.getcensus.com/destinations/sync-a-swag).",True,False,False,dataengineering,t5_36en4,46969,public,self,Show your love for SQL!,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rfldjv/show_your_love_for_sql/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,wytesmurf,,,[],,,,text,t2_ldunk,False,False,False,[],False,False,1639416256,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rfkwy1/using_python_for_data_engineering/,{},rfkwy1,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rfkwy1/using_python_for_data_engineering/,False,,,6,1639416267,1,I have always stuck to basic ETL tools and SQL for data loading. I read articles about people doing massive data loads with Python. What libraries do people use for massive data loads with Python. Around 10 million to 100 million records a day. I keep researching and everyone is saying use Pandas which is a huge memory hog on large parallel loads.,True,False,False,dataengineering,t5_36en4,46967,public,self,Using Python for Data Engineering,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rfkwy1/using_python_for_data_engineering/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,nobru_2712,,,[],,,,text,t2_8wuwb9ua,False,False,False,[],False,False,1639415979,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rfkt1g/fact_tables_in_databricks_using_delta/,{},rfkt1g,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rfkt1g/fact_tables_in_databricks_using_delta/,False,,,6,1639415989,1,"Hi all,

I'm working with Delta Lake and Databricks and we're currently in process of building dimension and fact tables (gold layer). I'm new to this world so I'd like to know what do you recommend for creating fact tables? Is it recommended to truncate them first and then fill with new data if we want to have only last information? I have experience in traditional DWH where we'd just truncate fact, but speaking with more experienced colleagues it seems to me that truncate is not a best practice in Delta world. We're currently using MERGE for updating and inserting facts, but in some cases it seems that truncate would do the work in most convenient way.

Any suggestions on this topic are welcome! Thanks",True,False,False,dataengineering,t5_36en4,46967,public,self,Fact tables in Databricks using Delta,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rfkt1g/fact_tables_in_databricks_using_delta/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Simonaque,,,[],,,,text,t2_cj1g2,False,False,False,[],False,False,1639415507,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rfkmf3/which_databricks_certification_is_more_valuable/,{},rfkmf3,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rfkmf3/which_databricks_certification_is_more_valuable/,False,,,6,1639415518,1,Databricks Certified Associate Developer for Apache Spark 3.0 or Databricks Certified Professional Data Engineer?,True,False,False,dataengineering,t5_36en4,46965,public,self,Which Databricks certification is more valuable for a Data Engineer Career?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rfkmf3/which_databricks_certification_is_more_valuable/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,SerialBussy,,,[],,,,text,t2_bycusncg,False,False,False,[],False,False,1639413346,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rfjrxo/ive_built_a_service_for_our_community/,{},rfjrxo,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rfjrxo/ive_built_a_service_for_our_community/,False,,,6,1639413356,1,"Hi. I've built a service that may be helpful for new and seasoned DE's. It's free but I want to make it exclusive for our community, how can I check if a user belongs to r/dataengineering? Maybe someone had prior experience with a similar idea?",True,False,False,dataengineering,t5_36en4,46962,public,self,I've built a service for our community,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rfjrxo/ive_built_a_service_for_our_community/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,DataStackAcademy,,,[],,,,text,t2_gsfor44i,False,False,False,[],False,False,1639412797,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rfjkb8/data_engineering_webinars_mentorship/,{},rfjkb8,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rfjkb8/data_engineering_webinars_mentorship/,False,,,6,1639412807,1,"Hi all - First, appreciate this community so much! 

It’s not the intention to promote but we have webinars that would be beneficial to those looking to get into Data Engineering or even career switches. I truly believe this is needed in the industry (coming from a seasoned Data Engineer that’s been in the field for 20+ years). 

You’re welcomed to join our upcoming free webinars: 

\&gt; 12/14 [Engineering Career Path ](https://www.eventbrite.com/myevent?eid=222634484737)

\&gt; 12/16 [Google Cloud Data Engineering Services Explained](https://www.eventbrite.com/myevent?eid=222625959237)

\&gt; 12/21 [Engineering Career Path ](https://www.eventbrite.com/myevent?eid=222635166777)

\&gt; 12/23 [Intro to Docker &amp; using Docker in Cloud Data Engineering](https://www.eventbrite.com/myevent?eid=222626892027)

Also, **let us know what you’re looking to learn**. I’d love to speak with anyone that has DE questions or needs mentorship. We can answer questions here, too. Thanks, all! 

\~[Parham](https://www.linkedin.com/in/parvister/)",True,False,False,dataengineering,t5_36en4,46960,public,self,Data Engineering Webinars + Mentorship,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rfjkb8/data_engineering_webinars_mentorship/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Data_Cog,,,[],,,,text,t2_9edta7qq,False,False,False,[],False,False,1639387484,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rfbuu8/data_engineering_jargon_part_3/,{},rfbuu8,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rfbuu8/data_engineering_jargon_part_3/,False,,,6,1639387495,1,"Hi - this is the next 10. 

1-10 is [here](https://www.reddit.com/r/dataengineering/comments/rdw3b3/data_engineering_jargon/?utm_source=share&amp;utm_medium=web2x&amp;context=3)

11-20 is [here](https://www.reddit.com/r/dataengineering/comments/rem26j/data_engineering_jargon_part_2/)

**21. Batch Processing**

An automated way of processing millions of data transactions at the same time. This is generally carried out overnight with the help of “batch jobs”.

*Loading all the customer’s data that bought a particular item on the day.*

**22. T-SQL**

SQL is a Structured Query Language or, simply put, a language used to manage databases. T-SQL is Transact-SQL which is a proprietary Microsoft extension of the SQL language.

*T-SQL can be used MS SQL Server or Azure SQL Database to write a statement as follows “SELECT customer\_name from tbl\_customer\_information where customer\_city = “London”. This provides the result of all the customer names where customers are based in London.*

**23. NoSQL**

Although SQL has been around for decades, NoSQL (not only SQL) is a concept designed for non-relational databases, particularly to store unstructured data like documents.

*Storing an Outlook email file in XML with key-value pair on a MongoDB document database.*

**24. BTEQ**

Batch Teradata Query (like SQL) is simply a utility and query tool for Teradata, which is a relational database system

*Creating a BTEQ script to load data from a flat file.*

**25. Cloud**

Delivery of computing services such as servers, networking, analytics etc., over the internet instead of using a dedicated data centre for an organisation.

*Storing data on Microsoft’s Azure Cloud service instead of on an on-premise solution.*

**26.** **Data Architecture**

The discipline of managing the people, processes and technologies relating to data; includes data strategy, data capture processes and technical patterns to derive insight from the data.

*A Data Architect creates a framework for an enterprise to manage its data flow end to end.*

**27. Data Visualisation**

A practice for visualising large amounts of data to derive key insights to drive business decisions.

*An executive dashboard that clearly outlines the sales performance of a certain team.*

**28. Data Centres**

A dedicated space (nowadays millions of sqft of space) which houses servers and systems for the organisation’s critical applications

*Microsoft Data Centre to host all the company’s critical applications.*

**29. Data Integration**

Usually, the hardest part of the project, where multiple sources of data are integrated into a singular application/data warehouse.

*Integrating finance and customer relationship systems integrating into an MS SQL server database.*

**30. Data Migration**

The practice of migrating the data from source to destination

*Migrating data from MS SQL server database to an Amazon Relational Database service*",True,False,False,dataengineering,t5_36en4,46937,public,self,Data Engineering Jargon - Part 3,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rfbuu8/data_engineering_jargon_part_3/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,theManag3R,,,[],,,,text,t2_1znmmuk4,False,False,False,[],False,False,1639387290,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rfbt65/optimizing_write_to_s3/,{},rfbt65,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rfbt65/optimizing_write_to_s3/,False,,,6,1639387300,1,"Greetings. We are generating an ORC outputfile to a customer but the export job takes 1,5 hour to complete. The export job is only fetching the latest dataset (parquet) and storing it to S3 as ORC. The issue is that the customer wants to have only one file, which means that we have to use repartition(1) during the write operation. If I have understood correctly, repartition(1) collects the data to a single node and therefore all other nodes stand idle and is not optimal. Is there a way to optimize this?
Thank you.",True,False,False,dataengineering,t5_36en4,46937,public,self,Optimizing write to S3,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rfbt65/optimizing_write_to_s3/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,DataDucky,,,[],,,,text,t2_hhqziebt,False,False,False,[],False,False,1639387046,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rfbr3u/a_facebook_group_for_swedish_it_people_in_the/,{},rfbr3u,False,False,False,False,False,False,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rfbr3u/a_facebook_group_for_swedish_it_people_in_the/,False,,,6,1639387056,1,[removed],True,False,False,dataengineering,t5_36en4,46937,public,self,A facebook group for Swedish IT people in the Data/Sci/Engineering/analyst field (or interested to switch into it),0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rfbr3u/a_facebook_group_for_swedish_it_people_in_the/,all_ads,6,,,automod_filtered,,,,,,,,,,,,,,,
[],False,maverick0196,,,[],,,,text,t2_5rfvu3v,False,False,False,[],False,False,1639378381,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rf9lav/data_engineer_imposter_syndrome/,{},rf9lav,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rf9lav/data_engineer_imposter_syndrome/,False,,,6,1639378392,1,"Hello everyone! I'm currently having 3.5 years of experience in IT and I'm into my second company(moved out after 3 years with the first one).
So far I believed that I was a data engineer creating data pipelines to ingest, transform and curate data but off late I'm having an imposter syndrome.

At both my companies I am coming to realise that I never created any data pipelines from scratch nor wrote any code for this. They used to have their respective framework/jobs to ingest and curate the data. All I did was to add a custom Json config with the details of the source I'm ingesting into the repo and the framework used to take care of the rest by picking up data specified in the config and transforming it according to the parameters specified in the config.i have no clue on the logic/spark code doing all the heavy lifting in the background. Basically all my infrastructure is already in place and I'm just adding a few lines in a separate config for my source.

Is such a profile common in the data engineering domain or is my imposter syndrome theory actually correct and would negatively impact my career over the long run.

Any advice and discussion greatly appreciated.

TLDR: both companies I have worked in have infrastructure already in place for ingestion, transformation and curation. As I'm just adding custom Json configs with details around my source I believe I'm having imposter syndrome since I'm not writing any code to develop data pipelines from scratch.",True,False,False,dataengineering,t5_36en4,46931,public,self,Data Engineer Imposter syndrome,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rf9lav/data_engineer_imposter_syndrome/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,SweetKenny12,,,[],,,,text,t2_cwnqin3b,False,False,False,[],False,False,1639368692,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rf6sew/software_engineer_become_data_engineer/,{},rf6sew,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rf6sew/software_engineer_become_data_engineer/,False,,,6,1639368702,1,"i have work as software engineer for about 4 years (3 years as backend engineer + 1 year as fullstack) and I want to change my career to be data engineer

&amp;#x200B;

What should I learn and how I convince the recruiters that I'm serious to change my career ?",True,False,False,dataengineering,t5_36en4,46924,public,self,Software Engineer Become Data Engineer,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rf6sew/software_engineer_become_data_engineer/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,neatpeter33,,,[],,,,text,t2_yvifa,False,False,False,[],False,False,1639368673,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rf6s7c/how_hard_would_it_be_to_get_a_data_scientist_job/,{},rf6s7c,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rf6s7c/how_hard_would_it_be_to_get_a_data_scientist_job/,False,,,6,1639368683,1,"Let’s say I can choose between a data engineer and and data scientist position. If I accept the data engineer position and work for 1/2 years and decide it’s not for me, how hard would it be to get a data scientist job?",True,False,False,dataengineering,t5_36en4,46924,public,self,How hard would it be to get a data scientist job if I work as a data engineer?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rf6s7c/how_hard_would_it_be_to_get_a_data_scientist_job/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Buffalo_times_eight,,,[],,,,text,t2_4deyr4u4,False,False,False,[],False,False,1639368137,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rf6m1c/skilling_up_in_data_engineering/,{},rf6m1c,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rf6m1c/skilling_up_in_data_engineering/,False,self,"{'enabled': False, 'images': [{'id': 'wbh_bc9Po9I9GU6gNtv1NLSuvNt0VnxqDXLwL54SuWQ', 'resolutions': [{'height': 56, 'url': 'https://external-preview.redd.it/loL5DzHwQtXABc-BvbYg1d37O4w4rrA5NwCpinKELbw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5b458669762b327c55fe8fa79ea6dfa7e6180430', 'width': 108}, {'height': 113, 'url': 'https://external-preview.redd.it/loL5DzHwQtXABc-BvbYg1d37O4w4rrA5NwCpinKELbw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d1a1dbdea4aae0d44bfb2b3a9ceacc1ef40a1c5c', 'width': 216}, {'height': 168, 'url': 'https://external-preview.redd.it/loL5DzHwQtXABc-BvbYg1d37O4w4rrA5NwCpinKELbw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b404cb0a74eacc7c0373886ef7f73b9e1939f22a', 'width': 320}, {'height': 336, 'url': 'https://external-preview.redd.it/loL5DzHwQtXABc-BvbYg1d37O4w4rrA5NwCpinKELbw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9b27e3f5fed9d2f0b2124be5cb1d30d98fddc59d', 'width': 640}, {'height': 504, 'url': 'https://external-preview.redd.it/loL5DzHwQtXABc-BvbYg1d37O4w4rrA5NwCpinKELbw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ceec55c14ffee6bac4d0aeb60eca2ce3de1c99c1', 'width': 960}, {'height': 567, 'url': 'https://external-preview.redd.it/loL5DzHwQtXABc-BvbYg1d37O4w4rrA5NwCpinKELbw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b4fe42eb82127f663c7312a3b558484bee9c601c', 'width': 1080}], 'source': {'height': 630, 'url': 'https://external-preview.redd.it/loL5DzHwQtXABc-BvbYg1d37O4w4rrA5NwCpinKELbw.jpg?auto=webp&amp;s=d9c1f5444afbf2180cf839759545af8466173842', 'width': 1200}, 'variants': {}}]}",6,1639368147,1,"I'm interested in skilling up for data engineering for an [AI Alignment Lab](https://jobs.lever.co/Anthropic/11601154-a5d2-4217-8d3d-a30d014e56fe) and think the stack &amp; skills gained will translate to other AI labs. 

I can buy the book ""Designing Data-Intensive Applications"" as suggested elsewhere in the sub.
Given the list of job requirements below, would you suggest a MOOC/certificate, or focus on a project?

  - High performance, large-scale compute systems
  - Kubernetes, python, OS internals  
  - Networking
  - Large-scale ETL, Spark
  - Pytorch, deep learning

**MOOC/Certificate:** It's not clear this would be helpful, but maybe I've missed a course/cert that covers the above.
**Project:** They're running models on massive clusters so I don't know if my small project would translate.

_**Background:** Data Scientist for 3 years, Solid python skills with some deep learning in my spare time.
**Timing:** I'm in this for the long haul and don't think I'm ready for the above role today. More about building the skills today for the next opportunity_ :)",True,False,False,dataengineering,t5_36en4,46924,public,self,Skilling up in Data Engineering,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rf6m1c/skilling_up_in_data_engineering/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Ladivcr,,,[],,,,text,t2_8iqt8tl5,False,False,False,[],False,False,1639362992,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rf4ywe/is_there_a_problem_if_i_signed_an_intern_contract/,{},rf4ywe,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rf4ywe/is_there_a_problem_if_i_signed_an_intern_contract/,False,,,6,1639363003,1,"Currently I'm on a intern as a backend developer but other company offers me a job as a Data engineer but I need to join with them before 10th January. But my intern finish on 1st February. 

&amp;#x200B;

My principal professional objective is be a data engineer, so I'm consider a lot leave my intern and bet all for this position as a data engineer jr.",True,False,False,dataengineering,t5_36en4,46923,public,self,Is there a problem if I signed an intern contract for 3 months but I can't finish?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rf4ywe/is_there_a_problem_if_i_signed_an_intern_contract/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,morpho4444,,,[],,,,text,t2_5qg3y,False,False,False,[],False,False,1639353406,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rf1tkv/interview_question_dilemma/,{},rf1tkv,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rf1tkv/interview_question_dilemma/,False,,,6,1639353417,1,"What comes to your mind the first time you listen the instructions:

What is the third largest number of this sequence?

1,2,3,3,3,4,7,8,9,9,10

You say is 9 or 8? What's the number you would get after applying max() and removing it three times?",True,False,False,dataengineering,t5_36en4,46913,public,self,Interview question dilemma,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rf1tkv/interview_question_dilemma/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,dekrepitbirth,,,[],,,,text,t2_e0ft8,False,False,False,[],False,False,1639341054,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rexkhb/choosing_a_columnar_db/,{},rexkhb,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rexkhb/choosing_a_columnar_db/,False,,,6,1639341064,1,"I have a single table with about 20 integer columns with billions of rows, goal is to support fast queries that select all columns, but can filter on any combination of columns. What would be a good columnar db for this? The data isn't live (gets a full refresh once a month), so I'm not sure dbs like ClickHouse/Pinot/Druid are the right choice since they seem to place a lot of emphasis on real-time ingestion with Kafka, but that might be a wrong impression.",True,False,False,dataengineering,t5_36en4,46899,public,self,Choosing a columnar db,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rexkhb/choosing_a_columnar_db/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,statistical_engineer,,,[],,,,text,t2_3plcyhxk,False,False,False,[],False,False,1639336168,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/revuyv/how_well_do_data_engineering_skills_transfer_to/,{},revuyv,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/revuyv/how_well_do_data_engineering_skills_transfer_to/,False,,,6,1639336179,1,"I spent my whole career in data so far: 1 year in data science and 3 years as a data engineer.  I definitely find the work being done as a data engineer more interesting than data science.



Considering that a lot of our skills and engineering processes already overlap with software engineering, is there a lot of overlap between both careers?",True,False,False,dataengineering,t5_36en4,46894,public,self,How well do data engineering skills transfer to software engineering?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/revuyv/how_well_do_data_engineering_skills_transfer_to/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Thresher_XG,,,[],,,,text,t2_dxpdw,False,False,False,[],False,False,1639329061,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/retcmt/da_to_be_interviewed_by_de/,{},retcmt,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/retcmt/da_to_be_interviewed_by_de/,False,,,6,1639329071,1,"I have an upcoming DA interview with a DE I will be working with. I am comfortable with SQL and proficient in C# and python. What kind of questions should I expect? I am not sure how technical the interview might get. I have also not done much advanced stats in my previous positions. Is there anything I should brush up on? The job description is mostly ETL, SQL and tableau. So not even sure if stats questions will be asked but just want to be sure.

Any help is greatly appreciated!

Thanks!",True,False,False,dataengineering,t5_36en4,46886,public,self,DA to be interviewed by DE,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/retcmt/da_to_be_interviewed_by_de/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,jliang99,,,[],,,,text,t2_2e95cks3,False,False,False,[],False,False,1639323047,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rerava/how_data_engineers_do_software_design_invitation/,{},rerava,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rerava/how_data_engineers_do_software_design_invitation/,False,self,"{'enabled': False, 'images': [{'id': 'C-owJ3M1hv-CPmcJIUoU5A9onQJUcX_UXM8kVISfIEc', 'resolutions': [{'height': 56, 'url': 'https://external-preview.redd.it/s7g3jTRN379OOEhpSuu4pRE94hnS3faEh8rhPoER_u8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1f5a9252fda90ae1464343c129f4ee339e81da24', 'width': 108}, {'height': 113, 'url': 'https://external-preview.redd.it/s7g3jTRN379OOEhpSuu4pRE94hnS3faEh8rhPoER_u8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0ab95c6b2800ac3ffa9320d2ac5caf14793e5931', 'width': 216}, {'height': 168, 'url': 'https://external-preview.redd.it/s7g3jTRN379OOEhpSuu4pRE94hnS3faEh8rhPoER_u8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9a3b53266442800f6edd3a2baa02066ebf07331e', 'width': 320}, {'height': 336, 'url': 'https://external-preview.redd.it/s7g3jTRN379OOEhpSuu4pRE94hnS3faEh8rhPoER_u8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=01a1d54df3c2cdde53fbccc1c202e1197dcca656', 'width': 640}, {'height': 504, 'url': 'https://external-preview.redd.it/s7g3jTRN379OOEhpSuu4pRE94hnS3faEh8rhPoER_u8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0663fcaf4be4dafe9e5c116f7347f46f3a1d7c9d', 'width': 960}, {'height': 567, 'url': 'https://external-preview.redd.it/s7g3jTRN379OOEhpSuu4pRE94hnS3faEh8rhPoER_u8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a6d30e84de20179ffc38560dc9d916346b9c23fb', 'width': 1080}], 'source': {'height': 630, 'url': 'https://external-preview.redd.it/s7g3jTRN379OOEhpSuu4pRE94hnS3faEh8rhPoER_u8.jpg?auto=webp&amp;s=00df6e509f08ff8dbe19661bcee98a9b11d432ca', 'width': 1200}, 'variants': {}}]}",6,1639323058,1,"Hi r/dataengineering!

I’m a part of a group of researchers from Carnegie Mellon University, University of Washington, and George Mason University studying software design. Your expertise on how solving data engineering problems impacts the design decisions you make would be very useful in our survey. **Link:** [https://forms.gle/KFtgDNNR6arRZYJ96](https://forms.gle/KFtgDNNR6arRZYJ96).

Your input in this field will help us better understand software design, which we hope to share with other researchers and technology companies to inspire new innovations in developer tools and static analysis tools for software architecture.",True,False,False,dataengineering,t5_36en4,46878,public,self,How Data Engineers Do Software Design? -- Invitation to participate in a research study,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rerava/how_data_engineers_do_software_design_invitation/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,MoniDataWizard,,,[],,,,text,t2_dmrf6npg,False,False,False,[],False,False,1639320363,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/reqexw/which_certification_would_you_recommend_me_to_take/,{},reqexw,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/reqexw/which_certification_would_you_recommend_me_to_take/,False,,,6,1639320373,1,"Hi guys,

I did Microsoft  Ignite Cloud Skills Challenge and got a free certification exam but as DP203 is not available on  eligible exams list I would like to ask you for your suggestions - which one would be the most useful one for data engineer?

AZ-104: Microsoft Azure Administrator

AZ-204: Developing Solutions for Microsoft Azure

DP-300: Administering Relational Databases on Microsoft Azure

AZ-800: Administering Windows Server Hybrid Core Infrastructure

AZ-801: Configuring Windows Server Hybrid Advanced Services 

 PL-400: Microsoft Power Platform Developer 

I know that these certificats aren't directly connected to my job as a beginner data engineer but still it would be good to expend my knowledge about Azure because I have contact with it on daily basic :)",True,False,False,dataengineering,t5_36en4,46874,public,self,Which certification would you recommend me to take?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/reqexw/which_certification_would_you_recommend_me_to_take/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,DevData3,,,[],,,,text,t2_hhkuy7xe,False,False,False,[],False,False,1639320082,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/reqbsc/am_i_a_data_developer_bi_developer_etl/,{},reqbsc,False,False,False,False,False,False,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/reqbsc/am_i_a_data_developer_bi_developer_etl/,False,,,6,1639320092,1,[removed],True,False,False,dataengineering,t5_36en4,46874,public,self,Am I a Data Developer ? Bi Developer ? ETL ?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/reqbsc/am_i_a_data_developer_bi_developer_etl/,all_ads,6,,,automod_filtered,,,,,,,,,,,,,,,
[],False,TheFragan,,,[],,,,text,t2_a6b9szlc,False,False,False,[],False,False,1639305616,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/remiju/spark_jobs_unitfunctional_test_and_qa/,{},remiju,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/remiju/spark_jobs_unitfunctional_test_and_qa/,False,,,6,1639305626,1,"Hello,

I was wondering how do you guys test your Spark Batch Jobs.

Im a Junior DE and i work at a huge bank-insurance company, they have a ""legacy"" data platform running on MapR distribution of Hadoop/Spark. They have 2 clusters one for Q&amp;A and another one for Production.

They problem is that the Data in the Q&amp;A environment isn't enough to cover all use cases, so they ask Data Engineers to write data inside it in order to test new features and submit those test to the Business Analysts for qualification.

I find this process bit annoying, like everytime, i develop my new features in like 1-2 hours and then spend a day or two to generate a proper dataset for testing (as it generally involves like 10 tables or so).

I'd like to know how to guys test your Spark Jobs and how do you convince the Business Analysts that what you developped actually does what was specified.

Regards,",True,False,False,dataengineering,t5_36en4,46863,public,self,Spark Jobs Unit/Functional Test and Q&amp;A,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/remiju/spark_jobs_unitfunctional_test_and_qa/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Data_Cog,,,[],,,,text,t2_9edta7qq,False,False,False,[],False,False,1639303577,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rem26j/data_engineering_jargon_part_2/,{},rem26j,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rem26j/data_engineering_jargon_part_2/,False,,,6,1639303587,1,"Hi - this is the next 10. First ten is [here:](https://www.reddit.com/r/dataengineering/comments/rdw3b3/data_engineering_jargon/?utm_source=share&amp;utm_medium=web2x&amp;context=3) 

**11. Ingestion**

Generally, the first step in a data pipeline, where data is ingested into the platform.

*A pipeline where customer address data is ingested from source A.*

**12. Extract, Transform, Load (ETL)**

A 3-step process of extracting data and transforming it (by applying some kind of logic like aggregation) and loading the new information into the destination. It could be used as ELT where the destination tables transform the data instead.

*An extract of customer address data is taken from the customer relationship management tool and is then aggregated according to their cities and this new information is loaded into destination B.*

**13. Data Models**

A way of organising the data in a way that it can be understood in a real-world scenario.

*Taking a huge amount of data and logically grouping it into customer, product and location data.*

**14. Normalisation**

A method of organising the data in a granular enough format that it can be utilised for different purposes over time. Usually, this is done by normalising the data into different forms such as 1NF (normal form) or 3NF (3rd normal form) which is the most common.

*Taking customer order data and* *creating granular information model; order in one table, item ordered in another table, customer contact in another table, payment of the order in another table.* *This allows for the data to be re-used for different purposes over time.*

**15. Star schema**

The simplest way to model data into different quantitative and qualitative data is called facts and dimensions. Usually, the fact table is interpreted with the help of a dimensions table resembling a star.

*A Star schema of sales data with dimensions such as customer, product &amp; time.*

**16. Facts**

A data warehousing term for quantitative information.

*The* *number of orders* *placed by a customer.*

**17. Dimensions**

A data warehousing term for qualitative information.

*Name of the customer* *or their* *country of residence.*

**18. Schemas**

A term for a collection of database objects. These are generally used to logically separate data within the database and apply access controls.

*Storing HR data in HR schema allows logical segregation from other data in the organisation.*

**19. SCD Type 1–6**

A method to deal with changes in the data over time in a data warehouse. Type 1 is when history is overwritten whereas Type 2 (most common) is when history is maintained each time a change occurs.

*When a customer changes their address; SCD Type 1 would overwrite the old address with the new one, whereas Type 2 would store both addresses to maintain history.*

**20. Business Intelligence**

A slightly out of date term for a combination of practices to derive business insights from data by predominantly using data warehousing, analytics and dashboarding.

*Creating a management dashboard to show customer demographics across the country.*",True,False,False,dataengineering,t5_36en4,46862,public,self,Data Engineering Jargon - Part 2,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rem26j/data_engineering_jargon_part_2/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,raghukveer,,,[],,,,text,t2_2m03eqtu,False,False,False,[],False,False,1639296839,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rekhhn/guide_to_read_the_data_warehouse_toolkit_to_save/,{},rekhhn,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rekhhn/guide_to_read_the_data_warehouse_toolkit_to_save/,False,self,"{'enabled': False, 'images': [{'id': '8c3XTXo4Z4acPfM6XMzxVehkehOLvHdqaiwcSxP8KnQ', 'resolutions': [{'height': 52, 'url': 'https://external-preview.redd.it/REhshAvTCUybbsMK05shAiDTre4EN5Gdf_Sw4TPTP-c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3a74f2d52d57b361d772b3f44bbb5b7d5b373003', 'width': 108}, {'height': 105, 'url': 'https://external-preview.redd.it/REhshAvTCUybbsMK05shAiDTre4EN5Gdf_Sw4TPTP-c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=db0bdd376d24fb2bd02bf0076c5bbdcea9a33c60', 'width': 216}, {'height': 156, 'url': 'https://external-preview.redd.it/REhshAvTCUybbsMK05shAiDTre4EN5Gdf_Sw4TPTP-c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=422fb7cc42a10890d6435692c0dc8033616caf31', 'width': 320}, {'height': 312, 'url': 'https://external-preview.redd.it/REhshAvTCUybbsMK05shAiDTre4EN5Gdf_Sw4TPTP-c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3f0936184d196be860bdb5c684617b4ad824b733', 'width': 640}, {'height': 468, 'url': 'https://external-preview.redd.it/REhshAvTCUybbsMK05shAiDTre4EN5Gdf_Sw4TPTP-c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=efffa9ca65f98804ff43b80aa5f124db9865dac7', 'width': 960}], 'source': {'height': 500, 'url': 'https://external-preview.redd.it/REhshAvTCUybbsMK05shAiDTre4EN5Gdf_Sw4TPTP-c.jpg?auto=webp&amp;s=8ea909014c848b42414dd636bbc50376e959375b', 'width': 1024}, 'variants': {}}]}",6,1639296850,1,[https://www.holistics.io/blog/how-to-read-data-warehouse-toolkit/](https://www.holistics.io/blog/how-to-read-data-warehouse-toolkit/),True,False,False,dataengineering,t5_36en4,46855,public,self,Guide to read The Data Warehouse Toolkit to save you from reading cover-cover and outdated topics.,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rekhhn/guide_to_read_the_data_warehouse_toolkit_to_save/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,skyReact,,,[],,,,text,t2_gryqo472,False,False,False,[],False,False,1639295254,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rek3vv/career_progression_to_de/,{},rek3vv,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rek3vv/career_progression_to_de/,False,,,6,1639295265,1,"I don't see a lot of junior DE positions in my area. Most of them requires around 5yrs of IT experience.

What is the easiest way to get into DE? What are the jobs that I should take as a stepping stone for becoming a DE? Would a Back End Web Development be a good stepping stone or I should take Data Analysis jobs instead? How about being an SQL dev? I'm a career shifter currently studying Front End Web Development. I hope you can help me. Thank you!",True,False,False,dataengineering,t5_36en4,46853,public,self,Career progression to DE?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rek3vv/career_progression_to_de/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,ChuckFinleyy,,,[],,,,text,t2_7uw88,False,False,False,[],False,False,1639290224,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/reivg0/looking_for_video_course_on_effective_sql_queries/,{},reivg0,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/reivg0/looking_for_video_course_on_effective_sql_queries/,False,,,6,1639290235,1,"Hi guys,

I  see a lot of these posts looking for learning pathways for new data engineers, however a lot of them point to articles or sql exercise websites. I enjoy courses because that helps structure my learning, ensuring i am introduced to concepts in appropriate order etc. I am after recommendations for a paid/free course that offers video explanations and exercises that covers;

**Effective SQL queries**

SQL statements (unions, sub queries, group by having)

What are CTE’s, UDF’s, how to use them

**Database administration**

Optimisation - Data modelling, normalisation, indexing

&amp;#x200B;

Also; I am interesting in data pipeline infrastructure. anything that covers setting up and maintaining pipelines within the azure environment and utilising docker / airflow would be great.",True,False,False,dataengineering,t5_36en4,46848,public,self,Looking for Video Course on effective SQL queries and database administration in 2021,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/reivg0/looking_for_video_course_on_effective_sql_queries/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,levsell,,,[],,,,text,t2_9y5fpiut,False,False,False,[],False,False,1639276328,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/reez7p/how_to_best_learn_data_engineering/,{},reez7p,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,1,0,False,all_ads,/r/dataengineering/comments/reez7p/how_to_best_learn_data_engineering/,False,,,6,1639276339,1,"I have no technical background (business major) but I would like to develop a deeper understanding of data engineering. I'm in the finance / investing field and think that having a deeper understanding of the technicalities would give me an edge. 

I've been reading whitepapers, research reports, high level tutorials, etc. but feel as if I still don't understand it quite as well as needed as you can only get so far with reading other people's research- I think the roadblock is that:  
A) I'm not a daily user of data engineering tools (Snowflake, SQL, etc.)  
B) I don't know how to code, develop programs, apps, etc. 

Do you think that having a true understanding of different DE tools, concepts, frameworks, overall industry, and ability to judge different tools / trends would require me to actually partake in some technical learning (learning how to code, etc.). For ex. even the technical parts of whitepapers I don't understand. 

Thanks for your advice.",True,False,False,dataengineering,t5_36en4,46841,public,self,How to best learn data engineering?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/reez7p/how_to_best_learn_data_engineering/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,squaricle,,,[],,,,text,t2_3cfsa44,False,False,False,[],False,False,1639269491,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/recw91/does_anyone_have_experience_documenting_airflow/,{},recw91,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/recw91/does_anyone_have_experience_documenting_airflow/,False,,,6,1639269502,1,"I tend to use Sphinx to document my projects and so far, I haven't found any useful extensions to get Sphinx to recognize the doc_mds in the DAGs and tasks. Has anyone else found helpful tools for this?",True,False,False,dataengineering,t5_36en4,46837,public,self,Does anyone have experience documenting Airflow projects with Sphinx?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/recw91/does_anyone_have_experience_documenting_airflow/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,SeaworthinessFit7893,,,[],,,,text,t2_4bor0kj8,False,False,False,[],False,False,1639265911,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rebs6q/how_does_apache_airflow_create_pipelines/,{},rebs6q,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rebs6q/how_does_apache_airflow_create_pipelines/,False,,,6,1639265921,1,So apache airflow is a workflow management framework correct? So how does it aid in construction of etl pipelines?,True,False,False,dataengineering,t5_36en4,46836,public,self,How does apache airflow create pipelines?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rebs6q/how_does_apache_airflow_create_pipelines/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,anecdotal_yokel,,,[],,,,text,t2_13s8ro,False,False,False,[],False,False,1639265706,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rebpzt/in_your_opinion_what_skillsexperience_do_you/,{},rebpzt,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rebpzt/in_your_opinion_what_skillsexperience_do_you/,False,,,6,1639265717,1,"In my current position(not data engineering), I do what I would consider novice data engineering but then I read posts here and find a much more complex set of skills. I’d like to transition into the field but wanted to get an opinion on what the absolute minimum would be versus a “desired” skills listing you see in job postings. What basic skills and experiences did you have that made you think of yourself as a legit “data engineer”?",True,False,False,dataengineering,t5_36en4,46836,public,self,"In your opinion, what skills/experience do you consider a minimum qualification for data engineering?",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rebpzt/in_your_opinion_what_skillsexperience_do_you/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,etl_boi,,,[],,,,text,t2_f16v22yv,False,False,False,[],False,False,1639261800,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/reahdb/moving_to_a_consulting_role_without_much/,{},reahdb,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/reahdb/moving_to_a_consulting_role_without_much/,False,,,6,1639261810,1,"Hey guys,

I have aboht 1 YOE and am looking to start applying to analytics engineering or data engineering consultant positions.

I don’t have much experience in the modem tech stack (dbt, cloud tools, fivetran, airflow, etc).  My current company uses a proprietary system, however much of it is very similar to these other tools listed on job postings.

My current expertise is in spark, python, ETL, DWH.  Beyond my technical skills, I’ve been on many *very* high impact projects and am used to interacting with very senior management because our org is very underdeveloped when it comes to tech professionals, so I’ve gotten a lot of experiences that many new grads haven’t (especially with stakeholder/client management).

I’ve been using free trials to explore other tools, become familiar with them, but I’m certainly no expert in them.

Is this enough to transition to consulting?  What other steps should I take to get this next role?",True,False,False,dataengineering,t5_36en4,46834,public,self,Moving to a consulting role without much knowledge of Modern Tech Stack,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/reahdb/moving_to_a_consulting_role_without_much/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,clairegiordano,,,[],,,,text,t2_28vl989,False,False,False,[],False,False,1639253642,techcommunity.microsoft.com,https://www.reddit.com/r/dataengineering/comments/re7t48/uk_covid19_dashboard_built_using_postgres_and/,{},re7t48,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/re7t48/uk_covid19_dashboard_built_using_postgres_and/,False,link,"{'enabled': False, 'images': [{'id': 'ISterghm5V7GN27qt1MnWIdSCPBDIpnlRfCWEQeZvxU', 'resolutions': [{'height': 60, 'url': 'https://external-preview.redd.it/gIo3Cb1ZG4R25DVrccnqJ7vjJP6gM1jpQevbcsTWSQQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3b02341f74d36fc0452559819afef07e52f84510', 'width': 108}, {'height': 121, 'url': 'https://external-preview.redd.it/gIo3Cb1ZG4R25DVrccnqJ7vjJP6gM1jpQevbcsTWSQQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=020bbc92f6dc4b85b5c3d2be005a170fe899e0bc', 'width': 216}, {'height': 180, 'url': 'https://external-preview.redd.it/gIo3Cb1ZG4R25DVrccnqJ7vjJP6gM1jpQevbcsTWSQQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=67e74653ea532b110358cabf7dfc17dfa7d4acf7', 'width': 320}, {'height': 360, 'url': 'https://external-preview.redd.it/gIo3Cb1ZG4R25DVrccnqJ7vjJP6gM1jpQevbcsTWSQQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1ae6b74eb5c6ca59c5dff8f8d97471c954907a17', 'width': 640}, {'height': 540, 'url': 'https://external-preview.redd.it/gIo3Cb1ZG4R25DVrccnqJ7vjJP6gM1jpQevbcsTWSQQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4cb6886d6d0d703a096c86cbae30c670da71375a', 'width': 960}, {'height': 607, 'url': 'https://external-preview.redd.it/gIo3Cb1ZG4R25DVrccnqJ7vjJP6gM1jpQevbcsTWSQQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=45dd975ab68e1eaeffdca66b151600c93f969399', 'width': 1080}], 'source': {'height': 1080, 'url': 'https://external-preview.redd.it/gIo3Cb1ZG4R25DVrccnqJ7vjJP6gM1jpQevbcsTWSQQ.jpg?auto=webp&amp;s=ed28b0a4e080274f4e6540e7c2cd94d99464a046', 'width': 1920}, 'variants': {}}]}",6,1639253652,1,,True,False,False,dataengineering,t5_36en4,46820,public,https://a.thumbs.redditmedia.com/8_wXiPp2oWcA6LEx9J2m91UT_WYoSwV_M3msewjsz70.jpg,UK COVID-19 dashboard built using Postgres and Citus for distributed scale (cross post from r/SQL),0,[],1.0,https://techcommunity.microsoft.com/t5/azure-database-for-postgresql/uk-covid-19-dashboard-built-using-postgres-and-citus-for/ba-p/3036276,all_ads,6,,,,,,78.0,140.0,https://techcommunity.microsoft.com/t5/azure-database-for-postgresql/uk-covid-19-dashboard-built-using-postgres-and-citus-for/ba-p/3036276,,,,,,,,,,
[],False,ckdatanerd,,,[],,,,text,t2_985ca477,False,False,False,[],False,False,1639248697,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/re64t2/what_certifications_are_worth_the_investment/,{},re64t2,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/re64t2/what_certifications_are_worth_the_investment/,False,,,6,1639248707,1,"I’m a data engineer with 6 months of experience and I really like the kind of work I do. My day to day tech stack is Azure &amp; Databricks (I work a lot with developing pipelines for moving data from SQLDB &amp; SQLDW to Cosmos, along with optimizing spark code for ML models and managing some ADF pipelines). This month (now until mid-Jan) my boss is having my team start no new projects and to focus on standardizing our processes and training. I am trying to figure out what certifications I should study for and if they’re worth it. My current plan is below:

1. Azure DP-900 (Azure Data Fundamentals)
2. Azure DP-203 (Azure Data Engineering)
3. Databricks Certified Associate Developer for Apache Spark

It’s overwhelming trying to study for so many exams at once, but when I’ve looked over the corse material for each, I already know how to do 70% of it because of my daily work. Are getting these certifications worth it? Or are they something future jobs won’t care about? As much as I like my current job, I plan on leaving it in the summer for better pay.",True,False,False,dataengineering,t5_36en4,46813,public,self,What certifications are worth the investment?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/re64t2/what_certifications_are_worth_the_investment/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,SeaworthinessFit7893,,,[],,,,text,t2_4bor0kj8,False,False,False,[],False,False,1639243780,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/re4h19/opinions_on_snowflake/,{},re4h19,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/re4h19/opinions_on_snowflake/,False,,,6,1639243790,1,What's your guy's thoughts on the cloud based data warehousing service snowflake. Good? Bad? Do you use it even?,True,False,False,dataengineering,t5_36en4,46811,public,self,Opinions on snowflake,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/re4h19/opinions_on_snowflake/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,BoiElroy,,,[],,,,text,t2_r8dyi,False,False,True,[],False,False,1639243746,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/re4gnx/how_much_is_a_data_engineer_expected_to_know/,{},re4gnx,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/re4gnx/how_much_is_a_data_engineer_expected_to_know/,False,,,6,1639243756,1,"I understand what it is. And where it can be applied. But I don't really know anything about implementation. 
Do you guys work with data engineers that build pipelines that already include CI/CD? Or do they just write the testing logic and hand it off? I don't know how to webhook :(",True,False,False,dataengineering,t5_36en4,46811,public,self,How much is a data engineer expected to know about CI/CD?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/re4gnx/how_much_is_a_data_engineer_expected_to_know/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,padmasaran_s,,,[],,,,text,t2_4ld6m52a,False,False,False,[],False,False,1639242740,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/re44n1/consulting_vs_saas_role/,{},re44n1,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/re44n1/consulting_vs_saas_role/,False,,,6,1639242750,1,"Hello!!

I work currently as a BI Developer with an analytics consulting firm and primarily work on DWH projects as Data Engineer with 2 years exp. Tech stack is around Azure with ADF, Synapse and SQL DB.

I have be offered a Data Engineer role with a 5 year old SaaS startup. Role looks to be focused on Data ingestion, pipelining and preliminary processing. Heavy lifting on Data processing is done by SDEs in Java. Tech stack is ADF, Python and SQL.

New role offers a decent hike. I am still unclear on the pros and cons on both sides - consulting vs SaaS - whether SaaS role will restrict my tech stack as I may get snowflake &amp; dbt work in consulting role if req. arises.

Need advise on deciding on the new role.",True,False,False,dataengineering,t5_36en4,46811,public,self,Consulting vs SaaS role,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/re44n1/consulting_vs_saas_role/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,data_nerd_ind,,,[],,,,text,t2_hgkub7kr,False,False,False,[],False,False,1639242464,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/re41ac/consulting_vs_saas_data_engineer_role/,{},re41ac,False,False,False,False,False,False,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/re41ac/consulting_vs_saas_data_engineer_role/,False,,,6,1639242475,1,[removed],True,False,False,dataengineering,t5_36en4,46811,public,self,Consulting vs SaaS Data Engineer role,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/re41ac/consulting_vs_saas_data_engineer_role/,all_ads,6,,,automod_filtered,,,,,,,,,,,,,,,
[],False,Automatic-Carpenter6,,,[],,,,text,t2_72sftj7a,False,False,False,[],False,False,1639238503,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/re2p7z/dimensional_model_for_dropbox_interview_question/,{},re2p7z,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/re2p7z/dimensional_model_for_dropbox_interview_question/,False,,,6,1639238514,1,"Hey,

This sub is great. I have learnt lots from it so far and hopefully someone can point me in the right direction for my question.

I am prepping for a DE interview and read tips online to practice building out dimensional models for the likes of Uber and Dropbox.

I find the Uber one straightforward (grain being a trip) but am a bit lost with Dropbox. I was thinking the grain could be a file upload or else an action (file upload, download, move, delete). Would you treat dim location as a ragged hierarchy?

How would you model dropbox?

&amp;#x200B;

TIA",True,False,False,dataengineering,t5_36en4,46808,public,self,Dimensional Model for Dropbox - Interview question,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/re2p7z/dimensional_model_for_dropbox_interview_question/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,cyclopster,,,[],,,,text,t2_9ywtxreq,False,False,False,[],False,False,1639230417,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/re044y/data_engineering_medium_paywall_is_it_worth_it/,{},re044y,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/re044y/data_engineering_medium_paywall_is_it_worth_it/,False,,,6,1639230428,1,"I've been a software developer for almost a decade and have been increasingly doing data engineering year after year. Last year I made the switch and am now a full time data engineer. However, since switching I've noticed that the topics I research for my job are increasingly behind a Medium paywall, unlike when I was doing back-end or front-end development.  


I built my career off of Google searches, StackOverflow, Twitter and other free sources so it's unfamiliar to me that tech blogs would cost money to read. Does anyone in this space subscribe to Medium for data engineering? Have they found it helpful? For example, I'm seeing many blogs by towardsdatascience behind this Medium paywall.",True,False,False,dataengineering,t5_36en4,46797,public,self,Data Engineering Medium Paywall. Is it worth it?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/re044y/data_engineering_medium_paywall_is_it_worth_it/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Data_Cog,,,[],,,,text,t2_9edta7qq,False,False,False,[],False,False,1639214383,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rdw3b3/data_engineering_jargon/,{},rdw3b3,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rdw3b3/data_engineering_jargon/,False,,,6,1639214394,1,"I wrote a list of 50 - I'll share ten at a time.

**1. Data Dump**

A file or a table containing a significant amount of data to be analysed or transferred.

*A table containing the ""data dump"" of all customer addresses.*

**2. Data Pipelines**

A data processing method akin to a pipeline, which starts with data ingestion then processing then completion.

*A pipeline where customer address data is ingested from source A and then aggregated according to their cities and this new information is loaded into destination B.*

**3. DBA**

Database Administrator is an admin role that understands the particular database technology and how to get the best out of it. This includes improving performance, backups and recovery.

*Performance tuning the database to respond better to particular complex data queries.*

**4. Data Warehouse**

A method of organising data to make it easy to analyse and report to make business decisions

*Oracle data warehouse. Organising customer data in a data warehouse to be able to report the number of newly acquired customers.*

**5. Data Mart**

A subset of a data warehouse, created for a very specific business use case.

*Finance data mart storing all the relevant financial information required by the Accounting team to process their month-end cycles.*

**6. ODS**

Operational data store generally stores limited and current information to help simple queries. Unable to handle historical or complex data queries.

*An ODS for daily stock fluctuations in a warehouse help the warehouse manager decide what to prioritise in the next order delivery.*

**7. EDW**

The same as a data warehouse except it includes all the data within an organisation. This means that the entire enterprise can rely on this warehouse for their business decisions.

*Organising sales, customer, marketing and finance data in an enterprise data warehouse to be able to create several key management reports.*

**8. RDBMS**

Relational database management system. All of the above examples are RDBMS, meaning they store data in a structured format using rows and columns.

*A Microsoft SQL server database.*

**9. In-memory DB**

Traditional databases have been used for complex calculations and queries. They store information on the actual disk in the computer. In-memory DB stores all the information on their memory (RAM), this allows for rapid calculations without read and write a function to a normal disk.

*A drill-down functionality of a live dashboard.*

**10. Data Lake**

A repository for all kinds of structured and unstructured data. Mainly based on Hadoop storage technology. Called a lake as it is flexible enough to store anything from raw data to unstructured email files.

*Hadoop Data Lake. Storing logs of all customers called into the inbound call centre including call duration.*",True,False,False,dataengineering,t5_36en4,46787,public,self,Data Engineering Jargon,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rdw3b3/data_engineering_jargon/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,piyushag,,,[],,,,text,t2_6gblabhe,False,False,False,[],False,False,1639187063,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rdohlg/data_cleansing_tools_used_by_large_enterprises/,{},rdohlg,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rdohlg/data_cleansing_tools_used_by_large_enterprises/,False,,,6,1639187074,1,"What are the most popular data cleansing/validation tools used by mid/large companies, especially around their ETL pipelines and Data Warehouses?",True,False,False,dataengineering,t5_36en4,46749,public,self,Data Cleansing tools used by large enterprises,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rdohlg/data_cleansing_tools_used_by_large_enterprises/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,garrrikkotua,,,[],,,,text,t2_8yz625qv,False,False,False,[],False,False,1639177714,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rdlc0f/what_do_you_think_about_automating_data_assets/,{},rdlc0f,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rdlc0f/what_do_you_think_about_automating_data_assets/,False,,,6,1639177724,1,"Hi, my name is Igor.

I am thinking of startup ideas in the data governance space. What are the problems do you see in the space?

What do you think about data catalogs / data documentation? How does data documentation process look like in your company?

Would really appreciate if you can check my idea here: [https://datafresh.unicornplatform.page/](https://datafresh.unicornplatform.page/)",True,False,False,dataengineering,t5_36en4,46734,public,self,What do you think about automating data assets documentation? It is a big problem? How does data documentation process look like in your company?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rdlc0f/what_do_you_think_about_automating_data_assets/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,m123av,,,[],,,,text,t2_c6m5h81g,False,False,False,[],False,False,1639165594,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rdgub0/data_engineering_course_help/,{},rdgub0,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,1,0,False,all_ads,/r/dataengineering/comments/rdgub0/data_engineering_course_help/,False,,,6,1639165604,1," 

Hello Everyone,

I have a IT background. My last job was .Net developer but that was 2 years ago. I cannot find a job since then. I have decided to get into Data engineering and getting my gcp. Does anyone have any suggestions on botocamps that guarantee a job? Or any courses that can be helpful for me?

Thanks",True,False,False,dataengineering,t5_36en4,46717,public,self,Data engineering course help,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rdgub0/data_engineering_course_help/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,king_booker,,,[],,,,text,t2_ryny2,False,False,False,[],False,False,1639162854,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rdftum/data_engineer_at_a_big_gaming_company_vs_an/,{},rdftum,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rdftum/data_engineer_at_a_big_gaming_company_vs_an/,False,,,6,1639162865,1," I have two job offers. One at a big gaming company (think Ubisoft, EA) the other at a boutique analytical firm that does analytics consulting.

In my head, I think an analytical firm is better to learn things but always liked gaming and I think analysing game data would be a lot more fun. But the tech stack does seem a bit outdated. (Hadoop, Spark, cassandra etc and  its on prem)

Anyone has any experience or suggestions regarding this?",True,False,False,dataengineering,t5_36en4,46718,public,self,Data Engineer at a big gaming company vs an analytical firm,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rdftum/data_engineer_at_a_big_gaming_company_vs_an/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,magu01,,,[],,,,text,t2_h0k60d4,False,False,False,[],False,False,1639162768,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rdfsds/data_engineering_how_to_or_starter_guide/,{},rdfsds,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rdfsds/data_engineering_how_to_or_starter_guide/,False,,,6,1639162778,1,"So i work for a business unit who gets dbms view dumps on a montly basis. Not too large maybe 4 databes, and only in the millions of rows per month.  So far ive been to just used excels power query to load the data, then store in a data model and create reports. Recently ppl have been very happy with reporting and are requesting more reports. 

Recently we got redshift avialable for use on premise. And im trying to figure out how to set this up.  I have a couple of years worth of data. However its not consistent. Like the dbms views have changed maybe once or 2x. 

I was thinking pushing all the dbms views into into a staging area for simpler access and to be able to create a pipeline to transform data push it into datamarts  then from datamarts create reports. 

So ideas for this project? Does it sound right a bunch of independent datamarts with a single staging area. Then use a pipeline to populate datamarts and create reports. 

Any ideas for redshift architecture. Courses on how to set up pods.

Any help is welcomed.",True,False,False,dataengineering,t5_36en4,46718,public,self,Data engineering how to or starter guide,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rdfsds/data_engineering_how_to_or_starter_guide/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,sparkInfinitely,,,[],,,,text,t2_gnk5pg6w,False,False,False,[],False,False,1639153178,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rdc7ho/airflow_scalability/,{},rdc7ho,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rdc7ho/airflow_scalability/,False,,,6,1639153188,1,"Hello, I am working on an ETL process where I created more than 200 dags on airflow and unfortunately, the dags break when they run simultaneously (tasks fail but the logs are empty, so it's probably the scheduler or worker's fault). but when I run a few dags it never breaks...  
I wonder how much Airflow can manage and depending on what?  
Did anyone encounter such Scalability problems with Airflow?",True,False,False,dataengineering,t5_36en4,46702,public,self,Airflow scalability,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rdc7ho/airflow_scalability/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,dsingh-in,,,[],,,,text,t2_7996oj4f,False,False,False,[],False,False,1639149823,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rdazyq/apache_airflow_custom_image_on_gcp/,{},rdazyq,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rdazyq/apache_airflow_custom_image_on_gcp/,False,,,6,1639149834,1,"Hi,
Has anyone created a custom image for Apache airflow on Google cloud? (Not cloud composer).
Would be great if you can share your experiences with me.",True,False,False,dataengineering,t5_36en4,46699,public,self,Apache airflow custom image on GCP,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rdazyq/apache_airflow_custom_image_on_gcp/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Exostrike,,,[],,,,text,t2_f29rr,False,False,False,[],False,False,1639135746,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rd6nt5/onprem_only_data_engineering/,{},rd6nt5,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rd6nt5/onprem_only_data_engineering/,False,,,6,1639135756,1,"I see that most discussion in the data engineering/database field is always about cloud (Azure, AWS etc). But as someone who's company is on-prem only due to data protection requirements, is there much happening in the on-prem sphere or is the area (and my career) falling behind the curve? It feels like pretty much all new technolgy or company is cloud dependent.",True,False,False,dataengineering,t5_36en4,46690,public,self,On-prem only data engineering?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rd6nt5/onprem_only_data_engineering/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,masek94,,,[],,,,text,t2_2llofc3r,False,False,False,[],False,False,1639129033,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rd52tv/multiprocessing_for_data_pipeline_optimization/,{},rd52tv,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rd52tv/multiprocessing_for_data_pipeline_optimization/,False,,,6,1639129044,1,"Hello there, 

&amp;#x200B;

I have a very simple pipeline that consists of three steps:

\- extract data from mongo dB (with pagination of 1000 documents)

\- transform data (takes a 1000 document from the step above) and do some pandas transformation

\- save data to excel (writes pandas data frame chunk from the step above to excel)

This pipeline goes with the lazy approach over the whole MongoDB storage in 1k chunks. 

So I did all kind of optimization in each step (mongo query is very optimized, I have their indexes that makes query very fast). For pandas transformation the same, It's improved from 2-3sec per 1000k documents to 0.5 seconds. And same for excel, I use openpyxl with write mode only which makes it also much faster. But still, excel is their pain in the ass. It takes like 80-90% of the pipeline execution.

So let's assume that I have 5kk records in mongo, so extract, transform part will take 10-12h, but saving to excel 7 days :( 

And one more thing is that my excels can have a maximum of 250k records (more records cause some memory issues on my cloud)

&amp;#x200B;

So I am thinking about multiprocessing excels. 

I wonder about something like:

\- count the number of documents in mongo.

\- split storage in equal sizes 

\- start pipelines in the multi processes

Does it make any sense? One issue is that I will run the whole pipeline at the same time, but in the end, I need to only process excel in pareall. 

But as the Pipeline is made as a generator of three steps: 1000k mongo documents -&gt; transform -&gt; write excel - &gt; once more. Then it's hard to start multiprocessing at the excel step.

Probably  I can't store data anywhere - I tried to load data first to sqlite, and part of saving of excels was taking part after everything was extracted from mongo. But it took to much storage and I needed to leave that solution.

&amp;#x200B;

Can you help me a little bit with some potential solutions for my issue :D Would be great!",True,False,False,dataengineering,t5_36en4,46683,public,self,Multiprocessing for data pipeline optimization?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rd52tv/multiprocessing_for_data_pipeline_optimization/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Ok_Advance_2714,,,[],,,,text,t2_cwrfl9jz,False,False,False,[],False,False,1639117423,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rd28dg/is_faang_data_engineer_not_a_far_fetched_dream/,{},rd28dg,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rd28dg/is_faang_data_engineer_not_a_far_fetched_dream/,False,,,6,1639117434,1,"I have seen many people from small companies who know nothing in the early stage get selected as a Data engineer in FAANG later in their careers. But not many Data scientists. Is it very competitive to grow as a data scientist or does it requires more complex skills??

Is it because data engineer has a specific set of skills to master whereas data scientist needs a lot of combined skills  and continuous updation?",True,False,False,dataengineering,t5_36en4,46667,public,self,Is FAANG data engineer not a far fetched dream??,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rd28dg/is_faang_data_engineer_not_a_far_fetched_dream/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,ononimo,,,[],,,,text,t2_h5jsr,False,False,False,[],False,False,1639105547,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rcykbx/data_analyst_interview_with_the_data_engineering/,{},rcykbx,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/rcykbx/data_analyst_interview_with_the_data_engineering/,False,,,6,1639105557,1,"So, I've recently applied for a Data Analyst position and scheduled to proceed with interview with the Data Engineering team. 

My question would be what kind of qualities and skills do you expect from a Data Analyst in your team? Any tips is much appreciated. Thanks in advance!",True,False,False,dataengineering,t5_36en4,46661,public,self,Data Analyst interview with the Data Engineering team,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rcykbx/data_analyst_interview_with_the_data_engineering/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Complex-Stress373,,,[],,,,text,t2_9va3r6a5,False,False,False,[],False,False,1639097665,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rcvzts/how_to_manage_changes_in_a_dimension_field_when/,{},rcvzts,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rcvzts/how_to_manage_changes_in_a_dimension_field_when/,False,,,6,1639097675,1,"Let's say that I have a sale (a fact entry) made by a user (a dimension). initially I would insert this sale+user into my big flat table in my data warehouse. So far sounds very standard

But my doubt is about what should I do when a dimension entry have a change?, for example, the user change his address.

How should I proceed in this scenario?, should I introduce a new  sale, using the same data as before, with the user information updated,  into my big flat table?

NOTE: I'm speaking about dimensions, etc, but I'm not pretending to  use at all an star-schema as an intermediate step (at least for my  learning problem)",True,False,False,dataengineering,t5_36en4,46655,public,self,how to manage changes in a dimension field when using big flat table,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rcvzts/how_to_manage_changes_in_a_dimension_field_when/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,BlancBryn,,,[],,,,text,t2_1i178g26,False,False,False,[],False,False,1639093595,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rcunrc/data_format_solution_for_storing_nested_json_data/,{},rcunrc,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rcunrc/data_format_solution_for_storing_nested_json_data/,False,self,"{'enabled': False, 'images': [{'id': '3Kjii3APGih4eMA3B2T11cd9KdyEgxjLpj8dulXnrEs', 'resolutions': [{'height': 54, 'url': 'https://external-preview.redd.it/m1i4bIreYtBA-maQig-YdVO4A2Zd9v__jD3wMxWM6x0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c9310127ace20a48bdf7535b6af741bfd48b8448', 'width': 108}, {'height': 108, 'url': 'https://external-preview.redd.it/m1i4bIreYtBA-maQig-YdVO4A2Zd9v__jD3wMxWM6x0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2dcfe8c82e8374138e50de0d3781bd8e63e8deb5', 'width': 216}, {'height': 160, 'url': 'https://external-preview.redd.it/m1i4bIreYtBA-maQig-YdVO4A2Zd9v__jD3wMxWM6x0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2dab1ffaf31884e483ddefa6d8f340968c5547c5', 'width': 320}], 'source': {'height': 300, 'url': 'https://external-preview.redd.it/m1i4bIreYtBA-maQig-YdVO4A2Zd9v__jD3wMxWM6x0.jpg?auto=webp&amp;s=4c5348186ddc4d002011da298d5bab294c6a2fb2', 'width': 600}, 'variants': {}}]}",6,1639093606,1,"Hello,

I need to create a data pipeline using Python that retrieves data from a REST API and writes it to a database.

Currently I have a first version running using a Microsoft SQL database.

The problem is that the JSON Responses of the API is a complex nested JSON, in which there are several arrays that change frequently in the number of their elements.

As a result, I have to constantly adjust the pipeline and database tables to accommodate the new fields and changes. 

&amp;#x200B;

Do you know a way how I can build a pipeline in a way that I can also save changing arrays as fields, so that the query of the target data works without problems and without having to make manual changes?

I'm assuming SQL Server just isn't designed for this. Would you recommend me e.g. MongoDB for this or using Parquet Files?  

&amp;#x200B;

Translated with [www.DeepL.com/Translator](https://www.DeepL.com/Translator) (free version)",True,False,False,dataengineering,t5_36en4,46655,public,self,Data format solution for storing nested json data in a handy way? Ideally to be able to query with SQL,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rcunrc/data_format_solution_for_storing_nested_json_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,srajeevan89,,,[],,,,text,t2_25nt9ty6,False,False,False,[],False,False,1639080769,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rcq47i/interested_in_learning_together/,{},rcq47i,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rcq47i/interested_in_learning_together/,False,,,6,1639080780,1,"Hi,

I have been trying to move in to DE roles for the last few months.It’s not that easy to switch tech stack I understand.
Without having any solid side projects to include in the resume it is hard to get in to a DE roles or something similar.
I follow the  SeattleDataGuy YouTube channel and his medium posts.He has some good info and posts about how to start building side projects and general direction on how to move into the DE space slowly and master tools and technologies.
He has a medium post “Data Engineering Roadmap for 2021”.
Based on that I am planing to build a Python Flask API as a first project and then slowly build things on top of it.
That’s the plan.

Anyone interested in getting into this.?
We can follow and do the different steps/projects mentioned in that post.

Please let me know in comments.",True,False,False,dataengineering,t5_36en4,46639,public,self,Interested in learning together,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rcq47i/interested_in_learning_together/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,notqualifiedforthis,,,[],,,,text,t2_13rwsm,False,False,False,[],False,False,1639078346,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rcp7vt/presenting_one_to_many_record_comparisons_w/,{},rcp7vt,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rcp7vt/presenting_one_to_many_record_comparisons_w/,False,,,6,1639078356,1,"Hey all, this question might be borderline off-topic for this sub but I think the follower's have here are most likely to have been in this situation.

I've been working on a large data matching exercise where we're trying to identify a company from a legacy system in a modernized system.  We've knocked out the simple &amp; straight forward matches using company identifiers and we're down to fuzzy matching company names, addresses, etc.  We're getting good results but my output at the moment is an Excel spreadsheet that non-data team members are struggling to wrap their head around. 

I'm looking for advice on how to present the results in an intuitive way to allow my team members to approve or deny a match quickly &amp; accurately.  Legacy company has 10 attributes while the company in the modernized system has 10 attributes &amp; many addresses that would need compared.  I could spend a couple days spinning up a lightweight web app but I keep thinking something is already out there.",True,False,False,dataengineering,t5_36en4,46635,public,self,Presenting One to Many Record Comparisons w/ Approve &amp; Deny options,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rcp7vt/presenting_one_to_many_record_comparisons_w/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Complex-Stress373,,,[],,,,text,t2_9va3r6a5,False,False,False,[],False,False,1639076747,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rcomqy/how_do_you_usually_manage_schema_evolution_in/,{},rcomqy,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,1,0,False,all_ads,/r/dataengineering/comments/rcomqy/how_do_you_usually_manage_schema_evolution_in/,False,,,6,1639076758,1,"I'm building a data model, just a learning project.

My project has some schemas that helps me to validate data and extract fields from some files. Then I built my data model. 

Now I know that if my schema change, this needs to be reflected in my star data model. For example, if a new column is added, I have to set null values for previous values. If a column is removed I have to remove the column, ....things like that.

However, how do you usually approach this?. Do you compare column by column?, is there something elegant out there or an starndard way of doing this?

Thanks in advance, honestly this channel is being very helpful, thanks",True,False,False,dataengineering,t5_36en4,46633,public,self,how do you usually manage schema evolution in your star data model?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rcomqy/how_do_you_usually_manage_schema_evolution_in/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,unsaltedrhino,,,[],,,,text,t2_9or07,False,False,False,[],False,False,1639073209,/r/dataengineering/comments/rcncy3/carto_databricks_bringing_spatial_analysis_to_the/,https://www.reddit.com/r/dataengineering/comments/rcncy3/carto_databricks_bringing_spatial_analysis_to_the/,{},rcncy3,False,True,False,False,False,True,False,True,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rcncy3/carto_databricks_bringing_spatial_analysis_to_the/,False,hosted:video,"{'enabled': False, 'images': [{'id': 'GGeS9Bdv3H18X3V3XaFJPidB8A6v8EultwEqxvGra8M', 'resolutions': [{'height': 52, 'url': 'https://external-preview.redd.it/kUBdPBEqKhEmlaITVX-cN-h0HlF59aPXi3nCsMu1SEE.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d1b4947de670d174ba4caf77171b8ac27fc857ec', 'width': 108}, {'height': 104, 'url': 'https://external-preview.redd.it/kUBdPBEqKhEmlaITVX-cN-h0HlF59aPXi3nCsMu1SEE.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=85568be83bde5ec12308fd2c9f5086f64b6299a2', 'width': 216}, {'height': 155, 'url': 'https://external-preview.redd.it/kUBdPBEqKhEmlaITVX-cN-h0HlF59aPXi3nCsMu1SEE.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=bf9814187501585e0ed319a7e046f132de4a777f', 'width': 320}, {'height': 310, 'url': 'https://external-preview.redd.it/kUBdPBEqKhEmlaITVX-cN-h0HlF59aPXi3nCsMu1SEE.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=28e5756597f0670ec113b659ba1d0be109758dbd', 'width': 640}, {'height': 465, 'url': 'https://external-preview.redd.it/kUBdPBEqKhEmlaITVX-cN-h0HlF59aPXi3nCsMu1SEE.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=aef75bd3a255f944144a1bd228572446832ea70a', 'width': 960}, {'height': 523, 'url': 'https://external-preview.redd.it/kUBdPBEqKhEmlaITVX-cN-h0HlF59aPXi3nCsMu1SEE.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=384ecd1f7adc73a6aeae370a1d6a2826205ad737', 'width': 1080}], 'source': {'height': 620, 'url': 'https://external-preview.redd.it/kUBdPBEqKhEmlaITVX-cN-h0HlF59aPXi3nCsMu1SEE.png?format=pjpg&amp;auto=webp&amp;s=40ad93ad5dbe69bdc0f7484a1e150114bacfc747', 'width': 1278}, 'variants': {}}]}",6,1639073219,1,,True,False,False,dataengineering,t5_36en4,46628,public,https://b.thumbs.redditmedia.com/o4uthgMPccjmeTqbJGU4CEp9klsJcsmHNJt93TYCFJs.jpg,CARTO &amp; Databricks: Bringing Spatial Analysis to the Lakehouse Platform - link in comments,0,[],1.0,https://v.redd.it/wp0qsf8e4k481,all_ads,6,,,,,,67.0,140.0,https://v.redd.it/wp0qsf8e4k481,,,,,,,,,,
[],False,hananmaqbool8,,,[],,,,text,t2_48u7vp3a,False,False,False,[],False,False,1639068701,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rclq50/course_recommendations_to_learn_data_engineering/,{},rclq50,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rclq50/course_recommendations_to_learn_data_engineering/,False,,,6,1639068712,1,I understand are a lot of courses out there but I would like to get course recommendations from data engineers themselves. I already know Python and can work with a bit of data but want to go through a course that'll teach me DE from start to finish. Thanks!,True,False,False,dataengineering,t5_36en4,46622,public,self,Course recommendations to learn Data Engineering?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rclq50/course_recommendations_to_learn_data_engineering/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Sloth_unleashed121,,,[],,,,text,t2_2844caf2,False,False,False,[],False,False,1639066137,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rcku64/hi_everyone_need_your_advise/,{},rcku64,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rcku64/hi_everyone_need_your_advise/,False,,,6,1639066148,1,"Im not exactly a Data Engineer, but I’m very interested in the field. I’m currently trying to run a highload pet project.

I have a Python scripts That generates large volume of Data ( about several Million rows) and sends it to the database using airflow once a day. 

Now I want to send batches of data based on a certain event( which in my case is just a random integer). I was told that airflow is not a good solution for that. Can you give any comments? 

Another thing is, I want to aggregate data along the way, to take down the size of batches. But I’m not sure how to address the memory issue. Will airflow store all the million rows during the aggregation? Is there some limit on the storage it provides? Can’t really find that information anywhere. 


Thank you in advance!",True,False,False,dataengineering,t5_36en4,46617,public,self,"Hi everyone, need your advise",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rcku64/hi_everyone_need_your_advise/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,CodeSigner1010,,,[],,,,text,t2_g40hvsw5,False,False,False,[],False,False,1639065461,youtube.com,https://www.reddit.com/r/dataengineering/comments/rcklao/seems_like_a_common_syntax_error_to_me/,{},rcklao,False,False,False,False,False,False,False,False,#ff66ac,[],dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1,Meme,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rcklao/seems_like_a_common_syntax_error_to_me/,False,rich:video,"{'enabled': False, 'images': [{'id': 'C_UKsl_3lcOku4D6nKqya7n9XE2k_P5KhDw4PuLv6eY', 'resolutions': [{'height': 81, 'url': 'https://external-preview.redd.it/iuSonzjxUomrAChaXIAFdN4exaRnkNBzcY5AcJiubxo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=23b37f8120c60b6ae24158b3a2800402c8a31dda', 'width': 108}, {'height': 162, 'url': 'https://external-preview.redd.it/iuSonzjxUomrAChaXIAFdN4exaRnkNBzcY5AcJiubxo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e01345fdfc7068a8204c887ba91352fb478c85cd', 'width': 216}, {'height': 240, 'url': 'https://external-preview.redd.it/iuSonzjxUomrAChaXIAFdN4exaRnkNBzcY5AcJiubxo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f3ba04065b05057bcf6c493b7961b8a3e54a4e72', 'width': 320}], 'source': {'height': 360, 'url': 'https://external-preview.redd.it/iuSonzjxUomrAChaXIAFdN4exaRnkNBzcY5AcJiubxo.jpg?auto=webp&amp;s=6974d5a1658692f1dd4f23cb88fbaa8045085735', 'width': 480}, 'variants': {}}]}",6,1639065471,1,,True,False,False,dataengineering,t5_36en4,46613,public,https://b.thumbs.redditmedia.com/6YLfcO5oTBTtwjXFrFykk6WZPwZHsZFs8DhyJEjLKog.jpg,Seems like a common syntax error to me 😂,0,[],1.0,https://www.youtube.com/watch?v=Gp0GzK4OHUc,all_ads,6,"{'oembed': {'author_name': '#SingYourCode', 'author_url': 'https://www.youtube.com/channel/UCFZjxwAmxfDOU6XYA46Zl0g', 'height': 200, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/Gp0GzK4OHUc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/Gp0GzK4OHUc/hqdefault.jpg', 'thumbnail_width': 480, 'title': '#SingYourCode - Corporate Software Policies Gone Wrong', 'type': 'video', 'version': '1.0', 'width': 356}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/Gp0GzK4OHUc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'scrolling': False, 'width': 356}",automod_filtered,"{'oembed': {'author_name': '#SingYourCode', 'author_url': 'https://www.youtube.com/channel/UCFZjxwAmxfDOU6XYA46Zl0g', 'height': 200, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/Gp0GzK4OHUc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/Gp0GzK4OHUc/hqdefault.jpg', 'thumbnail_width': 480, 'title': '#SingYourCode - Corporate Software Policies Gone Wrong', 'type': 'video', 'version': '1.0', 'width': 356}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/Gp0GzK4OHUc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/rcklao', 'scrolling': False, 'width': 356}",105.0,140.0,https://www.youtube.com/watch?v=Gp0GzK4OHUc,,,,,,,,,,
[],False,adnanrahic,,,[],,,,text,t2_nrqscal,False,False,False,[],False,False,1639064846,cube.dev,https://www.reddit.com/r/dataengineering/comments/rckdx2/heres_how_i_built_clickhouse_data_visualization/,{},rckdx2,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rckdx2/heres_how_i_built_clickhouse_data_visualization/,False,,,6,1639064857,1,,True,False,False,dataengineering,t5_36en4,46611,public,default,Here's how I built ClickHouse data visualization with a metrics API layer,0,[],1.0,https://cube.dev/blog/building-a-clickhouse-visualization-with-altinity-and-cube/,all_ads,6,,,,,,,,https://cube.dev/blog/building-a-clickhouse-visualization-with-altinity-and-cube/,,,,,,,,,,
[],False,Cool_Telephone,,,[],,,,text,t2_87zfi59a,False,False,False,[],False,False,1639061796,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rcjcz2/a_quick_data_modelling_question/,{},rcjcz2,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rcjcz2/a_quick_data_modelling_question/,False,,,6,1639061807,1,"Hi all, hoping for a confirmation (or discussion) on Kimball Methodology. How do you handle an accumulating snapshot fact, linked to a SCD2 dimension? Do you add the record to the fact based on the first key, and then simply update the accumulating columns, always pointing to the first version of the dim, or do you rebuild the fact from scratch each update using the key from the ‘current’ version of the dim? Or something else?",True,False,False,dataengineering,t5_36en4,46611,public,self,A quick data modelling question,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rcjcz2/a_quick_data_modelling_question/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Competitive-Reason35,,,[],,,,text,t2_8dnte4zw,False,False,False,[],False,False,1639057450,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rchygc/ways_of_productionising_data_engineering_skills/,{},rchygc,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rchygc/ways_of_productionising_data_engineering_skills/,False,,,6,1639057461,1,"As a 5YOE freelance data engineer, i’m looking for ways to make B2B/B2C products using my data engineering skills.

E.g. my girlfriend, who is a designer, creates templates for websites like Webflow. 

What are similar ways to productionise data engineering skills? Any ideas?",True,False,False,dataengineering,t5_36en4,46609,public,self,Ways of ‘productionising’ data engineering skills.,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rchygc/ways_of_productionising_data_engineering_skills/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,demince,,,[],,,,text,t2_umfhx4d,False,False,False,[],False,False,1639057365,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rchxkn/doing_easy_data_ingestion/,{},rchxkn,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rchxkn/doing_easy_data_ingestion/,False,self,"{'enabled': False, 'images': [{'id': 'BOuxpRdI-mIACiAoz8XXZieOr7l-7OdOSXRRyA9RhSc', 'resolutions': [{'height': 81, 'url': 'https://external-preview.redd.it/cDBAA4e1bbPdEW-svjWMLnM-LXjS6quZcpLTZxmElHA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=944fa0e6ee20e52e3f79f7427eebd6bb8d512b98', 'width': 108}, {'height': 162, 'url': 'https://external-preview.redd.it/cDBAA4e1bbPdEW-svjWMLnM-LXjS6quZcpLTZxmElHA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8d28c8faa8d89d3ac03486d26d393ddab4fe5943', 'width': 216}, {'height': 240, 'url': 'https://external-preview.redd.it/cDBAA4e1bbPdEW-svjWMLnM-LXjS6quZcpLTZxmElHA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7be416dcb1f92a1c5f92f567da215ef4a5f25b85', 'width': 320}], 'source': {'height': 360, 'url': 'https://external-preview.redd.it/cDBAA4e1bbPdEW-svjWMLnM-LXjS6quZcpLTZxmElHA.jpg?auto=webp&amp;s=6fef445dfa0854b65c01316f729428997831c1b6', 'width': 480}, 'variants': {}}]}",6,1639057376,1,Simplify the data ingestion: [https://www.youtube.com/watch?v=JRV\_5cxVQDU&amp;ab\_channel=VersatileDataKit](https://www.youtube.com/watch?v=JRV_5cxVQDU&amp;ab_channel=VersatileDataKit),True,False,False,dataengineering,t5_36en4,46608,public,self,Doing easy data ingestion,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rchxkn/doing_easy_data_ingestion/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,thewizzzy,,,[],,,,text,t2_bsujdgr,False,False,False,[],False,False,1639042107,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rce4du/handling_unicode_dash_hyphen_minus/,{},rce4du,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rce4du/handling_unicode_dash_hyphen_minus/,False,,,6,1639042118,1,"We are using various system for our reporting and processing.
SQL Server for storing reporting data
PowerBI for Dashboarding
Parquet + Hive + Azure Synapse for big data and ELT layer
C# for some custom web app

Often time we receive ""weird"" data especially from our non-english native speaking country.
They have Hyphen vs  Dash vs Minus, open paratheses vs round bracket, etc.

this drive data reporting mad.
as SQL and PowerBI by default is not case sensitive, nor accent sensitive. but C#, Hive, Parquet is.

User is OK for losing some details, e.g. harmonize hyphen, dash, and minus to just normal "" - "".
and we can't have full list of ""odd"" characters to handle.


Any good suggestion how to sanitize this data?
Any good library or something to clean this?",True,False,False,dataengineering,t5_36en4,46599,public,self,Handling UNICODE - Dash Hyphen Minus,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rce4du/handling_unicode_dash_hyphen_minus/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,dudeydudee,,,[],,,,text,t2_i6ld8,False,False,False,[],False,False,1639037004,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rcd05d/is_powerapps_c_or_the_power_platform_ecosystem/,{},rcd05d,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rcd05d/is_powerapps_c_or_the_power_platform_ecosystem/,False,,,6,1639037015,1,"I need to have a cloud-hosted application to read and convert Sharepoint files with very simple scripts. 

I would normally do this in a jupyter notebook, but we don't currently have anywhere that could host the code so anybody could run it. 

Do PowerApps in Sharepoint provide similar or good functionality for data manipulation and ETL down the road? I would have to learn C# but I don't mind if it provides some utility later on in my career. Sorry if this is a dumb question and thanks so much for the help!",True,False,False,dataengineering,t5_36en4,46595,public,self,Is PowerApps (C#) or the Power Platform ecosystem worth learning as a data analyst/engineer?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rcd05d/is_powerapps_c_or_the_power_platform_ecosystem/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Noe_Achache,,,[],,,,text,t2_86tovp8e,False,False,False,[],False,False,1639036974,sicara.ai,https://www.reddit.com/r/dataengineering/comments/rcczuu/launch_experiments_on_databricks_from_your_local/,{},rcczuu,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rcczuu/launch_experiments_on_databricks_from_your_local/,False,link,"{'enabled': False, 'images': [{'id': 'knZkCRf7VD1Q9KcykW_6i38KtkhBUp5zqHcbxCoA1ic', 'resolutions': [{'height': 60, 'url': 'https://external-preview.redd.it/RK_pm4C1008Xc7LHE6875YXa-c84FrPRktnGCD87us4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=35ab0a02f15a829aaa9f623dbc7a75180bb4b30f', 'width': 108}, {'height': 121, 'url': 'https://external-preview.redd.it/RK_pm4C1008Xc7LHE6875YXa-c84FrPRktnGCD87us4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=43c661fe0141825c7a5411d7c45d17e5a1b571a5', 'width': 216}, {'height': 179, 'url': 'https://external-preview.redd.it/RK_pm4C1008Xc7LHE6875YXa-c84FrPRktnGCD87us4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c384131a26cc8ba40061256188de04c1e6f41317', 'width': 320}, {'height': 359, 'url': 'https://external-preview.redd.it/RK_pm4C1008Xc7LHE6875YXa-c84FrPRktnGCD87us4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3d2161cd61cc1f882c2aa963f24ad9a6c127fa68', 'width': 640}, {'height': 539, 'url': 'https://external-preview.redd.it/RK_pm4C1008Xc7LHE6875YXa-c84FrPRktnGCD87us4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3130bd1d3c6d4188a1fab9391bc52b87bcfd5e26', 'width': 960}], 'source': {'height': 562, 'url': 'https://external-preview.redd.it/RK_pm4C1008Xc7LHE6875YXa-c84FrPRktnGCD87us4.jpg?auto=webp&amp;s=115a4d3932b3a2fb754d9045fbbb8112330761c2', 'width': 1000}, 'variants': {}}]}",6,1639036985,1,,False,False,False,dataengineering,t5_36en4,46595,public,https://a.thumbs.redditmedia.com/o6bs-fOdqZmmfTRM9CB6RtLqxfb69Gl6GQQrvBMMu08.jpg,Launch experiments on Databricks from your local environment,0,[],1.0,https://www.sicara.ai/blog/launch-experiments-databricks-local-environment,all_ads,6,,,,,,78.0,140.0,https://www.sicara.ai/blog/launch-experiments-databricks-local-environment,,,,,,,,,,
[],False,BoiElroy,,,[],,,,text,t2_r8dyi,False,False,True,[],False,False,1639034232,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rcccmg/delta_transaction_log_for_non_parquet_data/,{},rcccmg,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rcccmg/delta_transaction_log_for_non_parquet_data/,False,,,6,1639034243,1,"I'm curious how difficult it would be to create a Delta log for non parquet data. Specifically for lots of blob data. Images or audio etc.

Delta log is quite good but I just don't like that it turns image files that would be in directories like /images/cat/img002.png, images/dog/img356.png into --&gt; parquet-part0002.parquet.
The original structure is of course retrievable but it obscures the ability to be able to just look at what data is there unless you run an operation that writes back out from the parquet data or something.

Just using MD5 hashes, timestamps, and transaction audits would it really be so difficult to come up with a system that works as well without needing to use parquet?",True,False,False,dataengineering,t5_36en4,46592,public,self,Delta / transaction log for non parquet data,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rcccmg/delta_transaction_log_for_non_parquet_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,nfrankel,,,[],,,,text,t2_ayl6m,False,False,False,[],False,False,1639033827,discuss.elastic.co,https://www.reddit.com/r/dataengineering/comments/rcc97b/seven_tricks_to_building_fast_reliable_search/,{},rcc97b,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rcc97b/seven_tricks_to_building_fast_reliable_search/,False,link,"{'enabled': False, 'images': [{'id': 'dFVuZGAy7c4nZyBqOBWPNi9qryGzxp4lihDx7BzrHZU', 'resolutions': [{'height': 108, 'url': 'https://external-preview.redd.it/PzgCyzOas5L23az-KkQ2b404DupsBhsML5qeMN4zCqg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=da0f1fd2ed184b977d88bfa47869f9dca14a0942', 'width': 108}, {'height': 216, 'url': 'https://external-preview.redd.it/PzgCyzOas5L23az-KkQ2b404DupsBhsML5qeMN4zCqg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0625db4e68b7a24c1174cb03a4f2e09ed3b1612c', 'width': 216}, {'height': 320, 'url': 'https://external-preview.redd.it/PzgCyzOas5L23az-KkQ2b404DupsBhsML5qeMN4zCqg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ad98e9b8d209244d5a7cf95df578120e9680bb50', 'width': 320}], 'source': {'height': 512, 'url': 'https://external-preview.redd.it/PzgCyzOas5L23az-KkQ2b404DupsBhsML5qeMN4zCqg.jpg?auto=webp&amp;s=4fe3e30245394b1a2ca9cda4660f17f9af406bce', 'width': 512}, 'variants': {}}]}",6,1639033837,1,,True,False,False,dataengineering,t5_36en4,46591,public,https://a.thumbs.redditmedia.com/zJv0JlHY1gIF8pc5oW0QB157OToZcGUXt9FR5Pu3iB0.jpg,"Seven tricks to building fast, reliable search data integrations",0,[],1.0,https://discuss.elastic.co/t/dec-7th-2021-en-seven-tricks-to-building-fast-reliable-search-data-integrations/289184,all_ads,6,,,,,,140.0,140.0,https://discuss.elastic.co/t/dec-7th-2021-en-seven-tricks-to-building-fast-reliable-search-data-integrations/289184,,,,,,,,,,
[],False,SeaworthinessFit7893,,,[],,,,text,t2_4bor0kj8,False,False,False,[],False,False,1639018122,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rc7t4f/free_time/,{},rc7t4f,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rc7t4f/free_time/,False,,,6,1639030381,1,Do you guys get time off from your jobs regularly. Like what are the average D.E's work schedule look like?,True,False,False,dataengineering,t5_36en4,46588,public,self,Free time,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rc7t4f/free_time/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,10xbek,,,[],,,,text,t2_gtqs4z53,False,False,False,[],False,False,1639016704,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rc7d86/a_little_help/,{},rc7d86,False,False,False,False,False,False,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rc7d86/a_little_help/,False,,,6,1639030068,1,[removed],True,False,False,dataengineering,t5_36en4,46588,public,self,A little help,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rc7d86/a_little_help/,all_ads,6,,,moderator,,,,,,,,,,,,,,,
[],True,garrrikkotua,,,[],,,,text,t2_8yz625qv,False,False,False,[],False,False,1639001776,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rc2gnr/data_catalog_with_sqltotext_will_it_help_business/,{},rc2gnr,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,6,0,False,all_ads,/r/dataengineering/comments/rc2gnr/data_catalog_with_sqltotext_will_it_help_business/,False,,,6,1639026594,1,"Hi there, my name is Igor. 

I am testing different startup ideas in data governance space.

Sometimes business users have problems with data trust - a dashboard looks strange and they don't know whether it's a technical problem or a business one. They want to understand, how particular metric is calculated. Usually, these metrics are calculated via SQL, so I think SQL-To-Text might be usefull here to automatically generate descriptions and explanations for dashboards, reports, etc.

Do you think it's a big and important problem which is worth solving? Will natural language processing (SQL-To-Text) really help here?

Any thoughts are welcome :)",True,False,False,dataengineering,t5_36en4,46586,public,self,Data Catalog with SQL-To-Text. Will it help business users with data discovery and observability?,0,[],0.67,https://www.reddit.com/r/dataengineering/comments/rc2gnr/data_catalog_with_sqltotext_will_it_help_business/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,StandardDull3128,,,[],,,,text,t2_9hg5hon1,False,False,False,[],False,False,1639000893,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rc25d9/can_i_be_an_extrovert_and_still_be_a_data_engineer/,{},rc25d9,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,10,0,False,all_ads,/r/dataengineering/comments/rc25d9/can_i_be_an_extrovert_and_still_be_a_data_engineer/,False,,,6,1639026361,0,"I am thinking about transitioning to a full on data engineering career and that is what eventually led me to this subreddit. In my current job (1 1/2 years) I am in a mixed role... analyst/scientist/engineer with some project management responsibilities as well. Before I was in a BI developer role for about 3 years. So as I work with data I find that I maybe take more pleasure in the data engineering tasks, such as DW design and ETL processes, as opposed to DS projects. I do enjoy doing analyst work, but comparing salaries this seems to be the least payed role of the three (DE, DS, DA). The plan is to do the Udacity nano-degree course to fill in the missing tools I need for a fully pledged DE, see if I really like this kind of work and then try to find a full on DE job. While I search I intend to ask for more DE tasks in my current position.  


This brings me to my question. I was looking at some videos from the Seattle Data Guy on YT and in one of the videos he mentions that a DE job might be a fit for you if you are naturally introverted. Personally I enjoy working with people a lot. So the occasional meet and presentation I do in my current role is very welcome. I fear that in the DE role that I am pursuing I will find myself staring at the screen all day and not really interacting with people.   


Is there anyone here that has transitioned from a Data Analyst or BI developer into a Data Engineer and can share their experiences from a social perspective. Do you work with people less then in your previous role? If yes, how does this affect you? Do you miss human interaction?  


Thanks and cheers  


PS: Really loving this subreddit. You guys are great! :)",True,False,False,dataengineering,t5_36en4,46587,public,self,Can I be an extrovert and still be a data engineer?,0,[],0.5,https://www.reddit.com/r/dataengineering/comments/rc25d9/can_i_be_an_extrovert_and_still_be_a_data_engineer/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,SeaworthinessFit7893,,,[],,,,text,t2_4bor0kj8,False,False,False,[],False,False,1639000532,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rc20bm/what_do_i_need/,{},rc20bm,False,False,False,False,False,False,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,6,0,False,all_ads,/r/dataengineering/comments/rc20bm/what_do_i_need/,False,,,6,1639026262,2,[removed],True,False,False,dataengineering,t5_36en4,46587,public,default,What do I need?,0,[],0.63,https://www.reddit.com/r/dataengineering/comments/rc20bm/what_do_i_need/,all_ads,6,,,moderator,,,,,,,,,,,,,,,
[],False,nileg,,,[],,,,text,t2_78c5q0kj,False,False,False,[],False,False,1638992476,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rbz79j/can_unethically_produced_data_be_used_ethically/,{},rbz79j,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rbz79j/can_unethically_produced_data_be_used_ethically/,False,self,"{'enabled': False, 'images': [{'id': 'meZ03ZH6qHj0X75IlM3QBlilviTXPgXQR1lAchle3OY', 'resolutions': [{'height': 56, 'url': 'https://external-preview.redd.it/5CEavYVSCdEREmOI-5V61uNPT8Ft8DevhuY_31tUf8k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3c308ffd31456ec7163328666b759632aeba634e', 'width': 108}, {'height': 113, 'url': 'https://external-preview.redd.it/5CEavYVSCdEREmOI-5V61uNPT8Ft8DevhuY_31tUf8k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=603c65237b0181676f228b7a858d3e445c9dba24', 'width': 216}, {'height': 168, 'url': 'https://external-preview.redd.it/5CEavYVSCdEREmOI-5V61uNPT8Ft8DevhuY_31tUf8k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a0b22dedc618efd08cea123a702c38a67f3be465', 'width': 320}, {'height': 336, 'url': 'https://external-preview.redd.it/5CEavYVSCdEREmOI-5V61uNPT8Ft8DevhuY_31tUf8k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=310ab42346f2e74842dadb7df59d024fd6a36fbc', 'width': 640}, {'height': 504, 'url': 'https://external-preview.redd.it/5CEavYVSCdEREmOI-5V61uNPT8Ft8DevhuY_31tUf8k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bede0622d368c9dc210392d080bed9a04b17b4e8', 'width': 960}, {'height': 567, 'url': 'https://external-preview.redd.it/5CEavYVSCdEREmOI-5V61uNPT8Ft8DevhuY_31tUf8k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e5975f8fa6b7d41be3a4ffeb8800b92100fa3e87', 'width': 1080}], 'source': {'height': 630, 'url': 'https://external-preview.redd.it/5CEavYVSCdEREmOI-5V61uNPT8Ft8DevhuY_31tUf8k.jpg?auto=webp&amp;s=5810cb0dd333dbf420a33b621caba4ef11faec73', 'width': 1200}, 'variants': {}}]}",6,1639024241,0,"*Can Unethically Produced Data be Used Ethically?* This survey serves as the backbone of my research paper which tries to examine the question of ethics in scientific research and its advancement. Please take a minute to fill it out.

[https://forms.gle/eKak1dpWGUbZ5bAZ7](https://forms.gle/eKak1dpWGUbZ5bAZ7)",True,False,False,dataengineering,t5_36en4,46586,public,self,Can Unethically Produced Data be Used Ethically? Survey. Please do fill it out,0,[],0.5,https://www.reddit.com/r/dataengineering/comments/rbz79j/can_unethically_produced_data_be_used_ethically/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,ndz0r,,,[],,,,text,t2_6uvk2eb,False,False,False,[],False,False,1638989578,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rby6mv/is_it_possible_to_analyse_datasets_that_contains/,{},rby6mv,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,3,0,False,all_ads,/r/dataengineering/comments/rby6mv/is_it_possible_to_analyse_datasets_that_contains/,False,,,6,1639023508,5,"I'm trying to learn more about data science from the (too basic) courses I took during my past bachelor's degree in computer science.

Until now I have mainly worked with already strucured datasets that I found in various places. I always cleaned these datasets with Pandas and then applied supervised or semi-supervised methods just by manipulating variables names. The structure of the dataframes has always been the same, for example :

&amp;#x200B;

||id|gender|country|age|
|:-|:-|:-|:-|:-|
|0|242|M|Brazil|40|
|...|...|...|...|...|
|n|815|F|Canada|38|

&amp;#x200B;

So I decided to challenge myself by building my own dataset with indicators over the years I found from different free databases. After a first batch of headaches with data processing and data cleaning I finally managed to obtain two DataFrames with the same structure (years as index and names as column headers) except for the values which are different (one indicator per DF), here is an example:

**First DataFrame with weight**

||Name #1|Name #2|Name #...|Name #n|
|:-|:-|:-|:-|:-|
|Year 1|138|129|185|130|
|Year ...|...|...|...|...|
|Year n|174|155|134|220|

**Second DataFrame with size**

||Name #1|Name #2|Name #...|Name #n|
|:-|:-|:-|:-|:-|
|Year 1|49|51|49|55|
|Year ...|...|...|...|...|
|Year n|62|61|59|64|

&amp;#x200B;

I don't have the impression that a merge or a concat are possible since I have almost 10 000 individuals over about 50 years, with the names of the columns as labels of the individuals themselves. Where I block is how to obtain same kind of structure that in the first exemple in order to apply some ML methods as usual ? I am wondering about combining all 3 dataframes for multi-indexing, but how would I do my analysis ? 

I have turned the problem around in several directions and I must have made a mistake somewhere but without managing where exactly, yet I dare to assume that this problem must be common in data science, unless I am wrong ? How can you perform an analysis with python in this case ?",True,False,False,dataengineering,t5_36en4,46586,public,self,Is it possible to analyse datasets that contains only one value per column over the years ?,0,[],0.86,https://www.reddit.com/r/dataengineering/comments/rby6mv/is_it_possible_to_analyse_datasets_that_contains/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,thisaburneryall,,,[],,,,text,t2_h27uhz36,False,False,False,[],False,False,1638989282,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rby2w3/any_warnings_before_i_break_something/,{},rby2w3,False,False,False,False,False,False,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rby2w3/any_warnings_before_i_break_something/,False,,,6,1639023431,1,[removed],True,False,False,dataengineering,t5_36en4,46586,public,default,any warnings before i break something,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rby2w3/any_warnings_before_i_break_something/,all_ads,6,,,reddit,,,,,,,,,,,,,,,
[],False,Twinkletoespipay,,,[],,,,text,t2_hcphzbnq,False,False,False,[],False,False,1638983622,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rbw05r/mba_and_de/,{},rbw05r,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,7,0,False,all_ads,/r/dataengineering/comments/rbw05r/mba_and_de/,False,,,6,1639021917,6,Is having a master’s degree in business beneficial to a DE career in the future?,True,False,False,dataengineering,t5_36en4,46586,public,self,MBA and DE,0,[],0.81,https://www.reddit.com/r/dataengineering/comments/rbw05r/mba_and_de/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,TreapPeep,,,[],,,,text,t2_8nuqwg5q,False,False,False,[],False,False,1638976914,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rbtn1s/shopifys_monolithic_architecture_averaged_30tbmin/,{},rbtn1s,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,9,0,False,all_ads,/r/dataengineering/comments/rbtn1s/shopifys_monolithic_architecture_averaged_30tbmin/,False,self,"{'enabled': False, 'images': [{'id': 'EEr0GHHdEcfaC9G4UV1oa147ssu_R4yLLxPgyD7M-TQ', 'resolutions': [{'height': 108, 'url': 'https://external-preview.redd.it/-X5Rxdth4vMsCCyHZKEHT52pXqsFZAP0eK5aikuL0ic.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8aa907698c2871235e0ea047cd78310957a4eb4d', 'width': 108}], 'source': {'height': 140, 'url': 'https://external-preview.redd.it/-X5Rxdth4vMsCCyHZKEHT52pXqsFZAP0eK5aikuL0ic.jpg?auto=webp&amp;s=7a862f5a38ad20eb3ab41bb738c36aabf4caa1ed', 'width': 140}, 'variants': {}}]}",6,1639020222,50,"Pretty interesting stats from tweetstorm out of Shopify's Engineering twitter.

I'm a new-comer DE, any idea how they were able to achieve this? 

[https://twitter.com/ShopifyEng/status/1465806691543531525](https://twitter.com/ShopifyEng/status/1465806691543531525)",True,False,False,dataengineering,t5_36en4,46585,public,self,Shopify's Monolithic Architecture Averaged ~30TB/min in egress,0,[],0.98,https://www.reddit.com/r/dataengineering/comments/rbtn1s/shopifys_monolithic_architecture_averaged_30tbmin/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,rmoff,,,[],,,,text,t2_bvkm0,False,False,False,[],False,False,1638975323,dev.to,https://www.reddit.com/r/dataengineering/comments/rbt2zj/kafka_summit_london_2022_call_for_papers_closes/,{},rbt2zj,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rbt2zj/kafka_summit_london_2022_call_for_papers_closes/,False,link,"{'enabled': False, 'images': [{'id': 'bZS6v9byS5pGVujl2qFxnnXhs8E_KWd6EmtePlFLHGo', 'resolutions': [{'height': 54, 'url': 'https://external-preview.redd.it/-4-4c2iEsm3O426DkpwcRPSNOONQQ3ILGbh9SuOiJ1E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7cc7928ad1e9e6475f572b81226d0429af434dbd', 'width': 108}, {'height': 108, 'url': 'https://external-preview.redd.it/-4-4c2iEsm3O426DkpwcRPSNOONQQ3ILGbh9SuOiJ1E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3bf2096235b495ac2363849f386fa6e3b0af01c5', 'width': 216}, {'height': 160, 'url': 'https://external-preview.redd.it/-4-4c2iEsm3O426DkpwcRPSNOONQQ3ILGbh9SuOiJ1E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8546c25e31b836b63daeb8e08d89bf9308b3b500', 'width': 320}, {'height': 320, 'url': 'https://external-preview.redd.it/-4-4c2iEsm3O426DkpwcRPSNOONQQ3ILGbh9SuOiJ1E.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=155eeb1f4ad7990126f91856f747c77bfee9b518', 'width': 640}, {'height': 480, 'url': 'https://external-preview.redd.it/-4-4c2iEsm3O426DkpwcRPSNOONQQ3ILGbh9SuOiJ1E.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d57acf3f1a07c58ddd9f9bcd8c1a13c2070268b9', 'width': 960}], 'source': {'height': 500, 'url': 'https://external-preview.redd.it/-4-4c2iEsm3O426DkpwcRPSNOONQQ3ILGbh9SuOiJ1E.jpg?auto=webp&amp;s=acb7aea268b5c173182564eb59c12c432a3228a3', 'width': 1000}, 'variants': {}}]}",6,1639019820,1,,True,False,False,dataengineering,t5_36en4,46585,public,https://b.thumbs.redditmedia.com/cHXcl8qFtZlFFPHc9wL2SqCTqOhnOmoPh3BD3wNOiTI.jpg,Kafka Summit London 2022- Call for Papers closes soon,0,[],1.0,https://dev.to/confluentinc/kafka-summit-london-2022-call-for-papers-closes-soon-475l,all_ads,6,,,,,,70.0,140.0,https://dev.to/confluentinc/kafka-summit-london-2022-call-for-papers-closes-soon-475l,,,,,,,,,,
[],False,[deleted],,,,,,dark,,,False,,,[],False,False,1638968006,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rbqplh/api_snowflake/,{},rbqplh,False,False,False,False,False,False,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,6,0,False,all_ads,/r/dataengineering/comments/rbqplh/api_snowflake/,False,,,6,1639018131,3,[deleted],True,False,False,dataengineering,t5_36en4,46585,public,default,API —-&gt; snowflake,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rbqplh/api_snowflake/,all_ads,6,,,deleted,,,,,,,,,,,,,,,
[],False,Prestigious-Tone5114,,,[],,,,text,t2_bpstpigt,False,False,False,[],False,False,1638959830,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rboj3k/data_governance_management_template/,{},rboj3k,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,18,0,False,all_ads,/r/dataengineering/comments/rboj3k/data_governance_management_template/,False,,,6,1639016626,21,"I am working at an organization where we deal with really bad data on a daily basis. The main reason we found is the way the data is being entered in the source system. 

For this reason I feel data governance is the need of the hour and was wondering is there any specific data governance template that I can use and start documenting the steps and policies?",True,False,False,dataengineering,t5_36en4,46586,public,self,Data governance/ management template,0,[],0.94,https://www.reddit.com/r/dataengineering/comments/rboj3k/data_governance_management_template/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Nishant_UIC,,,[],,,,text,t2_dvfuo3no,False,False,False,[],False,False,1638951921,umbrellainfocare.com,https://www.reddit.com/r/dataengineering/comments/rbml7s/do_you_want_to_know_how_cdk_pipelines_for_data/,{},rbml7s,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rbml7s/do_you_want_to_know_how_cdk_pipelines_for_data/,False,link,"{'enabled': False, 'images': [{'id': '30klUQil-_3pYHrOIGs9clKHgr2OlEXSgVxAdEhajG8', 'resolutions': [{'height': 54, 'url': 'https://external-preview.redd.it/GVsacjrZzSfZfhXln9S9sBhzovTsrc71b2B4JGYFCZ8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=92e0a77550e36cffffb93720dda854dc4940ea2a', 'width': 108}, {'height': 109, 'url': 'https://external-preview.redd.it/GVsacjrZzSfZfhXln9S9sBhzovTsrc71b2B4JGYFCZ8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=beb746ebde54bc235a0b16878490653755440b7e', 'width': 216}, {'height': 161, 'url': 'https://external-preview.redd.it/GVsacjrZzSfZfhXln9S9sBhzovTsrc71b2B4JGYFCZ8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d835b237ef692b7adcf879bd82ac0c9090787490', 'width': 320}, {'height': 323, 'url': 'https://external-preview.redd.it/GVsacjrZzSfZfhXln9S9sBhzovTsrc71b2B4JGYFCZ8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8cb1ed2876fa58901ce5d90f7e95f9cf60ee3efd', 'width': 640}, {'height': 484, 'url': 'https://external-preview.redd.it/GVsacjrZzSfZfhXln9S9sBhzovTsrc71b2B4JGYFCZ8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=cdbbb760305d14bcbe439e8b3ed5d38b8ebeaf71', 'width': 960}], 'source': {'height': 510, 'url': 'https://external-preview.redd.it/GVsacjrZzSfZfhXln9S9sBhzovTsrc71b2B4JGYFCZ8.jpg?auto=webp&amp;s=5c0e14b1187c8108639e35c50ddaf411be20b36d', 'width': 1010}, 'variants': {}}]}",6,1639015271,0,,True,False,False,dataengineering,t5_36en4,46585,public,https://b.thumbs.redditmedia.com/nas0Sk-EVyhCppgXEgh7u8LEZhy125OIMomF1I25laQ.jpg,"Do you want to know how CDK pipelines for data lake deployment bring scalability, automation and centralized management? Find out in this article",0,[],0.33,https://www.umbrellainfocare.com/blogs/efficacy-of-cdk-pipelines-for-etl-jobs-in-aws-data-lake,all_ads,6,,,,,,70.0,140.0,https://www.umbrellainfocare.com/blogs/efficacy-of-cdk-pipelines-for-etl-jobs-in-aws-data-lake,,,,,,,,,,
[],False,saulgv,,,[],,,,text,t2_fn649,False,False,False,[],False,False,1638937305,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rbiijx/how_do_you_handle_small_files_in_data_lake/,{},rbiijx,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,20,0,False,all_ads,/r/dataengineering/comments/rbiijx/how_do_you_handle_small_files_in_data_lake/,False,,,6,1639012395,15,"Hi, everyone!

I’m new in DE, I’ve been working the past month doing a Data lake to ingest raw data from APIs (json and xml formats) in daily basis. Once the data is deposited in the raw zone, all the data is processed from it into cleansed zone in parquet format.

So far, so good. But what can’t keep me with a peace of mind is that every file has like max ~140Kb inside the following partitions

cleansed_zone/source/user_id=xxxx/year=xxxx/month=xx/day=xx/&lt;parquet file&gt;

From my perspective, it handles very well idempotency because every file belongs to a specific day.

In terms of performance, I’ve read that it leads to slower queries when using presto, Redshift spectrum, etc.

Is there something that I am missing out?

Thanks!

Update #1:
- The number of users is like 5,000 but it is expected to grow to 100,000 by the end of the year.
- Each user has an average of ~433MB data each year.
- We expect to query data 18 months back from the present for each user at least four times a month.",True,False,False,dataengineering,t5_36en4,46583,public,self,How do you handle small files in Data Lake?,0,[],0.95,https://www.reddit.com/r/dataengineering/comments/rbiijx/how_do_you_handle_small_files_in_data_lake/,all_ads,6,,,,,,,,,,,,,,,,1638983362.0,,
[],False,[deleted],,,,,,dark,,,False,,,[],False,False,1638936306,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rbi7db/how_to_handle_small_files_in_data_lake/,{},rbi7db,False,False,False,False,False,False,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rbi7db/how_to_handle_small_files_in_data_lake/,False,,,6,1639012175,1,,True,False,False,dataengineering,t5_36en4,46583,public,default,How to handle small files in Data lake?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rbi7db/how_to_handle_small_files_in_data_lake/,all_ads,6,,,deleted,,,,,,,,,,,,,,moderators,
[],True,rororo-your-boat,,,[],,,,text,t2_10aflw,False,False,False,[],False,False,1638931318,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rbgit3/whats_your_preferred_data_integration_method_from/,{},rbgit3,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,6,0,False,all_ads,/r/dataengineering/comments/rbgit3/whats_your_preferred_data_integration_method_from/,False,,,6,1639010992,2,"I’m trying to get a sense of what the preferred method data engineers these days use to extract data from SaaS products. 

What method do you like to work with the most? Is it:

* APIs
* JDBC
* Streamed (via Kinesis or similar)
* Something else?",True,False,False,dataengineering,t5_36en4,46581,public,self,What’s your preferred data integration method from a SaaS vendor?,0,[],0.76,https://www.reddit.com/r/dataengineering/comments/rbgit3/whats_your_preferred_data_integration_method_from/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,gman1023,,,[],,,,text,t2_3yozg,False,False,False,[],False,False,1638909196,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rb8ru5/was_anything_interesting_announced_at_aws/,{},rb8ru5,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,24,0,False,all_ads,/r/dataengineering/comments/rb8ru5/was_anything_interesting_announced_at_aws/,False,,,6,1639003763,13,I didn't get a chance to go to reInvent this year and didn't see too many changes regarding DE but I really appreciate the new serverless Redshift coming on-board. That should cut down costs in our DW.  Anything else that was cool?,True,False,False,dataengineering,t5_36en4,46570,public,self,Was anything interesting announced at AWS reInvent 2021 for data engineering?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rb8ru5/was_anything_interesting_announced_at_aws/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,dataengineerdude,,,[],,,,text,t2_4wrevs7q,False,False,False,[],False,False,1638903444,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rb6k1t/aws_outage_my_pipeline_is_down/,{},rb6k1t,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,20,0,False,all_ads,/r/dataengineering/comments/rb6k1t/aws_outage_my_pipeline_is_down/,False,,,6,1638998484,29,"Just curious how many other Data Engineers are prepared for AWS outages. All my stuff MWAA and Databricks runs on us-east-1. Of course, it's down. No Data pipeline. I feel like it's a small price to pay for the reduced complexity of High Availability. My business can handle a day gone. What about y'all?",True,False,False,dataengineering,t5_36en4,46567,public,self,AWS Outage ... My Pipeline is Down.,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rb6k1t/aws_outage_my_pipeline_is_down/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,wytesmurf,,,[],,,,text,t2_ldunk,False,False,False,[],False,False,1638898049,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rb3tst/datavault_2_retrieving_data/,{},rb3tst,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,16,0,False,all_ads,/r/dataengineering/comments/rb3tst/datavault_2_retrieving_data/,False,,,6,1638993340,3,"Has anyone figured out a good way to extract point in time data in DataVault 2? Using Lead and Lag, I have to include a sort operator which is really expensive over a large dataset. Does anyone have any tricks to get the data extracted faster. Ours is built on SQL server, but the approach for other systems could be ported to work on SQL server.",True,False,False,dataengineering,t5_36en4,46563,public,self,DataVault 2 Retrieving data,0,[],0.81,https://www.reddit.com/r/dataengineering/comments/rb3tst/datavault_2_retrieving_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Strijdhagen,,,[],,,,text,t2_49nbu,False,False,False,[],False,False,1638895763,datai.jobs,https://www.reddit.com/r/dataengineering/comments/rb34wu/looking_for_some_feedback_on_a_new_job_board_ive/,{},rb34wu,False,True,False,False,False,True,False,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,False,38,0,False,all_ads,/r/dataengineering/comments/rb34wu/looking_for_some_feedback_on_a_new_job_board_ive/,False,link,"{'enabled': False, 'images': [{'id': 'WVbW0Rd3cz8xEZ1O1kyb75MgQLaplPAruuUrE_PHHI4', 'resolutions': [{'height': 108, 'url': 'https://external-preview.redd.it/Gl7PV504yJ2Q730F5Au5BNDfMKKCyJxy8Q6dF1o_7Pw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d7082b332d54da2a9eddaf2523dfa1944d485856', 'width': 108}, {'height': 216, 'url': 'https://external-preview.redd.it/Gl7PV504yJ2Q730F5Au5BNDfMKKCyJxy8Q6dF1o_7Pw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9dd16834185973c6cbf35ad7c16217291aa9a146', 'width': 216}, {'height': 320, 'url': 'https://external-preview.redd.it/Gl7PV504yJ2Q730F5Au5BNDfMKKCyJxy8Q6dF1o_7Pw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8f13245725e09e44ed5c95c6476e38470c5f5f04', 'width': 320}], 'source': {'height': 500, 'url': 'https://external-preview.redd.it/Gl7PV504yJ2Q730F5Au5BNDfMKKCyJxy8Q6dF1o_7Pw.jpg?auto=webp&amp;s=5d562f6fbd9301bf1c659a53638f75ab959a48b7', 'width': 500}, 'variants': {}}]}",6,1638991310,65,,True,False,False,dataengineering,t5_36en4,46561,public,https://b.thumbs.redditmedia.com/XFyKXAqvCtCBAb-_Z60Xw3TXpkSUMcgE9pal-0VZ6CM.jpg,Looking for some feedback on a new job board I've created for Data Engineering! New jobs are listed automatically 4 times per day.,0,[],0.95,https://datai.jobs/jobs/?filter_job_listing_category=data-engineering,all_ads,6,,,,,,140.0,140.0,https://datai.jobs/jobs/?filter_job_listing_category=data-engineering,,,,,,,,,,
[],False,mannu_11,,,[],,,,text,t2_3dsjwu75,False,False,False,[],False,False,1638894898,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rb2wag/airflow_noob_question/,{},rb2wag,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,24,0,False,all_ads,/r/dataengineering/comments/rb2wag/airflow_noob_question/,False,,,6,1638990586,2,How do I define multiple dag owners in airflow.,True,False,False,dataengineering,t5_36en4,46558,public,self,Airflow Noob Question,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rb2wag/airflow_noob_question/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,mohaidoss,,,[],,,,text,t2_9k40zzr,False,False,False,[],False,False,1638894066,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rb2nzv/books_to_learn_scala/,{},rb2nzv,False,False,False,False,False,False,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rb2nzv/books_to_learn_scala/,False,,,6,1638989865,0,[removed],True,False,False,dataengineering,t5_36en4,46555,public,default,Books to learn Scala,0,[],0.5,https://www.reddit.com/r/dataengineering/comments/rb2nzv/books_to_learn_scala/,all_ads,6,,,moderator,,,,,,,,,,,,,,,
[],False,jsxgd,,,[],,,,text,t2_3jryuv2u,False,False,False,[],False,False,1638893177,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rb2esq/how_do_you_make_idempotent_etl_jobs_for_a/,{},rb2esq,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,24,0,False,all_ads,/r/dataengineering/comments/rb2esq/how_do_you_make_idempotent_etl_jobs_for_a/,False,,,6,1638989103,40,"I have a relational OLTP database that supports an app. Records are created and updated, and there's a created timestamp and an updated timestamp that tells us when the record was last changed.

&amp;#x200B;

My ETL jobs at the moment have just been identifying which records have changed/been created since the last ETL based on the updated timestamp and then updating the reporting database for those records, but this isn't idempotent.

&amp;#x200B;

How are you writing your jobs for this kind of source?",True,False,False,dataengineering,t5_36en4,46555,public,self,How do you make idempotent ETL jobs for a relational database where records are updated?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rb2esq/how_do_you_make_idempotent_etl_jobs_for_a/,all_ads,6,,,,,,,,,,,,,,,,,,
[],True,Party_Farm,,,[],,,,text,t2_69a2rla8,False,False,False,[],False,False,1638892481,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rb26t1/version_control_source_tables_and_downstream/,{},rb26t1,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/rb26t1/version_control_source_tables_and_downstream/,False,,,6,1638988505,1,"We are about to start using Fivetran for bringing data into our Snowflake instance from various sources, and it seems like it automatically detects schema changes from the source and propagates it down to the target tables. I'm curious if those of you that use Fivetran still maintain a git repository of the schema DDL scripts, and if so, what that process looks like? I've looked into alembic, Flyway, schemachange, etc., but it seems redundant keeping track of migration files if Fivetran is doing that automatically for us. At the end of the day, our goal is to maintain a set of DDL scripts so that we can rebuild the database schemas from scratch (with or without Fivetran). Perhaps Terraform + basic SQL scripts maintained in a git repository?

In searching through this subreddit, it seems like DBT is a beneficial module to use for objects built on top of the source tables/data, such as views, ETLs, additional tables, etc. And it also acts as a schema versioning module for the aforementioned. Is my understanding of this correct?",True,False,False,dataengineering,t5_36en4,46554,public,self,Version Control: Source tables and downstream tables/views,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rb26t1/version_control_source_tables_and_downstream/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,[deleted],,,,,,dark,,,False,,,[],False,False,1638891718,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rb1y7v/can_i_call_myself_a_data_engineer/,{},rb1y7v,False,False,False,False,False,False,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rb1y7v/can_i_call_myself_a_data_engineer/,False,,,6,1638987846,1,[deleted],True,False,False,dataengineering,t5_36en4,46554,public,default,Can I call myself a data engineer?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rb1y7v/can_i_call_myself_a_data_engineer/,all_ads,6,,,deleted,,,,,,,,,,,,,,,
[],True,francesco1093,,,[],,,,text,t2_zlyww,False,False,False,[],False,False,1638879027,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/raxsop/snowflake_migrating_from_oracle_and_tool_stack/,{},raxsop,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,14,0,False,all_ads,/r/dataengineering/comments/raxsop/snowflake_migrating_from_oracle_and_tool_stack/,False,,,6,1638976732,6,"Hi all,

I currently have the need to migrate a DWH based on Oracle Exadata to Cloud. The target architecture is either Snowflake on AWS or Redshift.

We are considering Snowflake for cost and performance reasons but I am not sure about a few points.

Concerning the migration, my understanding is that we would need to export data from Oracle as csv (or other formats) to a Cloud Storage (S3) and then use Snowpipe/COPY to import. Is that correct? Which tool can we use to convert DDLs? (I know that for Redshift they have SCT and DMT that it seems would simplify the migration quite a bit). 

Also, we were thinking about using dbt as it seems the most popular transformation tool for Snowflake. Any easy way to migrate Oracle PL/SQL procedures to dbt models? 

Thanks a lot!",True,False,False,dataengineering,t5_36en4,46547,public,self,Snowflake: migrating from Oracle and tool stack,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/raxsop/snowflake_migrating_from_oracle_and_tool_stack/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Uchihaandnow,,,[],,,,text,t2_gm6fdsqd,False,False,False,[],False,False,1638874585,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rawkxa/airflow_azure_key_vault/,{},rawkxa,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rawkxa/airflow_azure_key_vault/,False,,,6,1638874595,1,"Hi,

Has anyone configured Azure key vault as backend secrets for airflow ? I am running into trouble. I keep getting this error. I create app registration and generated client id and client secret and provided the access to key vault api. I provided my vault url in airflow.cfg file. I don't know what else I am doing wrong? Any help appreciated

 

    [2021-12-07, 10:44:43 UTC] {environment.py:98} INFO - No environment configuration found. [2021-12-07, 10:44:43 UTC] {managed_identity.py:85} INFO - ManagedIdentityCredential will use IMDS [2021-12-07, 10:44:43 UTC] {connection.py:404} ERROR - Unable to retrieve connection from secrets backend (AzureKeyVaultBackend). Checking subsequent secrets backend. Traceback (most recent call last):   File ""/home/airflowadmin/.local/lib/python3.8/site-packages/azure/core/pipeline/transport/_base.py"", line 582, in format_url     base = self._base_url.format(**kwargs).rstrip(""/"") KeyError: 'vaultBaseUrl'",True,False,False,dataengineering,t5_36en4,46447,public,self,Airflow - Azure Key Vault,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rawkxa/airflow_azure_key_vault/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,tallbrownglasses,,,[],,,,text,t2_f3w5u04o,False,False,False,[],False,False,1638872528,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/raw32o/help_on_azure_synapse_learning/,{},raw32o,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,5,0,False,all_ads,/r/dataengineering/comments/raw32o/help_on_azure_synapse_learning/,False,,,6,1638872539,1,"[https://docs.microsoft.com/en-us/learn/modules/design-multidimensional-schema-to-optimize-analytical-workloads/3-exercise-create-star-schema](https://docs.microsoft.com/en-us/learn/modules/design-multidimensional-schema-to-optimize-analytical-workloads/3-exercise-create-star-schema)

Hi , I was learning for Azure Data Engineer associate and was trying to do this exercise. the first sql script to create fact and dimension tables is throwing an error for me as below

what can be the issue.

https://preview.redd.it/30chubmvj3481.png?width=1770&amp;format=png&amp;auto=webp&amp;s=692e4d07ce9ac58e00e0bace6514e2538d170314",True,False,False,dataengineering,t5_36en4,46445,public,https://b.thumbs.redditmedia.com/oJPw3KszewTZ4PPlN9jVIBkNCNXBw3oDGxUF-Ucn-Sc.jpg,Help on Azure Synapse Learning,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/raw32o/help_on_azure_synapse_learning/,all_ads,6,,,,,,119.0,140.0,,"{'30chubmvj3481': {'e': 'Image', 'id': '30chubmvj3481', 'm': 'image/png', 'p': [{'u': 'https://preview.redd.it/30chubmvj3481.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2a48e96c4538c9b9f3b42a7b217f173183cdece9', 'x': 108, 'y': 92}, {'u': 'https://preview.redd.it/30chubmvj3481.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=be3b50510fd395ff238735b8f974c88ad39e6f84', 'x': 216, 'y': 184}, {'u': 'https://preview.redd.it/30chubmvj3481.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ad644677976606a112bc4233b9053f439e34d750', 'x': 320, 'y': 272}, {'u': 'https://preview.redd.it/30chubmvj3481.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=18b17582423a2b64012849b0b1b8add630fd404a', 'x': 640, 'y': 545}, {'u': 'https://preview.redd.it/30chubmvj3481.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b690ad012344f548a3e234ed55c5dd7fc033bce8', 'x': 960, 'y': 817}, {'u': 'https://preview.redd.it/30chubmvj3481.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5802480e7972275cf0ddaad7531b2a10cc7eea5e', 'x': 1080, 'y': 920}], 's': {'u': 'https://preview.redd.it/30chubmvj3481.png?width=1770&amp;format=png&amp;auto=webp&amp;s=692e4d07ce9ac58e00e0bace6514e2538d170314', 'x': 1770, 'y': 1508}, 'status': 'valid'}}",,,,,,,,,
[],False,e4ds,,,[],,,,text,t2_empiv8dh,False,False,False,[],False,False,1638870334,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ravjy8/event_driven_data_validation_with_google_cloud/,{},ravjy8,False,False,False,False,False,False,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/ravjy8/event_driven_data_validation_with_google_cloud/,False,self,"{'enabled': False, 'images': [{'id': '-MhwPnbfhP1HjSg5yHNV88yYnnn7rd2eguVdzuZGZAo', 'resolutions': [{'height': 30, 'url': 'https://external-preview.redd.it/Um9pBJkO-nd7GZCEISlbrtzoaFmHtjoxwDkMDq1FQSs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=da16b3369d7804de325918ca9db3281a42b1cfeb', 'width': 108}, {'height': 61, 'url': 'https://external-preview.redd.it/Um9pBJkO-nd7GZCEISlbrtzoaFmHtjoxwDkMDq1FQSs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e85055793f48b08efa49b907dd6e47642ac14372', 'width': 216}, {'height': 90, 'url': 'https://external-preview.redd.it/Um9pBJkO-nd7GZCEISlbrtzoaFmHtjoxwDkMDq1FQSs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3ad8127bed79a2c284e1f3e10a7ceb1b2020458b', 'width': 320}, {'height': 181, 'url': 'https://external-preview.redd.it/Um9pBJkO-nd7GZCEISlbrtzoaFmHtjoxwDkMDq1FQSs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e0f132d9fca2613456d4e449ba5678b4ebfd8dc2', 'width': 640}], 'source': {'height': 261, 'url': 'https://external-preview.redd.it/Um9pBJkO-nd7GZCEISlbrtzoaFmHtjoxwDkMDq1FQSs.jpg?auto=webp&amp;s=1ac3c6f993acbefa8a08dfc479889698baeca0e4', 'width': 920}, 'variants': {}}]}",6,1638870345,1,[removed],True,False,False,dataengineering,t5_36en4,46440,public,self,Event Driven Data Validation with Google Cloud Functions and Great Expectations,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ravjy8/event_driven_data_validation_with_google_cloud/,all_ads,6,,,automod_filtered,,,,,,,,,,,,,,,
[],False,AkosiAnton,,,[],,,,text,t2_12hsuj,False,False,False,[],False,False,1638866237,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/raul4m/advice_on_current_data_stack_and_ways_to_improve/,{},raul4m,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,6,0,False,all_ads,/r/dataengineering/comments/raul4m/advice_on_current_data_stack_and_ways_to_improve/,False,,,6,1638866248,1,"Hi Everyone,

I'm working on trying to improve our data stack and any advices, study buddy or mentorship is highly appreciated. To give some background I'm the first data engineer for the company coming from a front-end developer background comfortable with advanced SQL stuff and beginner python. Before me was a BI developer who created data models and dashes in Sisense (on-prem) which is what I use now as well. 

**Current state**

95% of my teams data requirements are coming from our mySQL db, which is maintained by our software dev team. Any current API ingestion is done by them and stored in tables.

We do have an analytical db which has views and queries from our main OLTP db, the sizes of data here are around 1m - 10m rows of orders/member/transactional data 

Remainder of the data is Excel/CSV files, some are static some are updated from time to time by other members of the org

We ingest the data into Sisense and build a data model, transformations can be done either in the OLAP db or the model itself

The models are then scheduled to built at certain intervals, some take a few seconds and the biggest ones around 4 hours. Which is then accessed by Sisense to create the dashboards and email reports. 

We do have a data scientist who is doing his work in BigQuery, though I have absolutely have no collaboration with him and no idea what he is doing as well. Not saying I won't collaborate with him just that haven't reached out yet since this modernization task I'm doing is just research I'm preparing for the final proposal to higher ups.

Lastly, in terms of ecosystem our company uses a lot of Microsoft services as well. 

**What I'm envisioning to use and reasons**

Please understand I don't have real experience on these tools, this is more on research and basing my judgement on comparison articles, guides, some tuts I did. 

Cloud solution - Snowflake on Azure or Azure Synapse

* Already in Microsoft ecosystem and integration to tools is easier and recommendation to higher      ups
* I'm split on the two because  snowflake seems to be easier to implement, note I'm the only one who will be doing this on-top of other tasks so it's a balance of ease and delivery      of requirements. 
* If ever I would go for Azure Synapse will be utilizing tools in their ecosystem [https://docs.microsoft.com/en-us/azure/architecture/example-scenario/dataplate2e/data-platform-end-to-end](https://docs.microsoft.com/en-us/azure/architecture/example-scenario/dataplate2e/data-platform-end-to-end)

BI Solution - Sisense since we're already using it, just now it will get data from the DW or if ever can use Power BI

&amp;#x200B;

* Airflow - I don't think I  would be needing this because you could schedule jobs already in Azure      Data Factory
* fivetran - I don't believe I need this for now basing on my current data sources, but I can anticipate      in the future API integration role could be moved from software devs to me in the future. But that's like maybe a year from now. 
* dbt - Will utilize this as well, having no documentation, version control, reusability etc is making      my head hurt right now. 

Any other tools I'm missing? Any advice you could give? I appreciate all the help given. Thank you!",False,False,False,dataengineering,t5_36en4,46438,public,self,Advice on current data stack and ways to improve,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/raul4m/advice_on_current_data_stack_and_ways_to_improve/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,pemens,,,[],,,,text,t2_zak9c,False,False,False,[],False,False,1638860957,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ratanv/transition_from_mle_to_ds_role/,{},ratanv,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/ratanv/transition_from_mle_to_ds_role/,False,,,6,1638860968,1,"Hi, I am ML Engineer and mainly in charge of the full cycle: PoC, data collection, preprocessing, model architecture coding, or use existing toolboxes, tuning and etc., but not deploying it. Recently I have a blast doing all the dataset management and preprocessing work (90% images/videos). 

From the brief overview of the Data Science positions in my country, the skillset is varied from one company to another and the overall tech stack looks not that standardized. 

If anyone had experienced it before, how hard is it to transit from MLE to DS role? Usually, I don't use any databases for the work and it's definitely something I need to learn about. But other than that?",True,False,False,dataengineering,t5_36en4,46436,public,self,Transition from MLE to DS role.,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ratanv/transition_from_mle_to_ds_role/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Overall_Shine_3928,,,[],,,,text,t2_cbva6a6a,False,False,False,[],False,False,1638858238,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/raslg0/how_pii_data_is_handled_and_secured_in_azure_data/,{},raslg0,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,12,0,False,all_ads,/r/dataengineering/comments/raslg0/how_pii_data_is_handled_and_secured_in_azure_data/,False,,,6,1638858248,1,"Hello community,

I would like to know how do you all handle PII data in a solution where you use Azure Data Factory to extract data and copy to Azure Data Lake and from data lake to Azure SQL database/ Azure synapse data warehouse.

The requirement is to encrypt PII data **before it leaves the organization’s premise**. What are the approaches you guys' follow to fulfill such requirement? Since data factory is not having a built in feature to encrypt PII data, would like to know how such scenarios are handled.

 Thank you",True,False,False,dataengineering,t5_36en4,46428,public,self,How PII data is handled and secured in Azure data platforms?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/raslg0/how_pii_data_is_handled_and_secured_in_azure_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,smart_potato34,,,[],,,,text,t2_g0nhkcyi,False,False,False,[],False,False,1638852703,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rar2dx/data_engineer_interview_at_metafacebook/,{},rar2dx,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,7,0,False,all_ads,/r/dataengineering/comments/rar2dx/data_engineer_interview_at_metafacebook/,False,,,6,1638852713,1,"I have a technical interview coming up with Meta (Facebook) for their Data Engineer role. I've been practicing some advanced SQL (PostgreSQL) and Python questions.

Did anyone do a similar interview with them recently or before? Can you give me some sample questions you remember that I can practice coding. Thank you.",True,False,False,dataengineering,t5_36en4,46421,public,self,Data Engineer Interview at Meta/Facebook,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rar2dx/data_engineer_interview_at_metafacebook/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Nokita_is_Back,,,[],,,,text,t2_5dri898p,False,False,False,[],False,False,1638851563,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/raqql9/what_languages_do_i_need_to_learn_to_get_ready/,{},raqql9,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,7,0,False,all_ads,/r/dataengineering/comments/raqql9/what_languages_do_i_need_to_learn_to_get_ready/,False,,,6,1638851574,1,"I need some advice on the Engineering part of DS. What are the common tools used for ETL/Data Pipelines in the private sector? Thus far I'm planning on learning:

Pyspark
Pyspark for SQL
SSID/ SQL
Cloud ETL Solutions from azure/aws/google
+BI Tools ETL Solutions

Would that be enough or do I need NoSQL too/have I forgot something? Are most ETL jobs nowadays done in the cloud or a company SQL/NoSQL Server? I'm a year in learning to code and thus far I'm fairly proficient in SQL and Python.

Thank you for your time",True,False,False,dataengineering,t5_36en4,46420,public,self,What Languages do I need to learn to get ready for the ETL part of Datascience?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/raqql9/what_languages_do_i_need_to_learn_to_get_ready/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,yummypoutine,,,[],,,,text,t2_8t7dr,False,False,True,[],False,False,1638831913,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rakdjr/what_are_your_biggest_data_pipelineetl_tool/,{},rakdjr,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/rakdjr/what_are_your_biggest_data_pipelineetl_tool/,False,,,6,1638831926,1,"I currently work for a company that does ETL/ELT (I am not here to advertise, so I am not mentioning it) and I am looking to improve the demo experience for prospects. Currently the demos are generic and very one-sided. I'd like to put together some more relevant use-cases /examples that are more relevant.

What are some of the bigger challenges / pain points do you guys run into when building pipelines or using ETL tools?

If you were looking for a new ETL or ELT tool, what would you like to see most?",True,False,False,dataengineering,t5_36en4,46400,public,self,What are your biggest data pipeline/ETL tool problems?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rakdjr/what_are_your_biggest_data_pipelineetl_tool/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,cpardl,,,[],,,,text,t2_fb1s1pke,False,False,False,[],False,False,1638831303,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rak5lr/what_is_the_modern_data_stack/,{},rak5lr,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,7,0,False,all_ads,/r/dataengineering/comments/rak5lr/what_is_the_modern_data_stack/,False,,,6,1638831313,1,"Hey everyone, 

on December 15th at 4PM ET - 1PM PT we'll be hosting as part of the Data Stack Show live panel where we'll discuss about the Modern Data Stack. 

The panel will have people from **dbt**, **Fivetran**, **Databricks**, **Hinge** and **Essence VC**. 

We wanted to have all the stakeholders share their views on what the Modern Data Stack is and why it's important. [You can register here for the live event](https://rudderstack.com/video-library/the-data-stack-show-live-what-is-the-modern-data-stack/). [POAP](https://poap.xyz/) will be used as a proof of attendance and based on that there will also be some gifts given! (Not sure what yet, still working on it but I promise it will be worth it)

In case you can't make it or just don't want to participate in the live event, the recording will be published on the [Data Stack Show](https://datastackshow.com/) anyway.

I would also like to ask for your help! 

I'd love to hear from the community what to ask the people from these companies, so please leave a message with your questions on this thread!

Thanks!",True,False,False,dataengineering,t5_36en4,46400,public,self,What is the Modern Data Stack,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rak5lr/what_is_the_modern_data_stack/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,castor-metadata,,,[],,,,text,t2_bu8cw718,False,False,False,[],False,False,1638830127,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rajqpq/data_lineage_use_cases/,{},rajqpq,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/rajqpq/data_lineage_use_cases/,False,self,"{'enabled': False, 'images': [{'id': 'JgzKdUJf1o6sw58o32EZlcTC-jFpYZFg9bbDzIJ-wmE', 'resolutions': [{'height': 56, 'url': 'https://external-preview.redd.it/1-XzApZCeMY7b1lRCYbTxWQIb8EcxClPCvNbqCFs5t0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3b77c2db6d182abc9e6250f767a71d6b78237e49', 'width': 108}, {'height': 113, 'url': 'https://external-preview.redd.it/1-XzApZCeMY7b1lRCYbTxWQIb8EcxClPCvNbqCFs5t0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=eeb5a3aa6ed930c1f4ec56ffde9a1c075bab62c9', 'width': 216}, {'height': 168, 'url': 'https://external-preview.redd.it/1-XzApZCeMY7b1lRCYbTxWQIb8EcxClPCvNbqCFs5t0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=313e032a043aa38b0e7fda312a7a5c38084e6ce0', 'width': 320}, {'height': 336, 'url': 'https://external-preview.redd.it/1-XzApZCeMY7b1lRCYbTxWQIb8EcxClPCvNbqCFs5t0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=342c7f4bf5558c0d16291587a164107d0dd612e7', 'width': 640}, {'height': 504, 'url': 'https://external-preview.redd.it/1-XzApZCeMY7b1lRCYbTxWQIb8EcxClPCvNbqCFs5t0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b3aedbd11f6137d31adf3ddf26f62e44e3b6c9b7', 'width': 960}, {'height': 567, 'url': 'https://external-preview.redd.it/1-XzApZCeMY7b1lRCYbTxWQIb8EcxClPCvNbqCFs5t0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6a3ecf59ac5e53fbcc73b94b15a79ee2d41ffd14', 'width': 1080}], 'source': {'height': 630, 'url': 'https://external-preview.redd.it/1-XzApZCeMY7b1lRCYbTxWQIb8EcxClPCvNbqCFs5t0.jpg?auto=webp&amp;s=498f2f5f888b112baae7b547b08df77565792dfb', 'width': 1200}, 'variants': {}}]}",6,1638830138,1,"I am working on a Data Lineage tools benchmark and deep analysis. I am trying to add a survey on data lineage use-cases to an article I published on TDS: [https://towardsdatascience.com/data-lineage-explained-to-my-grandmother-6545cad08c41](https://towardsdatascience.com/data-lineage-explained-to-my-grandmother-6545cad08c41). The opinion of the data engineering community would help me a lot.

**What is the most common data lineage use case? (answer in comments)**

  
You can find more modern data stack analysis and benchmark [here](https://notion.castordoc.com/):

\- [Benchmark for Data Catalogs](https://notion.castordoc.com/catalog-of-catalogs)

\- [Benchmark for Reverse ETL](https://notion.castordoc.com/catalog-reverse-etl)

\- [Benchmark for Data Quality](https://notion.castordoc.com/catalog-of-data-quality)

\- [Benchmark for ETL](https://notion.castordoc.com/catalog-of-etl-tools)

[View Poll](https://www.reddit.com/poll/rajqpq)",True,False,False,dataengineering,t5_36en4,46400,public,self,Data Lineage Use Cases,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rajqpq/data_lineage_use_cases/,all_ads,6,,,,,,,,,,,,,"{'is_prediction': False, 'options': [{'id': '12316764', 'text': 'Data Troubleshooting'}, {'id': '12316765', 'text': 'Impact analysis'}, {'id': '12316766', 'text': 'Discovery and Trust'}, {'id': '12316767', 'text': 'Metadata Propagation\u200d'}, {'id': '12316768', 'text': 'Data Privacy Regulation (GDPR and PII mapping)'}, {'id': '12316769', 'text': 'Data assets clean up or technology migration'}], 'prediction_status': None, 'resolved_option_id': None, 'total_stake_amount': None, 'total_vote_count': 0, 'tournament_id': None, 'user_selection': None, 'user_won_amount': None, 'vote_updates_remained': None, 'voting_end_timestamp': 1639262127567}",,,,,
[],False,_BearHawk,,,[],,,,text,t2_ekkeg,False,False,False,[],False,False,1638829317,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rajg2i/first_job_teaching_me_informatica/,{},rajg2i,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,False,2,0,False,all_ads,/r/dataengineering/comments/rajg2i/first_job_teaching_me_informatica/,False,,,6,1638829328,1,"Hi there, I am in my first job after college in a sort of hybrid DE/SWE role and for the ETL stuff I’ll be doing it’s mostly all in Informatica. I work at a large university in my state and the pay/benefits are good as well as work/life balance, but I can’t help but feel like it’s not the best to be learning Informatica when Airflow/azure/etc is so much more popular.

I don’t have any other experience with any sort of ETL technology from school and this is my first real work experience (didnt do any internships), so I can’t really complain. Should I just try to teach myself to work with airflow or azure outside of work? Bring up using something like Airflow instead of Informatica once I’ve been here longer? 

Looking for any advice ty.",True,False,False,dataengineering,t5_36en4,46402,public,self,First job teaching me Informatica,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rajg2i/first_job_teaching_me_informatica/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,elevern11,,,[],,,,text,t2_9chprpg8,False,False,False,[],False,False,1638828183,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/raj0vf/how_would_you_surface_graph_data_eg_neo4j_aws/,{},raj0vf,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,3,0,False,all_ads,/r/dataengineering/comments/raj0vf/how_would_you_surface_graph_data_eg_neo4j_aws/,False,,,6,1638828195,1,"I'm trying to create an interface that will allow a non-technical user (little to no SQL knowledge) to search for entities within a graph db. What would be a low cost or easy to develop solution for this?

Current proof of concept has raw data as a dump file that is loaded into neo4j desktop. Data is queried using Python and Cypher. I thought of using plotly + streamlit but what are some other options?",True,False,False,dataengineering,t5_36en4,46398,public,self,"How would you surface graph data (e.g. neo4j, AWS Neptune) for business users?",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/raj0vf/how_would_you_surface_graph_data_eg_neo4j_aws/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,ijpck,,,[],,,,text,t2_15cq8l,False,False,False,[],False,False,1638821784,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ragkyg/what_is_a_good_pipeline_to_use_alongside/,{},ragkyg,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,5,0,False,all_ads,/r/dataengineering/comments/ragkyg/what_is_a_good_pipeline_to_use_alongside/,False,,,6,1638821795,1,Could a business’ entire ETL pipelines be built out through Databricks jobs?,True,False,False,dataengineering,t5_36en4,46387,public,self,What is a good pipeline to use alongside Databricks notebooks?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ragkyg/what_is_a_good_pipeline_to_use_alongside/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Vitruvius_Corvinus,,,[],,,,text,t2_4tzty4ve,False,False,False,[],False,False,1638810520,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/rac5kd/new_role_requires_usage_of_infrastructure_as_a/,{},rac5kd,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,32,0,False,all_ads,/r/dataengineering/comments/rac5kd/new_role_requires_usage_of_infrastructure_as_a/,False,,,6,1638810531,1,"I got offered a new role which I accepted, I will be working as a Data Engineer. My boss said that  ""usage of Infrastructure as a code and building of CI/CD pipelines"" are part of the tasks. I don't know much about these, but hey I wanted to try something new :D

We will be using the Azure stack: Synapse Workspace, Databricks, SQL Pool, Data Factory. How does ""code as infrastructure"" and CI/CD fit into this realm? Can you point me towards some useful resources?

(I'm also new to Azure, been working with IBM stack for 1,5yrs)",True,False,False,dataengineering,t5_36en4,46376,public,self,"New role requires: ""usage of Infrastructure as a code and building of CI/CD pipelines."" What are these in terms of Data Engineering?",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/rac5kd/new_role_requires_usage_of_infrastructure_as_a/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,dataengineerdude,,,[],,,,text,t2_4wrevs7q,False,False,False,[],False,False,1638810100,confessionsofadataguy.com,https://www.reddit.com/r/dataengineering/comments/rabzj8/hive_metastore_in_databricks_what_to_know/,{},rabzj8,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/rabzj8/hive_metastore_in_databricks_what_to_know/,False,link,"{'enabled': False, 'images': [{'id': 'QPuiMColcLcm_uTCEPJ70Uq2zS_cQQkVweakR8Dsj7A', 'resolutions': [{'height': 72, 'url': 'https://external-preview.redd.it/6S1Z7TFV4h4FcOuWtnCZPTjn3j2_NNfqMYIQa9LJd1Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fa07f100d73b559023a3b978d45bc42079c13ada', 'width': 108}, {'height': 144, 'url': 'https://external-preview.redd.it/6S1Z7TFV4h4FcOuWtnCZPTjn3j2_NNfqMYIQa9LJd1Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=16a51e8416727c39dfab5e32c18780132aa5a7c0', 'width': 216}, {'height': 213, 'url': 'https://external-preview.redd.it/6S1Z7TFV4h4FcOuWtnCZPTjn3j2_NNfqMYIQa9LJd1Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e9ef344595e61061cc69d9f75904b5e35980291f', 'width': 320}, {'height': 426, 'url': 'https://external-preview.redd.it/6S1Z7TFV4h4FcOuWtnCZPTjn3j2_NNfqMYIQa9LJd1Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=605200093b1b635fc8228e6b1d0ca2bb8fbe4cfd', 'width': 640}, {'height': 640, 'url': 'https://external-preview.redd.it/6S1Z7TFV4h4FcOuWtnCZPTjn3j2_NNfqMYIQa9LJd1Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9cb1c901ebbf97da8909d343bf3a4c0b11c33826', 'width': 960}], 'source': {'height': 687, 'url': 'https://external-preview.redd.it/6S1Z7TFV4h4FcOuWtnCZPTjn3j2_NNfqMYIQa9LJd1Q.jpg?auto=webp&amp;s=7ded21fd63a1d03cddac81e9b1141931e37b3b01', 'width': 1030}, 'variants': {}}]}",6,1638810111,1,,True,False,False,dataengineering,t5_36en4,46376,public,https://a.thumbs.redditmedia.com/JjhxckWNfiZB9R0ZoQ2AwC1FriybdqSHrDIebaa94c4.jpg,Hive Metastore in Databricks – What To Know.,0,[],1.0,https://www.confessionsofadataguy.com/hive-metastore-in-databricks-what-to-know/,all_ads,6,,,,,,93.0,140.0,https://www.confessionsofadataguy.com/hive-metastore-in-databricks-what-to-know/,,,,,,,,,,
[],False,coding_up_a_storm,,,[],,,,text,t2_2ts0nofm,False,False,False,[],False,False,1638806601,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/raampc/personal_project_for_interviews_list_of_things_to/,{},raampc,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,False,12,0,False,all_ads,/r/dataengineering/comments/raampc/personal_project_for_interviews_list_of_things_to/,False,,,6,1638806612,1,"I want to start planning a new personal project for the show-and-tell portion of interviews. I have only ever made personal projects geared towards generic junior level software developer gigs, but I want to start one geared more towards data engineering.  I am looking to make a checklist of attributes.

&amp;#x200B;

I will not be starting from scratch, but will recycle parts of another project and build it out.

&amp;#x200B;

**My general proposal**

To make a program that reads astronomy API data (completed) and interprets the data into visual animation (completed). My older project did not store this data into a database, so that's the obvious addition. I am thinking of creating a snapshot table that the API data gets logged to FirstTable daily, and a feature to run the animation at a certain timestamp. Data will be transformed and piped to SecondTable. The transformation is used to convert astronomy values to animation values. The animation script will query SecondTable.

&amp;#x200B;

**AWS and CI/CD**

The new project will run from AWS so I can show basic competency of the service(I am doing Coursera training on the subject). I want to be able to show I can run basic virtual environments, linting, automated testing, git triggered events, and other CI/CD practices(these are things I am learning, but don't use at work).  

&amp;#x200B;

**Documentation**

The project will be well documented. This will include a general write up, system diagrams, readme's, code comments, and source code. All this will be visible from the project's websites.

&amp;#x200B;

What am I missing from this general formula?

&amp;#x200B;

**About me**

I am a Data Analyst recently turned Data Engineer with a CS degree who will target DE jobs with lots of coding and other SWE practices. I don't know anything about big data tools like Hadoop, Spark, and all that jazz.",True,False,False,dataengineering,t5_36en4,46372,public,self,Personal project for interviews: List of things to include?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/raampc/personal_project_for_interviews_list_of_things_to/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Georgehwp,,,[],,,,text,t2_5okt4lzw,False,False,False,[],False,False,1638801393,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ra8orw/is_hdf5_a_good_bet_for_storing_model_embeddings/,{},ra8orw,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,12,0,False,all_ads,/r/dataengineering/comments/ra8orw/is_hdf5_a_good_bet_for_storing_model_embeddings/,False,,,6,1638801404,1,"Need to be able to append to the structure without loading it into memory, need to be able to index by a key. Need quick load and writes. 

Primary use case is for nearest neighbour queries with torch.cdist where I want to be able to filter by some metadata (e.g. whether example is already labelled or not for use in active learning). Secondary use case is to train small CNNs (hence the reason why I don't just want to use FAISS / annoy).",True,False,False,dataengineering,t5_36en4,46368,public,self,Is HDF5 a good bet for storing model embeddings?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ra8orw/is_hdf5_a_good_bet_for_storing_model_embeddings/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,cookiethunderstorm30,,,[],,,,text,t2_6jzfhr15,False,False,False,[],False,False,1638799502,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ra81br/should_i_use_spark_or_a_python_script/,{},ra81br,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,5,0,False,all_ads,/r/dataengineering/comments/ra81br/should_i_use_spark_or_a_python_script/,False,,,6,1638799513,1,"Hi, all. I am building a pipeline that needs to batch process around 10GB of raw data sitting in s3, cleans and transforms the data so that it is ready to be loaded into Redshift. This needs to be done every HOUR. So I could do this by either running a python script in EC2, or by running spark with Glue or EMR.

What is the better solution here? Is 10GB too small to justify using spark? In what situations would you absolutely need spark instead of a python script?

Thank you.",True,False,False,dataengineering,t5_36en4,46367,public,self,Should I use spark or a python script?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ra81br/should_i_use_spark_or_a_python_script/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Complex-Stress373,,,[],,,,text,t2_9va3r6a5,False,False,False,[],False,False,1638798402,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ra7n79/schema_evolution_in_this_case_how_should_i/,{},ra7n79,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,4,0,False,all_ads,/r/dataengineering/comments/ra7n79/schema_evolution_in_this_case_how_should_i/,False,,,6,1638798413,1,"I have a personal project, learning project, an ELT system.

I fetch my data incrementally from a few mysql tables. Transformed,...usual stuff. Is already done.

Then I have this big question: ""how should I proceed if I add, rename or delete a column from any of these mysql tables""?. I know this is a question related to schema evolution, but what is your usual approach?",True,False,False,dataengineering,t5_36en4,46367,public,self,schema evolution in this case: how should I implement it?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ra7n79/schema_evolution_in_this_case_how_should_i/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,twopairisgood,,,[],,,,text,t2_d50wl,False,False,False,[],False,False,1638793608,lakefs.io,https://www.reddit.com/r/dataengineering/comments/ra64xg/the_guide_to_data_versioning/,{},ra64xg,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/ra64xg/the_guide_to_data_versioning/,False,link,"{'enabled': False, 'images': [{'id': 'aA22oUHG_5ZTKX97Lk6k_g8Dx4s7u2OVZyKvX7u3TSA', 'resolutions': [{'height': 60, 'url': 'https://external-preview.redd.it/qlydUxuz0fn0q1A_WM6OllmWwOZwzlwOdw6oQsQyjgs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9a41c8a8f849c7ca8be43cf728b51aa5311d3152', 'width': 108}, {'height': 121, 'url': 'https://external-preview.redd.it/qlydUxuz0fn0q1A_WM6OllmWwOZwzlwOdw6oQsQyjgs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d1ab1c375f957a94c0468c34986c32015295e192', 'width': 216}, {'height': 180, 'url': 'https://external-preview.redd.it/qlydUxuz0fn0q1A_WM6OllmWwOZwzlwOdw6oQsQyjgs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1cb9c2ed9c4b58bfd32bec8283f53e0145ac2aca', 'width': 320}, {'height': 360, 'url': 'https://external-preview.redd.it/qlydUxuz0fn0q1A_WM6OllmWwOZwzlwOdw6oQsQyjgs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=28c7ba4467cf55765910a94cb06b8684060f0793', 'width': 640}, {'height': 540, 'url': 'https://external-preview.redd.it/qlydUxuz0fn0q1A_WM6OllmWwOZwzlwOdw6oQsQyjgs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1bf3cb6441b76f9bd4d4f6c6919ac9550fe6c186', 'width': 960}, {'height': 607, 'url': 'https://external-preview.redd.it/qlydUxuz0fn0q1A_WM6OllmWwOZwzlwOdw6oQsQyjgs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a47f5abbd3d7be7fd90fa0183e4e054ed5e6c29f', 'width': 1080}], 'source': {'height': 1080, 'url': 'https://external-preview.redd.it/qlydUxuz0fn0q1A_WM6OllmWwOZwzlwOdw6oQsQyjgs.jpg?auto=webp&amp;s=30f86678b8cf09a71f1743d1e3d89f7b571dcb89', 'width': 1920}, 'variants': {}}]}",6,1638793620,1,,True,False,False,dataengineering,t5_36en4,46364,public,https://a.thumbs.redditmedia.com/71o9jGNAcNawHP0dRqILAjGuYaTkzPmGjYw_wYvSLm4.jpg,The Guide to Data Versioning,0,[],1.0,https://lakefs.io/data-versioning/,all_ads,6,,,,,,78.0,140.0,https://lakefs.io/data-versioning/,,,,,,,,,,
[],False,McUluld,,,[],,,,text,t2_37lhpoay,False,False,False,[],False,False,1638793448,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ra637c/properly_displaying_wiki_information/,{},ra637c,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,3,0,False,all_ads,/r/dataengineering/comments/ra637c/properly_displaying_wiki_information/,False,,,6,1638793459,1,"Hey guys, 

I have started going through the sub's wiki, but I am having a hard time having pages and tables displayed properly. 

When I click on a link of the homepage of the wiki a vertical section opens on the right side of the web page with the content of a new page.

This leaves about a third of the width of the screen to display the content I want to read. If this new page contains a table, I basically can only see two columns of the table, and I have to use the sliders to scroll horizontally. 

The button ""View larger version"" doesn't do anything. 

At first I figured I had something breaking the site with my browser or my extensions, but I have the same behavior on all browsers. 

Am I doing something wrong? Maybe there is a way to collapse the unneeded sections to free up space on my screen?

Thanks for your help",True,False,False,dataengineering,t5_36en4,46364,public,self,Properly displaying wiki information,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ra637c/properly_displaying_wiki_information/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,frenchdic,,,[],,,,text,t2_toatvig,False,False,False,[],False,False,1638792447,google.com,https://www.reddit.com/r/dataengineering/comments/ra5srs/statistical_data_visualization_with_seaborn_from/,{},ra5srs,False,True,False,False,False,True,False,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/ra5srs/statistical_data_visualization_with_seaborn_from/,False,link,"{'enabled': False, 'images': [{'id': 'LPxpns63tyYvZTIdA_0_g6ESiijc11wLIUN7u0vplX4', 'resolutions': [{'height': 60, 'url': 'https://external-preview.redd.it/LhgUysAbpakcZQNgDSELijuhNrT2XeRRsRCen9mxE80.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ab08d5272a62f78693828017f50e31d90a325a31', 'width': 108}, {'height': 120, 'url': 'https://external-preview.redd.it/LhgUysAbpakcZQNgDSELijuhNrT2XeRRsRCen9mxE80.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d0ec5bf1d8b8d4d7bb0561fb173fb1876c0cbc32', 'width': 216}, {'height': 178, 'url': 'https://external-preview.redd.it/LhgUysAbpakcZQNgDSELijuhNrT2XeRRsRCen9mxE80.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b35094fde3054f2dcec73eba18530fabf924c011', 'width': 320}, {'height': 356, 'url': 'https://external-preview.redd.it/LhgUysAbpakcZQNgDSELijuhNrT2XeRRsRCen9mxE80.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=97f2116ecc053caf6bebeebaeb13adaa6f47a109', 'width': 640}], 'source': {'height': 445, 'url': 'https://external-preview.redd.it/LhgUysAbpakcZQNgDSELijuhNrT2XeRRsRCen9mxE80.jpg?auto=webp&amp;s=a2c30f0ef14ebd4c88d6ae382dd5856813e40e36', 'width': 800}, 'variants': {}}]}",6,1638792459,1,,True,False,False,dataengineering,t5_36en4,46363,public,https://b.thumbs.redditmedia.com/0tyTaL-QFLS6Quy4yjvAfl03Wh-c1QqdTjxt0BtfG2g.jpg,Statistical Data Visualization with Seaborn From UST - Free Guided Project,0,[],1.0,https://www.google.com/amp/s/onlinecoursesgalore.com/coursera-guided-projects/amp/,all_ads,6,,,,,,77.0,140.0,https://www.google.com/amp/s/onlinecoursesgalore.com/coursera-guided-projects/amp/,,,,,,,,,,
[],False,Marksfik,,,[],,,,text,t2_24uok5oi,False,False,False,[],False,False,1638791511,ververica.com,https://www.reddit.com/r/dataengineering/comments/ra5j2m/6_things_to_consider_when_defining_your_apache/,{},ra5j2m,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/ra5j2m/6_things_to_consider_when_defining_your_apache/,False,link,"{'enabled': False, 'images': [{'id': 'NT5CnPQwj_42Llqg2Jpdxi3oICYl4jk3Kyu6NiUKX-8', 'resolutions': [{'height': 60, 'url': 'https://external-preview.redd.it/5HxSKuIu8PWkocd8QhIqdV0c6qnQI0xPIeNVF73HOXo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0d51b662315af73a566080023bb8cb9ec95ec57b', 'width': 108}, {'height': 121, 'url': 'https://external-preview.redd.it/5HxSKuIu8PWkocd8QhIqdV0c6qnQI0xPIeNVF73HOXo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=18154f1c885abf06072d28bd19f9cf7a9010f5d8', 'width': 216}, {'height': 180, 'url': 'https://external-preview.redd.it/5HxSKuIu8PWkocd8QhIqdV0c6qnQI0xPIeNVF73HOXo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=07047497a0206561b348a850e4d8eedf2a8ecd5b', 'width': 320}], 'source': {'height': 315, 'url': 'https://external-preview.redd.it/5HxSKuIu8PWkocd8QhIqdV0c6qnQI0xPIeNVF73HOXo.jpg?auto=webp&amp;s=8197ee484110f49986ea42616f6e2b3ad6018c07', 'width': 560}, 'variants': {}}]}",6,1638791522,1,,True,False,False,dataengineering,t5_36en4,46363,public,https://a.thumbs.redditmedia.com/9folvvey7sl1QCWJ-GQlPzGOuM7o7SpuPSrBZA0cZy4.jpg,6 things to consider when defining your Apache Flink cluster size,0,[],1.0,https://www.ververica.com/blog/6-things-to-consider-when-defining-your-apache-flink-cluster-size,all_ads,6,,,,,,78.0,140.0,https://www.ververica.com/blog/6-things-to-consider-when-defining-your-apache-flink-cluster-size,,,,,,,,,,
[],False,PhSon,,,[],,,,text,t2_u1xh7n5,False,False,False,[],False,False,1638789837,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/ra53wc/should_i_quit/,{},ra53wc,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,50,0,False,all_ads,/r/dataengineering/comments/ra53wc/should_i_quit/,False,,,6,1638789849,1,"My first job as a Data Engineer isn't what I expected, not learning much about popular DE tools and more how to use cloud techs like GCP. And my jobs consist mainly of upgrading, maintaining, our current data pipeline since our main project is already done when I joined, and it's been liek this for 6 months now. Not much related to actual data engineering works like writing ETL, data transformation,... I think. Thinking of quitting after only 6 months, I don't want to continue for 1 year to just end up with not much actual experience in this field. Looking for advice, thank you!",True,False,False,dataengineering,t5_36en4,46363,public,self,Should i quit?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/ra53wc/should_i_quit/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Mulkek,,,[],,,,text,t2_61gixadz,False,False,False,[],False,False,1638784956,youtube.com,https://www.reddit.com/r/dataengineering/comments/ra3v80/types_of_matrices_with_examples/,{},ra3v80,False,False,False,False,False,False,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/ra3v80/types_of_matrices_with_examples/,False,rich:video,"{'enabled': False, 'images': [{'id': 'UnW6wtW8cNgWMzlEKvouWPk7WxyHwx3vln9XxjKunbQ', 'resolutions': [{'height': 81, 'url': 'https://external-preview.redd.it/BaJ7usoREuxqL4AwOY6HgLHSaCwZcLxu1M3K0GmvKiU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=24991dd362df5cac95a8683d7940197589541236', 'width': 108}, {'height': 162, 'url': 'https://external-preview.redd.it/BaJ7usoREuxqL4AwOY6HgLHSaCwZcLxu1M3K0GmvKiU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=92e99aac496d6a33ea79b32be03373f42fdba91d', 'width': 216}, {'height': 240, 'url': 'https://external-preview.redd.it/BaJ7usoREuxqL4AwOY6HgLHSaCwZcLxu1M3K0GmvKiU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c096c8b17a09bbcbc64f960ad733405d1ab74dcb', 'width': 320}], 'source': {'height': 360, 'url': 'https://external-preview.redd.it/BaJ7usoREuxqL4AwOY6HgLHSaCwZcLxu1M3K0GmvKiU.jpg?auto=webp&amp;s=c37a815fa61dc3ff39fc8dd51ea1cda4542da45c', 'width': 480}, 'variants': {}}]}",6,1638784966,1,,True,False,False,dataengineering,t5_36en4,46359,public,https://b.thumbs.redditmedia.com/ax6qfOLuePz3-wTmXIeW7TsnjDmNPsduYL_OF_1wTSU.jpg,Types of Matrices with Examples,0,[],1.0,https://youtube.com/watch?v=Kbv7rw6sUBo&amp;feature=share,all_ads,6,"{'oembed': {'author_name': 'Mulkek', 'author_url': 'https://www.youtube.com/c/Mulkek', 'height': 200, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/Kbv7rw6sUBo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/Kbv7rw6sUBo/hqdefault.jpg', 'thumbnail_width': 480, 'title': 'Types of Matrices with Examples', 'type': 'video', 'version': '1.0', 'width': 356}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/Kbv7rw6sUBo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'scrolling': False, 'width': 356}",reddit,"{'oembed': {'author_name': 'Mulkek', 'author_url': 'https://www.youtube.com/c/Mulkek', 'height': 200, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/Kbv7rw6sUBo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/Kbv7rw6sUBo/hqdefault.jpg', 'thumbnail_width': 480, 'title': 'Types of Matrices with Examples', 'type': 'video', 'version': '1.0', 'width': 356}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/Kbv7rw6sUBo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/ra3v80', 'scrolling': False, 'width': 356}",105.0,140.0,https://youtube.com/watch?v=Kbv7rw6sUBo&amp;feature=share,,,,,,,,,,
[],False,warclaw133,,,[],,,,text,t2_md7fn,False,False,False,[],False,False,1638756511,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r9vsji/overwhelmed_so_much_to_do_so_little_time/,{},r9vsji,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,29,0,False,all_ads,/r/dataengineering/comments/r9vsji/overwhelmed_so_much_to_do_so_little_time/,False,,,6,1638756522,1,"Just needing to vent a bit. Im the only DE at a decent sized tech company. Too many data sources, not enough time to manage it all. Manager (the only DBA/sysadmin/other data person at the company for over 5 years) is leaving. I'm getting the bulk of their responsibilities on top of my own, with half the experience. 

I like the company and the work I do. Upper management all says they will do everything they can to support me. They are working on hiring an experienced data manager (and another Sr DE soon), but I feel like it's going to take forever to find someone in this environment, and then also train them on our environment and data. 

Just overwhelmed at the moment, and the light at the end of the tunnel is so damn far away.",True,False,False,dataengineering,t5_36en4,46337,public,self,"Overwhelmed... So much to do, so little time",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r9vsji/overwhelmed_so_much_to_do_so_little_time/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,informaticanonimo,,,[],,,,text,t2_h9jx8tq1,False,False,False,[],False,False,1638755303,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r9vdoj/es_consideran_importante_la_informática_en_la/,{},r9vdoj,False,False,False,False,False,False,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r9vdoj/es_consideran_importante_la_informática_en_la/,False,,,6,1638755314,1,[removed],True,False,False,dataengineering,t5_36en4,46337,public,self,[ES] ¿Consideran importante la informática en la vida del ser humano?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r9vdoj/es_consideran_importante_la_informática_en_la/,all_ads,6,,,automod_filtered,,,,,,,,,,,,,,,
[],False,mistanervous,,,[],,,,text,t2_7ec0q,False,False,False,[],False,False,1638748938,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r9t9ae/macs_for_data_engineering/,{},r9t9ae,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,31,0,False,all_ads,/r/dataengineering/comments/r9t9ae/macs_for_data_engineering/,False,,,6,1638748949,1,Tomorrow I start my first full time data engineering job and they gave me a MacBook to work with as my daily driver. I’ve never used a Mac for anything professional. Any tips for someone coming from programming on windows? Any limitations or things to keep in mind that are different from windows in the data engineering space? I know the stack is snowflake oriented.,True,False,False,dataengineering,t5_36en4,46330,public,self,Macs for data engineering,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r9t9ae/macs_for_data_engineering/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,ronald_r3,,,[],,,,text,t2_zdh50,False,False,False,[],False,False,1638748334,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r9t1oe/how_is_data_from_an_api_processed_once_inside_an/,{},r9t1oe,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,18,0,False,all_ads,/r/dataengineering/comments/r9t1oe/how_is_data_from_an_api_processed_once_inside_an/,False,,,6,1638748345,1,"I don't if discussion is the best one to pick but that should do lol.
So taking something like an e-commerce website for example... When a customer creates an account then I know (I hope 😅) that their data gets sent through an API to the OLTP database. Now does this data just get thrown into like a stored procedure where it's normalized? And where the logic determines whether the data needs to be inserted or updated (incremental and full refresh/overwrites) in the correct tables?
I'm just curious because I can never seem to find any information on it and it's as if the people that build these sorts of systems were part of some secret society lol.
And a side question why isn't it important for a data engineer to know this sort of thing? Or Is it and I should just keep looking for those resources?",True,False,False,dataengineering,t5_36en4,46329,public,self,How is data from an API processed once inside an OLTP db?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r9t1oe/how_is_data_from_an_api_processed_once_inside_an/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,BoiElroy,,,[],,,,text,t2_r8dyi,False,False,True,[],False,False,1638733503,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r9nm0z/how_often_are_you_moving_data_from_local_to_cloud/,{},r9nm0z,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,17,0,False,all_ads,/r/dataengineering/comments/r9nm0z/how_often_are_you_moving_data_from_local_to_cloud/,False,,,6,1638733514,1,"It feels like I'm doing this an unreasonable amount of the time and tooling and libraries don't do a while lot because the bottleneck usually ends up being network bandwidth. Multi threading can help to a point but still. I'm sitting here ready to use Spark and DBT and all this fun stuff but  I end up having to do a an rclone which is just...less exciting 

Curious if this is common or just my experience.",True,False,False,dataengineering,t5_36en4,46322,public,self,How often are you moving data from local to cloud?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r9nm0z/how_often_are_you_moving_data_from_local_to_cloud/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Infinite_Ad7323,,,[],,,,text,t2_gpliibio,False,False,False,[],False,False,1638726022,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r9ktw4/do_i_need_orchestration_for_a_fivetrandbt_stack/,{},r9ktw4,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,29,0,False,all_ads,/r/dataengineering/comments/r9ktw4/do_i_need_orchestration_for_a_fivetrandbt_stack/,False,,,6,1638726039,1,"If my stack consists of Fivetran for EL and dbt for T, do I need to use an orchestration tool, like Airflow?

It seems like GitHub Actions would be enough. You can set workflows to run on schedule or upon merging to a particular branch. But maybe I'm missing something about benefits of orchestration.",True,False,False,dataengineering,t5_36en4,46306,public,self,Do I need orchestration for a Fivetran-dbt stack?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r9ktw4/do_i_need_orchestration_for_a_fivetrandbt_stack/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,nitesh_ahuja,,,[],,,,text,t2_7o7huxh0,False,False,False,[],False,False,1638719905,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r9imst/difference_between_data_scientist_analyst_and/,{},r9imst,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,6,0,False,all_ads,/r/dataengineering/comments/r9imst/difference_between_data_scientist_analyst_and/,False,,,6,1638719916,1,"&amp;#x200B;

https://preview.redd.it/6gw6cihpxq381.png?width=300&amp;format=png&amp;auto=webp&amp;s=32033173641e705f0040a4ee5f52d29f3a42299e

[https://medium.com/alphaa-ai/data-analyst-engineer-scientist-86c5cfdf4c41](https://medium.com/alphaa-ai/data-analyst-engineer-scientist-86c5cfdf4c41)",True,False,False,dataengineering,t5_36en4,46298,public,https://a.thumbs.redditmedia.com/BxW97A35vBQwbJwezY4umUKtIxgX-oh4DEYYAV3JCK0.jpg,"Difference between Data Scientist, Analyst, and Engineer",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r9imst/difference_between_data_scientist_analyst_and/,all_ads,6,,,,,,140.0,140.0,,"{'6gw6cihpxq381': {'e': 'Image', 'id': '6gw6cihpxq381', 'm': 'image/png', 'p': [{'u': 'https://preview.redd.it/6gw6cihpxq381.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=47f7a3f0ad62896baf4a2f5e6be204900ef288ab', 'x': 108, 'y': 108}, {'u': 'https://preview.redd.it/6gw6cihpxq381.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b6adf8b1ac186bfad4fe90fef449c0161e3b6300', 'x': 216, 'y': 216}], 's': {'u': 'https://preview.redd.it/6gw6cihpxq381.png?width=300&amp;format=png&amp;auto=webp&amp;s=32033173641e705f0040a4ee5f52d29f3a42299e', 'x': 300, 'y': 300}, 'status': 'valid'}}",,,,,,,,,
[],False,samsamuel121,,,[],,,,text,t2_315knf3e,False,False,False,[],False,False,1638718389,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r9i4kf/framework_to_manage_and_log_tasks/,{},r9i4kf,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,8,0,False,all_ads,/r/dataengineering/comments/r9i4kf/framework_to_manage_and_log_tasks/,False,,,6,1638718401,1,"It may sound trivial but I can't find a tool to serve my purpose.

I have a long list of URLs (\~5000) I want to download files from, apply a simple operation on each file and store the result in another place. Each download and operation is generally slow (takes minutes).

Right now I have a multiprocessing script (Python) to do things in parallel. But I want to somehow keep track of what has been downloaded and processed because in case the script is interrupted for whatever reason I will be able to restart it and only process the items in the URL list that have not been processed. For this reason I use a SQLite table where I log completed tasks but it feels a little hacky and it's another thing to maintain.

Is there a tool for this kind of tasks? I'm looking for something relatively simple to setup but can't find anything",True,False,False,dataengineering,t5_36en4,46297,public,self,Framework to manage and log tasks,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r9i4kf/framework_to_manage_and_log_tasks/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,YahiaFhl,,,[],,,,text,t2_8u5svlj8,False,False,False,[],False,False,1638715846,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r9h9jc/preparing_for_google_cloud_certification_cloud/,{},r9h9jc,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,17,0,False,all_ads,/r/dataengineering/comments/r9h9jc/preparing_for_google_cloud_certification_cloud/,False,,,6,1638715856,1,"Helll community, I hope you're doing well. I wanna become a data engineer i'm currently in final year of my bachelor degree. do you recommend me this course to build a background as a beginner ? Also I don't know if i'll go for the cloud or technologies like hadoop, spark ...",True,False,False,dataengineering,t5_36en4,46294,public,self,Preparing for Google cloud certification: cloud data engineer,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r9h9jc/preparing_for_google_cloud_certification_cloud/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,boboshoes,,,[],,,,text,t2_5z43s,False,False,False,[],False,False,1638711369,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r9fvaf/moving_tbs_of_json_files_from_gcs_to_aws_s3/,{},r9fvaf,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,6,0,False,all_ads,/r/dataengineering/comments/r9fvaf/moving_tbs_of_json_files_from_gcs_to_aws_s3/,False,,,6,1638711380,1,"Hi, 

does anyone have experience moving a large amount of data from GCS to AWS S3? any insight here would be helpful. Thanks!",True,False,False,dataengineering,t5_36en4,46294,public,self,Moving TBs of json files from GCS to AWS S3,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r9fvaf/moving_tbs_of_json_files_from_gcs_to_aws_s3/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,lppier2,,,[],,,,text,t2_5rkwo3dn,False,False,False,[],False,False,1638699939,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r9cy2k/dagster_in_place_of_airflow/,{},r9cy2k,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,7,0,False,all_ads,/r/dataengineering/comments/r9cy2k/dagster_in_place_of_airflow/,False,,,6,1638699950,1,"Hi, in my work place we have 2 teams, which are using dagster (running in EKS) and airflow (MWAA) respectively. Overall, I have more experience with Dagster (I'm in that team) and have a fairly positive experience with it. I have also tried Airflow MWAA and after 2 weekends of trying out I find that I'm still barely getting anything done with it. But it may be due to a lack of airflow knowledge on my part. In this respect Dagster is really intuitive, I've actually had an intern create a pipeline to automate a ml workflow within his stay with us. 

My use case would be : 

1. ETL data bought from various data vendors for consumption by internal apps
2. Most jobs are fairly small (not big data) and can run as a EKS python job
3. Minority of the jobs need Spark, more for concurrency rather than data size. 

Would like to seek the opinions of those who have tried and used both.  

Should I forge ahead to use dagster for things like triggering AWS EMR for spark jobs? (on top of our EKS based workloads which we are currently using)",True,False,False,dataengineering,t5_36en4,46288,public,self,Dagster in place of airflow?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r9cy2k/dagster_in_place_of_airflow/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Koushik5586,,,[],,,,text,t2_wqct3,False,False,False,[],False,False,1638688590,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r9a7o6/data_engineering_resume_template/,{},r9a7o6,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,14,0,False,all_ads,/r/dataengineering/comments/r9a7o6/data_engineering_resume_template/,False,,,6,1638688601,1,I was scouting around places for a relevant Data engineering resume template but all I find is for SDE and backend SE. I would like to know if anyone has any sample resumes or any reference sites to help me build my resume?,True,False,False,dataengineering,t5_36en4,46277,public,self,Data Engineering Resume Template,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r9a7o6/data_engineering_resume_template/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Abject_Entrance_8847,,,[],,,,text,t2_f8zea8yq,False,False,False,[],False,False,1638682448,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r98mro/airflow_pipeline_that_loads_data_from_bigquery/,{},r98mro,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r98mro/airflow_pipeline_that_loads_data_from_bigquery/,False,,,6,1638682459,1,"I have created a datatable in GCP from existing bigquery sample table:

    SELECT
    cast(repository_created_at as datetime) as dt,
    repository_name,
    count(repository_open_issues) as cnt_open_issues,
    count(repository_watchers) as cnt_watchers,
    ARRAY_AGG(STRUCT(repository_url, repository_open_issues, repository_watchers )) as repo
    FROM bigquery-public-data.samples.github_timeline
    group by repository_created_at, repository_name

Now, i would like to automate the process by creating the Airflow pipeline that loads data from the bigquery sample to that created table. 

My pipeline looks like following:

    import airflow
    from airflow import DAG
    from airflow.contrib.operators.bigquery_operator import BigQueryOperator
    from datetime import datetime
    
    default_args = {
    'owner': 'Testing',
    'depends_on_past': False,
    'start_date': datetime('test_datetime'),
    'email': ['test_email'],
    'email_on_failure': True,
    'email_on_retry': False,
    }
    
    
    dag = DAG('test_bigquery_timeline',
    	default_args=default_args,
    	catchup=False,
    	schedule_interval='test_interval',
    	template_searchpath = 'tmpl_search_path'
    )
    
    BQ_CONN_ID = ""test_connection_id""
    BQ_PROJECT_ID = ""test_project_id""
    BQ_DATASET = ""test_table_schema""
    
    sample_task = BigQueryOperator(
    	task_id = ""test_load_data_into_table_schema"",
    ###### I KNOW THAT SOURCE OBJECT IS FOR UPLOADING FILE FROM EXTERNAL DATABASE BUT HOW TO MAKE IT SO THAT IT GETS NEW DATA FROM ['bigquery-public-data.samples.github_timeline'] ? 
    	source_objects = ['bigquery-public-data.samples.github_timeline'],
    	destination_dataset_table = BQ_PROJECT_ID/BQ_DATASET/TABLE
      write_disposition = WRITE_APPEND
    	bigquery_conn_id=BQ_CONN_ID,
            dag=dag
    )
    
    
    sample_task",True,False,False,dataengineering,t5_36en4,46274,public,self,Airflow pipeline that loads data from bigquery sample dataset into manually created table,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r98mro/airflow_pipeline_that_loads_data_from_bigquery/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,iamgeoknight,,,[],,,,text,t2_avt84u4i,False,False,False,[],False,False,1638682439,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r98mnv/ogr2ogr_a_simple_and_powerful_command_line_tool/,{},r98mnv,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/r98mnv/ogr2ogr_a_simple_and_powerful_command_line_tool/,False,self,"{'enabled': False, 'images': [{'id': 'IUFUOcdmdT-9wv-CkhKm26eo6F3elxQhPoitqgfRB6c', 'resolutions': [{'height': 119, 'url': 'https://external-preview.redd.it/iyc8PLZcOiaOJU0d__-Dnn0GNY9svQxavuo101Oi5ZQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=605e95b3400cf9324a56904547b7ec6c3abddfe5', 'width': 108}, {'height': 238, 'url': 'https://external-preview.redd.it/iyc8PLZcOiaOJU0d__-Dnn0GNY9svQxavuo101Oi5ZQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1d6b226534e440ecd33940c9b25795a7edf67552', 'width': 216}, {'height': 353, 'url': 'https://external-preview.redd.it/iyc8PLZcOiaOJU0d__-Dnn0GNY9svQxavuo101Oi5ZQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=257b7cfd4b80a691bd7b3bf216f8f2c7287f3fe3', 'width': 320}, {'height': 706, 'url': 'https://external-preview.redd.it/iyc8PLZcOiaOJU0d__-Dnn0GNY9svQxavuo101Oi5ZQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0e723e71257cf2dde959b49a48425857db5d1a39', 'width': 640}], 'source': {'height': 1024, 'url': 'https://external-preview.redd.it/iyc8PLZcOiaOJU0d__-Dnn0GNY9svQxavuo101Oi5ZQ.jpg?auto=webp&amp;s=12b3ff24f59dc42511a746bb78be7d7f16d38891', 'width': 927}, 'variants': {}}]}",6,1638682450,1,"Click on following link to know about ogr2ogr and how you can use it to transform your Geographic data

[https://spatial-dev.guru/2021/12/04/ogr2ogr-a-simple-command-line-tool-to-transform-your-gis-data/](https://spatial-dev.guru/2021/12/04/ogr2ogr-a-simple-command-line-tool-to-transform-your-gis-data/)",True,False,False,dataengineering,t5_36en4,46274,public,self,ogr2ogr – A simple and powerful command line tool to transform your Geographic Data,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r98mnv/ogr2ogr_a_simple_and_powerful_command_line_tool/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,citizenofacceptance,,,[],,,,text,t2_xo4dr,False,False,False,[],False,False,1638679279,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r97pw9/when_we_think_about_cost_is_the_main/,{},r97pw9,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,20,0,False,all_ads,/r/dataengineering/comments/r97pw9/when_we_think_about_cost_is_the_main/,False,,,6,1638679289,1,"Would be awesome to see like back of the envelope what calculation used to let’s say pipe data from the backend of a product via api into a lake then warehouse. 

As well , detail on the query cost after.",True,False,False,dataengineering,t5_36en4,46272,public,self,"When we think about cost, is the main consideration the MB per second a cpu can process for a pipeline ?",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r97pw9/when_we_think_about_cost_is_the_main/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,_Marwan02,,,[],,,,text,t2_9iw7pa3b,False,False,False,[],False,False,1638665217,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r93io4/ressources_for_leveling_up_with_spark/,{},r93io4,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,6,0,False,all_ads,/r/dataengineering/comments/r93io4/ressources_for_leveling_up_with_spark/,False,,,6,1638665233,1,"Hi, 

I have 1year if expérience with spark and i read Spark : The definitive guide.
How can i get to the next level ? Do you have book to recommend me ?",True,False,False,dataengineering,t5_36en4,46260,public,self,Ressources for leveling up with Spark,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r93io4/ressources_for_leveling_up_with_spark/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Affectionate-Fuel521,,,[],,,,text,t2_a6xh2lu9,False,False,False,[],False,False,1638662781,ferozedaud.blogspot.com,https://www.reddit.com/r/dataengineering/comments/r92sps/avro_logical_types_with_parquet_hive_and_spark/,{},r92sps,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r92sps/avro_logical_types_with_parquet_hive_and_spark/,False,link,"{'enabled': False, 'images': [{'id': 'GBYRhJVivlsszhjjYJE4DmW1huxVLWP1Q6hjr0IxUxo', 'resolutions': [{'height': 36, 'url': 'https://external-preview.redd.it/LilyxSrlfHN18IqHf1d4qLQ2zmcZMpBO_05pN3ViYM4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4ee904c9432da80cde8658d6a85765d94c481af3', 'width': 108}, {'height': 73, 'url': 'https://external-preview.redd.it/LilyxSrlfHN18IqHf1d4qLQ2zmcZMpBO_05pN3ViYM4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3b77963bfe2f71800dd4f889057aad11ddca3350', 'width': 216}, {'height': 109, 'url': 'https://external-preview.redd.it/LilyxSrlfHN18IqHf1d4qLQ2zmcZMpBO_05pN3ViYM4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d2f7c339d3f74d95501aa2ec919185ab7d8eddff', 'width': 320}, {'height': 218, 'url': 'https://external-preview.redd.it/LilyxSrlfHN18IqHf1d4qLQ2zmcZMpBO_05pN3ViYM4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d13b19efdff4a818997f7560ccba92fba5d699dc', 'width': 640}, {'height': 327, 'url': 'https://external-preview.redd.it/LilyxSrlfHN18IqHf1d4qLQ2zmcZMpBO_05pN3ViYM4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=07a7544a0db34e767d25686bd3dd382aa3d4001d', 'width': 960}, {'height': 368, 'url': 'https://external-preview.redd.it/LilyxSrlfHN18IqHf1d4qLQ2zmcZMpBO_05pN3ViYM4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fdfb2d0a23a5cccf33dfe8e37d1e01c690b3ef6a', 'width': 1080}], 'source': {'height': 409, 'url': 'https://external-preview.redd.it/LilyxSrlfHN18IqHf1d4qLQ2zmcZMpBO_05pN3ViYM4.jpg?auto=webp&amp;s=ca2037edf5cd8b4b5da81ab1ca8db00f51981932', 'width': 1200}, 'variants': {}}]}",6,1638662792,1,,True,False,False,dataengineering,t5_36en4,46259,public,https://b.thumbs.redditmedia.com/2l5seDZcGcWBlfRrYdsKH2SegpdlmOYm2dofa1ctnOI.jpg,"Avro Logical Types with Parquet, Hive and Spark",0,[],1.0,https://ferozedaud.blogspot.com/2021/12/avro-logical-types-with-parquet-hive_01981333308.html,all_ads,6,,,,,,47.0,140.0,https://ferozedaud.blogspot.com/2021/12/avro-logical-types-with-parquet-hive_01981333308.html,,,,,,,,,,
[],False,arezki123,,,[],,,,text,t2_1nwq6i64,False,False,False,[],False,False,1638657860,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r915uj/any_recommended_course_on_dbt/,{},r915uj,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,6,0,False,all_ads,/r/dataengineering/comments/r915uj/any_recommended_course_on_dbt/,False,,,6,1638657871,1,"After being convinced by the spread and popularity of DBT I would like to start learning but I do not find any good resources. I watched a playlist on YouTube (the only one) but I found the content very limited and Snowflake oriented.

Can someone please recommend any useful resources?",True,False,False,dataengineering,t5_36en4,46252,public,self,Any recommended course on dbt?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r915uj/any_recommended_course_on_dbt/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,wingtales,,,[],,,,text,t2_5s16o,False,False,False,[],False,False,1638655333,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r90avg/tricks_for_dealing_with_comma_errors_in_dbt_sql/,{},r90avg,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,3,0,False,all_ads,/r/dataengineering/comments/r90avg/tricks_for_dealing_with_comma_errors_in_dbt_sql/,False,self,"{'enabled': False, 'images': [{'id': 'Ct5k1_Cn-gwracKP8esEQOlDEa7p7e1Ysvs2MFDgLXI', 'resolutions': [{'height': 108, 'url': 'https://external-preview.redd.it/Zy1Wnh1zJnsuDUoNGR97RqGGom4Uo_MUVvzBcsmWxNc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ac5a556f4cfcd2b50e6672a9b00a6594acad126f', 'width': 108}], 'source': {'height': 128, 'url': 'https://external-preview.redd.it/Zy1Wnh1zJnsuDUoNGR97RqGGom4Uo_MUVvzBcsmWxNc.jpg?auto=webp&amp;s=33b381b361550ab1ebc4c63fca0e4922c7a3bb9c', 'width': 128}, 'variants': {}}]}",6,1638655344,1,"I find myself often struggling with missing/too-many-commas in dbt. Does anyone have any tips for highlighting such errors? I've tried my hand at writing a regex string to highlight a few of them (using the [VSCode Regex Highlighter extension](https://marketplace.visualstudio.com/items?itemName=fabiospampinato.vscode-highlight)), but it's a pretty complex to catch them all, and the plugin does not support a lot of necessary regex syntax (like nested capture groups). Happy to share the scripts if anyone is interested.

A couple of examples:

```sql
select
    field1,
    field2 -- &lt;--- missing comma here
    field3
from
    my_model`
```

```sql
select
    field1,
    field2,
    field3, -- &lt;--- shouldn't be a comma here
from 
    my_model
```

Does anyone have any ideas? I haven't seen any IDEs that handle highlighting like this, but maybe I don't know what I'm looking for? I know about the convention of writing the comma first, but my brain doesn't particularly like starting to use that way. (I write a lot of python normally)",True,False,False,dataengineering,t5_36en4,46251,public,self,Tricks for dealing with comma errors in dbt / sql,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r90avg/tricks_for_dealing_with_comma_errors_in_dbt_sql/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Delicious_Attempt_99,,,[],,,,text,t2_ci308gob,False,False,False,[],False,False,1638649240,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r8y70l/what_are_few_good_degrees_i_can_get_in_machine/,{},r8y70l,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/r8y70l/what_are_few_good_degrees_i_can_get_in_machine/,False,,,6,1638649251,1,"I’m looking for some good institutions for pg or diplomas on machine learning and cloud! I saw about upgrad, its reviews are neutral! Any similar kind of degrees that I can get online?

Thanks in advance",True,False,False,dataengineering,t5_36en4,46245,public,self,What are few good degrees I can get in machine learning and cloud in india,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r8y70l/what_are_few_good_degrees_i_can_get_in_machine/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,thegarlicknight,,,[],,,,text,t2_n7un0,False,False,False,[],False,False,1638647438,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r8xk8c/good_resources_that_help_to_give_the_big_picture/,{},r8xk8c,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,8,0,False,all_ads,/r/dataengineering/comments/r8xk8c/good_resources_that_help_to_give_the_big_picture/,False,,,6,1638647449,1,"I have a general interest in data engineering, but I am having trouble understanding the big picture. I generally find it easier to learn a topic if I have a better idea of what I am working towards. Are there any resources that help to break down some of the individual aspects of data engineering and what function they serve in the overall process?

A lot of resources tell you the skills you need to learn, but not necessarily what they are for or how these different skills are going to be used together.",True,False,False,dataengineering,t5_36en4,46245,public,self,Good resources that help to give the big picture for data engineering,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r8xk8c/good_resources_that_help_to_give_the_big_picture/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Olumider,,,[],,,,text,t2_17lhds6,False,False,False,[],False,False,1638644379,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r8wg73/why_ibm_ibm_solutionsetc_is_not_loved_by_data/,{},r8wg73,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,16,0,False,all_ads,/r/dataengineering/comments/r8wg73/why_ibm_ibm_solutionsetc_is_not_loved_by_data/,False,,,6,1638644389,1,I was searching for reviews on a course that was provided by IBM and uses solutions provided by IBM and i was surprised how they are not liked at all and many users recommended to skip/avoid anything provided by IBM.,True,False,False,dataengineering,t5_36en4,46239,public,self,"Why IBM ""IBM solutions...etc"" is not loved by data engineers and architects?!",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r8wg73/why_ibm_ibm_solutionsetc_is_not_loved_by_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Eastern-Resolution41,,,[],,,,text,t2_fx3mnhnj,False,False,False,[],False,False,1638643313,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r8w2fm/test_etl_on_sample_data/,{},r8w2fm,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,5,0,False,all_ads,/r/dataengineering/comments/r8w2fm/test_etl_on_sample_data/,False,,,6,1638643323,1,"Hello, we have some ETL we are doing that we want to verify with sample data and expected output data. Is this something we can incorporate into DBT, and is this testing strategy appropriate/commonly used?",True,False,False,dataengineering,t5_36en4,46237,public,self,Test ETL on sample data,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r8w2fm/test_etl_on_sample_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,hairbear1234,,,[],,,,text,t2_1swxdf76,False,False,False,[],False,False,1638642295,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r8vpce/help_with_interview_prep_stakeholder/,{},r8vpce,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/r8vpce/help_with_interview_prep_stakeholder/,False,,,6,1638642306,1,"I have a followup interview for a Senior role that consists of two 1-hour interviews. I haven't been through one of these before and wanted to ask the community for guidance on how best to prepare and what sort of questions to expect. The interviews will be:

1) Stakeholder Management and Partnership

* High level case study about creating self-service analytics for stakeholders (PMs, SWEs, DS)
* How to be attentive to and address stakeholder needs? 
* Prioritizing tasks with time/resource constraints.
* Tradeoffs between avoiding tech debt, product stability, and delivering user value quickly.
* Anti-Patterns to avoid. 

&amp;#x200B;

2) Data Pipeline Design - Whiteboard style

* Given a set of data and SQL queries, design a pipeline. Thought process on why you would set it up that way.
* How to explain to stakeholders how they would use the pipeline? How analytically useful is it for them?
* How to assess how production quality the pipeline is - how to set it up to run, where are the bottlenecks, how to ensure data quality?
* How would you gather requirements?

&amp;#x200B;

I plan on going through Designing Data-Intensive Applications for the 2nd one. I'm starting to scour the internet for info regarding the 1st one, as well as reflecting on my own experience.

I'm super psyched for this company, so thank you in advance for your advice!",True,False,False,dataengineering,t5_36en4,46237,public,self,"Help with Interview Prep - Stakeholder Management/Partnership, and Data Pipeline Design",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r8vpce/help_with_interview_prep_stakeholder/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Abject_Entrance_8847,,,[],,,,text,t2_f8zea8yq,False,False,False,[],False,False,1638641452,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r8veih/how_to_understand_the_schema_below/,{},r8veih,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,5,0,False,all_ads,/r/dataengineering/comments/r8veih/how_to_understand_the_schema_below/,False,,,6,1638641462,1,"How to understand the schema below?

What is the difference between `count_likes` and `data_likes`

    data_name            STRING
    count_likes            INTEGER
    data                 RECORD &lt;-this column has nested columns
        data_likes       INTEGER 

On top of the schema written following info:

    Dataset is aggregated by the event",True,False,False,dataengineering,t5_36en4,46236,public,self,How to understand the schema below,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r8veih/how_to_understand_the_schema_below/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Sure-Fox9929,,,[],,,,text,t2_aruj7wiq,False,False,False,[],False,False,1638637902,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r8u5al/working_full_time_as_data_analyst_should_i_pursue/,{},r8u5al,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,9,0,False,all_ads,/r/dataengineering/comments/r8u5al/working_full_time_as_data_analyst_should_i_pursue/,False,self,"{'enabled': False, 'images': [{'id': '3FBogu43P1GDm9RfMJ3eQkbPzVLDswV1DeVVOKIDby0', 'resolutions': [{'height': 56, 'url': 'https://external-preview.redd.it/iG8zLWUgWWuazifAXWbtp3KFQPH2xE-lcM7snAMFlac.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=22ed8ae998f635ed6feeead41503139fed41840f', 'width': 108}, {'height': 113, 'url': 'https://external-preview.redd.it/iG8zLWUgWWuazifAXWbtp3KFQPH2xE-lcM7snAMFlac.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=733c30d6407ebac880873731cf6a4369c66c365c', 'width': 216}, {'height': 168, 'url': 'https://external-preview.redd.it/iG8zLWUgWWuazifAXWbtp3KFQPH2xE-lcM7snAMFlac.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=aec41dd9f04105884ad3f590a76fb9d86e0d71c5', 'width': 320}, {'height': 336, 'url': 'https://external-preview.redd.it/iG8zLWUgWWuazifAXWbtp3KFQPH2xE-lcM7snAMFlac.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=38a5beec1d3f9add9abc91552fdd86ef4c8c00fd', 'width': 640}, {'height': 504, 'url': 'https://external-preview.redd.it/iG8zLWUgWWuazifAXWbtp3KFQPH2xE-lcM7snAMFlac.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e9edb00414799bb490f294e5183b75d119469a62', 'width': 960}, {'height': 567, 'url': 'https://external-preview.redd.it/iG8zLWUgWWuazifAXWbtp3KFQPH2xE-lcM7snAMFlac.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d65c5aec52998ae7d9f0259647d763224a1f7a0d', 'width': 1080}], 'source': {'height': 630, 'url': 'https://external-preview.redd.it/iG8zLWUgWWuazifAXWbtp3KFQPH2xE-lcM7snAMFlac.jpg?auto=webp&amp;s=5cd925988c647d4af4d5e47cf03823dd9bf1fd33', 'width': 1200}, 'variants': {}}]}",6,1638637913,1,"I appreciate any career advice :)

Currently, I work as a data analyst for a sales operations team. I mostly work building Power BI dashboards and local data pipelines using Python and Jupyter notebooks. I have a master's degree in business analytics (which involved a mix of Python, SQL, Tableau, machine learning and statistics).

I am interested in transitioning to Data Engineering, however I feel that I lack engineering experience. Although I sometimes hear that you can break into DE without a comp sci degree, it seems that many DE jobs require it.

I have been considering applying to [UPenn's online MCIT degree](https://online.seas.upenn.edu/degrees/mcit-online/), which is a master's designed for people without an undergrad in CS. I feel that this program can help, however it would probably take three years to complete part time while working.

Do you all have any advice on whether pursuing this degree is worth it or would it be overkill for me? Are there any alternatives I should be considering?

Thank you for your perspectives - it is nice to have a community where I can ask these questions! :)",True,False,False,dataengineering,t5_36en4,46235,public,self,Working full time as data analyst. Should I pursue a Comp Sci degree to help make transition to Data Engineering?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r8u5al/working_full_time_as_data_analyst_should_i_pursue/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,AMGraduate564,,,[],,,,text,t2_57j4x5fp,False,False,False,[],False,False,1638633913,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r8sq0l/apache_spark_pandas_api_koalas_alone_is_enough/,{},r8sq0l,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,5,0,False,all_ads,/r/dataengineering/comments/r8sq0l/apache_spark_pandas_api_koalas_alone_is_enough/,False,,,6,1638633925,1,"Spark now includes the full pandas API called Koalas, can we solely use this API to code Spark computations for ETL jobs?

It is a lot easier to get people with Python and pandas skill, than to find someone with full Spark skills. So, if we can achieve the same results as using Spark by utilizing Koalas, then this will lower the bar to entry into Spark ETL workflow.

In other words, what are the limitations of Koalas compared to Core Spark?",True,False,False,dataengineering,t5_36en4,46230,public,self,Apache Spark: pandas API (Koalas) alone is enough for Spark functionalities?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r8sq0l/apache_spark_pandas_api_koalas_alone_is_enough/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,arezki123,,,[],,,,text,t2_1nwq6i64,False,False,False,[],False,False,1638623099,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r8pa3i/why_is_data_build_tool_dbt_is_so_popular_what_are/,{},r8pa3i,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,75,0,False,all_ads,/r/dataengineering/comments/r8pa3i/why_is_data_build_tool_dbt_is_so_popular_what_are/,False,,,6,1638623110,1,"Hi, can some one please explain why DBT is so popular?",True,False,False,dataengineering,t5_36en4,46222,public,self,Why is Data Build Tool (DBT) is so popular? What are some other alternatives?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r8pa3i/why_is_data_build_tool_dbt_is_so_popular_what_are/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Jemal,,,[],,,,text,t2_5p9gj,False,False,False,[],False,False,1638620237,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r8oilr/lambda_vs_kappa_architecture_pros_and_cons/,{},r8oilr,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r8oilr/lambda_vs_kappa_architecture_pros_and_cons/,False,,,6,1638620248,1,"I feel like Kappa has alot of upsides (lower latency, simpler codebase) and limited downsides (difficulty in implementation?).

Are there more to the story? In what situation should we choose lambda over kappa and vice versa?",True,False,False,dataengineering,t5_36en4,46220,public,self,Lambda vs Kappa architecture. Pros and cons?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r8oilr/lambda_vs_kappa_architecture_pros_and_cons/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Comprehensive-Set-77,,,[],,,,text,t2_8j06kn2a,False,False,False,[],False,False,1638613486,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r8murh/burnout/,{},r8murh,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,19,0,False,all_ads,/r/dataengineering/comments/r8murh/burnout/,False,self,"{'enabled': False, 'images': [{'id': 'rv4hy8XLtvk76lMd_0UcbOpAknGpeVSTbueACckrYJQ', 'resolutions': [{'height': 70, 'url': 'https://external-preview.redd.it/AtaFh09ozWB0accJbc64EFGxxJvY4oG08KBJ0IA3r7A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c0fa6a295bb810c585d9945756222046fcdf669a', 'width': 108}, {'height': 141, 'url': 'https://external-preview.redd.it/AtaFh09ozWB0accJbc64EFGxxJvY4oG08KBJ0IA3r7A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=87a6b1012142c79c95402f5bfc48c070af9cd7dc', 'width': 216}, {'height': 210, 'url': 'https://external-preview.redd.it/AtaFh09ozWB0accJbc64EFGxxJvY4oG08KBJ0IA3r7A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7e402256dca0e4cc8a39ac72b0427b8d7dab0996', 'width': 320}, {'height': 420, 'url': 'https://external-preview.redd.it/AtaFh09ozWB0accJbc64EFGxxJvY4oG08KBJ0IA3r7A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4fd525ba8893aa9a3f10d444f9d04e8897e5f8ad', 'width': 640}, {'height': 630, 'url': 'https://external-preview.redd.it/AtaFh09ozWB0accJbc64EFGxxJvY4oG08KBJ0IA3r7A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=63832c4099e861b6b5ca10c10fde1666c274f314', 'width': 960}, {'height': 709, 'url': 'https://external-preview.redd.it/AtaFh09ozWB0accJbc64EFGxxJvY4oG08KBJ0IA3r7A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0e8a64eec3a7d101cbf3b8aea5567437069a0227', 'width': 1080}], 'source': {'height': 1106, 'url': 'https://external-preview.redd.it/AtaFh09ozWB0accJbc64EFGxxJvY4oG08KBJ0IA3r7A.jpg?auto=webp&amp;s=a9180ae8e0b34825d2cfae42bcee5a484d855b9d', 'width': 1683}, 'variants': {}}]}",6,1638613497,1,"Ok just going to be transparent here. I have been working as a VP of sales, growth hacker and even a CMO. I discovered my passion for coding and data about 8 years ago and I did a 360 and became a data engineer 1 year ago.

I have never ever experienced any symptoms of burnout until now. The workload I ridiculous at least at my workplace. 

I started researching and came across this article: https://www.helpnetsecurity.com/2021/10/25/data-engineers-burnout/

It's basically saying that the 97% of DE is experiencing some kind of burnout.

Do u guys relate, or am I just doing this stuff wrong?",True,False,False,dataengineering,t5_36en4,46217,public,self,Burnout?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r8murh/burnout/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,CapitalistZ,,,[],,,,text,t2_99stunfc,False,False,False,[],False,False,1638605010,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r8kvqd/own_business/,{},r8kvqd,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,8,0,False,all_ads,/r/dataengineering/comments/r8kvqd/own_business/,False,,,6,1638605020,1,"Does anyone run their own business? I'm seeing ridiculous money being paid for integrations and reporting. I'm doing this from sales, design through to coding the solutions. Seems like a no-brainer to start doing it for myself.",True,False,False,dataengineering,t5_36en4,46214,public,self,Own Business,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r8kvqd/own_business/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Abject_Entrance_8847,,,[],,,,text,t2_f8zea8yq,False,False,False,[],False,False,1638603217,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r8kftx/task_consultation/,{},r8kftx,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,2,0,False,all_ads,/r/dataengineering/comments/r8kftx/task_consultation/,False,,,6,1638603228,1,"Guys, i am just starting out as a data engineer. I have a data table that looks like following:

    dt                     DATE
    repository_name,       STRING
    cnt_open_issue,        INTEGER
    cnt_watchers,          INTEGER
    repo                   RECORD -&gt; [
            repository_url            STRING 
            repository_open_issues    INTEGER
            repository_watchers       INTEGER
    ]
        

Now i need to write a airflow pipeline with BigQueryOperator that loads a data into the schema above using  **bigquery-public-data.samples.github\_timeline** 

Here is what i have tried. (Do not worry about the exact code, \[even pseudocode is fine\] rather i want to understand the overall flow of the process, see if i am on a right direction. This is my first pipeline)

    import airflow
    from airflow import DAG
    from airflow.contrib.operators.bigquery_operator import BigQueryOperator
    
    from datetime import datetime
    
    default_args = {
    'owner': 'Testing',
    'depends_on_past': False,
    'start_date': datetime('test_datetime'),
    'email': ['test_email'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=2),
    }
    
    
    dag = DAG('test_bigquery_timeline',
    default_args=default_args,
    schedule_interval='test_interval'
    )
    
    BQ_CONN_ID = ""test_connection_id""
    BQ_PROJECT_ID = ""test_project_id""
    BQ_DATASET = ""test_table_schema""
    
    sample_task = BigQueryOperator(
    	task_id = ""test_load_data_into_table_schema"",
            source_objects = ['bigquery-public-data.samples.github_timeline'],
    	schema_fields=[
    			{'name': 'dt', 'type': 'DATE', 'mode': 'NULLABLE'},
    			{'name': 'repository_name', 'type': 'STRING', 'mode': 'NULLABLE'},
    		        {'name': 'cnt_open_issues', 'type': 'INTEGER', 'mode': 'NULLABLE'},
    			{'name': 'cnt_watchers', 'type': 'INTEGER', 'mode': 'NULLABLE'},
    			{'name': 'repo', 'type': 'RECORD', 'mode': 'REPEATED', ""fields"": [
                {
                    ""name"": ""repository_url"",
                    ""type"": ""STRING"",
                    ""mode"": ""NULLABLE""
                },
                {
                    ""name"": ""repository_open_issues"",
                    ""type"": ""INTEGER"",
                    ""mode"": ""NULLABLE""
                },
                {
                    ""name"": ""repository_watchers"",
                    ""type"": ""INTEGER"",
                    ""mode"": ""NULLABLE""
                }] 
             }
    					],
    	destination_dataset_table = BQ_PROJECT_ID/BQ_DATASET/TABLE,
    	allow_large_results = True,
    	write_disposition = WRITE_APPEND
            bigquery_conn_id=BQ_CONN_ID,
            dag=dag	
    )
    
    
    sample_task

Is there anything wrong with this pipeline? 

&amp;#x200B;

Also, what makes me to have a doubt about it is, For example:

**bigquery-public-data.samples.github\_timeline** does not have a `cnt_open_issues` column, how this code would know what exacly to copy from the bigquery-public-data.sample.github\_timeline? I am sure i'm missing out something here. I am trying to understand how airflow pipeline works. I know there are bunch of course, i learn better by doing. Looking for advice/direction/correction from you guys. Thank you all",True,False,False,dataengineering,t5_36en4,46213,public,self,task consultation,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r8kftx/task_consultation/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Ok-Window-8625,,,[],,,,text,t2_dq0q0f5o,False,False,False,[],False,False,1638583368,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r8ew6i/how_would_you_prefer_a_sales_person_reach_you/,{},r8ew6i,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/r8ew6i/how_would_you_prefer_a_sales_person_reach_you/,False,,,6,1638583379,1,"How would you prefer a sales person reach you about a product for data processing

[View Poll](https://www.reddit.com/poll/r8ew6i)",True,False,False,dataengineering,t5_36en4,46197,public,self,How would you prefer a sales person reach you about a product for data processing.,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r8ew6i/how_would_you_prefer_a_sales_person_reach_you/,all_ads,6,,,,,,,,,,,,,"{'is_prediction': False, 'options': [{'id': '12251416', 'text': 'Email'}, {'id': '12251417', 'text': 'Call'}, {'id': '12251418', 'text': 'Text'}, {'id': '12251419', 'text': 'LinkedIn'}, {'id': '12251420', 'text': ""Don't contact me""}], 'prediction_status': None, 'resolved_option_id': None, 'total_stake_amount': None, 'total_vote_count': 0, 'tournament_id': None, 'user_selection': None, 'user_won_amount': None, 'vote_updates_remained': None, 'voting_end_timestamp': 1638842568907}",,,,,
[],False,CableComprehensive97,,,[],,,,text,t2_bt30tbpu,False,False,False,[],False,False,1638570293,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r8al65/i_have_a_list_5k_company_urls_im_wanting/,{},r8al65,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,4,0,False,all_ads,/r/dataengineering/comments/r8al65/i_have_a_list_5k_company_urls_im_wanting/,False,,,6,1638570598,0,"I know for example that from Crunchbase I could get stock symbols from the relevant URLs, then probably take that to another financial service to do the rest (unknown which one/I'm new to this) - then connect the data back to my main list....

But ideally I'm wanting to do it all in one system.

&amp;#x200B;

As mentioned too I'm new to working with market/stock related systems, and would prefer something free (but paid is fine too) - so suggestions on tools that first can associate company URLs to stock info, and have some flexibility in what I can query (more the merrier) is what I'm seeking.

Big thank you ahead for any help here too.",True,False,False,dataengineering,t5_36en4,46194,public,self,"I have a list (5k+) company URLs I'm wanting stock/market info on - at least for the ones that are public ---- what's the best service/tool to take this list of company URLs, bulk upload, and ideally then export chosen data I query based on the companies from the list that are public?",0,[],0.5,https://www.reddit.com/r/dataengineering/comments/r8al65/i_have_a_list_5k_company_urls_im_wanting/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,francesco1093,,,[],,,,text,t2_zlyww,False,False,False,[],False,False,1638565440,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r893rw/why_is_snowflake_so_popular/,{},r893rw,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,55,0,False,all_ads,/r/dataengineering/comments/r893rw/why_is_snowflake_so_popular/,False,,,6,1638565450,1,"Hi all,

I am just wondering why so many companies use Snowflake. As far as I know, it is a Cloud Datawarehouse, which means that the main competitors are platforms such as BigQuery, Redshift, Synapse. What makes Snowflake better than the rest?

Do you use it in your work? In that case, which kind of application do you have? And which tools do you integrate it with?

Thanks!",True,False,False,dataengineering,t5_36en4,46192,public,self,Why is Snowflake so popular?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r893rw/why_is_snowflake_so_popular/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,GuardiansBeer,,,[],,,,text,t2_4btxj,False,False,False,[],False,False,1638563924,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r88jt1/does_your_organization_have_a_personposition_that/,{},r88jt1,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/r88jt1/does_your_organization_have_a_personposition_that/,False,,,6,1638563934,1,"This would likely be limited to very large companies with a significant number of analysts and complex data, but i am curious what the role may be called and duties that the person or team would take on.

My employer (fortune 50, 1000+ analysts, dozens of co-related business lines with data to share across teams) has a role as described, but no good name for it and lots of opportunities to make it better.   The person (specific to a line of business) is a point-of-contact when you are looking for something or have questions, they have the roadmap for new data additions and prioritize building new tables/views/products.  The role helps people get access to systems and listens for the things that are difficult, then tries to make the process easier.",True,False,False,dataengineering,t5_36en4,46191,public,self,"Does your organization have a person/position that makes data easier to find, or serve as a advocate for analysts trying to identify the right data? or, as a bridge between analyst and data engineers to define what is most needed?",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r88jt1/does_your_organization_have_a_personposition_that/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,cpardl,,,[],,,,text,t2_fb1s1pke,False,False,False,[],False,False,1638560412,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r87a80/best_practices_to_track_and_control_costs_on/,{},r87a80,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,9,0,False,all_ads,/r/dataengineering/comments/r87a80/best_practices_to_track_and_control_costs_on/,False,,,6,1638560422,1,"Hey everyone,

I'm wondering what kind of best practices or mechanisms you are using to track and control costs on Snowflake. 

The platform offers a couple of different options to do that, e.g. querying system tables, check the UI and a quota system, but I feel that there's a lot to be done on top of that in a production system.",True,False,False,dataengineering,t5_36en4,46190,public,self,Best Practices to track and control costs on Snowflake,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r87a80/best_practices_to_track_and_control_costs_on/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Significant-Carob897,,,[],,,,text,t2_4x8s649h,False,False,False,[],False,False,1638554508,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r85306/dbt_pricing_for_an_analytics_stack_with_around/,{},r85306,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,4,0,False,all_ads,/r/dataengineering/comments/r85306/dbt_pricing_for_an_analytics_stack_with_around/,False,,,6,1638554519,1,"This is a noob question. And apologies if it gives you-didnot-do-enough-research vibes.


So in community, I see a lot of DBT recommendations and did some research if it suits our needs for an analytics stack that we are moving from mssql server on premise to gcp. mostly cloud functions, bigquery, looker. our data is currently around 2TB and we heavily rely on sql procedures that we plan to replace by bigquery routines. These sql procedures are poorly versioned (copy pasting to production kinda of thing) and a team of 10 data engineers work on different business requirements in the stack.

So I was testing if dbt will resolve issues of versioning and database schema handling (that continuously evolves). But I struggled with a couple of issues.

- $50/Data Engineer monthly felt expensive to me. Is it?
- Can anyone shed light on Enterprise version pricing.
- I read its open source and have a thriving community? Is it about dbt-cli?
- Do data security bothers you? I understand Google is not something to be trusted either but lets say company is okay to put their data in GCP, how to explain that dbt will also have access to the data now.

What ""dbt"" is everyone using that is so affordable and has all the features and looks like a data engineer's dream come true?

Again apologies if it all sounds too stupid.",True,False,False,dataengineering,t5_36en4,46185,public,self,DBT pricing for an analytics stack with around 2TB data. Is it worth the cost?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r85306/dbt_pricing_for_an_analytics_stack_with_around/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,tigermatos,,,[],,,,text,t2_gxkcvrzk,False,False,False,[],False,False,1638548003,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r82met/realtime_stream_sliding_window_vs_tumblingbatch/,{},r82met,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,7,0,False,all_ads,/r/dataengineering/comments/r82met/realtime_stream_sliding_window_vs_tumblingbatch/,False,,,6,1638548014,1,"Ok, I get it. Batching data into mini chunks makes sense in cases like presenting a histogram on a dashboard, or saving data in summaries to conserve storage.

But for event-driven applications, batch-processing.... mmm

Say you are comparing stats from the last minute, last hour, and last 24 hours in order to detect something, like an opportunity in the stock market, or a shift in crowd sentiment, or a possible hacker attack, etc. Tumbling window or batching has 2 major drawbacks:  
1 - it has to wait for the batch to fill (or closing time), which could cost precious time to react.   
2 - the specific patterns you are hoping to detect often get cut off into different batches at a point that makes them completely undetected! 

Sliding-window queries run continually, and guarantee that the pattern gets detected, but have a massive compute requirement. It's literally applying a formula each time a stream message is received. The bigger the window, the slower they get exponentially, which might be why sliding window queries aren't so popular. Too expensive. 

So I decided to make a new stream analytics engine that can query sliding windows at a constant speed (nanoseconds regardless of window size), so that we can build better event-driven applications on small VMs without spending a fortune.   
(And yes, this entire post is a desperate attempt to find collaborators.)

The app is free and open-source, and almost ready. 

If you are into this topic, let's have a discussion, people!

Right now I'm making a cutesy website for the project: [www.riodb.org](https://www.riodb.org)

There you can find out more about the app and myself. I suggest starting with About -&gt; FAQ -&gt; ""What makes it fast?""

If you aren't a programmer, no worries. I'm looking for thoughts, questions, use cases, experiments, benchmarks, etc. Or maybe you know producers and consumers tools (or dashboards) that would be fun to integrate.

Let's get this party started!

THANKS   
Tiger",True,False,False,dataengineering,t5_36en4,46182,public,self,Real-time stream: sliding window vs tumbling/batch,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r82met/realtime_stream_sliding_window_vs_tumblingbatch/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,ezia_stiva,,,[],,,,text,t2_8abl3scu,False,False,False,[],False,False,1638547129,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r82ad1/interview_on_tuesday/,{},r82ad1,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,18,0,False,all_ads,/r/dataengineering/comments/r82ad1/interview_on_tuesday/,False,,,6,1638547140,1,I have my final technical interview Tuesday morning for a job I’ll make a lot more in. Terrified of being berated for 90 min because I’ve never done a technical interview before. Just posting for well wishes and luck 🥲 I’ll be cramming a coursera course in this weekend.,True,False,False,dataengineering,t5_36en4,46179,public,self,Interview On Tuesday,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r82ad1/interview_on_tuesday/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,camelCaseInsensitive,,,[],,,,text,t2_ggg0wfmt,False,False,False,[],False,False,1638545833,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r81syz/help_with_managing_airflow_dev_git_branches/,{},r81syz,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r81syz/help_with_managing_airflow_dev_git_branches/,False,,,6,1638545845,1,"I have an interesting problem that some of you more experienced might be able to help with.

As a team we have to constantly merge our feature branches since Airflow expects a single DAG folder. 

Our CI/CD setup means that if I push a feature branch my branch becomes the current state of the Airflow instance. 

The issue with this is that if someone else makes a change in a separate branch that doesn’t have my updates they must merge in my branch before pushing otherwise my changes are overwritten in the dev DAG folder. 

Needless to say this defeats the purpose of branching. Since when I’m ready to merge into master I risk bringing along all the unfinished branches I’ve merged into mine during development.

Curious to hear how others have managed this setup? In an ideal world we would spin up an airflow instance for each feature branch when it is pushed with new changes but that makes small iterative changes slow since deployment with every update to the branch takes time.",True,False,False,dataengineering,t5_36en4,46179,public,self,Help with Managing Airflow Dev Git Branches,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r81syz/help_with_managing_airflow_dev_git_branches/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,throw_away_91028303,,,[],,,,text,t2_h6jurqcr,False,False,False,[],False,False,1638545667,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r81qqu/data_engineers_who_were_previously_java_based/,{},r81qqu,False,False,False,False,False,False,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/r81qqu/data_engineers_who_were_previously_java_based/,False,self,"{'enabled': False, 'images': [{'id': '_L34yknxK57AtozKOEDuYLcMs5EcnEfxIDk4JcYWqEw', 'resolutions': [{'height': 54, 'url': 'https://external-preview.redd.it/oRQy8Kxttm0hftzHpWw-AGCqMXWqjKEHNT1qzWqsG_E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=42ea95a2fa76929c3a054a731ae072f5fa3b656f', 'width': 108}, {'height': 108, 'url': 'https://external-preview.redd.it/oRQy8Kxttm0hftzHpWw-AGCqMXWqjKEHNT1qzWqsG_E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5469e98fc1392023d648dcbf31891e76cdb1c8cc', 'width': 216}, {'height': 160, 'url': 'https://external-preview.redd.it/oRQy8Kxttm0hftzHpWw-AGCqMXWqjKEHNT1qzWqsG_E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1d4580a98e789d8dd0fa8ac4cdcaab75dd49e896', 'width': 320}, {'height': 320, 'url': 'https://external-preview.redd.it/oRQy8Kxttm0hftzHpWw-AGCqMXWqjKEHNT1qzWqsG_E.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=619064d867133901351df149f521273e40f42c27', 'width': 640}, {'height': 480, 'url': 'https://external-preview.redd.it/oRQy8Kxttm0hftzHpWw-AGCqMXWqjKEHNT1qzWqsG_E.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=091d3c86938ef6201d08090e3419586bbba7c918', 'width': 960}, {'height': 540, 'url': 'https://external-preview.redd.it/oRQy8Kxttm0hftzHpWw-AGCqMXWqjKEHNT1qzWqsG_E.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=aa4c3457423773f5e7d33817de62824acc78568b', 'width': 1080}], 'source': {'height': 640, 'url': 'https://external-preview.redd.it/oRQy8Kxttm0hftzHpWw-AGCqMXWqjKEHNT1qzWqsG_E.jpg?auto=webp&amp;s=bfeb1cf89d6c4970b25fa831f89e38b4aae449e2', 'width': 1280}, 'variants': {}}]}",6,1638545679,1,[removed],True,False,False,dataengineering,t5_36en4,46179,public,self,"Data engineers who were previously Java based developers, can you please share your insights?",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r81qqu/data_engineers_who_were_previously_java_based/,all_ads,6,,,automod_filtered,,,,,,,,,,,,,,,
[],False,-80am,,,[],,,,text,t2_l5l2s7a,False,False,False,[],False,False,1638540017,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r7zqmg/best_way_to_permanently_store_snowflake_table/,{},r7zqmg,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,6,0,False,all_ads,/r/dataengineering/comments/r7zqmg/best_way_to_permanently_store_snowflake_table/,False,,,6,1638540035,1,"Hi all. Normally I just use Snowflake for analytics and technically, I could drop the whole DB and no actual data would be lost. I would just have to run (and wait) for pipelines to populate it again.
However, a client wants to take quarterly snapshots of a complex view (materialized as a table) for audit purposes. This is in case the underlying data ever changes, they will have a record of the exact data at the time they did audit reporting.
My instinct is to export the snapshot to version controlled S3 bucket and read it back in to Snowflake as an external table. This is so I could keep the practice of saying, ""Snowflake can be deleted. It is not the source of anything.""
Another colleague just wants to append it into another Snowflake table, but that feels like we are one accident away from dropping it. And now Snowflake is the source for this one audit data set.
Am I over cautious or thinking of Snowflake wrong? Advice or links to relevant docs/discussions welcome.",True,False,False,dataengineering,t5_36en4,46173,public,self,Best way to permanently store Snowflake table snapshots,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r7zqmg/best_way_to_permanently_store_snowflake_table/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,superconductiveKyle,,,[],,,,text,t2_3xnau4cx,False,False,False,[],False,False,1638539641,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r7zmam/distinguishing_critical_pipeline_tests_from/,{},r7zmam,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,False,3,0,False,all_ads,/r/dataengineering/comments/r7zmam/distinguishing_critical_pipeline_tests_from/,False,self,"{'enabled': False, 'images': [{'id': 'JP_5xoTXTXmRCV4JjfiX4k1eRU5CZSSNtZbL39ctUpU', 'resolutions': [{'height': 69, 'url': 'https://external-preview.redd.it/OM5LgREmIMdaSojGcNEcyPJ1NKoDhVwrH43JuO_8I-k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9c68df13379ae8a485625c1bbd78c4cdc89d215c', 'width': 108}, {'height': 138, 'url': 'https://external-preview.redd.it/OM5LgREmIMdaSojGcNEcyPJ1NKoDhVwrH43JuO_8I-k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9aa112d7c511f87d30713839489de06c95b9c269', 'width': 216}, {'height': 205, 'url': 'https://external-preview.redd.it/OM5LgREmIMdaSojGcNEcyPJ1NKoDhVwrH43JuO_8I-k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=57d0491c35796f19f0ced335aa1646d0afc17520', 'width': 320}, {'height': 411, 'url': 'https://external-preview.redd.it/OM5LgREmIMdaSojGcNEcyPJ1NKoDhVwrH43JuO_8I-k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0cec87781d20ec6ed4d9aa7450e6350176bf3b4a', 'width': 640}, {'height': 617, 'url': 'https://external-preview.redd.it/OM5LgREmIMdaSojGcNEcyPJ1NKoDhVwrH43JuO_8I-k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4d6bf71a3939e67f95acb8b292eaed88ae5b36b1', 'width': 960}, {'height': 694, 'url': 'https://external-preview.redd.it/OM5LgREmIMdaSojGcNEcyPJ1NKoDhVwrH43JuO_8I-k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=92da7855624bfa7df44dd526f7cf2884ef055dab', 'width': 1080}], 'source': {'height': 1260, 'url': 'https://external-preview.redd.it/OM5LgREmIMdaSojGcNEcyPJ1NKoDhVwrH43JuO_8I-k.jpg?auto=webp&amp;s=6fe874665734ac8a8e4813408ca5855a67c1aef3', 'width': 1960}, 'variants': {}}]}",6,1638539652,1,"[https://greatexpectations.io/blog/distinguishing-critical-pipeline-tests-from-metrics/](https://greatexpectations.io/blog/distinguishing-critical-pipeline-tests-from-metrics/)

We should all know at this point data quality and testing your data is important but I like the angle that this blog takes on avoiding altering fatigue. It's great that you set a system up but it's pretty easy to create a bunch of extra noise.",True,False,False,dataengineering,t5_36en4,46171,public,self,Distinguishing critical pipeline tests from metrics. How do you decide what to actually test?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r7zmam/distinguishing_critical_pipeline_tests_from/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Significant-Carob897,,,[],,,,text,t2_4x8s649h,False,False,False,[],False,False,1638538659,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r7zba9/version_control_data_pipelines_in_bigquery/,{},r7zba9,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,8,0,False,all_ads,/r/dataengineering/comments/r7zba9/version_control_data_pipelines_in_bigquery/,False,,,6,1638538669,1,"We are kicking off migration to BigQuery from mssql server for our analytics stack.

What is the best way to version control tables/views creation, routines creation, schema creation and changes over time.

I have checked out terraform but looks like it comes with its own set of limitations like when u add a column table is destroyed and created.  Something we cannot afford with production data.

Looking at github actions too, something like this: a merge to main branch runs bq command to do some action in gcp. 

Any similar setups and experiences. What are the best practices etc.

Thanks in advance.",True,False,False,dataengineering,t5_36en4,46170,public,self,Version Control Data Pipelines in Bigquery,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r7zba9/version_control_data_pipelines_in_bigquery/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,mister_patience,,,[],,,,text,t2_xt5zb,False,False,False,[],False,False,1638537682,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r7z0ic/creating_your_own_data_engineering_labs/,{},r7z0ic,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,6,0,False,all_ads,/r/dataengineering/comments/r7z0ic/creating_your_own_data_engineering_labs/,False,,,6,1638537694,1,"Has anyone got any experience or knowledge around creating your own labs? Either in Azure or AWS?

Thoughts, resources, experience all welcome!",True,False,False,dataengineering,t5_36en4,46168,public,self,Creating your own data engineering labs?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r7z0ic/creating_your_own_data_engineering_labs/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,pendulumpendulum,,,[],,,,text,t2_fz57elu,False,False,False,[],False,False,1638537353,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r7yx09/what_should_be_the_next_step_i_should_take_to/,{},r7yx09,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,10,0,False,all_ads,/r/dataengineering/comments/r7yx09/what_should_be_the_next_step_i_should_take_to/,False,,,6,1638537364,1,"At my company that I work at now, my business title is “data engineer”, but all I do is work in Informatica Powercenter. I almost never write any code. The only code that I do write is SQL, and I barely even do that. Pretty much my entire job is just creating workflows in Informatica powercenter. I am worried that this job is not giving me enough opportunity for growth and enough transferable skills to be able to be competitive in the job market. 

What should I do? Also I want to make it clear that it seems like my job role will not be changing anytime soon to include any new technologies or new rules. I genuinely think if I stayed in this job, I would be doing Informatica powercenter forever and never learning anything new.

My opinion is that I either need to get a new job at a new company so that I can learn new skills, or I need to transfer to a completely different team within my company, and possibly not be a “data engineer“ anymore since it seems like the way my company does data engineering is not very beneficial for career growth in my personal opinion.

But I would like to read opinions from people who are far more experienced than I am. What do you think I should do? I have about 15 months of experience so far, and this is the only job I’ve had since graduating.",True,False,False,dataengineering,t5_36en4,46167,public,self,What should be the next step I should take to improve my career prospects?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r7yx09/what_should_be_the_next_step_i_should_take_to/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,boggle_thy_mind,,,[],,,,text,t2_9d1jjuxh,False,False,False,[],False,False,1638534286,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r7y0dq/snowflake_infer_schema_for_external_tables/,{},r7y0dq,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,4,0,False,all_ads,/r/dataengineering/comments/r7y0dq/snowflake_infer_schema_for_external_tables/,False,,,6,1638534297,1,"Anyone knows if it is possible to use infer_schema to create external tables with a defined schema (by default it creates a table with a single VALUE column that contains all the data)? I have parquet files in ADSL that are pretty wide (200+ columns) and I was able to use infer schema to create normal tables with no issue and I can create external tables if I define the columns manually, but the same syntax to infer_schema does not work for  external tables.

I have a data factory job that loads the data into ADSL with a timestamp and I pick it up with snowpipe, but at the moment there is no need to keep all historical data, having the current value is good enough, so I was thinking I could use external table and simply overwrite the file in ADSL and read the latest through an external table.",True,False,False,dataengineering,t5_36en4,46165,public,self,Snowflake infer_schema for external tables,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r7y0dq/snowflake_infer_schema_for_external_tables/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,UnderCover_0,,,[],,,,text,t2_bbp2lw8f,False,False,False,[],False,False,1638521574,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r7uscs/spark_ml_biased_result_my_confusion_about_machine/,{},r7uscs,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,11,0,False,all_ads,/r/dataengineering/comments/r7uscs/spark_ml_biased_result_my_confusion_about_machine/,False,,,6,1638521585,1,"Hi guys,

I am new to data engineering domain, very challenging. I am still trying to understand a basic high-level overview on Spark ML (Machine Learning Parallelization). I came across a material, screenshot (attached):

&amp;#x200B;

https://preview.redd.it/p6des5g9ka381.png?width=1368&amp;format=png&amp;auto=webp&amp;s=86f6105df0e423e7ca9828035d9e0707ea3bb23c

My confusion is that **would such a way produce a bias model  because each node only worked on partial data instead of taking the  whole data as a consideration of learning?**

I can understand the Spark parallelization can benefit batch processing, map-reduce jobs, e.g., counting, sorting, aggregating:

Imagine a 30 GB dataset. By parallelizing to 3 x 10 GB subsets to 3 workers (nodes), it boosts the processing productivity. 

HOWEVER,  in the above case. We want to do a Linear Regression (Supervised  Learning) or SVD (Unsupervised Learning) from these 30GB data points.  Ideally, the algorithm digest the 30GB dataset as a whole to produce a  model (catching all the nuances, relationships, correlation, variances,  etc.), making a relatively accurate model. 

BUT, if we partitioned  to 3 x 10GB (3 subsets) separately into three different workers, each  worker products its models based on its assigned 10GB data. The system  combines the results with the final result. Wouldn’t the final model result be  very different from the model we train on the whole 30GB dataset?  (Because each work’s trained model is only based on its given partial  data)

Would you please correct me if I am wrong? Kind of confused? 

My question is: **will the Spark machine learning parallelization produce a biased model than the way train/learning on a whole original dataset?** 

&amp;#x200B;

Thanks!",True,False,False,dataengineering,t5_36en4,46159,public,https://b.thumbs.redditmedia.com/q6_F4sI9yK-HflhEQh4O9T6fbqGwyETzBQJjRbyKRdc.jpg,[Spark ML] Biased result? My Confusion about Machine Learning Parallelization?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r7uscs/spark_ml_biased_result_my_confusion_about_machine/,all_ads,6,,,,,,49.0,140.0,,"{'p6des5g9ka381': {'e': 'Image', 'id': 'p6des5g9ka381', 'm': 'image/png', 'p': [{'u': 'https://preview.redd.it/p6des5g9ka381.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=00597a41a1800ddbe9cea7916fb5111c84e802fc', 'x': 108, 'y': 37}, {'u': 'https://preview.redd.it/p6des5g9ka381.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=664df76e7b6bb709f8ac2a1ea0affd04720d1490', 'x': 216, 'y': 75}, {'u': 'https://preview.redd.it/p6des5g9ka381.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e2f5f460f03088f2f2da3781611fbb2f6722732c', 'x': 320, 'y': 112}, {'u': 'https://preview.redd.it/p6des5g9ka381.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e07642ef6ecbfe446ba6111690e98d269cb3f726', 'x': 640, 'y': 224}, {'u': 'https://preview.redd.it/p6des5g9ka381.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=11fb4cd1100a95deeba19e03ed8954ee05725b71', 'x': 960, 'y': 336}, {'u': 'https://preview.redd.it/p6des5g9ka381.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4b771351bb8a30522aff772594d4e8636bbd97d4', 'x': 1080, 'y': 378}], 's': {'u': 'https://preview.redd.it/p6des5g9ka381.png?width=1368&amp;format=png&amp;auto=webp&amp;s=86f6105df0e423e7ca9828035d9e0707ea3bb23c', 'x': 1368, 'y': 479}, 'status': 'valid'}}",,,,,,,,,
[],False,nobel-001,,,[],,,,text,t2_a3lkw6g4,False,False,False,[],False,False,1638520066,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r7uf01/using_both_snowflake_and_databricks/,{},r7uf01,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,26,0,False,all_ads,/r/dataengineering/comments/r7uf01/using_both_snowflake_and_databricks/,False,,,6,1638520078,1,"
Would it make sense to use both Snowflake and Databricks in my cloud data architecture

I would consider Snowflake as my main data warehouse and apply SQL transformations in it. (~70% of transformations)

While Databricks will be used for more complex data transformations that require python \ pyspark (~30% of transformations), as well as considering it as our Machine Learning experimentation platform, making use of its Notebooks, AutoML, ....

In this case I will not be using delta lack \ lack house, instead of this Databricks will be read and write the data from\to Snowflake.

Does this make sense, or I have to choose either Snowflake or Databricks?",True,False,False,dataengineering,t5_36en4,46156,public,self,Using both Snowflake and Databricks,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r7uf01/using_both_snowflake_and_databricks/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,whitet445,,,[],,,,text,t2_4hq1f4mk,False,False,False,[],False,False,1638511023,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r7s0yr/what_would_you_do_if_you_had_some_spare_time_in_a/,{},r7s0yr,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,9,0,False,all_ads,/r/dataengineering/comments/r7s0yr/what_would_you_do_if_you_had_some_spare_time_in_a/,False,,,6,1638511034,1," say you had 2-3 spare hours a day to spend doing anything that could contribute to your growth as a data scientist, possibly increase your income, increase job prospects, skills set, etc. What would that be?",True,False,False,dataengineering,t5_36en4,46146,public,self,what would you do if you had some spare time in a day?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r7s0yr/what_would_you_do_if_you_had_some_spare_time_in_a/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,pknerd,transparent,,[],fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b,Data Engineer,dark,text,t2_54sig,False,False,False,[],False,False,1638510681,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r7rxj1/data_engineering_roadmap_for_a_pythonetl/,{},r7rxj1,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,False,23,0,False,all_ads,/r/dataengineering/comments/r7rxj1/data_engineering_roadmap_for_a_pythonetl/,False,,,6,1638510692,1,"Hi  
I have been working in Python for a long time, even developed custom ETLs and in Airflow and a bit in Spark but so far I could not pursue the career properly. Now I just want to up from being an ETL Developer only to a full-fledged Data Engineering field. I need guidance in the following:

&amp;#x200B;

* What roadmap can I follow to learn more about data engineering in the next 3-4 months?
* Should I specialize in a certain tool like Spark or Airflow?
* Is there any data engineering certification that could help in both learning and jobs? 
* Any course?

I am sorry if I am not making sense but in short, I need a clear roadmap and a plan to execute things ASAP.

&amp;#x200B;

Thanks",True,False,False,dataengineering,t5_36en4,46146,public,self,Data Engineering Roadmap for a Python/ETL Developer? need guidance,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r7rxj1/data_engineering_roadmap_for_a_pythonetl/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,targetXING,,,[],,,,text,t2_6zw4hqke,False,False,False,[],False,False,1638505319,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r7qae4/s3_cost_spikes_with_glue_crawler_please_help/,{},r7qae4,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,3,0,False,all_ads,/r/dataengineering/comments/r7qae4/s3_cost_spikes_with_glue_crawler_please_help/,False,,,6,1638505330,1," I recently implemented a glue crawler to run every hour, before it was once a week. However, I noticed that my S3 cost, in the cost explorer, spike up last month and this month. My S3 buckets currently have less than 1GB so I don't think the cost is due to the storage. I am, however, using the glue meta-store in a spark job, every hour.

Then, I change the glue crawler to only check new folders. I am hoping that this will resolve this because the cost has been very high for me.",True,False,False,dataengineering,t5_36en4,46143,public,self,S3 Cost Spikes with Glue Crawler? Please Help,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r7qae4/s3_cost_spikes_with_glue_crawler_please_help/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,iamyuvraj0002,,,[],,,,text,t2_55aldhot,False,False,False,[],False,False,1638497909,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r7nsh3/about_data_engineer/,{},r7nsh3,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,3,0,False,all_ads,/r/dataengineering/comments/r7nsh3/about_data_engineer/,False,,,6,1638497921,1,So can any one tell me what skills are important for data Engineer? What's the roles and responsibilities of data engineer?,True,False,False,dataengineering,t5_36en4,46139,public,self,About data engineer,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r7nsh3/about_data_engineer/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,bhrd,,,[],,,,text,t2_112f7d,False,False,False,[],False,False,1638496951,aws.amazon.com,https://www.reddit.com/r/dataengineering/comments/r7ngsh/announcing_aws_data_exchange_for_apis_find/,{},r7ngsh,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/r7ngsh/announcing_aws_data_exchange_for_apis_find/,False,link,"{'enabled': False, 'images': [{'id': 'QQUyyF5eIUn1d1mxo8CBxlYbgYh_VChJ9h2Rf48z3P4', 'resolutions': [{'height': 54, 'url': 'https://external-preview.redd.it/x19ww9H5TiinGL28ceFCw4UHQ7slFkcFM8Yz_u00jJY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d5d435ba7c4cdb1d047fa37cc4fe1f8ff71c5ed2', 'width': 108}, {'height': 108, 'url': 'https://external-preview.redd.it/x19ww9H5TiinGL28ceFCw4UHQ7slFkcFM8Yz_u00jJY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=58ab43e9ce1ae93ab264f435b1bb549b962f69f3', 'width': 216}, {'height': 160, 'url': 'https://external-preview.redd.it/x19ww9H5TiinGL28ceFCw4UHQ7slFkcFM8Yz_u00jJY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f2e66844122253e1e6b866f231356bfe4343d10a', 'width': 320}, {'height': 320, 'url': 'https://external-preview.redd.it/x19ww9H5TiinGL28ceFCw4UHQ7slFkcFM8Yz_u00jJY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=75edc44b13f85c018612618908ac0b2fae4bb0d7', 'width': 640}], 'source': {'height': 400, 'url': 'https://external-preview.redd.it/x19ww9H5TiinGL28ceFCw4UHQ7slFkcFM8Yz_u00jJY.jpg?auto=webp&amp;s=b0ed6ecc67e45e051fe2500f43b3e8b2e7681186', 'width': 800}, 'variants': {}}]}",6,1638496962,1,,True,False,False,dataengineering,t5_36en4,46139,public,https://a.thumbs.redditmedia.com/Gm5mBqOea-TanNFbV_FtY_BvYloEaxhDTBYLYdjVxn8.jpg,"Announcing AWS Data Exchange for APIs: Find, Subscribe to, and Use Third-party APIs with Consistent Authentication | Amazon Web Services",0,[],1.0,https://aws.amazon.com/blogs/aws/data-exchange-for-apis-find-subscribe-use-third-party-apis-consistent-authentication/,all_ads,6,,,,,,70.0,140.0,https://aws.amazon.com/blogs/aws/data-exchange-for-apis-find-subscribe-use-third-party-apis-consistent-authentication/,,,,,,,,,,
[],False,the_budding_dawn,,,[],,,,text,t2_6mymkkro,False,False,False,[],False,False,1638491060,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r7li57/free_alternatives_to_cdata_drivers/,{},r7li57,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r7li57/free_alternatives_to_cdata_drivers/,False,,,6,1638491071,1,"Forgive me if I sound like a noob, because I am, but is there a free alternative to CData Drivers? I'm just an admin assistant trying to wow my boss by linking NetSuite customer records with an Access mail log and am wanting to have it so if you add a customer record in NetSuite it will add the customer to my Access database. Thanks for any help.",True,False,False,dataengineering,t5_36en4,46133,public,self,Free alternatives to CData Drivers?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r7li57/free_alternatives_to_cdata_drivers/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Veryzoned,,,[],,,,text,t2_159vou,False,False,False,[],False,False,1638489071,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r7kteu/data_engineering_vs_data_science/,{},r7kteu,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,4,0,False,all_ads,/r/dataengineering/comments/r7kteu/data_engineering_vs_data_science/,False,,,6,1638489082,1,"Heyy soooo im sure this is a very common question here, so sorry in d’avance if it is, but I would love some clarification from much more experienced people like you guys that I might have missed through just Googling! 

My question is more focused on the starter and education side of things, as in; what does an aspiring Data Engineer do/learn compared to a Data Scientist? I feel Data Engineering would suit me much better, but all my knowledge comes from the Data Science side of things. Are both fields very similar to each other when it comes to learning and its only once you get a job that you are given the title as Engineer or Scientist (like the roadmap towards becoming one or the other are similar?).

At the moment a short version of my roadmap WAS looking like this: Take a Data Science Bootcamp, get a starters job in the field while continuing into an Engineering degree at my Uni. What changes should I make, or add? 

Thanks in advance everyone, this would be such a life saver!",True,False,False,dataengineering,t5_36en4,46134,public,self,Data Engineering vs Data Science?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r7kteu/data_engineering_vs_data_science/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,liliumdog,,,[],,,,text,t2_2pe0d8uq,False,False,False,[],False,False,1638481931,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r7iaz8/advice_for_syncing_a_postgres_database_with_a/,{},r7iaz8,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,8,0,False,all_ads,/r/dataengineering/comments/r7iaz8/advice_for_syncing_a_postgres_database_with_a/,False,,,6,1638481942,1,"I work at a company where we store the majority of our data in several PostgreSQL databases. Some of our data is in the form of time series. We're in the process of trying to improve query performance, and after doing some benchmarking with TimescaleDB I've found it would be much more efficient than the other methods I've tried with just Postgres by itself.

At a previous company, we used Kafka to sync two databases that stored similar data that needed to be slightly reformatted before being inserted into the destination db. I'm considering using Kafka in this scenario as well to sync the data from Postgres to the TimescaleDB (we're on AWS so I can't simply just install the timescaledb extension to the existing host unfortunately). 

Does anyone know of any alternatives to Kafka to keep a Postgres host and a TimescaleDB host in sync? I've found documentation on streaming replication for TimescaleDB, but that seems to only be for the source/replica architecture.",True,False,False,dataengineering,t5_36en4,46124,public,self,Advice for syncing a Postgres database with a TimescaleDB database,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r7iaz8/advice_for_syncing_a_postgres_database_with_a/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,adrianturcu,,,[],,,,text,t2_4bma6c5u,False,False,False,[],False,False,1638478865,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r7h7fj/data_platform_generator_data_solution_blueprints/,{},r7h7fj,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r7h7fj/data_platform_generator_data_solution_blueprints/,False,self,"{'enabled': False, 'images': [{'id': 'LNMu_l2ovKOMj5XkCYT2UyRPvNBpvcyGROms-oVwg1k', 'resolutions': [{'height': 55, 'url': 'https://external-preview.redd.it/c9btwOn5R_cyMCeleOMkie8uJ209FFqf5ni5I9X4mZ8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=49e298a3ce173c0b0e9cdb83d7251849aefaef00', 'width': 108}, {'height': 110, 'url': 'https://external-preview.redd.it/c9btwOn5R_cyMCeleOMkie8uJ209FFqf5ni5I9X4mZ8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d74b289a81aa65497f007312abc93b77493d8f14', 'width': 216}, {'height': 163, 'url': 'https://external-preview.redd.it/c9btwOn5R_cyMCeleOMkie8uJ209FFqf5ni5I9X4mZ8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d5a382d3373ffba0c98439000a2eac1788487aad', 'width': 320}, {'height': 326, 'url': 'https://external-preview.redd.it/c9btwOn5R_cyMCeleOMkie8uJ209FFqf5ni5I9X4mZ8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d464f8c503872ef08f01b0e81be1b6c363e28b0d', 'width': 640}, {'height': 489, 'url': 'https://external-preview.redd.it/c9btwOn5R_cyMCeleOMkie8uJ209FFqf5ni5I9X4mZ8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0a08cf57a4d0bd8b1bf116e9b7c419936bc49b10', 'width': 960}, {'height': 550, 'url': 'https://external-preview.redd.it/c9btwOn5R_cyMCeleOMkie8uJ209FFqf5ni5I9X4mZ8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a6ba151a12f6df612884c4fa806ef478db3066b3', 'width': 1080}], 'source': {'height': 1034, 'url': 'https://external-preview.redd.it/c9btwOn5R_cyMCeleOMkie8uJ209FFqf5ni5I9X4mZ8.jpg?auto=webp&amp;s=7d3b2a8c3d6267e0a26fd679fe73217fe0a81c5a', 'width': 2027}, 'variants': {}}]}",6,1638478875,1,"Hi everyone,

We took inspiration from [Matt Turck’s Data &amp; AI landscape](https://mattturck.com/data2021/) and [Scott Brinker's Martech landscape](https://chiefmartec.com/2020/04/marketing-technology-landscape-2020-martech-5000/) to generate visual data solution blueprints to inspire you in the design process of your data platform and help you to find alternative combinations of data technologies.

Our idea was simple:

* Let's take some technologies from these landscapes and generate simple visual data solution blueprints to inspire other data aficionados in the design process of a data platform. 
* Also, expose them to different technologies that exist on the market which could be good alternatives to already better-known technologies
* Create a generator to display one picture at a time - we have millions of combinations. (Can you discover all of them?)
* We used for the first time No-Code applications like [Bubble](https://bubble.io/) and [Airtable](https://www.airtable.com/)

[We launched this page today](https://dataplatformgenerator.com/). This is a concept site we call ""**Picture Data Solution**"" for all the data aficionados out there.

Have fun, be inspired, and we hope you enjoy it. 

Don't forget to share it with your friends if you like it!

We also, appreciate your brutal and honest feedback",True,False,False,dataengineering,t5_36en4,46121,public,self,Data Platform Generator - Data Solution Blueprints,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r7h7fj/data_platform_generator_data_solution_blueprints/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,SlavKiwi,,,[],,,,text,t2_bunxw5qc,False,False,False,[],False,False,1638476551,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r7gdh1/scaling_data_ingest_for_nlp_models/,{},r7gdh1,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,3,0,False,all_ads,/r/dataengineering/comments/r7gdh1/scaling_data_ingest_for_nlp_models/,False,,,6,1638476562,1,"We were testing out an NLP pipeline, but most of the data test runs were done with data in Panda Dataframes.   
Now as we are happy with the results of the ML model, I wondered what would be the best way to scale the ingestion? As we have more than 5 million text files potentially. Would it have to be a file store of individual files or they could be grouped into something like parquets with the string column ?  


My background is primarily in SQL and data modelling, and only in past few months I've been exploring Apache Spark and Databricks as a potential solution for scaling.",True,False,False,dataengineering,t5_36en4,46119,public,self,Scaling Data Ingest for NLP models,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r7gdh1/scaling_data_ingest_for_nlp_models/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,No-Intern-6231,,,[],,,,text,t2_a08r6g6s,False,False,False,[],False,False,1638476402,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r7gbeg/which_cloud_certification_for_de/,{},r7gbeg,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/r7gbeg/which_cloud_certification_for_de/,False,,,6,1638476413,1,"Hello everyone,
Can you guys please share few suggestions which AWS certification will be more suitable and preferable for Data Engineering profile ?",True,False,False,dataengineering,t5_36en4,46117,public,self,Which Cloud Certification for DE,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r7gbeg/which_cloud_certification_for_de/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,sumkar,,,[],,,,text,t2_152dq2,False,False,False,[],False,False,1638473982,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r7ff69/spark_data_pipeline_design_considerations/,{},r7ff69,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/r7ff69/spark_data_pipeline_design_considerations/,False,,,6,1638473994,1,"Hi all,

 I have extensive experience in building etl/elt jobs . Typical etl job consists of multiple jobs with transformation logic , scheduling , auditing, logging .etc

 Iam new to data pipelines with spark , spark streaming. However I have worked  on few pocs( proof of concepts) on databrics spark jobs and found concepts are very similar.


While designing spark jobs what are the design decisions you consider ? Are they same as designing etl jobs like

Job dependencies /Orchestration ( can airflow be used on data bricks jobs  ?)

Master data loads ( dimension updates )

Job auditing and job logs (where and how the audits are stored .)

How to Size the cluster based on Valume of data ?

Thanks",True,False,False,dataengineering,t5_36en4,46117,public,self,Spark Data pipeline design considerations,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r7ff69/spark_data_pipeline_design_considerations/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,jana_50n,,,[],,,,text,t2_9771y6c2,False,False,False,[],False,False,1638470087,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r7dyom/what_percent_of_your_work_would_you_say_is_done/,{},r7dyom,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,123,0,False,all_ads,/r/dataengineering/comments/r7dyom/what_percent_of_your_work_would_you_say_is_done/,False,,,6,1638470099,1,What is your job title?,True,False,False,dataengineering,t5_36en4,46112,public,self,"What percent of your work would you say is done using Python, SQL, etc.?",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r7dyom/what_percent_of_your_work_would_you_say_is_done/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,secodaHQ,,,[],,,,text,t2_aiinah9q,False,False,False,[],False,False,1638466078,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r7chgj/5_steps_to_managing_and_prioritizing_data/,{},r7chgj,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,4,0,False,all_ads,/r/dataengineering/comments/r7chgj/5_steps_to_managing_and_prioritizing_data/,False,self,"{'enabled': False, 'images': [{'id': 'Qa7hW4NvO2AlqldJu-ESV0mdxxu0nTWVciCC4J2o6kM', 'resolutions': [{'height': 65, 'url': 'https://external-preview.redd.it/HmuvPAD8VOd73hwvHBB9-Q2UMAFmYRXAHziig_NsUPY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1812ce7cef2dbb7add4328912bc89da7d2401ad4', 'width': 108}, {'height': 131, 'url': 'https://external-preview.redd.it/HmuvPAD8VOd73hwvHBB9-Q2UMAFmYRXAHziig_NsUPY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a79c299a669f6ec0d5a60eaf34cd2b855da4cab5', 'width': 216}, {'height': 194, 'url': 'https://external-preview.redd.it/HmuvPAD8VOd73hwvHBB9-Q2UMAFmYRXAHziig_NsUPY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=710a4032aeb0c61aa37d92b4175537cc8909b203', 'width': 320}, {'height': 389, 'url': 'https://external-preview.redd.it/HmuvPAD8VOd73hwvHBB9-Q2UMAFmYRXAHziig_NsUPY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a5bd6dddf81f1f4dbddd778bbe112c11f309c6d8', 'width': 640}, {'height': 584, 'url': 'https://external-preview.redd.it/HmuvPAD8VOd73hwvHBB9-Q2UMAFmYRXAHziig_NsUPY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9c265a1827bae1648f474e9cfe22a94bcf647b34', 'width': 960}, {'height': 657, 'url': 'https://external-preview.redd.it/HmuvPAD8VOd73hwvHBB9-Q2UMAFmYRXAHziig_NsUPY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d6b4b233d3f1024ab3147d64698d992a4bde0000', 'width': 1080}], 'source': {'height': 740, 'url': 'https://external-preview.redd.it/HmuvPAD8VOd73hwvHBB9-Q2UMAFmYRXAHziig_NsUPY.jpg?auto=webp&amp;s=dd377d6ce2a5e81655a49673e3ce14b2e4a54198', 'width': 1216}, 'variants': {}}]}",6,1638466089,1,"As an early data hire at a fast-growing company, one of the first things that you’ll likely encounter is a backlog of questions from employees. This backlog, alongside all the other tasks associated with reporting, maintaining data and creating new pipelines can feel extremely overwhelming. These long lists of tasks and questions prevent many data teams from being proactive about analysis. Without a proper process in place that allows analysts to enable self-service, managing data requests proactively can feel like a never-ending battle against a current. Additionally, data teams that constantly need to answer the same question are not creating processes that help them manage the scale and complexity that companies experience as they grow. 

Below are some of our tips on how to manage the data requests backlog. We believe that great data teams should be proactive. They should adopt tools and processes that ensure that the data team never has to answer the same question twice. We hope this step by step list is helpful to all data teams that want to improve their efficiency and reduce their workload in the future:

**1. Set expectations about the data requests workflow with reasonable timelines**

The first step of setting expectations around a team is communicating the way that your team is working to the stakeholders in the company. We suggest adopting an Agile workflow with weekly or bi-weekly sprints. Although your Scrum team may be small at first, setting expectations about when certain requests will be answered with the sprint methodology can be helpful. With scrum, a product is built in a series of iterations called sprints that break down big, complex projects into bite-sized pieces. 

**2. Define your requests workflow**

Data requests are questions that employees have about data that exists in the organization or about new data that isn't being collected yet. Traditionally, data teams will take data requests through an intake form or a Slack channel where employees can ask for the request. Some of the requests are unique and difficult questions, which require the full attention of the data team. On the other hand, some data requests are repetitive and low priority, which doesn’t require much effort from the data team.

**3. Create a requests template**

We’ve created a data requests template for teams that are looking for a better way to manage inbound questions. Below is the template. Feel free to copy the template and use it in your team's workflow. Here's our template:

* *What is the business question you are trying to answer?*
* *What is the impact of this question and how will it help the company?*
* *Who will be using this data?*
* *What time frames are crucial here? (Example: Monthly, weekly, daily)*
* *What is the visualization you are trying to create?*
* *What interactions/drill-downs are required? (Ie. the type of use, revenue amount etc.)*
* *Are there any other details we should know about this data request?*

**4. Automate repetitive data requests**

Data teams that take the next step with their data requests process can start to think proactively about data requests. Customer support teams have been deflecting inbound questions for years using tools like Intercoms knowledge base and Ada automated customer support chatbots. Smart data teams realize that they can do the same. Data teams can automate and deflect common questions with tools that allow them to document data requests in the same place teammates are looking for answers.

**5. Measure the data requests workflow**

Lastly, you can’t improve anything you don’t measure. Taking the time to measure what your users are asking, which tables are used the most, and who is the most influential user in your organization is a great way to automate more common questions. 

If you found this useful, you can find the full article here: [https://www.secoda.co/blog/how-to-manage-and-prioritize-data-requests](https://www.secoda.co/blog/how-to-manage-and-prioritize-data-requests)",True,False,False,dataengineering,t5_36en4,46107,public,self,5 Steps to managing and prioritizing data requests (free template included),0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r7chgj/5_steps_to_managing_and_prioritizing_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,pong_pong_pong,,,[],,,,text,t2_8tntgtxc,False,False,False,[],False,False,1638460610,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r7aige/is_there_an_open_source_etl_tool_to_convert_a/,{},r7aige,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,14,0,False,all_ads,/r/dataengineering/comments/r7aige/is_there_an_open_source_etl_tool_to_convert_a/,False,,,6,1638460622,1,"Hello, I'm a QA Engineer trying to migrate our test database dump from Oracle to PostgreSQL in order to perform regression/sanity/smoke tests on a Postgres database with no success, there's always a certain set of sequences that do not function correctly rendering the system unfunctional and lots of modules of the product failing, the tools I tried to do the database migration with are Ora2Pg and DBEaver unsuccessfully. So I read that you can use Talend which is an ETL tool that can do just what I want it to do, but what I'm trying to find is if an open source alternative exists that can do what I need it to do? Thanks",True,False,False,dataengineering,t5_36en4,46102,public,self,Is there an open source ETL tool to convert a database dump from one DBMS to another?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r7aige/is_there_an_open_source_etl_tool_to_convert_a/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,LemurPrime,,,[],,,,text,t2_gtjfu,False,False,False,[],False,False,1638460417,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r7ag0g/cicd_in_data_engineering_help_a_noob/,{},r7ag0g,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,24,0,False,all_ads,/r/dataengineering/comments/r7ag0g/cicd_in_data_engineering_help_a_noob/,False,,,6,1638460428,1,"I'm trying to recommend some process improvements at my org as we start standing up a Snowflake DW and expand our Azure app footprint. Our legacy data infrastructure is pretty wild west so not a lot of source control or ""Ops"" thinking. 

Anyone here successfully implemented or worked in an environment that had particularly good tooling and process around data pipeline code review, deployment, and change control? What are some ""must haves"" in your opinion?",True,False,False,dataengineering,t5_36en4,46102,public,self,CI/CD in data engineering - help a noob,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r7ag0g/cicd_in_data_engineering_help_a_noob/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,cfcchamps09,,,[],,,,text,t2_4ngh82a8,False,False,False,[],False,False,1638456705,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r794l4/how_to_automate_python_extract_etl_script/,{},r794l4,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,12,0,False,all_ads,/r/dataengineering/comments/r794l4/how_to_automate_python_extract_etl_script/,False,,,6,1638456715,1,"I currently have a python script that pulls data from an API. I now need to store that data somewhere (either S3 or google drive or Mysql etc., haven't decided yet).

Once I decide on the destination and adapt the script to load the data, how do I automate the running of the script?

I don't want to use my local machine. I'm looking for both specifics and general knowledge and how to create an environment where your script is run.",False,False,False,dataengineering,t5_36en4,46101,public,self,How to automate python Extract (ETL) script,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r794l4/how_to_automate_python_extract_etl_script/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,demince,,,[],,,,text,t2_umfhx4d,False,False,False,[],False,False,1638453614,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r782oa/how_to_simplify_data_engineering_work/,{},r782oa,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r782oa/how_to_simplify_data_engineering_work/,False,self,"{'enabled': False, 'images': [{'id': 'xO4CFSly24dnDiLoKRP6w07g4AgM0wM0Vx8P-9Hqa40', 'resolutions': [{'height': 81, 'url': 'https://external-preview.redd.it/ckBkjuKDiYCZRhwHuIG561T2xbBxXrswuF_RgFFGwk0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8c7c75abf85dbdd87eca2d551499d7e1da9d9baa', 'width': 108}, {'height': 162, 'url': 'https://external-preview.redd.it/ckBkjuKDiYCZRhwHuIG561T2xbBxXrswuF_RgFFGwk0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=40f77c9240071d6613bdfe49a7a31fa585239ce5', 'width': 216}, {'height': 240, 'url': 'https://external-preview.redd.it/ckBkjuKDiYCZRhwHuIG561T2xbBxXrswuF_RgFFGwk0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=50521aba827ce8d44785515f11081ecb0d45413e', 'width': 320}], 'source': {'height': 360, 'url': 'https://external-preview.redd.it/ckBkjuKDiYCZRhwHuIG561T2xbBxXrswuF_RgFFGwk0.jpg?auto=webp&amp;s=54c550a3122740faf463698f7cb465be99b571f2', 'width': 480}, 'variants': {}}]}",6,1638453632,1,[https://youtu.be/457S98K6i60](https://youtu.be/457S98K6i60),True,False,False,dataengineering,t5_36en4,46097,public,self,How to simplify data engineering work?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r782oa/how_to_simplify_data_engineering_work/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,FoolishEnquiry,,,[],,,,text,t2_dnxyju9v,False,False,False,[],False,False,1638446154,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r75v2a/automatically_refresh_an_xlsx_on_google_drive/,{},r75v2a,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,4,0,False,all_ads,/r/dataengineering/comments/r75v2a/automatically_refresh_an_xlsx_on_google_drive/,False,self,"{'enabled': False, 'images': [{'id': 'nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0', 'resolutions': [{'height': 108, 'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1c8a90e5690a7186afdb269ad05279551994d09', 'width': 108}, {'height': 216, 'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=533bd055cdae7998d1b8f9cd9d7dedabc1715bda', 'width': 216}], 'source': {'height': 316, 'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;s=8cd5e918e2bde6ca72d4445d6fc007f203689799', 'width': 316}, 'variants': {}}]}",6,1638446165,1,"Hi! I have an excel spreadsheet on my google drive that I refresh every day, but since it takes 10-15 mins to complete, I was wondering if I could automate this say at 2 am right after the data is updated in the MSSQL database. Currently, the excel is connected to the database with a connection string containing my username to the database.

I am planning on doing it this way, while somehow adding the credentials into the mix:

[https://stackoverflow.com/questions/40893870/refresh-excel-external-data-with-python](https://stackoverflow.com/questions/40893870/refresh-excel-external-data-with-python)

Is there a better approach  to this?",True,False,False,dataengineering,t5_36en4,46086,public,self,Automatically refresh an xlsx on google drive from an MSSQL database,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r75v2a/automatically_refresh_an_xlsx_on_google_drive/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,pithuttar,,,[],,,,text,t2_gejjxxhg,False,False,False,[],False,False,1638441967,firebolt.io,https://www.reddit.com/r/dataengineering/comments/r74s4n/cloud_data_warehouse_the_full_guide/,{},r74s4n,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,4,0,False,all_ads,/r/dataengineering/comments/r74s4n/cloud_data_warehouse_the_full_guide/,False,,,6,1638441979,1,,True,False,False,dataengineering,t5_36en4,46078,public,default,Cloud Data Warehouse - The Full Guide,0,[],1.0,https://www.firebolt.io/blog/cloud-data-warehouse,all_ads,6,,,,,,,,https://www.firebolt.io/blog/cloud-data-warehouse,,,,,,,,,,
[],False,Substantial-Gain9199,transparent,,[],fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b,Data Engineer,dark,text,t2_78k9tce8,False,False,False,[],False,False,1638441848,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r74r25/py_spark_or_scala_spark/,{},r74r25,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,1,0,False,all_ads,/r/dataengineering/comments/r74r25/py_spark_or_scala_spark/,False,,,6,1638441859,1,Which would be much more useful in long term???,True,False,False,dataengineering,t5_36en4,46077,public,self,Py Spark or scala+ spark?????,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r74r25/py_spark_or_scala_spark/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,BaljenderSingh,,,[],,,,text,t2_h2pwrzbl,False,False,False,[],False,False,1638436019,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r73cmj/our_new_competition_with_nasa_ncbi_ncats_nih_is/,{},r73cmj,False,False,False,False,False,False,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r73cmj/our_new_competition_with_nasa_ncbi_ncats_nih_is/,False,self,"{'enabled': False, 'images': [{'id': 'fhZ1MttDTtA9D1H1CmkaeU3cWY0-Zr1OEVhD72d3Cak', 'resolutions': [{'height': 56, 'url': 'https://external-preview.redd.it/Cls9P-kLGa23V-eAD9cdTcaIFwDvyXcnkhnwoAoYpIg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ab5b5ae7ae3ff00fc116354858f011faea74094b', 'width': 108}, {'height': 113, 'url': 'https://external-preview.redd.it/Cls9P-kLGa23V-eAD9cdTcaIFwDvyXcnkhnwoAoYpIg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=108cb17ed4dbaec8e02ac1976dd0b50ce4560cb8', 'width': 216}, {'height': 168, 'url': 'https://external-preview.redd.it/Cls9P-kLGa23V-eAD9cdTcaIFwDvyXcnkhnwoAoYpIg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cb54f0bf384847ee50fab4ddf6e9f88428671d3c', 'width': 320}, {'height': 336, 'url': 'https://external-preview.redd.it/Cls9P-kLGa23V-eAD9cdTcaIFwDvyXcnkhnwoAoYpIg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=332885e86e93904ef60f5bad840a92fd64ad0d92', 'width': 640}, {'height': 504, 'url': 'https://external-preview.redd.it/Cls9P-kLGa23V-eAD9cdTcaIFwDvyXcnkhnwoAoYpIg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a12befb608b4344e1766f2f28fb359a02903dde1', 'width': 960}, {'height': 567, 'url': 'https://external-preview.redd.it/Cls9P-kLGa23V-eAD9cdTcaIFwDvyXcnkhnwoAoYpIg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6f696b255fd93d9d1f154bd23c7564110b3a3f6f', 'width': 1080}], 'source': {'height': 630, 'url': 'https://external-preview.redd.it/Cls9P-kLGa23V-eAD9cdTcaIFwDvyXcnkhnwoAoYpIg.jpg?auto=webp&amp;s=fe5c54cf364497cecf7db2f79ef29bfc0ca5249a', 'width': 1200}, 'variants': {}}]}",6,1638436030,1,[removed],True,False,False,dataengineering,t5_36en4,46073,public,self,"📢 Our new competition with NASA, NCBI, NCATS &amp; NIH is LIVE! 🙌 The competition itself is free to enter with a winning amount of $100,000",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r73cmj/our_new_competition_with_nasa_ncbi_ncats_nih_is/,all_ads,6,,,reddit,,,,,,,,,,,,,,,
[],False,Bubbly-Piece-8037,,,[],,,,text,t2_8kux3ewt,False,False,False,[],False,False,1638431756,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r72c1m/convincing_for_apache_stack/,{},r72c1m,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/r72c1m/convincing_for_apache_stack/,False,,,6,1638431767,1,"Hi folks, 

I'm currently building a DWH. We are gathering IIoT data from industrial machines with time series data and other stuff. IoT and consumption data only have time so I have to use cartesian product to match them with their respective jobId in every factory. Our system will be on-prem.

I know that we are not going to use distributed systems but currently researching apache druid to store our time series data and inmon modeled dwh. In short, OnPrem machine asks for IoT data every second to industrial machines (can be up to 40) and gets an array of all IoT change data (length of about 400 mostly NULL). Concurrently, JobID and consumption data will be gathered and all data will be matched with jobID every 10 minutes and ETL will trigger to load it to DWH.(our current system design) In velocity this seems a lot but not in size. I think Apache stack is cool but I cannot just develop it bc I want it. Given the fact that number of sensor, machine can increase or system expansion in ERP,etc. how can I convince my team leader to practice and learn those techs in prod enviroment?

In data industry, there is no exact definition of big data though so I can calculate velocity x volume x variety formula. When you look for requirements, you usually see example of very big tech companies with over 1 million/sec data flow which is not the case for %99.99 of companies. What would be your approach?",True,False,False,dataengineering,t5_36en4,46070,public,self,Convincing for Apache Stack,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r72c1m/convincing_for_apache_stack/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,amin1596,transparent,,[],fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b,Data Engineer,dark,text,t2_bh0jt48c,False,False,False,[],False,False,1638431714,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r72bne/databricks_developer_essentials_capstone/,{},r72bne,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/r72bne/databricks_developer_essentials_capstone/,False,,,6,1638431726,1,Anyone have some experience in getting through the 6 exercises in this Capstone? There seems to be some nuances and edge cases that cannot be easily googled. It also seems more difficult than a real exam. Any tips on how to get proficient enough with PySpark to get through these with ease?,True,False,False,dataengineering,t5_36en4,46070,public,self,Databricks Developer Essentials Capstone,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r72bne/databricks_developer_essentials_capstone/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Ok-Message1053,,,[],,,,text,t2_a4jh6m70,False,False,False,[],False,False,1638425342,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r70m6i/puppeteer_and_bot_blocking/,{},r70m6i,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/r70m6i/puppeteer_and_bot_blocking/,False,,,6,1638425354,1,"I have a script using puppeteer that works locally on my mac, however the same script doesn't work on my ubuntu VM. I'm running the browser in non-headless mode and can see that when it reaches a specific page, it just doesn't load at all. Has anyone ever encountered something like this? Does it seem logical that the site might have a bot that is blocking this on linux but not on mac? Any other reasons that this might be happening? Thanks",True,False,False,dataengineering,t5_36en4,46070,public,self,puppeteer and bot blocking,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r70m6i/puppeteer_and_bot_blocking/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,borjavb,,,[],,,,text,t2_90dc4r5o,False,False,False,[],False,False,1638396429,medium.com,https://www.reddit.com/r/dataengineering/comments/r6qm70/column_data_lineage_in_bigquery_with_zetasql_and/,{},r6qm70,False,False,False,False,False,False,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r6qm70/column_data_lineage_in_bigquery_with_zetasql_and/,False,link,"{'enabled': False, 'images': [{'id': 'sNKrflqkLOUhkpEuR7fiqYT0tFmDEnSCqhmM-ZRggOo', 'resolutions': [{'height': 68, 'url': 'https://external-preview.redd.it/QchdW46i64S1ilcIa00mveUm8i0zQrFog7E2KJVIRdM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4161e2c12e4f5330658335bf7253ada22bd9d875', 'width': 108}, {'height': 137, 'url': 'https://external-preview.redd.it/QchdW46i64S1ilcIa00mveUm8i0zQrFog7E2KJVIRdM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ef932e738f603295131842ab423849678d5899f0', 'width': 216}, {'height': 203, 'url': 'https://external-preview.redd.it/QchdW46i64S1ilcIa00mveUm8i0zQrFog7E2KJVIRdM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=869e86ed0fabf5d7c06a2e12de76a783243132e9', 'width': 320}, {'height': 406, 'url': 'https://external-preview.redd.it/QchdW46i64S1ilcIa00mveUm8i0zQrFog7E2KJVIRdM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=42bf152bfe2ea63dae535ffb87bcff429492a959', 'width': 640}, {'height': 610, 'url': 'https://external-preview.redd.it/QchdW46i64S1ilcIa00mveUm8i0zQrFog7E2KJVIRdM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=609594c1eb1f7df4feed81240baac99715a34b57', 'width': 960}, {'height': 686, 'url': 'https://external-preview.redd.it/QchdW46i64S1ilcIa00mveUm8i0zQrFog7E2KJVIRdM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=af078f334f358f8caa0788bc6667bb01e40829f0', 'width': 1080}], 'source': {'height': 763, 'url': 'https://external-preview.redd.it/QchdW46i64S1ilcIa00mveUm8i0zQrFog7E2KJVIRdM.jpg?auto=webp&amp;s=8a6e0007fa95ddfc18c9279d4dd846d13bbb61aa', 'width': 1200}, 'variants': {}}]}",6,1638396440,1,,True,False,False,dataengineering,t5_36en4,46049,public,https://b.thumbs.redditmedia.com/jE2d-G68me1nxKXnilNLvczNZulgfni9k8FlsuJTAiE.jpg,Column data lineage in BigQuery with ZetaSQL and dbt,0,[],1.0,https://medium.com/data-monzo/mapping-our-data-journey-with-column-lineage-56209c00606d,all_ads,6,,,automod_filtered,,,89.0,140.0,https://medium.com/data-monzo/mapping-our-data-journey-with-column-lineage-56209c00606d,,,,,,,,,,
[],False,Reasonable-Plan4470,,,[],,,,text,t2_a5lzl1a4,False,False,False,[],False,False,1638395888,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r6qduv/low_code_vs_sql_what_transforms_data_faster/,{},r6qduv,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,5,0,False,all_ads,/r/dataengineering/comments/r6qduv/low_code_vs_sql_what_transforms_data_faster/,False,self,"{'enabled': False, 'images': [{'id': 'X57brGTs1Zoj4Qn3UWVtE1dl9ujNRUXGOH_kInik5XQ', 'resolutions': [{'height': 57, 'url': 'https://external-preview.redd.it/kvdHWYqtsr3j16EnPTrFe2kGNqlEYS1ABjvMwOwl0ck.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7f4d8d7ab84216d6ca3849317578932461a4b6af', 'width': 108}, {'height': 115, 'url': 'https://external-preview.redd.it/kvdHWYqtsr3j16EnPTrFe2kGNqlEYS1ABjvMwOwl0ck.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=48dbf4830ce6e60aa9e0776e933d984eaf4d7524', 'width': 216}, {'height': 171, 'url': 'https://external-preview.redd.it/kvdHWYqtsr3j16EnPTrFe2kGNqlEYS1ABjvMwOwl0ck.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cc088fc8088698bce07af95230b98648e8781b64', 'width': 320}], 'source': {'height': 300, 'url': 'https://external-preview.redd.it/kvdHWYqtsr3j16EnPTrFe2kGNqlEYS1ABjvMwOwl0ck.jpg?auto=webp&amp;s=769bc2deb405ef7382c2d3eecc11df54febad0a9', 'width': 560}, 'variants': {}}]}",6,1638395899,1,Two data engineers are racing to get the same data model done before the end of the day: one using low code the other building it in SQL. Guess who makes it to happy hour?  [😂](https://emojipedia.org/face-with-tears-of-joy/)[https://youtu.be/UcXS54ZE2co](https://youtu.be/UcXS54ZE2co),True,False,False,dataengineering,t5_36en4,46049,public,self,Low code vs. SQL: What transforms data faster?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r6qduv/low_code_vs_sql_what_transforms_data_faster/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,kaingee,,,[],,,,text,t2_fxw7ag5g,False,False,False,[],False,False,1638394911,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r6q0gu/mac_or_pc/,{},r6q0gu,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,14,0,False,all_ads,/r/dataengineering/comments/r6q0gu/mac_or_pc/,False,,,6,1638394922,1,"Even though Mac isnt a requirement a lot of engineers use it. So Im wondering how feasible is it in your work place if everyone is not on the same system. 

[View Poll](https://www.reddit.com/poll/r6q0gu)",False,False,False,dataengineering,t5_36en4,46048,public,self,Mac or PC?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r6q0gu/mac_or_pc/,all_ads,6,,,,,,,,,,,,,"{'is_prediction': False, 'options': [{'id': '12201678', 'text': 'Mac'}, {'id': '12201679', 'text': 'PC'}], 'prediction_status': None, 'resolved_option_id': None, 'total_stake_amount': None, 'total_vote_count': 0, 'tournament_id': None, 'user_selection': None, 'user_won_amount': None, 'vote_updates_remained': None, 'voting_end_timestamp': 1638481311557}",,,,,
[],False,SeattleDataGuy,,,[],,,,text,t2_b003dzgv,False,False,False,[],False,False,1638391824,youtube.com,https://www.reddit.com/r/dataengineering/comments/r6otrn/why_you_should_become_a_data_engineer_and_not_a/,{},r6otrn,False,True,False,False,False,True,False,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,29,0,False,all_ads,/r/dataengineering/comments/r6otrn/why_you_should_become_a_data_engineer_and_not_a/,False,rich:video,"{'enabled': False, 'images': [{'id': 'Fowls-wGGBlQBx4esJHRbu4kKbLUYQd-moLwtiFZDTM', 'resolutions': [{'height': 81, 'url': 'https://external-preview.redd.it/ym_bT1pSf36TB9p6uMrXCEEHjJaSZgjRhleiDK4iQ90.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d052168098728e82b462f821c725ec89c7486a92', 'width': 108}, {'height': 162, 'url': 'https://external-preview.redd.it/ym_bT1pSf36TB9p6uMrXCEEHjJaSZgjRhleiDK4iQ90.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=92678ec2e3a849eb49fee389987d2bac2bbe9fb1', 'width': 216}, {'height': 240, 'url': 'https://external-preview.redd.it/ym_bT1pSf36TB9p6uMrXCEEHjJaSZgjRhleiDK4iQ90.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d2cb199aa0f48cf3ae57d40e3d27813e895a4ef9', 'width': 320}], 'source': {'height': 360, 'url': 'https://external-preview.redd.it/ym_bT1pSf36TB9p6uMrXCEEHjJaSZgjRhleiDK4iQ90.jpg?auto=webp&amp;s=252821fba21ce8d48fd8d46b417074d443690818', 'width': 480}, 'variants': {}}]}",6,1638391836,1,,True,False,False,dataengineering,t5_36en4,46045,public,https://b.thumbs.redditmedia.com/cUZAsxqF9PTxS2GhRHe4iU8j4wliBCQTQuBIoqRZaOQ.jpg,Why You Should Become A Data Engineer And Not A Data Scientist - Picking The Right Data Career,0,[],1.0,https://www.youtube.com/watch?v=KgdWvtppH50,all_ads,6,"{'oembed': {'author_name': 'Seattle Data Guy', 'author_url': 'https://www.youtube.com/c/SeattleDataGuy', 'height': 200, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/KgdWvtppH50?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/KgdWvtppH50/hqdefault.jpg', 'thumbnail_width': 480, 'title': 'Why You Should Become A Data Engineer And Not A Data Scientist - Picking The Right Data Career', 'type': 'video', 'version': '1.0', 'width': 356}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/KgdWvtppH50?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'scrolling': False, 'width': 356}",,"{'oembed': {'author_name': 'Seattle Data Guy', 'author_url': 'https://www.youtube.com/c/SeattleDataGuy', 'height': 200, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/KgdWvtppH50?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/KgdWvtppH50/hqdefault.jpg', 'thumbnail_width': 480, 'title': 'Why You Should Become A Data Engineer And Not A Data Scientist - Picking The Right Data Career', 'type': 'video', 'version': '1.0', 'width': 356}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/KgdWvtppH50?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/r6otrn', 'scrolling': False, 'width': 356}",105.0,140.0,https://www.youtube.com/watch?v=KgdWvtppH50,,,,,,,,,,
[],False,theant97,,,[],,,,text,t2_4xo4smxw,False,False,False,[],False,False,1638384621,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r6m0ia/how_do_you_load_large_datasets_to_redshift/,{},r6m0ia,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,5,0,False,all_ads,/r/dataengineering/comments/r6m0ia/how_do_you_load_large_datasets_to_redshift/,False,,,6,1638384632,1,I have around 500mb parquet or orc files in the s3 location around 3000 files . How can I plan to load the history data to redshift. For 100 mb file using copy command is taking forever.,True,False,False,dataengineering,t5_36en4,46036,public,self,How do you load large datasets to redshift .,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r6m0ia/how_do_you_load_large_datasets_to_redshift/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,king_booker,,,[],,,,text,t2_ryny2,False,False,False,[],False,False,1638383284,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r6lhgq/late_data_arrival/,{},r6lhgq,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,7,0,False,all_ads,/r/dataengineering/comments/r6lhgq/late_data_arrival/,False,,,6,1638383296,1,"I get asked this very often in interviews. How do you handle late arrival of data? Eg if a batch job has already ran and you have late data arrival for that job, how do you handle scenarios like this? 

How do you guys handle this in your products?",True,False,False,dataengineering,t5_36en4,46033,public,self,Late data arrival,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r6lhgq/late_data_arrival/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,parvister,,,[],,,,text,t2_glb4soe8,False,False,False,[],False,False,1638381483,reddit.com,https://www.reddit.com/r/dataengineering/comments/r6krnt/saw_this_on_the_rfunny_which_makes_us_data/,{},r6krnt,False,True,False,False,False,True,False,False,#ff66ac,[],dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1,Meme,light,text,False,False,True,4,0,False,all_ads,/r/dataengineering/comments/r6krnt/saw_this_on_the_rfunny_which_makes_us_data/,False,link,"{'enabled': False, 'images': [{'id': 'zBZ2xoXHDFqq3pf697b4MFku8LCMRAK7hZc8GPfLSP4', 'resolutions': [{'height': 135, 'url': 'https://external-preview.redd.it/eN8PYJZAEL7r1yxB1zuo4-9ZM9wpComKRBKat5RO8qE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a7a27561cd6c9d662f35a1bc6265b255b220ccef', 'width': 108}, {'height': 271, 'url': 'https://external-preview.redd.it/eN8PYJZAEL7r1yxB1zuo4-9ZM9wpComKRBKat5RO8qE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9c401ab6f38bc658cc3b897726c7987ce5f450a6', 'width': 216}, {'height': 402, 'url': 'https://external-preview.redd.it/eN8PYJZAEL7r1yxB1zuo4-9ZM9wpComKRBKat5RO8qE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2cd9669a73bf2d67e0f61611d78fb244b9f44413', 'width': 320}], 'source': {'height': 578, 'url': 'https://external-preview.redd.it/eN8PYJZAEL7r1yxB1zuo4-9ZM9wpComKRBKat5RO8qE.jpg?auto=webp&amp;s=ddc3d735bbf99ea8f784febf3464cf2924f76224', 'width': 460}, 'variants': {}}]}",6,1638381494,1,,True,False,False,dataengineering,t5_36en4,46034,public,https://b.thumbs.redditmedia.com/7tPhS6ul-tosrXlOllC9v-j7I-NHrlxRsl-no5ylseg.jpg,Saw this on the r/funny which makes us data engineering get a good laugh,0,[],1.0,https://www.reddit.com/r/terriblefacebookmemes/comments/qloe4k/so_funny/?utm_source=share&amp;utm_medium=ios_app&amp;utm_name=iossmf,all_ads,6,,,,,,140.0,140.0,https://www.reddit.com/r/terriblefacebookmemes/comments/qloe4k/so_funny/?utm_source=share&amp;utm_medium=ios_app&amp;utm_name=iossmf,,,,,,,,,,
[],False,thearsenalman97,,,[],,,,text,t2_6z54fbzr,False,False,False,[],False,False,1638380925,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r6kjs5/update_got_my_first_interview/,{},r6kjs5,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/r6kjs5/update_got_my_first_interview/,False,,,6,1638380936,1,"So my previous [post](https://www.reddit.com/r/dataengineering/comments/qjrajd/need_help_with_resume/)  on here was about how I could tailor my resume to get interviews (wasn't getting any). After incorporating all the feedback given in the comments, I finally got one (not that big of deal maybe, but is a huge break for me). 

It's my first interview for a big data engineering role (I'm current in an SWE role). The first round is a technical round for about an hour. Can someone let me know what kind of questions I can expect or topics I need to brush up on. 

This is the job description that I was sent : 

***Hands on job experience in Hadoop technologies (Pig/Spark).***

***Understanding of traditional ETL tools, RDBMS, SQL***

***Experience on Big Data and NoSQL technologies***

***Proficiency in Unix/Linux as well as scripting languages (Shell/Perl/Python).***

***Building non-relational data models on NoSQL data stores***

***Hands-on MapReduce coding, including Java, Python, Pig programming***

***Hands on experience on Cloud Technologies (AWS/GCP)***

***Good to have accreditations.***

***Good understanding on Agile way of working.***

&amp;#x200B;

Thanks in advance !",True,False,False,dataengineering,t5_36en4,46033,public,self,UPDATE : Got my first Interview,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r6kjs5/update_got_my_first_interview/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,AutoModerator,,,[],,,,text,t2_6l4z3,False,False,True,[],False,False,1638378025,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r6jfnm/quarterly_salary_discussion/,{},r6jfnm,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,109,0,False,all_ads,/r/dataengineering/comments/r6jfnm/quarterly_salary_discussion/,False,,,6,1638378038,1,"This is a recurring thread that happens quarterly and was created to help increase transparency around salary and compensation for Data Engineering. Please comment below and include the following:

1. Current title

2. Years of experience (YOE)

3. Location

4. Base salary &amp; currency (dollars, euro, pesos, etc.)

5. Bonuses/Equity (optional)

6. Industry (optional)",False,False,True,dataengineering,t5_36en4,46032,public,self,Quarterly Salary Discussion,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r6jfnm/quarterly_salary_discussion/,all_ads,6,,,,,,,,,,,,,,,,,,"[{'author_id': 't2_2tv9i42n', 'author_name': 'theporterhaus', 'collection_id': 'ef3eb514-328d-4549-a705-94c26963d79b', 'created_at_utc': 1621559056.076, 'description': '', 'display_layout': None, 'last_update_utc': 1638378025.773, 'link_ids': ['t3_npxcqc', 't3_pfwuyg', 't3_r6jfnm'], 'permalink': 'https://www.reddit.com/r/dataengineering/collection/ef3eb514-328d-4549-a705-94c26963d79b', 'subreddit_id': 't5_36en4', 'title': 'Data Engineering Salaries'}]"
[],False,EntropyRX,,,[],,,,text,t2_3fyu9j5o,False,False,False,[],False,False,1638377014,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r6j1u7/vertexai_to_build_an_api/,{},r6j1u7,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r6j1u7/vertexai_to_build_an_api/,False,,,6,1638377025,1,"I would like to hear some opinions from anyone who worked with VertexAI (on gcp).

Does it help to manage the end to end ML lifecycle for creating RESTFUL APIs?  Or it's just about the model development itself and offers no contribution to the serving phase (assuming I want to implement the ML into a web app and serve prediction with API calls)",True,False,False,dataengineering,t5_36en4,46031,public,self,VertexAI to build an API,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r6j1u7/vertexai_to_build_an_api/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,segtekdev,,,[],,,,text,t2_cpvcb0db,False,False,False,[],False,False,1638376604,blog.gitguardian.com,https://www.reddit.com/r/dataengineering/comments/r6iweu/how_to_optimizing_filtering_processes/,{},r6iweu,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r6iweu/how_to_optimizing_filtering_processes/,False,link,"{'enabled': False, 'images': [{'id': 'o4zgI_p-TMwc-L1GdxnN1xoIWBz5AvXpqXZsSi0fxQ4', 'resolutions': [{'height': 63, 'url': 'https://external-preview.redd.it/PHZ0T2a4hbdKSFqWfaQEcreLVfalSq0KORyHGoS1jZM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c442ecc4368fceb4989cb5581b35ca81be49d30b', 'width': 108}, {'height': 126, 'url': 'https://external-preview.redd.it/PHZ0T2a4hbdKSFqWfaQEcreLVfalSq0KORyHGoS1jZM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=44e4f7ec4fcd635e6d6ef3ef80833bea39c78940', 'width': 216}, {'height': 187, 'url': 'https://external-preview.redd.it/PHZ0T2a4hbdKSFqWfaQEcreLVfalSq0KORyHGoS1jZM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d1262ebadfc9501348c326fe8a43c4afe1079d46', 'width': 320}, {'height': 374, 'url': 'https://external-preview.redd.it/PHZ0T2a4hbdKSFqWfaQEcreLVfalSq0KORyHGoS1jZM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=97b73095d492f9b7c3215d4678c1cc6ba5b7d070', 'width': 640}, {'height': 561, 'url': 'https://external-preview.redd.it/PHZ0T2a4hbdKSFqWfaQEcreLVfalSq0KORyHGoS1jZM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7b8f1579b0c7a6e39be9a1e6533614a55737f85b', 'width': 960}, {'height': 631, 'url': 'https://external-preview.redd.it/PHZ0T2a4hbdKSFqWfaQEcreLVfalSq0KORyHGoS1jZM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0c7fbb9ab71d0f5e03703d7570dbba1676f7bdac', 'width': 1080}], 'source': {'height': 1380, 'url': 'https://external-preview.redd.it/PHZ0T2a4hbdKSFqWfaQEcreLVfalSq0KORyHGoS1jZM.jpg?auto=webp&amp;s=c3af6a2fadc591d1e9163d129a038a006e7d5554', 'width': 2360}, 'variants': {}}]}",6,1638376616,1,,True,False,False,dataengineering,t5_36en4,46030,public,https://b.thumbs.redditmedia.com/ZF3E_IqIJPIDJUmICJ3c8t-z6MYsvC5coUEhk-D7TMY.jpg,How to: Optimizing Filtering Processes,0,[],1.0,https://blog.gitguardian.com/fast-scans-return-earlier/,all_ads,6,,,,,,81.0,140.0,https://blog.gitguardian.com/fast-scans-return-earlier/,,,,,,,,,,
[],False,ConstructionBrief965,,,[],,,,text,t2_cvhxmfar,False,False,False,[],False,False,1638375793,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r6il4m/utilizing_dbt_and_monte_carlo_together/,{},r6il4m,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r6il4m/utilizing_dbt_and_monte_carlo_together/,False,self,"{'enabled': False, 'images': [{'id': 'qcpLUdFvzifDsOzWAmnBjUs_EyZYaZ_nsixfPKSk1RI', 'resolutions': [{'height': 108, 'url': 'https://external-preview.redd.it/0F88rbQZqwb1YdVUI9kQsHd2aXNd25-pgK122MUVbUc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a6172f49818d16845a3f4795cd9ed132f6744a11', 'width': 108}, {'height': 216, 'url': 'https://external-preview.redd.it/0F88rbQZqwb1YdVUI9kQsHd2aXNd25-pgK122MUVbUc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=63781ce20732aa4de78a4f2887a16bc29d67837a', 'width': 216}, {'height': 320, 'url': 'https://external-preview.redd.it/0F88rbQZqwb1YdVUI9kQsHd2aXNd25-pgK122MUVbUc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=dde6a3f67459c78ce3b6f3fc63c3275895c39bd7', 'width': 320}, {'height': 640, 'url': 'https://external-preview.redd.it/0F88rbQZqwb1YdVUI9kQsHd2aXNd25-pgK122MUVbUc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=52c7500c628d8fe1daee9967d8ca442890670d46', 'width': 640}, {'height': 960, 'url': 'https://external-preview.redd.it/0F88rbQZqwb1YdVUI9kQsHd2aXNd25-pgK122MUVbUc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e2b55fec512ff2ae8a50353aa68f229d3de5d3f7', 'width': 960}, {'height': 1080, 'url': 'https://external-preview.redd.it/0F88rbQZqwb1YdVUI9kQsHd2aXNd25-pgK122MUVbUc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4e84bf40dc3e89a9b4ac8bbfdd0885927f82828d', 'width': 1080}], 'source': {'height': 4096, 'url': 'https://external-preview.redd.it/0F88rbQZqwb1YdVUI9kQsHd2aXNd25-pgK122MUVbUc.jpg?auto=webp&amp;s=c8badc25ab569c2e4f80bd5a5845d31e3b38b69b', 'width': 4096}, 'variants': {}}]}",6,1638375805,1,Interesting read from the Autotrader UK team [https://engineering.autotrader.co.uk/2021/11/30/how-autotrader-ensures-end-to-end-data-trust-at-scale.html](https://engineering.autotrader.co.uk/2021/11/30/how-autotrader-ensures-end-to-end-data-trust-at-scale.html),False,False,False,dataengineering,t5_36en4,46030,public,self,Utilizing dbt and Monte Carlo together,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r6il4m/utilizing_dbt_and_monte_carlo_together/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,DataDudeDom,,,[],,,,text,t2_c7cky4a8,False,False,False,[],False,False,1638374446,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r6i287/data_modeling_versioning_for_video_image_data/,{},r6i287,False,False,False,False,False,False,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r6i287/data_modeling_versioning_for_video_image_data/,False,,,6,1638374456,1,[removed],True,False,False,dataengineering,t5_36en4,46029,public,self,Data Modeling &amp; Versioning for Video &amp; Image Data,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r6i287/data_modeling_versioning_for_video_image_data/,all_ads,6,,,automod_filtered,,,,,,,,,,,,,,,
[],False,humblesquirrelking,,,[],,,,text,t2_9exoicxf,False,False,False,[],False,False,1638366609,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r6f6d8/how_do_you_built_real_time_pipeline/,{},r6f6d8,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,16,0,False,all_ads,/r/dataengineering/comments/r6f6d8/how_do_you_built_real_time_pipeline/,False,,,6,1638366621,1,"Guys, I've been building data platform. But I really don't understand know how do I building real time pipelines

So basically I run jobs which picks data from sources which picks data from last day till current time. But how do I buit real time data platform..? 

Can anybody pls comment or shade some light. It'd be great if share some resources. I really want to build real time streaming data pipelines using Kafka etc",True,False,False,dataengineering,t5_36en4,46022,public,self,How do you built real time pipeline,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r6f6d8/how_do_you_built_real_time_pipeline/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,rafinirovannoe,,,[],,,,text,t2_1nv4lffu,False,False,False,[],False,False,1638366445,medium.com,https://www.reddit.com/r/dataengineering/comments/r6f4g7/building_data_platform_in_pyspark_part_1_python/,{},r6f4g7,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r6f4g7/building_data_platform_in_pyspark_part_1_python/,False,link,"{'enabled': False, 'images': [{'id': 'vowKXZpQRGMewRJ0HucPK6EeaUzkIh9NoDZn0ku_pYc', 'resolutions': [{'height': 72, 'url': 'https://external-preview.redd.it/ZMD7VWDS51pU6zzAapjUViPjgTpZteWAntnoWNNfDfU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0fc5f5d6c0408ed0f3024ee8f82f497f814074c2', 'width': 108}, {'height': 144, 'url': 'https://external-preview.redd.it/ZMD7VWDS51pU6zzAapjUViPjgTpZteWAntnoWNNfDfU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=56d8ac83e985d9a3c23cf013617ddbeea90a52a0', 'width': 216}, {'height': 213, 'url': 'https://external-preview.redd.it/ZMD7VWDS51pU6zzAapjUViPjgTpZteWAntnoWNNfDfU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=af63b5cbbd88b0d7a92d346efc8c42a7285ee85a', 'width': 320}, {'height': 426, 'url': 'https://external-preview.redd.it/ZMD7VWDS51pU6zzAapjUViPjgTpZteWAntnoWNNfDfU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d1e1c4041bd35738e02582b87434f5c9a360a53c', 'width': 640}, {'height': 640, 'url': 'https://external-preview.redd.it/ZMD7VWDS51pU6zzAapjUViPjgTpZteWAntnoWNNfDfU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=385ef2fd0bbb443ee8abc7dab6d818e3644b65ef', 'width': 960}, {'height': 720, 'url': 'https://external-preview.redd.it/ZMD7VWDS51pU6zzAapjUViPjgTpZteWAntnoWNNfDfU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=eb4f25c55d5d1b42b6cc3ba27aca4f7537e132ff', 'width': 1080}], 'source': {'height': 800, 'url': 'https://external-preview.redd.it/ZMD7VWDS51pU6zzAapjUViPjgTpZteWAntnoWNNfDfU.jpg?auto=webp&amp;s=197079a501841186d7e204a49840a8e4c971195c', 'width': 1200}, 'variants': {}}]}",6,1638366456,1,,True,False,False,dataengineering,t5_36en4,46022,public,https://b.thumbs.redditmedia.com/VmtOKwmDEurTwBL7qyd0n-ceJl7GkadrhZYRUxQCe8I.jpg,Building data platform in PySpark. Part 1 — Python and Scala interop,0,[],1.0,https://medium.com/joom/building-data-platform-in-pyspark-part-1-python-and-scala-interop-c52f96b7dc59,all_ads,6,,,,,,93.0,140.0,https://medium.com/joom/building-data-platform-in-pyspark-part-1-python-and-scala-interop-c52f96b7dc59,,,,,,,,,,
[],False,dataengineerdude,,,[],,,,text,t2_4wrevs7q,False,False,False,[],False,False,1638365595,confessionsofadataguy.com,https://www.reddit.com/r/dataengineering/comments/r6eudm/lessons_learned_from_merge_operations_with/,{},r6eudm,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,11,0,False,all_ads,/r/dataengineering/comments/r6eudm/lessons_learned_from_merge_operations_with/,False,link,"{'enabled': False, 'images': [{'id': 'ZHUtImemtlpHZaJI8Lxk8LlWjlqeQe_ryNzu1mW9gRU', 'resolutions': [{'height': 72, 'url': 'https://external-preview.redd.it/mlbJzK5A3HcEHQoQbItzA0hNg0gyff_50a96-zyCIHM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e3fe6f8b1e0e7971b094923672f23709265b9a23', 'width': 108}, {'height': 144, 'url': 'https://external-preview.redd.it/mlbJzK5A3HcEHQoQbItzA0hNg0gyff_50a96-zyCIHM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fc6f655c02df16d559a76486ad2952d03baf56e5', 'width': 216}, {'height': 213, 'url': 'https://external-preview.redd.it/mlbJzK5A3HcEHQoQbItzA0hNg0gyff_50a96-zyCIHM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d407a822e4621c0bce405de08a52fc6194dd55fe', 'width': 320}, {'height': 426, 'url': 'https://external-preview.redd.it/mlbJzK5A3HcEHQoQbItzA0hNg0gyff_50a96-zyCIHM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=da96611b94ae5bdbd4e506d0aa4fe054c0c9fb28', 'width': 640}, {'height': 640, 'url': 'https://external-preview.redd.it/mlbJzK5A3HcEHQoQbItzA0hNg0gyff_50a96-zyCIHM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ed46b71d7a3801825a9f8336c0eef25e02e34e9e', 'width': 960}], 'source': {'height': 687, 'url': 'https://external-preview.redd.it/mlbJzK5A3HcEHQoQbItzA0hNg0gyff_50a96-zyCIHM.jpg?auto=webp&amp;s=17165a6c7e76a580dfe4d18cd5a90592810df86f', 'width': 1030}, 'variants': {}}]}",6,1638365606,1,,True,False,False,dataengineering,t5_36en4,46022,public,https://b.thumbs.redditmedia.com/Pm1yRvhGWv68o8jiPQCj-EdyWsB13KkEAxnfGE4cXUc.jpg,Lessons Learned from MERGE operations with Billions of Records on Databricks Spark,0,[],1.0,https://www.confessionsofadataguy.com/lessons-learned-from-merge-operations-with-billions-of-records-on-databricks-spark/,all_ads,6,,,,,,93.0,140.0,https://www.confessionsofadataguy.com/lessons-learned-from-merge-operations-with-billions-of-records-on-databricks-spark/,,,,,,,,,,
[],False,Raph_Bellahs,,,[],,,,text,t2_8azi48xg,False,False,False,[],False,False,1638360264,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r6d99x/de_intern_interview_at_facebook/,{},r6d99x,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,5,0,False,all_ads,/r/dataengineering/comments/r6d99x/de_intern_interview_at_facebook/,False,,,6,1638360274,1,"Hi Everyone, I just passed my first round of interview in Meta , it was very basics SQL and Python questions ( LC Easy ) and I have my on site Virtual Interview in the next few days , wanted to know if someone has been there and know how to prepare the ""motivation interview"" , 30 min talking about why do I want to be there

Any extra help or tips will be received :)

Also , if there is some DE in  big company  willing to give me a little time to answer few of my questions on video call will be great ! ( in exchange of money of course , I know you are all busy ... )

Thanks !",True,False,False,dataengineering,t5_36en4,46019,public,self,DE Intern Interview at Facebook,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r6d99x/de_intern_interview_at_facebook/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,BlancBryn,,,[],,,,text,t2_1i178g26,False,False,False,[],False,False,1638354144,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r6bpjr/how_to_improve_etl_tech_stack_for_robust_api/,{},r6bpjr,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,21,0,False,all_ads,/r/dataengineering/comments/r6bpjr/how_to_improve_etl_tech_stack_for_robust_api/,False,,,6,1638354162,1,"Hey, I am relatively new to the data engineering environment in a smaller company. Until now we mainly use Pentaho for ETL pipelines and SQL procedures to load data from source systems (SAP, SQL Server, REST APIs) into the data warehouse. Die to personal changes and new hiring positions I have the opportunity to take our infrastructure to a new level, e.g.  we hardly use proper verisoning and often code is maintained twice and three times. We have a bad scheduling and monitoring for pipelines. I was thinking of Apache Airflow for orchestrating data pipelines and a gut repo to manage dag files and python/sql files seperdtly. I am still missing a robust solution to access a REST API to our CRM system several times a day and store the raw between and finally load the result into the data warehouse + performant historization. Do you guys also have the same issues? What does your tech stack look like to manage robust API calls including pagination and reliably get the results from the JSON into a SQL database (or parguet file,...)?",True,False,False,dataengineering,t5_36en4,46013,public,self,"How to improve ETL tech stack for robust API calls, historization, orchestration and versioning?",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r6bpjr/how_to_improve_etl_tech_stack_for_robust_api/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,HovercraftGold980,,,[],,,,text,t2_a6gn9tzs,False,False,False,[],False,False,1638330005,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r65gdo/anyone_here_work_or_apply_to_capital_one_de_roles/,{},r65gdo,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,3,0,False,all_ads,/r/dataengineering/comments/r65gdo/anyone_here_work_or_apply_to_capital_one_de_roles/,False,,,6,1638330015,1,I hear that have strong DE culture. Any tips to prepare for their first online Technical assessment?,True,False,False,dataengineering,t5_36en4,45989,public,self,Anyone here work or apply to Capital One DE roles ?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r65gdo/anyone_here_work_or_apply_to_capital_one_de_roles/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,TestPleaseIgnore69,,,[],,,,text,t2_4hod7s2m,False,False,False,[],False,False,1638319204,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r61zm1/what_to_prepare_for_on_de_interview/,{},r61zm1,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,3,0,False,all_ads,/r/dataengineering/comments/r61zm1/what_to_prepare_for_on_de_interview/,False,,,6,1638319217,1,"I really, really want to become a data engineer and have about 1.5 YOE as a Data Analyst. I talked with the hiring manager, took and passed the TA (SQL/Python), and am on my way to speaking with 4 individuals over a two-hour period. 

So far, so good. But I am wondering about other concepts and ideas I should prepare for with this next round of interviews? They know I don't have extensive DE experience but I am very eager to learn (they will be teaching us to help with the workload).

So far I have been google DE interview questions and analyzing every detail of the JD to get familiar with the tech stack. Any other aspects and ways I can better prepare for this? Thanks!",True,False,False,dataengineering,t5_36en4,45975,public,self,What to prepare for on DE Interview?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r61zm1/what_to_prepare_for_on_de_interview/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,noNSFWcontent,,,[],,,,text,t2_yagno,False,False,False,[],False,False,1638317814,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r61ir5/how_does_your_teamcompany_manage_documentation/,{},r61ir5,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,7,0,False,all_ads,/r/dataengineering/comments/r61ir5/how_does_your_teamcompany_manage_documentation/,False,,,6,1638317825,1,"We are part of a rapidly growing company and our team is growing too. In the process, we are also building a lot of tools to better build pipelines and other things. 

But we're lagging in how to document all the things that are being implemented or the changes that are brought in. There was no need earlier as the team just consisted of two or three people. But it's already a 6 people team now and we want to be future proof. 

How do you guys document changes or just document tools that are build in house?",True,False,False,dataengineering,t5_36en4,45974,public,self,How does your team/company manage documentation?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r61ir5/how_does_your_teamcompany_manage_documentation/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,kkjeb,,,[],,,,text,t2_4uhbgslt,False,False,False,[],False,False,1638312857,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r5zsq0/to_all_of_you_data_engineers/,{},r5zsq0,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,53,0,False,all_ads,/r/dataengineering/comments/r5zsq0/to_all_of_you_data_engineers/,False,,,6,1638312867,1,"A few questions for you.

Can you tell me about your day to day?
How do you use python in your jobs?
Who in the business do you typically work with?
Which technology do you recommend to someone interested in this line of work?",True,False,False,dataengineering,t5_36en4,45971,public,self,To all of you data engineers,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r5zsq0/to_all_of_you_data_engineers/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Apart_Climate_8516,,,[],,,,text,t2_7rkqnivf,False,False,False,[],False,False,1638305041,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r5wxs5/cs6400_database_systems_concepts_design_course/,{},r5wxs5,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,9,0,False,all_ads,/r/dataengineering/comments/r5wxs5/cs6400_database_systems_concepts_design_course/,False,,,6,1638305053,1,"looking for someone to do the course Database systems concepts and design by georgia tech on udacity  together .

it is totally free .

we can keep each other accountable and work on the project for the course.

###",True,False,False,dataengineering,t5_36en4,45955,public,self,CS6400 Database Systems Concepts &amp; Design COURSE BUDDY udacity,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r5wxs5/cs6400_database_systems_concepts_design_course/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Meriu,,,[],,,,text,t2_dbb2y,False,False,False,[],False,False,1638302728,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r5w2e3/supplying_application_back_with_data_lake_data/,{},r5w2e3,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,4,0,False,all_ads,/r/dataengineering/comments/r5w2e3/supplying_application_back_with_data_lake_data/,False,,,6,1638302739,1,"Hello Everyone! 
I’ve been long time lurker dealing with day to day data engineering tasks, but today out product owner asked for one I’m not sure how to beat. 

We’re gathering analytics data from various systems via API and calculating KPIs using Databricks on top of Azure Data Lake. Now we would want to feed the results back to newly created application. We haven’t been creating data warehouse ad delta lake has been sufficient for us, thus simple ETL process is a no-go, unfortunately. Team of backend devs is experienced in using rest api, and this is their protocol of choice. I’m wondering how would you approach it, especially do you see any possibility to create restful api to serve those data or would you recommend different way to serve those data. 

Thanks in advance!",True,False,False,dataengineering,t5_36en4,45951,public,self,Supplying application back with data lake data,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r5w2e3/supplying_application_back_with_data_lake_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,SatRipper,,,[],,,,text,t2_gi9h13c,False,False,False,[],False,False,1638296414,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r5tnm9/data_engineer_consulting/,{},r5tnm9,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,18,0,False,all_ads,/r/dataengineering/comments/r5tnm9/data_engineer_consulting/,False,,,6,1638296425,1,"Is consulting a good way to start a data engineering career right out of college? For example, would a job at a company such as Accenture or Capgemini set you up for good jobs 1 or 2 years down the line? Additionally, would this experience be enough to break into tech in 1-2 years time?",True,False,False,dataengineering,t5_36en4,45948,public,self,Data Engineer Consulting?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r5tnm9/data_engineer_consulting/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Agreeable-Flow5658,,,[],,,,text,t2_7uftpbv9,False,False,False,[],False,False,1638295409,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r5t9n3/data_pipeline_automation_on_azure_synapse/,{},r5t9n3,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,10,0,False,all_ads,/r/dataengineering/comments/r5t9n3/data_pipeline_automation_on_azure_synapse/,False,,,6,1638295420,1,"Hello everyone. I am new to azure cloud. Am trying to automate a pipeline. Basically, I want to trigger a notebook Everytime an excel file is added to my Data lake storage. The notebook runs fine but can not seem to get the name of the file that triggered the pipeline. I want to be able to get the file name, load it into a pandas or spark dataframe, process the file data and insert it into an Azure SQL database. I have figured out all the other parts. Just need to get the file name. Any help is highly appreciated",True,False,False,dataengineering,t5_36en4,45945,public,self,Data pipeline automation on Azure Synapse,0,[],0.99,https://www.reddit.com/r/dataengineering/comments/r5t9n3/data_pipeline_automation_on_azure_synapse/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,igorlukanin,,,[],,,,text,t2_6qpanfe,False,False,False,[],False,False,1638295211,cube.dev,https://www.reddit.com/r/dataengineering/comments/r5t6ps/building_a_metrics_dashboard_with_apache_superset/,{},r5t6ps,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r5t6ps/building_a_metrics_dashboard_with_apache_superset/,False,link,"{'enabled': False, 'images': [{'id': 'FDOx1S5nLHR8OZVhFRSq8VMZpo5-QzDgRxbH3oReT_g', 'resolutions': [{'height': 54, 'url': 'https://external-preview.redd.it/kQtDgWZoGTnwbG7ywKasp7d_FyBi0j7zDKkufaeBUvQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ef89f1f824f1dff3bf8def64df6a76206c5d661c', 'width': 108}, {'height': 108, 'url': 'https://external-preview.redd.it/kQtDgWZoGTnwbG7ywKasp7d_FyBi0j7zDKkufaeBUvQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=000dc52d45733ee61dae590c367572fd7bf71aea', 'width': 216}, {'height': 160, 'url': 'https://external-preview.redd.it/kQtDgWZoGTnwbG7ywKasp7d_FyBi0j7zDKkufaeBUvQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4981aabbde0c61aa66519b973539972da0962ec3', 'width': 320}, {'height': 320, 'url': 'https://external-preview.redd.it/kQtDgWZoGTnwbG7ywKasp7d_FyBi0j7zDKkufaeBUvQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=161ef4e9ef7acdfd6dbaec1cd59991a9be3e684b', 'width': 640}, {'height': 480, 'url': 'https://external-preview.redd.it/kQtDgWZoGTnwbG7ywKasp7d_FyBi0j7zDKkufaeBUvQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0cc1930e0555f2d537b490cd48f3ae5421b990ff', 'width': 960}, {'height': 540, 'url': 'https://external-preview.redd.it/kQtDgWZoGTnwbG7ywKasp7d_FyBi0j7zDKkufaeBUvQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=61d25b62fa81986934168a116a6ad88c3507bf01', 'width': 1080}], 'source': {'height': 960, 'url': 'https://external-preview.redd.it/kQtDgWZoGTnwbG7ywKasp7d_FyBi0j7zDKkufaeBUvQ.jpg?auto=webp&amp;s=9f59a139fb0742731bbab379efca88c255707865', 'width': 1920}, 'variants': {'obfuscated': {'resolutions': [{'height': 54, 'url': 'https://external-preview.redd.it/kQtDgWZoGTnwbG7ywKasp7d_FyBi0j7zDKkufaeBUvQ.jpg?width=108&amp;crop=smart&amp;blur=10&amp;format=pjpg&amp;auto=webp&amp;s=e4e8365aa4bf2f9d5d30e1a2384d3333b60f2a76', 'width': 108}, {'height': 108, 'url': 'https://external-preview.redd.it/kQtDgWZoGTnwbG7ywKasp7d_FyBi0j7zDKkufaeBUvQ.jpg?width=216&amp;crop=smart&amp;blur=21&amp;format=pjpg&amp;auto=webp&amp;s=17ebb4a717b6cd2319c070b916786b84f694010a', 'width': 216}, {'height': 160, 'url': 'https://external-preview.redd.it/kQtDgWZoGTnwbG7ywKasp7d_FyBi0j7zDKkufaeBUvQ.jpg?width=320&amp;crop=smart&amp;blur=32&amp;format=pjpg&amp;auto=webp&amp;s=03c0b5b64d3c29223910e309539bd2bb6e428535', 'width': 320}, {'height': 320, 'url': 'https://external-preview.redd.it/kQtDgWZoGTnwbG7ywKasp7d_FyBi0j7zDKkufaeBUvQ.jpg?width=640&amp;crop=smart&amp;blur=40&amp;format=pjpg&amp;auto=webp&amp;s=6472db8ab7c6eb11d164e130b74e6c2293747a24', 'width': 640}, {'height': 480, 'url': 'https://external-preview.redd.it/kQtDgWZoGTnwbG7ywKasp7d_FyBi0j7zDKkufaeBUvQ.jpg?width=960&amp;crop=smart&amp;blur=40&amp;format=pjpg&amp;auto=webp&amp;s=1e3298c5affb42ba549f6fce827c6bb818020f68', 'width': 960}, {'height': 540, 'url': 'https://external-preview.redd.it/kQtDgWZoGTnwbG7ywKasp7d_FyBi0j7zDKkufaeBUvQ.jpg?width=1080&amp;crop=smart&amp;blur=40&amp;format=pjpg&amp;auto=webp&amp;s=5531a314be71ded00a95b5db7a13c2a61e69bccc', 'width': 1080}], 'source': {'height': 960, 'url': 'https://external-preview.redd.it/kQtDgWZoGTnwbG7ywKasp7d_FyBi0j7zDKkufaeBUvQ.jpg?blur=40&amp;format=pjpg&amp;auto=webp&amp;s=c0a92bef0ca3aec5f86e94dc1e8bfb72ee13acf9', 'width': 1920}}}}]}",6,1638295229,1,,True,True,False,dataengineering,t5_36en4,45945,public,spoiler,Building a metrics dashboard with Apache Superset and Cube (spoiler: it will be blazing fast),0,[],1.0,https://cube.dev/blog/building-metrics-dashboard-with-superset/,all_ads,6,,,,,,70.0,140.0,https://cube.dev/blog/building-metrics-dashboard-with-superset/,,,,,,,,,,
[],False,Culpgrant21,,,[],,,,text,t2_1n3qfa0v,False,False,False,[],False,False,1638293172,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r5sdze/how_do_you_handle_deletes_when_doing_an/,{},r5sdze,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,6,0,False,all_ads,/r/dataengineering/comments/r5sdze/how_do_you_handle_deletes_when_doing_an/,False,,,6,1638293199,1,"Hey, I am currently designing an incremental refresh from a source system into our data warehouse using the high watermark technique. Basically, I just get the last modified date from the DW table and then refresh the last 3 days of data.

My one challenge is the source system has deletes that we want to delete out of the DW tables as well. What is the best way to handle this in your experience? 

I could pull all the PKs and delete the ones that are in the DW table but not the Source Table.",True,False,False,dataengineering,t5_36en4,45944,public,self,How do you handle Deletes when doing an Incremental Refresh,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r5sdze/how_do_you_handle_deletes_when_doing_an/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,dataengineerdude,,,[],,,,text,t2_4wrevs7q,False,False,False,[],False,False,1638283805,confessionsofadataguy.com,https://www.reddit.com/r/dataengineering/comments/r5ovzt/cte_vs_subquery/,{},r5ovzt,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,26,0,False,all_ads,/r/dataengineering/comments/r5ovzt/cte_vs_subquery/,False,link,"{'enabled': False, 'images': [{'id': 'MIXRoZVPXW8XFZGdA-EaeSrc9H3ZqHk0-gz3KHMKhmM', 'resolutions': [{'height': 65, 'url': 'https://external-preview.redd.it/_2UsfcTIqOCKVGvwhetqU47OlGiKTPaoSUpyTbUpeSQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=969bdb8f2a31efdcb61bc1c2abff8480fcdd599f', 'width': 108}, {'height': 130, 'url': 'https://external-preview.redd.it/_2UsfcTIqOCKVGvwhetqU47OlGiKTPaoSUpyTbUpeSQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9e639f4209acebbea46223d5d89e41da0b4c54f5', 'width': 216}, {'height': 193, 'url': 'https://external-preview.redd.it/_2UsfcTIqOCKVGvwhetqU47OlGiKTPaoSUpyTbUpeSQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7644a90158e56476c0570d8477eff9040eb6eaad', 'width': 320}, {'height': 387, 'url': 'https://external-preview.redd.it/_2UsfcTIqOCKVGvwhetqU47OlGiKTPaoSUpyTbUpeSQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2d05c7ca0da633682e955793f65505c91d566356', 'width': 640}, {'height': 581, 'url': 'https://external-preview.redd.it/_2UsfcTIqOCKVGvwhetqU47OlGiKTPaoSUpyTbUpeSQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7bae1136e2bc0c96b215fcb0e6428357899792bc', 'width': 960}], 'source': {'height': 624, 'url': 'https://external-preview.redd.it/_2UsfcTIqOCKVGvwhetqU47OlGiKTPaoSUpyTbUpeSQ.jpg?auto=webp&amp;s=1ec6c81e255e8fbc3cfb0181bd1a79f1e8861e0c', 'width': 1030}, 'variants': {}}]}",6,1638283816,1,,True,False,False,dataengineering,t5_36en4,45932,public,https://b.thumbs.redditmedia.com/A-4ZVOs3tNubG_SyvS-odeUj4ei2GpBbSOLQBfhSXEc.jpg,CTE vs SubQuery,0,[],1.0,https://www.confessionsofadataguy.com/cte-vs-subquery/,all_ads,6,,,,,,84.0,140.0,https://www.confessionsofadataguy.com/cte-vs-subquery/,,,,,,,,,,
[],False,Taerbit,,,[],,,,text,t2_gjsfv,False,False,False,[],False,False,1638280612,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r5ntbx/swe_transition_to_de/,{},r5ntbx,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/r5ntbx/swe_transition_to_de/,False,,,6,1638280623,1,"Hi all,

I have an upcoming interview for a De role at a large UK company. I am 1.5 years out of Univeristy with an Applied Computing degree having completed an Honours project in clinical ML dermatology. I have spent the time since working for a startup as the sole backend developer on projects. During this time, I took over product vision, gathered requirements and pushed for software best practises such as TDD and version control. However, I am a bit apprehensive about the interview given my absense from SQL querying in my past role (predominantly used ORM).

Just wanted to gauge those in the know if my experience will come across positively in a DE interview and if there is anything I should spotlight or not mention.

Cheers!

P.s. My SQL skills aren't horrendous but my syntax can be flakey and more advanced topics I'm not clued up on",True,False,False,dataengineering,t5_36en4,45932,public,self,SWE transition to DE,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r5ntbx/swe_transition_to_de/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,10xbek,,,[],,,,text,t2_gtqs4z53,False,False,False,[],False,False,1638238779,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r5c6q0/cloud_certs_for_a_beginner/,{},r5c6q0,False,False,False,False,False,False,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r5c6q0/cloud_certs_for_a_beginner/,False,,,6,1638238789,1,[removed],True,False,False,dataengineering,t5_36en4,45893,public,self,Cloud Certs for a beginner,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r5c6q0/cloud_certs_for_a_beginner/,all_ads,6,,,automod_filtered,,,,,,,,,,,,,,,
[],False,BerghainInMyVeins,,,[],,,,text,t2_91oo85c8,False,False,False,[],False,False,1638238288,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r5c0wy/apache_beam_and_cloud_data_flow/,{},r5c0wy,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,7,0,False,all_ads,/r/dataengineering/comments/r5c0wy/apache_beam_and_cloud_data_flow/,False,,,6,1638238299,1,"I’ve been trying to develop a simple pipeline on cloud data flow with Apache beam. 

I have the pipeline running locally and it’s working. The problem is when I try to run it on data flow. I am really struggling with templating my pipeline. I’m not sure how/if I can run it without templating it. 

All the info I’ve found regarding using the data flow runner seem incomplete. 

Do any of you guys use Apache beam with the data flow runner? 

How do you launch your pipelines? Do you template them? No? 

What resource(s) did you use to launch your pipelines at first? All the resources I have come across are not that great. 

Thank you.",True,False,False,dataengineering,t5_36en4,45893,public,self,Apache beam and cloud data flow,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r5c0wy/apache_beam_and_cloud_data_flow/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,cpardl,,,[],,,,text,t2_fb1s1pke,False,False,False,[],False,False,1638228583,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r58r49/the_data_stack_show_a_podcast_on_data_engineering/,{},r58r49,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,False,20,0,False,all_ads,/r/dataengineering/comments/r58r49/the_data_stack_show_a_podcast_on_data_engineering/,False,self,"{'enabled': False, 'images': [{'id': '_yoscnXb1RFxtTuBZnbzDMJHCehvluaPTHyNfrCbMkw', 'resolutions': [{'height': 60, 'url': 'https://external-preview.redd.it/Deqlntlo9qb5vgHPqHMHWcvNrdzsWAxeZ0GCv4dKMO8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0e577c0d8f3a992b1dc57675a3d26252ad81247a', 'width': 108}, {'height': 121, 'url': 'https://external-preview.redd.it/Deqlntlo9qb5vgHPqHMHWcvNrdzsWAxeZ0GCv4dKMO8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1763e65c857ad2771645d4ea3c2e850f17bfccef', 'width': 216}, {'height': 180, 'url': 'https://external-preview.redd.it/Deqlntlo9qb5vgHPqHMHWcvNrdzsWAxeZ0GCv4dKMO8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7d90b0dbd576e3eba2f0ad686163a85121f65603', 'width': 320}, {'height': 360, 'url': 'https://external-preview.redd.it/Deqlntlo9qb5vgHPqHMHWcvNrdzsWAxeZ0GCv4dKMO8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f5971a1ba699a90ffd3b8a8cfbdc3fd309954682', 'width': 640}, {'height': 540, 'url': 'https://external-preview.redd.it/Deqlntlo9qb5vgHPqHMHWcvNrdzsWAxeZ0GCv4dKMO8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0f89730a35d8dbeffc76af7828edd89f296f4ce7', 'width': 960}, {'height': 607, 'url': 'https://external-preview.redd.it/Deqlntlo9qb5vgHPqHMHWcvNrdzsWAxeZ0GCv4dKMO8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b71ab8f712333f16f45b3c81feba003de6126689', 'width': 1080}], 'source': {'height': 900, 'url': 'https://external-preview.redd.it/Deqlntlo9qb5vgHPqHMHWcvNrdzsWAxeZ0GCv4dKMO8.jpg?auto=webp&amp;s=34de8ecb2dda95ced12ebd7c21053e1e7981ba1d', 'width': 1600}, 'variants': {}}]}",6,1638228595,1,"Hey everyone, 

I'm one of the hosts of a podcast show, called the data stack show. We are focusing on data engineering related topics but we never say no to a guest who has something interesting to say around technology. We even had a guest where we chatted about communities.

We've been running the show for about a year now and never shared its existence with any community.

 I'd love to get feedback and suggestions from you. What questions you might have and what people you would like to listen to. 

Of course any constructive criticism is more than welcome!

&amp;#x200B;

You can check the show here: [The Data Stack Show](https://datastackshow.com/)",True,False,False,dataengineering,t5_36en4,45891,public,self,The Data Stack Show: A Podcast on Data Engineering,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r58r49/the_data_stack_show_a_podcast_on_data_engineering/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,redditthrowaway0315,,,[],,,,text,t2_clsgf4j4,False,False,False,[],False,False,1638222778,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r56p7j/how_do_you_deal_with_constant_business/,{},r56p7j,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,22,0,False,all_ads,/r/dataengineering/comments/r56p7j/how_do_you_deal_with_constant_business/,False,,,6,1638222790,1,"Hi friends,

Here is something that is constantly bugging me. Let's say you are working on some transformation for the T in ELT. The result is a view/table that gives the end-users data easy to query. There is a classification field, say ""Source"", that indicates the source of a measure (sales/views/whatever). Your analysts need this field to slice the Tableau views.

The problem is that 1) Sometimes the definition of ""Source"" changes, Source A may mean something before 2021-11-01 and something else afterwards; 2) Sometimes one layer of source is not enough, and you have to add one more columns to differentiate same sources (e.g. Source = '3rd-party', and you need a second field to tell which 3rd-party).

These make the table very messy, because you got all business rules implemented, so analysts have to remember the dates you made the changes and write big CASE WHEN THEN ELSE END.

How to solve this issue? I guess once we put so much information into one table it's inevitable that it becomes messy? Maybe I should remove this table and just expose the dim/fact structure to analysts? We already have a dim/fact DWH implemented and my table is just a boon to reduce the difficulty of writing a lot of joins.",True,False,False,dataengineering,t5_36en4,45884,public,self,How do you deal with constant business logic/requirement change?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r56p7j/how_do_you_deal_with_constant_business/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Thenbee,,,[],,,,text,t2_15h1ob,False,False,False,[],False,False,1638217415,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r54q05/usage_of_composite_key/,{},r54q05,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,6,0,False,all_ads,/r/dataengineering/comments/r54q05/usage_of_composite_key/,False,,,6,1638217427,1,"Hi,

I am studying on composite keys and its usage and I am not entirely sure what the advantages are over or a surrogate key, or a column which is a concatenation of the columns that would be your composite key. I understand that you would have 1 less column using composite key, but doesn’t make that 1 column save you lots of possible foreign table columns?",True,False,False,dataengineering,t5_36en4,45883,public,self,Usage of composite key,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r54q05/usage_of_composite_key/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,khantalha,,,[],,,,text,t2_12blgr,False,False,False,[],False,False,1638216078,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r547xr/installation_guide_of_apache_airflow_on_windows/,{},r547xr,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,False,3,0,False,all_ads,/r/dataengineering/comments/r547xr/installation_guide_of_apache_airflow_on_windows/,False,,,6,1638216090,1,"Hey, I've written this piece of information on the basis of training I took last days while becoming Data Engineer.

Here's the easiest guide to install (Apache Airflow) on Windows.

[Installation Guide of Apache Airflow on Windows | Super Fast &amp; Easy](http:// https://link.medium.com/KDlBBQ6gzlb)",True,False,False,dataengineering,t5_36en4,45883,public,self,Installation Guide of Apache Airflow on Windows | Fast &amp; Super Easy,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r547xr/installation_guide_of_apache_airflow_on_windows/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,liberal_hot_takes,,,[],,,,text,t2_bjf3oz5t,False,False,False,[],False,False,1638215219,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r53wgo/airflow_importing_a_python_module/,{},r53wgo,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,12,0,False,all_ads,/r/dataengineering/comments/r53wgo/airflow_importing_a_python_module/,False,,,6,1638215229,1,"I am trying to use an existing python module (praw) with a DAG.

I am at a loss of where to start.

I'm using the apache airflow docker-compose.yaml.

So do I just need to follow these instructions? - https://airflow.apache.org/docs/apache-airflow/stable/modules_management.html#",True,False,False,dataengineering,t5_36en4,45880,public,self,Airflow - importing a python module,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r53wgo/airflow_importing_a_python_module/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,felipeHernandez19,,,[],,,,text,t2_7sxdmzpt,False,False,False,[],False,False,1638208949,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r51lj9/resources/,{},r51lj9,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,14,0,False,all_ads,/r/dataengineering/comments/r51lj9/resources/,False,,,6,1638208960,1,"Guys I’m looking for data engineering resources. Such as masters, courses, online bootcamps, free videos, etc etc. something with a flow. Thanks in advance for sharing with me",True,False,False,dataengineering,t5_36en4,45872,public,self,Resources,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r51lj9/resources/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,me-noob,,,[],,,,text,t2_eijct,False,False,False,[],False,False,1638207270,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r50zcj/data_modeling_tools_with_realtime_online/,{},r50zcj,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,3,0,False,all_ads,/r/dataengineering/comments/r50zcj/data_modeling_tools_with_realtime_online/,False,,,6,1638207281,1,"I am planning a series of virtual (online) workshops to create and expand a logical data model together with a small group of business analysts and data modelers across the country. Goal is to first collaboratively refine scope, functionality, extensibility, and required data entities at the semantic layer before creating DDLs / table structures etc.  
The team is familiar with and has access to Visio licenses but I am wondering whether there are other / better tools for a virtual team to use in such scenario and what has worked for you. Any feedback is welcome.",True,False,False,dataengineering,t5_36en4,45869,public,self,Data Modeling Tools with Real-Time Online Collaboration Features?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r50zcj/data_modeling_tools_with_realtime_online/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,diegolujan1,,,[],,,,text,t2_2tclli02,False,False,False,[],False,False,1638203275,youtube.com,https://www.reddit.com/r/dataengineering/comments/r4ziej/ccsccus_technologies_capture_co2_emissions_at/,{},r4ziej,False,True,False,False,False,True,False,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,5,0,False,all_ads,/r/dataengineering/comments/r4ziej/ccsccus_technologies_capture_co2_emissions_at/,False,rich:video,"{'enabled': False, 'images': [{'id': 'nsfYMhPOHNvEaoVfFio9WVit_9dD1X1tTPC9TQ6bExk', 'resolutions': [{'height': 81, 'url': 'https://external-preview.redd.it/RJFE7lRYMPvEwNcPlTmAlEpA5bmu1GK3Mtmxsc2HIBk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=09a3f4ea8b3661bc973959cc359e64de18742d77', 'width': 108}, {'height': 162, 'url': 'https://external-preview.redd.it/RJFE7lRYMPvEwNcPlTmAlEpA5bmu1GK3Mtmxsc2HIBk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2acf53ea0099c9485707db24a3182f462cf44dd9', 'width': 216}, {'height': 240, 'url': 'https://external-preview.redd.it/RJFE7lRYMPvEwNcPlTmAlEpA5bmu1GK3Mtmxsc2HIBk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5db45998a5c7ab16170f3095e582be060901ecb4', 'width': 320}], 'source': {'height': 360, 'url': 'https://external-preview.redd.it/RJFE7lRYMPvEwNcPlTmAlEpA5bmu1GK3Mtmxsc2HIBk.jpg?auto=webp&amp;s=2742ea112ab0dc9009e65c916a36538d351f2050', 'width': 480}, 'variants': {}}]}",6,1638203294,1,,True,False,False,dataengineering,t5_36en4,45862,public,https://b.thumbs.redditmedia.com/9lQlzpaumnkuPI8MqMx0S1dK2iwOVZ9riW0UsQWzK3c.jpg,CCS/CCUS technologies capture CO2 emissions at source or directly from the air. CO2 emissions are then transported away and stored deep underground or turned into useful products. What are your thoughts on this technology?,0,[],1.0,https://www.youtube.com/watch?v=om-Czrru53I&amp;t=78s,all_ads,6,"{'oembed': {'author_name': 'Toy_Virtual_Structures', 'author_url': 'https://www.youtube.com/channel/UCr5Akn6LhGDin7coWM7dfUg', 'height': 200, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/om-Czrru53I?start=78&amp;feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/om-Czrru53I/hqdefault.jpg', 'thumbnail_width': 480, 'title': 'Investing in a Green Future: Carbon Capture &amp; Storage', 'type': 'video', 'version': '1.0', 'width': 356}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/om-Czrru53I?start=78&amp;feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'scrolling': False, 'width': 356}",,"{'oembed': {'author_name': 'Toy_Virtual_Structures', 'author_url': 'https://www.youtube.com/channel/UCr5Akn6LhGDin7coWM7dfUg', 'height': 200, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/om-Czrru53I?start=78&amp;feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/om-Czrru53I/hqdefault.jpg', 'thumbnail_width': 480, 'title': 'Investing in a Green Future: Carbon Capture &amp; Storage', 'type': 'video', 'version': '1.0', 'width': 356}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/om-Czrru53I?start=78&amp;feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/r4ziej', 'scrolling': False, 'width': 356}",105.0,140.0,https://www.youtube.com/watch?v=om-Czrru53I&amp;t=78s,,,,,,,,,,
[],False,zjffdu,,,[],,,,text,t2_38ntx,False,False,False,[],False,False,1638199866,zjffdu.medium.com,https://www.reddit.com/r/dataengineering/comments/r4yasq/deep_dive_into_delta_lake_via_apache_zeppelin/,{},r4yasq,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r4yasq/deep_dive_into_delta_lake_via_apache_zeppelin/,False,link,"{'enabled': False, 'images': [{'id': 'xLI0xpfAtNb-WKp0q8DRCz8gBv2PGLdmlnyC4TfzMRk', 'resolutions': [{'height': 60, 'url': 'https://external-preview.redd.it/lay5iVm2r0dtEkhHjGkJ60mzvDYNBO-dM011tHJyd40.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=81e68a604f228ab0f9a1a9d6eb8cd9a734098613', 'width': 108}, {'height': 120, 'url': 'https://external-preview.redd.it/lay5iVm2r0dtEkhHjGkJ60mzvDYNBO-dM011tHJyd40.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c6c52431fdc7ecc9cb3d4e9ef36de60207119fe0', 'width': 216}, {'height': 178, 'url': 'https://external-preview.redd.it/lay5iVm2r0dtEkhHjGkJ60mzvDYNBO-dM011tHJyd40.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=204b3a4b09f5e92b494b2245ff7289a9e987a2dc', 'width': 320}, {'height': 356, 'url': 'https://external-preview.redd.it/lay5iVm2r0dtEkhHjGkJ60mzvDYNBO-dM011tHJyd40.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a8a03ff88c8cc71ddf899cdf61d2b201c3cabb3d', 'width': 640}, {'height': 534, 'url': 'https://external-preview.redd.it/lay5iVm2r0dtEkhHjGkJ60mzvDYNBO-dM011tHJyd40.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3f5e631aa31525e435ad0479a763d97120eb2eb5', 'width': 960}, {'height': 601, 'url': 'https://external-preview.redd.it/lay5iVm2r0dtEkhHjGkJ60mzvDYNBO-dM011tHJyd40.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a0302297cde6bd4050d2ef3983c8832bb11079c2', 'width': 1080}], 'source': {'height': 668, 'url': 'https://external-preview.redd.it/lay5iVm2r0dtEkhHjGkJ60mzvDYNBO-dM011tHJyd40.jpg?auto=webp&amp;s=5c3c04ebd3129fd986ace3710a3e0c798b141423', 'width': 1200}, 'variants': {}}]}",6,1638199877,1,,True,False,False,dataengineering,t5_36en4,45861,public,https://b.thumbs.redditmedia.com/SBiwBqYEdooU1h9yydus6y74Sn3I8kzeBYAIY8ynJmo.jpg,Deep Dive into Delta Lake via Apache Zeppelin,0,[],1.0,https://zjffdu.medium.com/deep-dive-into-delta-lake-via-apache-zeppelin-d59db1673584,all_ads,6,,,,,,77.0,140.0,https://zjffdu.medium.com/deep-dive-into-delta-lake-via-apache-zeppelin-d59db1673584,,,,,,,,,,
[],False,oneequalsequalsone,,,[],,,,text,t2_ghfjevxv,False,False,False,[],False,False,1638199750,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r4y99g/whats_your_orgs_gcp_project_architecture_for/,{},r4y99g,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/r4y99g/whats_your_orgs_gcp_project_architecture_for/,False,,,6,1638199761,1,"For example, we're considering having data go through these 3 projects:

- landing -&gt; dev -&gt; prod

I'm working on designing this at the moment, and I'd like to hear different ideas. Any thoughts?",True,False,False,dataengineering,t5_36en4,45861,public,self,What's your org's GCP project architecture for pipelines?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r4y99g/whats_your_orgs_gcp_project_architecture_for/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,jpmfribeiro,,,[],,,,text,t2_aietkep,False,False,False,[],False,False,1638198909,jpmonteiro.substack.com,https://www.reddit.com/r/dataengineering/comments/r4xyva/codds_dream_a_tribute_to_the_father_of_relational/,{},r4xyva,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/r4xyva/codds_dream_a_tribute_to_the_father_of_relational/,False,link,"{'enabled': False, 'images': [{'id': 'yvKZurCvYfg0fNoMA9L7cpo0h4OAvzQskEUvQmZ89vw', 'resolutions': [{'height': 153, 'url': 'https://external-preview.redd.it/UiorTV-_W5OZjXMdO2LWfCUG2r2MEzITc-A476r27R4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=22b1be061ae1848857a8e15d8db07524eb8fbff3', 'width': 108}], 'source': {'height': 300, 'url': 'https://external-preview.redd.it/UiorTV-_W5OZjXMdO2LWfCUG2r2MEzITc-A476r27R4.jpg?auto=webp&amp;s=fbc82f8452611d4328b2d8090f4a726f619c505a', 'width': 211}, 'variants': {}}]}",6,1638198919,1,,True,False,False,dataengineering,t5_36en4,45860,public,https://b.thumbs.redditmedia.com/gR4XBxpfEWtT16OKk2COY01JGLOTq1QSW3JvCQT1_uw.jpg,Codd's Dream - A tribute to the father of relational databases and his quest to bring access to information to the casual user,0,[],1.0,https://jpmonteiro.substack.com/p/codds-dream,all_ads,6,,,,,,140.0,140.0,https://jpmonteiro.substack.com/p/codds-dream,,,,,,,,,,
[],False,DataGeek0,,,[],,,,text,t2_dqawdl1y,False,False,False,[],False,False,1638195839,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r4wx44/d_how_verizon_uses_ai_for_enhanced_business/,{},r4wx44,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,3,0,False,all_ads,/r/dataengineering/comments/r4wx44/d_how_verizon_uses_ai_for_enhanced_business/,False,self,"{'enabled': False, 'images': [{'id': 'zg7XmJmYuaKqz202No_XygSD6xeAETXV-wT6IWr16dg', 'resolutions': [{'height': 55, 'url': 'https://external-preview.redd.it/yVTSmJnURo9i0olWSFZaRrYjT8thIMt-nnUW2VSBw0w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=359b1e94b97dda7c579570dd16bfdadf65c2dfca', 'width': 108}, {'height': 111, 'url': 'https://external-preview.redd.it/yVTSmJnURo9i0olWSFZaRrYjT8thIMt-nnUW2VSBw0w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=88086b2818525f5cecbb9d68582b0e8f981513d5', 'width': 216}, {'height': 164, 'url': 'https://external-preview.redd.it/yVTSmJnURo9i0olWSFZaRrYjT8thIMt-nnUW2VSBw0w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8b2a17f345cace8a7ce044cfb57adc7e22b0efa0', 'width': 320}, {'height': 329, 'url': 'https://external-preview.redd.it/yVTSmJnURo9i0olWSFZaRrYjT8thIMt-nnUW2VSBw0w.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=477389af8cc042d99f5e32d9ca99f20694b44569', 'width': 640}, {'height': 494, 'url': 'https://external-preview.redd.it/yVTSmJnURo9i0olWSFZaRrYjT8thIMt-nnUW2VSBw0w.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=479e16683ec8d19f450dde9a2288b0b66fc7522a', 'width': 960}], 'source': {'height': 527, 'url': 'https://external-preview.redd.it/yVTSmJnURo9i0olWSFZaRrYjT8thIMt-nnUW2VSBw0w.jpg?auto=webp&amp;s=b747afbee199f3fbf18d12746f2c5d25479e0ed3', 'width': 1024}, 'variants': {}}]}",6,1638195849,1,"Hi r/dataengineering!

I wanted to share this webinar with you ""How Verizon Uses AI For Enhanced Business Decisions with Anil Kumar, Executive  Director – Head of AI Industrialization at Verizon"" on Thursday, December 2, 2021 at 11:30 AM ET.

I'm really interested in learning how a company as enormous as Verizon is using AI/ML at scale. What about you? 

Below is the information from the website:

**Featured Speaker: Anil Kumar, Executive Director, Head of AI Industrialization at Verizon.**

As companies are looking to leverage cognitive technology, and  deploying machine learning models, organizations need to make sure they  have the correct people, processes, and technology in place to succeed.  In this presentation, Anil will share what AI industrialization is and how Verizon is moving from pockets of AI/ML to AI/ML being implemented  across the organization. He will also share some of the unique  opportunities and challenges that adopting AI across an organization as  large as Verizon presents.

**Agenda:**

* 11:30-12:30pm: Featured Presentation
* 12:30-13:00pm: Your Q&amp;A and interaction

Link to website: [https://events.cognilytica.com/CLNTQwMHwyNA](https://events.cognilytica.com/CLNTQwMHwyNA)",True,False,False,dataengineering,t5_36en4,45861,public,self,"[D] How Verizon Uses AI For Enhanced Business Decisions with Anil Kumar, Executive Director – Head of AI Industrialization at Verizon Thursday, December 2, 2021 at 11:30 AM ET",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r4wx44/d_how_verizon_uses_ai_for_enhanced_business/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,TriscuitLuvr4Lyfe,,,[],,,,text,t2_e20malp1,False,False,False,[],False,False,1638195029,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r4wnlq/how_do_you_deploy_database_objects_in_a_cicd/,{},r4wnlq,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,12,0,False,all_ads,/r/dataengineering/comments/r4wnlq/how_do_you_deploy_database_objects_in_a_cicd/,False,,,6,1638195040,1,"I come from a MSSQL background so I'm familiar with the concept of building a dacpac file from source code and deploying that to a DB server via automation software. Microsoft and Visual Studio makes this rather easy.

But how do you do this with another RDBMS, like Postgres, or even Snowflake? If you have a bunch of .sql files that define your table structures, and you make a change to one of the tables, how do you auto-deploy that without getting the DBA involved or manually making change scripts? Is there a piece of software that makes this process easy?",True,False,False,dataengineering,t5_36en4,45859,public,self,How do you deploy database objects in a CI/CD pipeline?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r4wnlq/how_do_you_deploy_database_objects_in_a_cicd/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,PuzzleheadedOcelot3,,,[],,,,text,t2_3rcz8s5g,False,False,False,[],False,False,1638194649,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r4wixx/should_i_take_this_opportunity/,{},r4wixx,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,9,0,False,all_ads,/r/dataengineering/comments/r4wixx/should_i_take_this_opportunity/,False,,,6,1638194659,1,"Hi guys I'm currently working as an entry level data analyst in London

Background:
Currently working for a consulting company that just had 6 weeks of training on Excel, SQL, power BI, Tableau, Python, and R. Each software covered in a week (yes it was super intense as so much information was given 6 hours a day)

Was called by my manager that said she thinks I'd be capable of joining the data engineering pathway where we'll learn Scrum, Hadoop, spark and scala. Over the next 3 weeks once again at 6 hours a day.

My question is that is this a good opportunity to go into becoming a data engineer? That too when I haven't even mastered the softwares I've used as a data analyst.

How difficult are the softwares I've mentioned to learn compared to say Python other R? I was ok with python but R was extremely difficult for me. Even in python a concept such as a loop is still difficult for me. My strengths were in python pandas which consisted of EDA.

After all these weeks of training I feel quite burnt out but at the same time I don't want to miss this data engineering opportunity if it is good in the long run.

Im 28 years old with a kid and a mortgage (don't know if this is relevant)

The training will be paid, so I'll be able to support my family. The only thing is lack of time with them.

After the 3 weeks of initial training we have a project with a company called Quantexa that are innovating something called contextualised design intelligence and apparently that's something that will be huge in the future. This project will take 7-12 weeks of SELF LEARNING then we'll be placed in deloitte to used what we learnt with Quantexa to identify fraud in the finance department.

https://www.quantexa.com/press/deloitte-partnership/

Any insights would be greatly appreciated because I just don't know what to do. I'm at that age where I don't want to study anymore but I also don't want to miss an incredible opportunity.

Will it be difficult and should I take it?

Thank you!",True,False,False,dataengineering,t5_36en4,45859,public,self,Should I take this opportunity?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r4wixx/should_i_take_this_opportunity/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,dragonachu117,,,[],,,,text,t2_a7vbf8fe,False,False,False,[],False,False,1638191805,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r4vmsb/azure_databricks_best_practices_and_template_for/,{},r4vmsb,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/r4vmsb/azure_databricks_best_practices_and_template_for/,False,,,6,1638191816,1,"Hello Guys,

At my company, we are building a data lake in azure and is going to create snapshots from incremental data coming from various source by leveraging data bricks delta lakes(upsert ops). Can you guys please share your experiences, dos and don'ts or best practices when it comes to doing this on azure.

Please share any links if you have seen a template for doing this some where.",True,False,False,dataengineering,t5_36en4,45859,public,self,Azure Databricks best practices and template for incremental handling using delta lakes,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r4vmsb/azure_databricks_best_practices_and_template_for/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,geekyhumans,,,[],,,,text,t2_q7kko,False,False,False,[],False,False,1638190197,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r4v5er/how_to_find_contractfreelance_gigs_in_data/,{},r4v5er,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,7,0,False,all_ads,/r/dataengineering/comments/r4v5er/how_to_find_contractfreelance_gigs_in_data/,False,,,6,1638190207,1,"I'm a bit unsure where to find Data Engineering projects - Contract/Freelance? Is there any website that lists agencies?

P.S. Don't suggest freelancing websites.

Thanks",True,False,False,dataengineering,t5_36en4,45856,public,self,How to find contract/freelance gigs in Data Engineering?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r4v5er/how_to_find_contractfreelance_gigs_in_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Length-Working,,,[],,,,text,t2_d505p8is,False,False,False,[],False,False,1638188718,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r4uqk7/azure_arc_pseudoserverless_by_kubernetes/,{},r4uqk7,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/r4uqk7/azure_arc_pseudoserverless_by_kubernetes/,False,,,6,1638188729,1,"I'm currently developing an evolution architecture for my data platform and have landed on Azure Arc to power it. This tool seems extremely powerful in a security conscious, enterprise-wide environment. It lets me bring all my kubernetes deployments under one roof regardless of the platform they are on, which is nice, but more importantly (and currently a Preview feature) I can also run my Azure Function Apps and Azure Logic Apps directly on AKS via Arc. 

For reasons I won't go into, I'm locked within one vnet in a single region. Suddenly, I can get all the power of serverless, with all the cost savings of serverless for spiking workload, with the scalability of Azure Kubernetes Service for scaling baseline workloads. I'm currently estimating I can bring down the overall cost of our platform by literally 90%. I'm unable to directly deploy serverless normally because it inherantly is multi-region but this seems like a better alternative.

Has anyone got any experience with Arc? I'm keen to hear if there are any downsides besides the usual ""it's in Preview"".",True,False,False,dataengineering,t5_36en4,45855,public,self,Azure Arc: Pseudo-serverless by Kubernetes,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r4uqk7/azure_arc_pseudoserverless_by_kubernetes/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,saif3r,,,[],,,,text,t2_fw1zu,False,False,False,[],False,False,1638179317,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r4seun/best_way_to_store_aggregations_in_dynamodb/,{},r4seun,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/r4seun/best_way_to_store_aggregations_in_dynamodb/,False,,,6,1638179328,1,"Hi All,

I have stumbled upon an use case, that processes raw daily data, into weekly data and needs to be stored in dynamodb. Part of raw data looks like this:

    W01	Service1	01-01-2021  Rainy
    W01	Service1	02-01-2021  Snowy
    W01	Service1	03-01-2021  Rainy
    W01	Service1	04-01-2021  Sunny
    W01	Service1	05-01-2021  Rainy
    W01	Service1	06-01-2021  Sunny
    W01	Service1	07-01-2021  Sunny

I need to aggregate each week into one row and store it. The final data structure would look like this ( Week and Wether\_Service are keys):

    Week    Weather_Service    Country    Region    Weather                          Occurences
    W01     Service1           UK         North     ['Rainy', 'Sunny', 'Snowy']      [3, 3, 1]
    W01     Service2           UK         South     ['Sunny', 'Rainy', 'Snowy']      [4, 2, 1]

Raw data will be pulled using glue/python and then once processes, it will be pulled from dynamodb using python for further processing.

Is there a better way to store Weather and Occurences than this? Assuming the rest of the data structure has to remain as is?",True,False,False,dataengineering,t5_36en4,45851,public,self,Best way to store aggregations in DynamoDB.,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r4seun/best_way_to_store_aggregations_in_dynamodb/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,torner374,,,[],,,,text,t2_fb0k4j0i,False,False,False,[],False,False,1638178106,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r4s4q5/interview_for_data_engineer_intern_role_how_can_i/,{},r4s4q5,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/r4s4q5/interview_for_data_engineer_intern_role_how_can_i/,False,,,6,1638178117,1,"Officially the role is a software developer one but it’s applied to data, I feel like it’s a close fit to data engineering.

I’m a sophomore in university, what can I learn before the interview to maximize my chance of landing the role? I only have slight experience with sql querying and processing data with python.

Should I practice sql?",True,False,False,dataengineering,t5_36en4,45850,public,self,"Interview for data engineer intern role, how can I prepare?",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r4s4q5/interview_for_data_engineer_intern_role_how_can_i/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,killer_unkill,,,[],,,,text,t2_18oazxf4,False,False,False,[],False,False,1638176249,schemaverse.com,https://www.reddit.com/r/dataengineering/comments/r4rp5y/schemaverse_a_space_based_strategy_game_to_learn/,{},r4rp5y,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r4rp5y/schemaverse_a_space_based_strategy_game_to_learn/,False,,,6,1638176260,1,,True,False,False,dataengineering,t5_36en4,45850,public,default,"Schemaverse, a space based strategy game to learn PostgreSQL",0,[],1.0,https://schemaverse.com/,all_ads,6,,,,,,,,https://schemaverse.com/,,,,,,,,,,
[],False,tombradyisthegoat1q,,,[],,,,text,t2_e5uiru14,False,False,False,[],False,False,1638169414,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r4q0dl/interview_questions/,{},r4q0dl,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,6,0,False,all_ads,/r/dataengineering/comments/r4q0dl/interview_questions/,False,,,6,1638169425,1,"Hi all, I have an interview coming up for an entry-level (I think it's entry-level, they never specified)  data engineer position. I am brushing up on some conceptual stuff, mostly Hadoop and NoSQL. This is my first interview for a data engineer position and I was wondering if I should also brush up on some leetcode or if it will likely be conceptual. Please let me know what you think.",True,False,False,dataengineering,t5_36en4,45843,public,self,Interview Questions,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r4q0dl/interview_questions/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,In_Robarts_We_Stand,,,[],,,,text,t2_93sljz44,False,False,False,[],False,False,1638163063,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r4oa9c/do_you_guys_want_a_quick_stats_primer_heres_a/,{},r4oa9c,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,17,0,False,all_ads,/r/dataengineering/comments/r4oa9c/do_you_guys_want_a_quick_stats_primer_heres_a/,False,,,6,1638163075,1,"If you all need some simple stats here are the go-to. Hope it helps.

This was presented to graduate students in the Faculty of Forestry at the University of Toronto.

[https://www.youtube.com/watch?v=GDrEwz8r8Xc](https://www.youtube.com/watch?v=GDrEwz8r8Xc)

First, a general introduction to probability, assumptions of inference, and hypothesis testing. The second requirement is to perform the analyses, including managing the data using some statistical software.

At the vanguard of such software is the R statistical software [https://www.r-project.org/;](https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbWVHOVNHcG95V1ZFcWF5SDdsdXVFUFgwVk9YZ3xBQ3Jtc0ttYUZYVmQzQ1E4MkRaNTFqN2NjNXE4TXNqMW5UZ1BuZGhmZUk3SjUwQ3RFT24tLW5IZkdpczJ6T0t5dWdlU1FaOFJDVngxVUlrYWRnOVREZVhXaHNQaXk0VWN1dHVXWTRRWFBxS1ZKY0RvNTRyTjRlWQ&amp;q=https%3A%2F%2Fwww.r-project.org%2F%3B)​ and its integrative environment Rstudio [https://www.rstudio.com/](https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqazB0NjQtTlZvRlp4cDZudWM0dzJfOTQyaEViQXxBQ3Jtc0tsY201bG5pbGltSWhWOVl2YUlvQlNQbk9Delg2bWVHRlluOGNQNUFqUlhqeEVBUEtwcHFNMDRjR2ZsZGV4YzVac0pzV050TzVGbElPdEN3MThyTGFLODFiRlJMOWdESkc5R0NOdW53SGZuMDAtRHBwaw&amp;q=https%3A%2F%2Fwww.rstudio.com%2F)​. Few programs constitute the virtues of open-sources highly integrated and powerful programs as the R statistical software - taught to the modern student in most institutions.

The following video comprises the time-stamped functions and analyses:

1. Intro to data management, probability, and data presentation ([0:00](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=0s)​-[13:00](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=780s)​ min)
2. Intro to producing graphs with R studio ([13:15](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=795s)​ - [17:00](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=1020s)​ min)
3. Subsetting the data ([23:20](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=1400s)​)
4. 'attach' function in R ([23:47](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=1427s)​ min)
5. 'summarySE' ([24:45](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=1485s)​ min)
6. Using the plot function ([28:00](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=1680s)​ - [34:00](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=2040s)​ min)
7. Package 'sciplot' for bar graphs 'bargraph.CI' function ([34:00](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=2040s)​ - [36:00](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=2160s)​ min)
8. Analysis of Variance including Tukey HSD (ANOVA) ([36:00](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=2160s)​ - [40:00](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=2400s)​ min)
9. Creating Line Plots and Scatter plot ( [42:00](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=2520s)​-[45:00](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=2700s)​ min)
10. Regression analyses via 'lm' function - linear - ([45:00](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=2700s)​ - [48:00](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=2880s)​ mins)
11. Correlation analysis ([48:00](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=2880s)​ -[51:00](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=3060s)​ mins)

The video finishes with a summation of what we've learned. If you like this material, then please like and subscribe to this channel.",True,False,False,dataengineering,t5_36en4,45838,public,self,Do you guys want a quick stats primer? Here's a simple guide with the R,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r4oa9c/do_you_guys_want_a_quick_stats_primer_heres_a/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,BoiElroy,,,[],,,,text,t2_r8dyi,False,False,True,[],False,False,1638161724,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r4nvpk/ive_always_been_curious_about_this_one_thing/,{},r4nvpk,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,9,0,False,all_ads,/r/dataengineering/comments/r4nvpk/ive_always_been_curious_about_this_one_thing/,False,,,6,1638161735,1,"Sorry for the click-bait-y title.

Given: a columnar storage system, like an OLAP database like Snowflake. Or even a columnar storage format like parquet.

If you had a column that had time as a text string, e.g. ""11:30"", would it improve database normalization to split that into one field with ""11"", one with "":"", and the last with ""30""? My thought is that the number of unique values you need to store in the original case is 720, assuming a 12-hr format, and 1440 assuming a 24-hr format. For the latter case, even though you would have more column objects, the number of unique values you have to carry is 73 , assuming a 12-hr format and 85, assuming a 24-hr format.

Wouldn't that compress better? and not degrade the information persistence.

This is assuming a dataset that is written to significantly more than its read so joining the fields back when queried isn't that big of a deal.

If so, would it be possible to write a program  that just does this basic level of scanning through a database and finds ways to optimize its storage",True,False,False,dataengineering,t5_36en4,45835,public,self,I've always been curious about this one thing regarding database normalization...,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r4nvpk/ive_always_been_curious_about_this_one_thing/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,maxell505,,,[],,,,text,t2_fixpx,False,False,False,[],False,False,1638158431,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r4mw09/current_data_analyst_looking_to_transition_into/,{},r4mw09,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,5,0,False,all_ads,/r/dataengineering/comments/r4mw09/current_data_analyst_looking_to_transition_into/,False,,,6,1638158446,1,"Hey guys, I'm currently a data analyst at a small company in the southeast region. Most of my work is done in excel or writing basic select statements to pull data. Sometimes I will dig through existing sql queries and edit the queries to get the data I need.   


That being said, I am looking for a higher salary and more interesting work. I would like to transition to data engineering but don't really know where to start. I have seen some roadmaps but don't know how I could really fit those in with my current state of knowledge. I know basic sql (T-SQL in SQL server) and I know VERY basic python.   


Do you guys recommend taking a Udacity course on data engineering or something similar? I like having a set curriculum or just a nice and in depth path that will allow me to have things to check off in a list. I know data engineering is pretty broad and there is no set way of learning the fundamentals but I'm sure there is something out there that would fit in with me right?",True,False,False,dataengineering,t5_36en4,45833,public,self,Current Data Analyst looking to transition into Data Engineer,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r4mw09/current_data_analyst_looking_to_transition_into/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,SatRipper,,,[],,,,text,t2_gi9h13c,False,False,False,[],False,False,1638152235,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r4kvde/junior_data_engineer_salary/,{},r4kvde,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,3,0,False,all_ads,/r/dataengineering/comments/r4kvde/junior_data_engineer_salary/,False,,,6,1638152246,1,I recently received a junior DE offer for 85k for remote work. I am graduating this year and this would be my first full time job. Is this a fair salary? What would be a good counter offer to make?,True,False,False,dataengineering,t5_36en4,45825,public,self,Junior Data Engineer salary,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r4kvde/junior_data_engineer_salary/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Scandalous_Andalous,,,[],,,,text,t2_3n5b9ant,False,False,False,[],False,False,1638134205,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r4em5p/interview_coming_up/,{},r4em5p,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,10,0,False,all_ads,/r/dataengineering/comments/r4em5p/interview_coming_up/,False,,,6,1638134216,1,"Hey all - I have an internal interview coming up for a Data Engineer job - I’m currently a data analyst in the same division etc.

We’re pretty immature data-wise e.g. have external consultants doing our data warehousing &amp; modelling etc. whilst our own BI Devs / Engineers are learning from them. These guys were just the traditional SQL gurus who’ve been put into the team. Don’t use Python or APIs or anything like that

Just wondering what tips would you guys have for the interview? Some things/concepts that might help me stand out.

Appreciate any help at all!",True,False,False,dataengineering,t5_36en4,45806,public,self,Interview coming up,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r4em5p/interview_coming_up/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,ora_600,,,[],,,,text,t2_9u62au14,False,False,False,[],False,False,1638132290,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r4dw99/how_prevalent_is_oracle_in_the_data_engineering/,{},r4dw99,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,12,0,False,all_ads,/r/dataengineering/comments/r4dw99/how_prevalent_is_oracle_in_the_data_engineering/,False,,,6,1638132301,1,"I have a lot of experience with on-premise Oracle databases in a (mainly legacy) enterprise environment, and I am wondering how much Oracle you guys are exposed to in your daily work?

Or has everyone moved to something like Snowflake or Postgres?",True,False,False,dataengineering,t5_36en4,45804,public,self,How prevalent is Oracle in the data engineering space?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r4dw99/how_prevalent_is_oracle_in_the_data_engineering/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,tekiN24,,,[],,,,text,t2_dq866ytm,False,False,False,[],False,False,1638131467,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r4dl6o/is_macbook_air_2020_m1_8_gb_enough_for_data/,{},r4dl6o,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,18,0,False,all_ads,/r/dataengineering/comments/r4dl6o/is_macbook_air_2020_m1_8_gb_enough_for_data/,False,,,6,1638131478,1,"Currently, I am a computer technologies student and I want to develop a career in data engineering. I started my journey with Lenovo and this week, I have encountered 3 blue screen errors and I am scared of getting things worse. I wanted to upgrade my laptop to mac but the currency difference between TL and USD hit me hard. 

Thus I am looking for entry-level MacBook air for myself but I am curious whether 8 gb ram is enough or not. Should I buy 16 gb? Maybe I can try learning photoshop, movie and premiere for the freelance job but this is just  an idea.  And this must last longer because it is so hard to buy a new computer in my country, so I want to use the laptop that I want to buy.

Thanks for your time. Have a nice day.",True,False,False,dataengineering,t5_36en4,45803,public,self,Is MacBook Air 2020 M1 8 GB enough for data engineering?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r4dl6o/is_macbook_air_2020_m1_8_gb_enough_for_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Scalar_Mikeman,,,[],,,,text,t2_io9vf,False,False,False,[],False,False,1638127272,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r4c1bk/how_does_normalization_increase_performance/,{},r4c1bk,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,9,0,False,all_ads,/r/dataengineering/comments/r4c1bk/how_does_normalization_increase_performance/,False,,,6,1638127282,1,"Reading up on Normalization vs Denormalization it keeps coming up that Normalizing your data reduces redundancy and increases performance. The reduces redundancy I understand, but the example in my mind is a web log and I don't see how Normalizing this data would increase performance. You may get location, IP Address, Date and Time, Page Requested, OS and Browser information from a weblog. If you Normalize this data you might make a Date Table, a Time Table, a Browser Table, and an OS Table. If you made these separate tables and made them foreign keys in the fact table (web log table) wouldn't that significantly SLOW down the performance of writing the data? You would need to parse the log, lookup the value for the date, time, OS and browser, create them if they don't exist and then construct a row for the fact table with the updated row containing the foreign keys. 

Could someone provide a concrete example of when/how Normalization speeds up performance?",True,False,False,dataengineering,t5_36en4,45796,public,self,How Does Normalization Increase Performance?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r4c1bk/how_does_normalization_increase_performance/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Greg_Z_,,,[],,,,text,t2_1z5jdh5h,False,False,False,[],False,False,1638121122,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r49ovk/combining_data_from_multiple_data_sources/,{},r49ovk,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,13,0,False,all_ads,/r/dataengineering/comments/r49ovk/combining_data_from_multiple_data_sources/,False,,,6,1638121133,1,"How often do you need to combine/merge heterogenous data fetched from REST API or loaded from files with the data retrieved from databases? What do you use for it? What are the challenges?

[View Poll](https://www.reddit.com/poll/r49ovk)",True,False,False,dataengineering,t5_36en4,45791,public,self,Combining data from multiple data sources,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r49ovk/combining_data_from_multiple_data_sources/,all_ads,6,,,,,,,,,,,,,"{'is_prediction': False, 'options': [{'id': '12134558', 'text': 'Never'}, {'id': '12134559', 'text': 'Rare'}, {'id': '12134560', 'text': 'Quite often'}, {'id': '12134561', 'text': 'Every day'}], 'prediction_status': None, 'resolved_option_id': None, 'total_stake_amount': None, 'total_vote_count': 0, 'tournament_id': None, 'user_selection': None, 'user_won_amount': None, 'vote_updates_remained': None, 'voting_end_timestamp': 1638380322632}",,,,,
[],False,intexAqua,,,[],,,,text,t2_9szvrwyl,False,False,False,[],False,False,1638120890,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r49lr5/i_am_looking_for_a_roadmap_on_getting_into_data/,{},r49lr5,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,52,0,False,all_ads,/r/dataengineering/comments/r49lr5/i_am_looking_for_a_roadmap_on_getting_into_data/,False,,,6,1638120902,1,"I know this roadmap https://www.reddit.com/r/TheInsaneApp/comments/pjt1le/data_engineering_roadmap/?utm_medium=android_app&amp;utm_source=share

It is just massive and I can't hope to do all of that in 1-2 years (I have a full time job too). It looks like a multi year experience check list. Can anyone share a shorter curated roadmap for getting into the field?",True,False,False,dataengineering,t5_36en4,45791,public,self,I am looking for a roadmap on getting into Data Engineering. I can't hope to follow the popular roadmap shared on this sub.,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r49lr5/i_am_looking_for_a_roadmap_on_getting_into_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,bubhrara,,,[],,,,text,t2_tchra,False,False,False,[],False,False,1638115372,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r47iyb/smart_solution_to_reduce_an_impractical_number_of/,{},r47iyb,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/r47iyb/smart_solution_to_reduce_an_impractical_number_of/,False,,,6,1638115383,1,"Hello folks,

I know this might not be the most appropriate sub for this discussion but no harm in trying, right?

We have a timeseries based ML solution that helps with anomaly detection and finding the root cause for this anomaly.

How it currently operates is, if I see an anomaly in a metric (if the actual is far from prediction), we try to examine the associated dimensions and associate the anomaly to its root cause accordingly.

For example, considering Covid related deaths across the US, if our model senses an anomaly (steep rise/fall), we try to break down the available data by state and see which state is responsible for such a steep change. In this example, there is only one dimension which is the state. So our model, in essence, iterates over 50 states - hence 50 iterations.

When we try to scale this up to multivariate that involves for example 5 dimensions (to the already existing ""State"", say we add gender, age group, income group, health rating), the number of iterations is going to explode. What was earlier 50, is now going to become,

50 States \* 4 Genders \* 10 Income groups \* 12 Health Ratings = 24,000 Iterations.

When such a model has to detect anomalies in say e-commerce data where the number of dimensions is going to be larger I hope you see how can't our model scale (at least without parallelization).

How do you guys think can we solve this? Any suggestion is greatly appreciated.

Thanks!",True,False,False,dataengineering,t5_36en4,45789,public,self,Smart solution to reduce an impractical number of iterations in a time-series model,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r47iyb/smart_solution_to_reduce_an_impractical_number_of/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,vananth22,,,[],,,,text,t2_10v76s,False,False,False,[],False,False,1638109136,dataengineeringweekly.com,https://www.reddit.com/r/dataengineering/comments/r45ajb/data_engineering_weekly_a_year_in_review_of_2021/,{},r45ajb,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/r45ajb/data_engineering_weekly_a_year_in_review_of_2021/,False,link,"{'enabled': False, 'images': [{'id': 'uqnyU4Y1Xpn88qdWZEh0DzDUMGQ3K7T8QZozfZDfnt4', 'resolutions': [{'height': 46, 'url': 'https://external-preview.redd.it/n8orOcmtF4QVXpTANmrsuidFR9VyjLttmEZ0G-t7f8o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c629ef5f87cea1a8eb34db9b67592a82b2a5d57c', 'width': 108}, {'height': 92, 'url': 'https://external-preview.redd.it/n8orOcmtF4QVXpTANmrsuidFR9VyjLttmEZ0G-t7f8o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f02f64d96725525e299b43a0343012e2597ef5f0', 'width': 216}, {'height': 137, 'url': 'https://external-preview.redd.it/n8orOcmtF4QVXpTANmrsuidFR9VyjLttmEZ0G-t7f8o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d98f803a8be412a82597baa502ed0b0b4744bcb6', 'width': 320}, {'height': 274, 'url': 'https://external-preview.redd.it/n8orOcmtF4QVXpTANmrsuidFR9VyjLttmEZ0G-t7f8o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=49658ef52bbe3ef61942899b0338bfd42a06444a', 'width': 640}, {'height': 412, 'url': 'https://external-preview.redd.it/n8orOcmtF4QVXpTANmrsuidFR9VyjLttmEZ0G-t7f8o.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=401039acd1a5c50bd145c748761477f67ad7a69a', 'width': 960}, {'height': 463, 'url': 'https://external-preview.redd.it/n8orOcmtF4QVXpTANmrsuidFR9VyjLttmEZ0G-t7f8o.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7b4131ef4232cc342366bbf37685ecacbfea1f8c', 'width': 1080}], 'source': {'height': 515, 'url': 'https://external-preview.redd.it/n8orOcmtF4QVXpTANmrsuidFR9VyjLttmEZ0G-t7f8o.jpg?auto=webp&amp;s=330a95566e5629e6bc5b8b5eac1aaca5fd14cd93', 'width': 1200}, 'variants': {}}]}",6,1638109147,1,,True,False,False,dataengineering,t5_36en4,45778,public,https://b.thumbs.redditmedia.com/Q_tkvFucdZO4MMYro5j3-_pKt3nd_3HLTohtPi1MrhU.jpg,Data Engineering Weekly - A Year in Review of 2021,0,[],1.0,https://www.dataengineeringweekly.com/p/data-engineering-weekly-a-year-in,all_ads,6,,,,,,60.0,140.0,https://www.dataengineeringweekly.com/p/data-engineering-weekly-a-year-in,,,,,,,,,,
[],False,Pristine_Ad_1411,,,[],,,,text,t2_gpanp3u4,False,False,False,[],False,False,1638103810,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r43mic/new_grad_struggling_with_a_large_migration_task/,{},r43mic,False,False,False,False,False,False,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r43mic/new_grad_struggling_with_a_large_migration_task/,False,self,"{'enabled': False, 'images': [{'id': 'vCeVk_w9nfTURsJ_iSrQgJkUr5kDzPPPw36VyaU-5-k', 'resolutions': [{'height': 192, 'url': 'https://external-preview.redd.it/HY67yeY0vWXJeQgt16aawvDeabTCbc7SfLmuGJ7uBos.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=200e22fd798c361a62da185869a04d2d487cd7d6', 'width': 108}, {'height': 385, 'url': 'https://external-preview.redd.it/HY67yeY0vWXJeQgt16aawvDeabTCbc7SfLmuGJ7uBos.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7c8d35419ad4e9ee6ed931c349b03704aae0b11e', 'width': 216}, {'height': 571, 'url': 'https://external-preview.redd.it/HY67yeY0vWXJeQgt16aawvDeabTCbc7SfLmuGJ7uBos.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4afd71a756edd1d2f1c86575e6d71e38451c9498', 'width': 320}], 'source': {'height': 913, 'url': 'https://external-preview.redd.it/HY67yeY0vWXJeQgt16aawvDeabTCbc7SfLmuGJ7uBos.jpg?auto=webp&amp;s=702cd0e6cf6cc3ba96c7c9362cdbeafbe089630c', 'width': 511}, 'variants': {}}]}",6,1638103821,1,[removed],True,False,False,dataengineering,t5_36en4,45775,public,self,New grad struggling with a large migration task,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r43mic/new_grad_struggling_with_a_large_migration_task/,all_ads,6,,,automod_filtered,,,,,,,,,,,,,,,
[],False,izner82,,,[],,,,text,t2_67s1slvl,False,False,False,[],False,False,1638082006,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r3y3qv/how_does_data_looks_like_when_stored_in_a/,{},r3y3qv,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/r3y3qv/how_does_data_looks_like_when_stored_in_a/,False,,,6,1638082017,1,"    Given the table: Bonuses
        ID         Last    First   Bonus
        1          Doe     John    8000
        2          Smith   Jane    4000
        3          Beck    Sam     1000
    
    row oriented database(e.g. postgresql):
        1,Doe,John,8000;2,Smith,Jane,4000;3,Beck,Sam,1000;
    
    column oriented database(e.g. mariadb):
        1,2,3;Doe,Smith,Beck;John,Jane,Sam;8000,4000,1000;
    
    column family database(e.g. cassandra):
        ""Bonuses"" : {
            row1 : { ""ID"":1, ""Last"":""Doe"", ""First"":""John"", ""Bonus"":8000},
            row2 : { ""ID"":2, ""Last"":""Smith"", ""First"":""Jane"", ""Bonus"":4000},
            row3 : { ""ID"":3, ""Last"":""Beck"", ""First"":""Sam"", ""Bonus"":1000}
        }
    
    document based database (e.g. mongodb):
        document1  
            { 
                ""ID"": 1,
                ""Last"": ""Doe"",
                ""First"": ""John"",
                ""Bonus"": 8000
            }
        document2
            {
                ""ID"": 2,
                ""Last"": ""Smith"",
                ""First"": ""Jane"",
                ""Bonus"": 4000
            } 
        document3
            {
                ""ID"": 3,
                ""Last"": ""Beck"",
                ""First"": ""Sam"",
                ""Bonus"": 1000
            }
    
    graph database(e.g. neo4j): ???
    
    key-value database(e.g. redis: ???",True,False,False,dataengineering,t5_36en4,45760,public,self,How does data looks like when stored in a key-value and graph nosql databases,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r3y3qv/how_does_data_looks_like_when_stored_in_a/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,tallbrownglasses,,,[],,,,text,t2_f3w5u04o,False,False,False,[],False,False,1638076442,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r3wkeh/anyone_have_sqlpad_membership/,{},r3wkeh,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r3wkeh/anyone_have_sqlpad_membership/,False,,,6,1638076453,1,"Hey Greetings everyone, any one of you have SQLPad membership so that we could share.",True,False,False,dataengineering,t5_36en4,45757,public,self,anyone have SQLPad membership?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r3wkeh/anyone_have_sqlpad_membership/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Koushik5586,,,[],,,,text,t2_wqct3,False,False,False,[],False,False,1638075923,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r3wf43/data_engineering_interview_prep/,{},r3wf43,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,17,0,False,all_ads,/r/dataengineering/comments/r3wf43/data_engineering_interview_prep/,False,,,6,1638075934,1,I am planning to take interview to switch to a better company and i wanted to clarify one thing. Does Data structures and algorithms have more weightage in a data engineering interview similar to a SDE role or is it more focused in SQL and good programming skills so that i can focus more on sql and data warehousing rather than DSA?,True,False,False,dataengineering,t5_36en4,45756,public,self,Data Engineering Interview Prep,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r3wf43/data_engineering_interview_prep/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Ok_Region_422,,,[],,,,text,t2_98t49uxi,False,False,False,[],False,False,1638070380,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r3us0x/any_data_engineers_who_work_in_florida/,{},r3us0x,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,5,0,False,all_ads,/r/dataengineering/comments/r3us0x/any_data_engineers_who_work_in_florida/,False,,,6,1638070391,1,"wanted to know your view on the market in florida, if you like what you do, and what's your pay if you don't mind sharing?",True,False,False,dataengineering,t5_36en4,45750,public,self,Any data engineers who work in florida?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r3us0x/any_data_engineers_who_work_in_florida/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Pack_Otherwise,,,[],,,,text,t2_6cy1x7a8,False,False,False,[],False,False,1638063354,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r3si7k/snowflake_stored_procedure_vs_tasks/,{},r3si7k,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,10,0,False,all_ads,/r/dataengineering/comments/r3si7k/snowflake_stored_procedure_vs_tasks/,False,,,6,1638063365,1,Wondering what the best practice is for scheduling operations in snowflake. Should I create a single stored procedure that performs multiple operations and schedule it with a task? Or should I break it up into smaller pieces and chain them together with multiple tasks? I don’t have DBT or airflow so I need to find a snowflake native solution,True,False,False,dataengineering,t5_36en4,45742,public,self,Snowflake stored procedure vs tasks,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r3si7k/snowflake_stored_procedure_vs_tasks/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,tigermatos,,,[],,,,text,t2_gxkcvrzk,False,False,False,[],False,False,1638051493,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r3oggi/looking_for_data_hobbyistsresearchersevangelists/,{},r3oggi,False,False,False,False,False,False,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r3oggi/looking_for_data_hobbyistsresearchersevangelists/,False,,,6,1638051504,1,[removed],True,False,False,dataengineering,t5_36en4,45737,public,self,Looking for data hobbyists/researchers/evangelists to try a new stream analytics tool,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r3oggi/looking_for_data_hobbyistsresearchersevangelists/,all_ads,6,,,automod_filtered,,,,,,,,,,,,,,,
[],False,AmritaOS,,,[],,,,text,t2_gwyx0bqa,False,False,False,[],False,False,1638050659,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r3o693/tips_az900_and_dp900/,{},r3o693,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,3,0,False,all_ads,/r/dataengineering/comments/r3o693/tips_az900_and_dp900/,False,,,6,1638050669,1,"Hi everyone. I’m planning on doing these certifications. I have a few questions and am looking for advise! 
Will studying the MS documentation be enough? 
Will these help me increase my chances of getting a job as a BI developer? Data Analyst? Data engineer?
Thanks looking for any tips!",True,False,False,dataengineering,t5_36en4,45735,public,self,Tips - AZ-900 and DP-900,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r3o693/tips_az900_and_dp900/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,mister_patience,,,[],,,,text,t2_xt5zb,False,False,False,[],False,False,1638042470,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r3lao0/post_dp203_azure_project/,{},r3lao0,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,7,0,False,all_ads,/r/dataengineering/comments/r3lao0/post_dp203_azure_project/,False,,,6,1638042481,1,"What would be a good project to engage with post DP203? I'd like to sink my teeth into something that could take a few days or even up to a week. Any thoughts here would be fantastic, or specific projects/courses I could buy that would take me through something significant would be fab.",True,False,False,dataengineering,t5_36en4,45725,public,self,Post DP203 Azure Project,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r3lao0/post_dp203_azure_project/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,NerdWannabe01,,,[],,,,text,t2_eh46d1ia,False,False,False,[],False,False,1638038218,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r3jrwr/best_data_engineer_courses_2021/,{},r3jrwr,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/r3jrwr/best_data_engineer_courses_2021/,False,,,6,1638038230,1,"Hi everyone,

So I am a recent master's graduate in Business Analytics, where 80% of my academic projects heavily relied on Python/ ML, and the other 20% relied on SQL. Over time, I fell in love with the coding aspect of data science.

After 2 months of struggling to find a Data Analyst job, I got myself a position at a start-up firm. However, the reality hit me. While the job description mentioned lots of SQL and Python, I still have to do lots of Excel tasks and have limited chances to work with Python/ SQL.

Therefore, recently, I have been looking into Data Engineering roles as well as their requirements. Unfortunately, I became overwhelmed by the number of tools and concepts that I need to learn. Its not that I feel lazy or anything, its just that I do not know where to start/ the correct learning path. Therefore, I strongly believe that having good courses can get me off to a good start.

I have been researching and found some courses such as ""Data Engineer Nanodegree"" or ""Preparing for Google Cloud Certification: Cloud Data Engineer Professional Certificate"". If you have done these courses, would you still recommend me do these courses in 2022? What about other online courses that you have found helpful for beginners?

Thank you for reading my long-ass essay!!!!",True,False,False,dataengineering,t5_36en4,45717,public,self,Best Data Engineer Courses 2021!,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r3jrwr/best_data_engineer_courses_2021/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,28percentbattery,,,[],,,,text,t2_kgulr,False,False,False,[],False,False,1638034273,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r3icjv/skills_to_learn_before_my_data_engineering/,{},r3icjv,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,8,0,False,all_ads,/r/dataengineering/comments/r3icjv/skills_to_learn_before_my_data_engineering/,False,,,6,1638034284,1,"Hi,

I have just been made an offer for a data engineering graduate scheme in the UK, and I was wondering what some things I should focus on/ get to grips with to get a bit of a head start?

Cheers",True,False,False,dataengineering,t5_36en4,45710,public,self,Skills to learn before my data engineering graduate scheme starts in ~3 months?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r3icjv/skills_to_learn_before_my_data_engineering/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Fragrant-Lobster4276,,,[],,,,text,t2_7wcm51fu,False,False,False,[],False,False,1638028385,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r3ganj/rule_based_categorisation_on_free_text_fields_in/,{},r3ganj,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/r3ganj/rule_based_categorisation_on_free_text_fields_in/,False,,,6,1638028397,1,"Working on a scebario where I need to categorise free text fields into n categories based on the presence of certain key words for each category
The key words will belong to [a-zA-Z]

So if free text contains a key word from group 1 then label 1
And so on..

The DA in the draft implememtation /requirement has implemented this with a nested case statement with more nested like string matches

Apart from being inefficient from a performance perspective
This also makes the design very inflexible if in future morr key words are added



My approach considering spark as the processing engine
Is first transform the free text field to individual words using a regextokenizer and strip away numeric characters and append a new column to the dataframe1

Further for the category groups create a dataframe2 with the key words stored in an array type that will be broadcast joined with the dataframe1


And then use an array_intersect between the two columns to find a match
Each category will correspond to another case statement in the spark sql , but the list of category key words will be abstracted to a separate reference file

This will allow easy modification of the keywords in the refernce files

Any suggestions or alternate approached for effectively performing  pattern matching using spark(pyspark preferable)?",True,False,False,dataengineering,t5_36en4,45702,public,self,Rule based categorisation on free text fields in spark,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r3ganj/rule_based_categorisation_on_free_text_fields_in/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,eemamedo,,,[],,,,text,t2_o0s8m,False,False,False,[],False,False,1638026955,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r3ftia/clarification_on_uber_data_infrastructure/,{},r3ftia,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,2,0,False,all_ads,/r/dataengineering/comments/r3ftia/clarification_on_uber_data_infrastructure/,False,self,"{'enabled': False, 'images': [{'id': 'Odoxr0IusUBgx0XkmQvEQ7kj8HkiLtPaaYObi4CJxjw', 'resolutions': [{'height': 60, 'url': 'https://external-preview.redd.it/2nZZiXI1N-vlKRhSJtpK_m0LbfyP4iwJxb0DOLYmNqc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3617d329e93d054996a54432db24eb02bf0b0cf1', 'width': 108}, {'height': 121, 'url': 'https://external-preview.redd.it/2nZZiXI1N-vlKRhSJtpK_m0LbfyP4iwJxb0DOLYmNqc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=77a9f6762968ce8d0d9dfd3a237d16f1c7857d84', 'width': 216}, {'height': 180, 'url': 'https://external-preview.redd.it/2nZZiXI1N-vlKRhSJtpK_m0LbfyP4iwJxb0DOLYmNqc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c102ca26b96d892bf014e09b0fdbc0723b414c7b', 'width': 320}, {'height': 360, 'url': 'https://external-preview.redd.it/2nZZiXI1N-vlKRhSJtpK_m0LbfyP4iwJxb0DOLYmNqc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d54432bdec28ee7cd082b4184f36e28c36a09c44', 'width': 640}, {'height': 540, 'url': 'https://external-preview.redd.it/2nZZiXI1N-vlKRhSJtpK_m0LbfyP4iwJxb0DOLYmNqc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6b65e8942235129ba4936d6cbf16dfa8a60b19ee', 'width': 960}, {'height': 607, 'url': 'https://external-preview.redd.it/2nZZiXI1N-vlKRhSJtpK_m0LbfyP4iwJxb0DOLYmNqc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8a26158970d91b64e40c18035004fa61b939c571', 'width': 1080}], 'source': {'height': 1080, 'url': 'https://external-preview.redd.it/2nZZiXI1N-vlKRhSJtpK_m0LbfyP4iwJxb0DOLYmNqc.jpg?auto=webp&amp;s=08d8b753f678641b6cd68cf30eb5eafd6a057a64', 'width': 1920}, 'variants': {}}]}",6,1638026966,1,"I was reading this article from Uber: https://eng.uber.com/uber-big-data-platform/ and I don't understand one part:

While moving ETL and modeling into Hadoop made this process more scalable, these steps were still bottlenecks since these ETL jobs had to recreate the entire modeled table in every run. Adding to the problem, both ingestion of the new data and modeling of the related derived table were based on creating new snapshots of the entire dataset and swapping the old and new tables to provide users with access to fresh data.

The part I don't understand is that why did they need to recreate the entire modeled table with every run? They have schema; why not just append to existing table? 

2nd question is related: Why do they need to swap the old and new table instead of using partitions with respect to time where time will indicate the table that data was loaded into DB?",True,False,False,dataengineering,t5_36en4,45698,public,self,Clarification on Uber Data Infrastructure description,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r3ftia/clarification_on_uber_data_infrastructure/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,shanik1986,,,[],,,,text,t2_12r9etm,False,False,False,[],False,False,1638026214,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r3fkkv/looking_for_experienced_data_engineers_for_a_45/,{},r3fkkv,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r3fkkv/looking_for_experienced_data_engineers_for_a_45/,False,self,"{'enabled': False, 'images': [{'id': 'uWsORwGjWdSGsuygKZBpTk03y7qmfT8GL4_NFB7T0Do', 'resolutions': [{'height': 56, 'url': 'https://external-preview.redd.it/oSqpJqKaFvbcJQWNxeU65KAcjxOI-jTVkzS2UkGtQzo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6fc93ae083ba8447c52cff011a9cc263c7aad694', 'width': 108}, {'height': 113, 'url': 'https://external-preview.redd.it/oSqpJqKaFvbcJQWNxeU65KAcjxOI-jTVkzS2UkGtQzo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3bac9d6c21ab6b14f0ad1992c257fab13934eb7c', 'width': 216}, {'height': 168, 'url': 'https://external-preview.redd.it/oSqpJqKaFvbcJQWNxeU65KAcjxOI-jTVkzS2UkGtQzo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=12a2dac5dec9f3e4da7d7e40781ede35b1d287ac', 'width': 320}, {'height': 336, 'url': 'https://external-preview.redd.it/oSqpJqKaFvbcJQWNxeU65KAcjxOI-jTVkzS2UkGtQzo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=dce2e9147458a5c22a70cf24d75ced7bcdaff6ac', 'width': 640}, {'height': 504, 'url': 'https://external-preview.redd.it/oSqpJqKaFvbcJQWNxeU65KAcjxOI-jTVkzS2UkGtQzo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4c95daa11c17470b131849d4fc41b6bd604a0532', 'width': 960}, {'height': 567, 'url': 'https://external-preview.redd.it/oSqpJqKaFvbcJQWNxeU65KAcjxOI-jTVkzS2UkGtQzo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=eb0965fa31fcd6198b0bec5111f1536d927c814f', 'width': 1080}], 'source': {'height': 945, 'url': 'https://external-preview.redd.it/oSqpJqKaFvbcJQWNxeU65KAcjxOI-jTVkzS2UkGtQzo.jpg?auto=webp&amp;s=c45a9aaabb8d5f2b8e2c1d4e524b8017a41845f7', 'width': 1800}, 'variants': {}}]}",6,1638026225,1,[https://www.userinterviews.com/projects/fSlm7OekIQ/apply](https://www.userinterviews.com/projects/fSlm7OekIQ/apply),True,False,False,dataengineering,t5_36en4,45698,public,self,Looking for experienced Data Engineers for a 45 minute product interview on DE best practices- 100$ in Amazon Gift Cards,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r3fkkv/looking_for_experienced_data_engineers_for_a_45/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,ampankajsharma,,,[],,,,text,t2_6en6384s,False,False,False,[],False,False,1638025836,tryblackfriday.com,https://www.reddit.com/r/dataengineering/comments/r3fg97/get_access_to_3000_courses_on_coursera_for_1/,{},r3fg97,False,True,False,False,False,True,False,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r3fg97/get_access_to_3000_courses_on_coursera_for_1/,False,link,"{'enabled': False, 'images': [{'id': 'Ki6VrrRcSE_fsmyAiZAczseQWjiN8rcO7BWU4pfTOpo', 'resolutions': [{'height': 60, 'url': 'https://external-preview.redd.it/5xygftyf4I6GAIVPnHFNvxARw5iHON3JB63wmSsOaI8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=200f0ad215cba2add8e68f3b51dcb5d377ed483e', 'width': 108}, {'height': 120, 'url': 'https://external-preview.redd.it/5xygftyf4I6GAIVPnHFNvxARw5iHON3JB63wmSsOaI8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=77c8dd5dd5c895f0d7dc1872668a4251ada7d0bc', 'width': 216}, {'height': 178, 'url': 'https://external-preview.redd.it/5xygftyf4I6GAIVPnHFNvxARw5iHON3JB63wmSsOaI8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ae42bd4fb9dca0c540dd4a5b631ab4c48a43e58e', 'width': 320}, {'height': 356, 'url': 'https://external-preview.redd.it/5xygftyf4I6GAIVPnHFNvxARw5iHON3JB63wmSsOaI8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6cb81f9af3766a42e19268f01fac9708b202f50a', 'width': 640}], 'source': {'height': 445, 'url': 'https://external-preview.redd.it/5xygftyf4I6GAIVPnHFNvxARw5iHON3JB63wmSsOaI8.jpg?auto=webp&amp;s=3cbe6d5f5a741abb45ebed3c1bb7603d9924f6a4', 'width': 800}, 'variants': {}}]}",6,1638025847,1,,True,False,False,dataengineering,t5_36en4,45697,public,https://a.thumbs.redditmedia.com/rJhlBdAusFKrbF_HHeP74K2c0Ux_-wkXox_1xGBvOn4.jpg,"Get access to 3,000+ courses on Coursera for $1 [Limited-time offer]",0,[],1.0,https://www.tryblackfriday.com/2021/11/coursera-plus-subscription-offer.html,all_ads,6,,,,,,77.0,140.0,https://www.tryblackfriday.com/2021/11/coursera-plus-subscription-offer.html,,,,,,,,,,
[],False,Bubbly_Cook_4690,,,[],,,,text,t2_arhpv8tl,False,False,False,[],False,False,1638019220,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r3df55/data_engineering_course_recomendation/,{},r3df55,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,6,0,False,all_ads,/r/dataengineering/comments/r3df55/data_engineering_course_recomendation/,False,,,6,1638019231,1,I have the basics of SQL and have worked a bit in Python. I would like to learn more into the Data Engineering. I am looking for a comprehensive course that would teach me the basics as well as more advanced issues. As there are Black Friday promotions now - what course do you currently advise?,True,False,False,dataengineering,t5_36en4,45691,public,self,Data Engineering Course Recomendation,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r3df55/data_engineering_course_recomendation/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,izner82,,,[],,,,text,t2_67s1slvl,False,False,False,[],False,False,1638012771,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r3bskh/why_doesnt_nosql_and_data_warehousing_doesnt_go/,{},r3bskh,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,19,0,False,all_ads,/r/dataengineering/comments/r3bskh/why_doesnt_nosql_and_data_warehousing_doesnt_go/,False,,,6,1638012782,1,"For OLTPs i have a few criterias in determining whether to use SQL or NoSQL, e.g. for structured data + acid compliant + small to medium data use SQL, for unstructured/semi-structured data + big data use NoSQL.

But for data warehousing, I don't understand why almost all companies are using SQL, what could be the reason behind it?",True,False,False,dataengineering,t5_36en4,45685,public,self,Why doesn't NoSQL and Data Warehousing doesn't go hand in hand?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r3bskh/why_doesnt_nosql_and_data_warehousing_doesnt_go/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Professional_End_979,,,[],,,,text,t2_a6t9ksvy,False,False,False,[],False,False,1638010856,robertsahlin.com,https://www.reddit.com/r/dataengineering/comments/r3bbs8/serverless_dbt_on_google_cloud_platform/,{},r3bbs8,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r3bbs8/serverless_dbt_on_google_cloud_platform/,False,,,6,1638010867,1,,False,False,False,dataengineering,t5_36en4,45684,public,default,Serverless dbt on Google Cloud Platform,0,[],1.0,https://robertsahlin.com/serverless-dbt-on-google-cloud-platform/,all_ads,6,,,,,,,,https://robertsahlin.com/serverless-dbt-on-google-cloud-platform/,,,,,,,,,,
[],False,oneequalsequalsone,,,[],,,,text,t2_ghfjevxv,False,False,False,[],False,False,1638008753,i.redd.it,https://www.reddit.com/r/dataengineering/comments/r3au4g/reasonable_experience_requirement/,{},r3au4g,False,True,False,False,True,True,False,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,23,0,False,all_ads,/r/dataengineering/comments/r3au4g/reasonable_experience_requirement/,False,image,"{'enabled': True, 'images': [{'id': 'WQxpnY6K4-V0WtFViAMSm_LfpZ9lvYUIbMz-AkVbvjk', 'resolutions': [{'height': 51, 'url': 'https://preview.redd.it/bkkmrpfh74281.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=765c464bf39657017a15787fa2cd7480b8a6f62b', 'width': 108}, {'height': 102, 'url': 'https://preview.redd.it/bkkmrpfh74281.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a18a7fd81f1758d4a20b45e9ece8f43dfed425a8', 'width': 216}, {'height': 151, 'url': 'https://preview.redd.it/bkkmrpfh74281.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=346316813e4a946b9becc72b914f346f622ef813', 'width': 320}, {'height': 303, 'url': 'https://preview.redd.it/bkkmrpfh74281.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=aea4dd186fa728f129ec2fb6710677fa7cf44233', 'width': 640}], 'source': {'height': 356, 'url': 'https://preview.redd.it/bkkmrpfh74281.jpg?auto=webp&amp;s=6c691b0ea81d5f5c055fcbe76a0c4921d5622a85', 'width': 750}, 'variants': {}}]}",6,1638008764,1,,True,False,False,dataengineering,t5_36en4,45683,public,https://b.thumbs.redditmedia.com/pubu_YM4-gOaOToRtN89ACSNSWL0FaQBhybEKRYOkJg.jpg,Reasonable experience requirement,0,[],1.0,https://i.redd.it/bkkmrpfh74281.jpg,all_ads,6,,,,,,66.0,140.0,https://i.redd.it/bkkmrpfh74281.jpg,,,,,,,,,,
[],False,GLTBR,,,[],,,,text,t2_558xhm29,False,False,False,[],False,False,1637999938,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r38qds/scaling_airflow_with_a_celery_cluster_using/,{},r38qds,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,7,0,False,all_ads,/r/dataengineering/comments/r38qds/scaling_airflow_with_a_celery_cluster_using/,False,,,6,1637999949,1,"

As the title says, i want to setup Airflow that would run on a cluster (1 master, 2 nodes) using Docker swarm.

Current setup:

Right now i have Airflow setup that uses the CeleryExecutor that is running on single EC2. I have a Dockerfile that pulls Airflow's image and `pip install -r requirements.txt`  
. From this Dockerfile I'm creating a local image and this image is used  in the docker-compose.yml that spins up the different services Airflow  need (webserver, scheduler, redis, flower and some worker. metadb is  Postgres that is on a separate RDS). The docker-compose is used in docker swarm mode ie. `docker stack deploy . airflow_stack`

Required Setup:

I want to scale the current setup to 3 EC2s (1 master, 2 nodes) that  the master would run the webserver, schedule, redis and flower and the  workers would run in the nodes. After searching and web and docs, there are a few things that are still  not clear to me that I would love to know

&amp;#x200B;

1. from what i understand, in order for the nodes to run the workers,  the local image that I'm building from the Dockerfile need to be pushed  to some repository (if it's really needed, i would use AWS ECR) for the  airflow workers to be able to create the containers from that image. is  that correct?
2. syncing volumes and env files, right now, I'm mounting the volumes  and insert the envs in the docker-compose file. would these mounts and  envs be synced to the nodes (and airflow workers containers)? if not,  how can make sure that everything is sync as airflow requires that all  the components (apart from redis) would have all the dependencies, etc.
3. one of the envs that needs to be set when using a CeleryExecuter is  the broker\_url, how can i make sure that the nodes recognize the redis  broker that is on the master

I'm sure that there are a few more things that i forget, but what i wrote is a good start. Any help or recommendation would be greatly appreciated

Thanks!

Dockerfile:

    FROM apache/airflow:2.1.3-python3.9
    USER root
    RUN apt update;
    RUN apt -y install build-essential;
    USER airflow
    COPY requirements.txt requirements.txt
    COPY requirements.airflow.txt requirements.airflow.txt
    RUN pip install --upgrade pip;
    RUN pip install --upgrade wheel;
    RUN pip install -r requirements.airflow.txt
    RUN pip install -r requirements.txt
    EXPOSE 8793 8786 8787

docker-compose.yml:

    version: '3.8'
    x-airflow-celery: &amp;airflow-celery
      image: local_image:latest
      volumes:
        -some_volume
      env_file:
        -some_env_file
    
    services:
      webserver:
        &lt;&lt;: *airflow-celery
        command: airflow webserver
        restart: always
        ports:
          - 80:8080
        healthcheck:
          test: [ ""CMD-SHELL"", ""[ -f /opt/airflow/airflow-webserver.pid ]"" ]
          interval: 10s
          timeout: 30s
          retries: 3
    
      scheduler:
        &lt;&lt;: *airflow-celery
        command: airflow scheduler
        restart: always
        deploy:
          replicas: 2
    
      redis:
        image: redis:6.0
        command: redis-server --include /redis.conf
        healthcheck:
          test: [ ""CMD"", ""redis-cli"", ""ping"" ]
          interval: 30s
          timeout: 10s
          retries: 5
        ports:
          - 6379:6379
        environment:
          - REDIS_PORT=6379
    
      worker:
        &lt;&lt;: *airflow-celery
        command: airflow celery worker
        deploy:
          replicas: 16
    
      flower:
        &lt;&lt;: *airflow-celery
        command: airflow celery flower
        ports:
          - 5555:5555",True,False,False,dataengineering,t5_36en4,45676,public,self,Scaling Airflow with a Celery cluster using Docker swarm,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r38qds/scaling_airflow_with_a_celery_cluster_using/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,GLTBR,,,[],,,,text,t2_558xhm29,False,False,False,[],False,False,1637999418,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r38lu0/scaling_airflow_with_a_celery_cluster_using/,{},r38lu0,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r38lu0/scaling_airflow_with_a_celery_cluster_using/,False,,,6,1637999430,1,"As the title says, i want to setup Airflow that would run on a cluster (1 master, 2 nodes) using Docker swarm.

Current setup:

Right now i have Airflow setup that uses the CeleryExecutor that is running on single EC2. I have a Dockerfile that pulls Airflow's image and pip install -r requirements.txt. From this Dockerfile I'm creating a local image and this image is used in the docker-compose.yml that spins up the different services Airflow need (webserver, scheduler, redis, flower and some worker. metadb is Postgres that is on a separate RDS). The docker-compose is used in docker swarm mode ie. docker stack deploy . airflow_stack

Required Setup:

I want to scale the current setup to 3 EC2s (1 master, 2 nodes) that the master would run the webserver, schedule, redis and flower and the workers would run in the nodes. After searching and web and docs, there are a few things that are still not clear to me that I would love to know

from what i understand, in order for the nodes to run the workers, the local image that I'm building from the Dockerfile need to be pushed to some repository (if it's really needed, i would use AWS ECR) for the airflow workers to be able to create the containers from that image. is that correct?
syncing volumes and env files, right now, I'm mounting the volumes and insert the envs in the docker-compose file. would these mounts and envs be synced to the nodes (and airflow workers containers)? if not, how can make sure that everything is sync as airflow requires that all the components (apart from redis) would have all the dependencies, etc.
one of the envs that needs to be set when using a CeleryExecuter is the broker_url, how can i make sure that the nodes recognize the redis broker that is on the master
I'm sure that there are a few more things that i forget, but what i wrote is a good start. Any help or recommendation would be greatly appreciated

Thanks!

Dockerfile:
``
FROM apache/airflow:2.1.3-python3.9
USER root


RUN apt update;
RUN apt -y install build-essential;

USER airflow
COPY requirements.txt requirements.txt
COPY requirements.airflow.txt requirements.airflow.txt

RUN pip install --upgrade pip;
RUN pip install --upgrade wheel;

RUN pip install -r requirements.airflow.txt
RUN pip install -r requirements.txt


EXPOSE 8793 8786 8787
``

docker-compose.yml:
``
version: '3.8'
x-airflow-celery: &amp;airflow-celery
  image: local_image:latest
  volumes:
    -some_volume
  env_file:
    -some_env_file

services:
  webserver:
    &lt;&lt;: *airflow-celery
    command: airflow webserver
    restart: always
    ports:
      - 80:8080
    healthcheck:
      test: [ ""CMD-SHELL"", ""[ -f /opt/airflow/airflow-webserver.pid ]"" ]
      interval: 10s
      timeout: 30s
      retries: 3

  scheduler:
    &lt;&lt;: *airflow-celery
    command: airflow scheduler
    restart: always
    deploy:
      replicas: 2

  redis:
    image: redis:6.0
    command: redis-server --include /redis.conf
    healthcheck:
      test: [ ""CMD"", ""redis-cli"", ""ping"" ]
      interval: 30s
      timeout: 10s
      retries: 5
    ports:
      - 6379:6379
    environment:
      - REDIS_PORT=6379

  worker:
    &lt;&lt;: *airflow-celery
    command: airflow celery worker
    deploy:
      replicas: 16

  flower:
    &lt;&lt;: *airflow-celery
    command: airflow celery flower
    ports:
      - 5555:5555
``",True,False,False,dataengineering,t5_36en4,45676,public,self,Scaling Airflow with a Celery cluster using Docker swarm,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r38lu0/scaling_airflow_with_a_celery_cluster_using/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,ProfessionalAd499,,,[],,,,text,t2_8xg1y446,False,False,False,[],False,False,1637968271,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r2zakk/beginner_question_de_with_focus_on_hadoop/,{},r2zakk,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,14,0,False,all_ads,/r/dataengineering/comments/r2zakk/beginner_question_de_with_focus_on_hadoop/,False,,,6,1637968282,1,"Hey :)

I don't know much about DE yet and pretty much stumbled into this path. I have tons of questions but would start with the (imo) most important one:

Does starting a DE career focused on Linux and Hadoop/Ansible/SQL provide a reasonable foundation?

Thank you so much!",False,False,False,dataengineering,t5_36en4,45662,public,self,Beginner Question: DE with focus on Hadoop?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r2zakk/beginner_question_de_with_focus_on_hadoop/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,XaviRoss93,,,[],,,,text,t2_fs9e23hr,False,False,False,[],False,False,1637966232,youtu.be,https://www.reddit.com/r/dataengineering/comments/r2ymez/ex_mckinsey_manager_explains_the_most_valuable/,{},r2ymez,False,True,False,False,False,True,False,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/r2ymez/ex_mckinsey_manager_explains_the_most_valuable/,False,rich:video,"{'enabled': False, 'images': [{'id': 'f-y-nc__087ToVJ2tpUbx-WFW_AOD7znqHTJZWopwL8', 'resolutions': [{'height': 81, 'url': 'https://external-preview.redd.it/wq_11e57Kb_aI1wK5ZQyWqgJ7FGlko4ih9_Z7o5KCzs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d53c93ffce8e1206c8cce349de3a19dd21e56da0', 'width': 108}, {'height': 162, 'url': 'https://external-preview.redd.it/wq_11e57Kb_aI1wK5ZQyWqgJ7FGlko4ih9_Z7o5KCzs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f32838152a192458be3ffb41175ce16a4e403ef5', 'width': 216}, {'height': 240, 'url': 'https://external-preview.redd.it/wq_11e57Kb_aI1wK5ZQyWqgJ7FGlko4ih9_Z7o5KCzs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=542e613a8289f6c392621bef24683b88367dc304', 'width': 320}], 'source': {'height': 360, 'url': 'https://external-preview.redd.it/wq_11e57Kb_aI1wK5ZQyWqgJ7FGlko4ih9_Z7o5KCzs.jpg?auto=webp&amp;s=56709e449912b46406ccb60dc6f5ed8acbd3290a', 'width': 480}, 'variants': {}}]}",6,1637966244,1,,True,False,False,dataengineering,t5_36en4,45660,public,https://a.thumbs.redditmedia.com/GsKwGoer8h2fZsXlaDdOGJ2u82XJJ3-S97MdfhszJ-8.jpg,Ex McKinsey manager explains the most valuable skills for management consulting,0,[],1.0,https://youtu.be/_-8qZaoUHt4,all_ads,6,"{'oembed': {'author_name': 'HUSTLE HUB', 'author_url': 'https://www.youtube.com/c/HUSTLEHUB-CHANNEL', 'height': 200, 'html': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/_-8qZaoUHt4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/_-8qZaoUHt4/hqdefault.jpg', 'thumbnail_width': 480, 'title': 'Ex McKinsey manager explains the most valuable skills for management consulting w/ @YOUinConsulting', 'type': 'video', 'version': '1.0', 'width': 267}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/_-8qZaoUHt4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'scrolling': False, 'width': 267}",,"{'oembed': {'author_name': 'HUSTLE HUB', 'author_url': 'https://www.youtube.com/c/HUSTLEHUB-CHANNEL', 'height': 200, 'html': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/_-8qZaoUHt4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/_-8qZaoUHt4/hqdefault.jpg', 'thumbnail_width': 480, 'title': 'Ex McKinsey manager explains the most valuable skills for management consulting w/ @YOUinConsulting', 'type': 'video', 'version': '1.0', 'width': 267}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""267"" height=""200"" src=""https://www.youtube.com/embed/_-8qZaoUHt4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/r2ymez', 'scrolling': False, 'width': 267}",105.0,140.0,https://youtu.be/_-8qZaoUHt4,,,,,,,,,,
[],False,TheFreeloader,,,[],,,,text,t2_6kimw,False,False,False,[],False,False,1637958896,youtu.be,https://www.reddit.com/r/dataengineering/comments/r2w6tb/why_data_engineering_is_so_hot_right_now/,{},r2w6tb,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r2w6tb/why_data_engineering_is_so_hot_right_now/,False,rich:video,"{'enabled': False, 'images': [{'id': 'S2BVO-LdTfZ_9ZWpMKjBZwYB_E6GHVu7o6Y4P6IUkOU', 'resolutions': [{'height': 81, 'url': 'https://external-preview.redd.it/MIvk7XKPyTi4pAaBlENGtKDqW1mNalv2BmVwjQoW4-Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ff341bf943c27fd2a3c1a658cd1a8987066b56ef', 'width': 108}, {'height': 162, 'url': 'https://external-preview.redd.it/MIvk7XKPyTi4pAaBlENGtKDqW1mNalv2BmVwjQoW4-Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5f6e9c498cf620f6744f316938363fbcdbec768f', 'width': 216}, {'height': 240, 'url': 'https://external-preview.redd.it/MIvk7XKPyTi4pAaBlENGtKDqW1mNalv2BmVwjQoW4-Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=94c5de951929d7a69f14ecb877c187b180be26ea', 'width': 320}], 'source': {'height': 360, 'url': 'https://external-preview.redd.it/MIvk7XKPyTi4pAaBlENGtKDqW1mNalv2BmVwjQoW4-Y.jpg?auto=webp&amp;s=2d2287204067ae94aec4c8d341fe229910a9a809', 'width': 480}, 'variants': {}}]}",6,1637958907,1,,True,False,False,dataengineering,t5_36en4,45653,public,https://b.thumbs.redditmedia.com/Iw6gaGHHRTpgzfNJLjV09qg1As33Rd0pr1_Rz5fVIiI.jpg,Why data engineering is so hot right now.,0,[],1.0,https://youtu.be/vwvdtXMcNzI,all_ads,6,"{'oembed': {'author_name': 'Ken Jee', 'author_url': 'https://www.youtube.com/c/KenJee1', 'height': 200, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/vwvdtXMcNzI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/vwvdtXMcNzI/hqdefault.jpg', 'thumbnail_width': 480, 'title': 'Why Is Data Engineering So HOT Right Now?', 'type': 'video', 'version': '1.0', 'width': 356}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/vwvdtXMcNzI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'scrolling': False, 'width': 356}",,"{'oembed': {'author_name': 'Ken Jee', 'author_url': 'https://www.youtube.com/c/KenJee1', 'height': 200, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/vwvdtXMcNzI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/vwvdtXMcNzI/hqdefault.jpg', 'thumbnail_width': 480, 'title': 'Why Is Data Engineering So HOT Right Now?', 'type': 'video', 'version': '1.0', 'width': 356}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/vwvdtXMcNzI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/r2w6tb', 'scrolling': False, 'width': 356}",105.0,140.0,https://youtu.be/vwvdtXMcNzI,,,,,,,,,,
[],False,BadGuyBadGuy,,,[],,,,text,t2_535yg,False,False,False,[],False,False,1637950509,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r2tbz6/college_question_already_have_a_bs_but_want_to/,{},r2tbz6,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,7,0,False,all_ads,/r/dataengineering/comments/r2tbz6/college_question_already_have_a_bs_but_want_to/,False,self,"{'enabled': False, 'images': [{'id': 'YLZTzkCJ7e0KFZE6Rnt0oOhDqVdRNnVDecyc27lkWB0', 'resolutions': [{'height': 54, 'url': 'https://external-preview.redd.it/6UnccMrBwCTgovSl8pCGNLaGGpyvx8lH0zP-VS_MGMg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e8df47c57fcd3e276ea129d405e68947c2dd55d7', 'width': 108}, {'height': 108, 'url': 'https://external-preview.redd.it/6UnccMrBwCTgovSl8pCGNLaGGpyvx8lH0zP-VS_MGMg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6a719c7d0416494262e960637d87da5cb801fd67', 'width': 216}, {'height': 160, 'url': 'https://external-preview.redd.it/6UnccMrBwCTgovSl8pCGNLaGGpyvx8lH0zP-VS_MGMg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d705751fce814a683cedda1331f3ee8fe0415681', 'width': 320}, {'height': 320, 'url': 'https://external-preview.redd.it/6UnccMrBwCTgovSl8pCGNLaGGpyvx8lH0zP-VS_MGMg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8ac70e5403ff065b01544b27575a9b6a19a82d16', 'width': 640}, {'height': 480, 'url': 'https://external-preview.redd.it/6UnccMrBwCTgovSl8pCGNLaGGpyvx8lH0zP-VS_MGMg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8b791263423ca4a2063254736b32a47dc3228066', 'width': 960}, {'height': 540, 'url': 'https://external-preview.redd.it/6UnccMrBwCTgovSl8pCGNLaGGpyvx8lH0zP-VS_MGMg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=47f3b439a601c5179a7de1a22ba4f11e95f1ee53', 'width': 1080}], 'source': {'height': 600, 'url': 'https://external-preview.redd.it/6UnccMrBwCTgovSl8pCGNLaGGpyvx8lH0zP-VS_MGMg.jpg?auto=webp&amp;s=8b9fb67617af211f539ebcef36e3e581da4ddc9b', 'width': 1200}, 'variants': {}}]}",6,1637950520,1,"To reiterate the crappy title - I have a bachelors of science in IT, but I want the full computer science education.

**Here's my question** - Would there be any benefit to pursuing an *accredited* CS post-bachelors?  Would that open any additional doors?  

If not, I'm thinking to just save the $30k and go through this free curriculum on Github:
[OSSU Computer Science](https://github.com/ossu/computer-science)

**For context:**

It's for self-satisfaction, but also to feel ""more rounded"" as I begin hunting for my first data engineering job.  I'm currently a data analyst with three years experience using Python.  We didn't have a data warehouse until now, so no actual SQL experience.  I've only used SQL in coursework at home - I'd say intermediate where I'm writing transformations / stored procedures.  This next year will be SQL heavy though at work, so I'm not worried there.  

Bonus question: If I complete the OSSU path, is that worth listing on a resume on top of my other stuff?  Or would that just prompt some eye rolls?  I don't know what the perception is on that kind of thing.  Maybe in conjunction with a home project?",True,False,False,dataengineering,t5_36en4,45643,public,self,"College question - already have a BS, but want to learn computer science curriculum",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r2tbz6/college_question_already_have_a_bs_but_want_to/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Substantial-Cow-8958,,,[],,,,text,t2_6lob61a5,False,False,False,[],False,False,1637949166,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r2sur6/data_streaming_idea/,{},r2sur6,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/r2sur6/data_streaming_idea/,False,,,6,1637949177,1,"Hey guys. 

I'm working in a POC of data streaming using Iceberg. The idea is to be able to execute row level operations every time some action happens at the transactional database.

It would be great to hear some opinions and improvements to be made. So, here is a summary of the streaming process, (there is also a flowchart):

The data streaming process begins at Postgres, where and when operations are made at the transactional database. The Postgres table simulation at the flowchart is just a script that makes inserts updates and deletes at the database randomly, to simulate a real-life database. The tables at Postgres have a catalog at Glue, and each one of them, gains two columns at Glue: deleted and timestamp. Those columns are used to make row-level operations. That is how the merge knows which column was deleted or updated. There is a Python script responsible for building those catalogs with the additional columns.

When some action occurs at Postgres (insert, update or delete), a Python script, that uses wall2json replication and is always listening to the database, send the operations to the correct table’s Data Delivery at Kinesis Firehose, adding the correct values for timestamp, and deleted, if that is the case.  
Kinesis Firehose saves the operations made at the database into s3 buckets specific for each table as parquet files.

When new files are saved at s3, a lambda function is triggered to create (if it doesn’t exist yet) a queue at SQS for each table, and to feed the queue with the reference to the data at s3, that was just saved as an operation made at the database.

After that, a Python script, responsible for processing the parquet files into iceberg tables, listens to the SQS queue, and when there are new messages, reads the message getting the parquet file reference, to get the file at s3 and process it using Spark, and the two fields that were added (timestamp and deleted).  
Through Spark, the row-level operations are made into the Iceberg tables, and they now reflect the updates made at the database.

Finally, the iceberg tables have Glue Catalogs, and through them, it's possible to query the tables at Athena.

  
There is also a flowchart that you can check. Every opinion and improvement will help a lot. 

Thanks for reading!

&amp;#x200B;

&amp;#x200B;

https://preview.redd.it/9cx1x1v0az181.png?width=8165&amp;format=png&amp;auto=webp&amp;s=c1dce524f67778a301e8eb663e26ef1be138cb20",True,False,False,dataengineering,t5_36en4,45643,public,https://b.thumbs.redditmedia.com/YWfJz4CxwUyMeFTx11iAQphqXVSLPZKh40XYxycYOBw.jpg,Data Streaming Idea,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r2sur6/data_streaming_idea/,all_ads,6,,,,,,52.0,140.0,,"{'9cx1x1v0az181': {'e': 'Image', 'id': '9cx1x1v0az181', 'm': 'image/png', 'p': [{'u': 'https://preview.redd.it/9cx1x1v0az181.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8146d74ec1d8c6f6592da27bf53529f5ef0fa398', 'x': 108, 'y': 40}, {'u': 'https://preview.redd.it/9cx1x1v0az181.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bc701b058ea140c0cb0a5175103bd62ea58a9fb7', 'x': 216, 'y': 81}, {'u': 'https://preview.redd.it/9cx1x1v0az181.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5105c77250f165798420bce7634712a2ae58f86a', 'x': 320, 'y': 120}, {'u': 'https://preview.redd.it/9cx1x1v0az181.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b57513c023b96c1aa21e8aaf9d1bcd7fc435f622', 'x': 640, 'y': 241}, {'u': 'https://preview.redd.it/9cx1x1v0az181.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4817c4055a8bbabb2f89ee95a7486f073dc256a9', 'x': 960, 'y': 362}, {'u': 'https://preview.redd.it/9cx1x1v0az181.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=63843f0a962d7df6ba3286f7234d6d64f5bffd63', 'x': 1080, 'y': 407}], 's': {'u': 'https://preview.redd.it/9cx1x1v0az181.png?width=8165&amp;format=png&amp;auto=webp&amp;s=c1dce524f67778a301e8eb663e26ef1be138cb20', 'x': 8165, 'y': 3083}, 'status': 'valid'}}",,,,,,,,,
[],False,polishchief,,,[],,,,text,t2_coktu7y4,False,False,False,[],False,False,1637946924,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r2s30r/plsql_developer_wanting_to_become_a_data_engineer/,{},r2s30r,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,27,0,False,all_ads,/r/dataengineering/comments/r2s30r/plsql_developer_wanting_to_become_a_data_engineer/,False,,,6,1637946935,1,"I basically write complex SQL and PL/SQL code for ETL purposes at my job. I have ~3 years of experience doing this. To be honest, I don’t even have to do much of extracting the data. It is already provided in flat files or tables, and that is what I use to transform the data. I don’t use cloud, any programming language, or any modern technologies. I want to do much more, and I won’t get that at my current job. 

I have been watching a ton of “what does a DE do?” videos on YouTube, and I know that is what I want to do. I have a Masters in CS (but bachelors in History), and I am comfortable with programming. 

I know that I don’t know much about DE, so I enrolled into Datacamp’s Data Engineering track yesterday, and plan to finish it within a couple of months. I know this will give me a high level overview of what a DE does and needs to know. 

I am not sure what to do after this. I like learning from books and doing exercises/projects. Are there any resources you guys recommend that will help me in my transition towards becoming a DE? I want to learn enough to be able to apply for junior or mid level DE positions. I will be applying to these positions as I study.",True,False,False,dataengineering,t5_36en4,45641,public,self,PLSQL Developer wanting to become a Data Engineer. What steps to take?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r2s30r/plsql_developer_wanting_to_become_a_data_engineer/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Individual-Weekend66,,,[],,,,text,t2_6cht0ltu,False,False,False,[],False,False,1637946363,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r2rvxe/should_data_engineers_work_on_operational_kind_of/,{},r2rvxe,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/r2rvxe/should_data_engineers_work_on_operational_kind_of/,False,,,6,1637946374,1,"I am wondering at a high level the title question. I will put a more specific case. We (me and my DEs colleagues) are helping the catalog team of the company. My company is a marketplace, and there is a hugely time-consuming task for the catalog team, which is the following:

A seller uploads a product to be on sale, but we don't have it in our products database/product management tool (thus, an event of no matching is created and sent to a microservice database). Then, the mission of the catalog team is to create those products which we didn't have but a seller wants to sell. 

And here is where we enter into the game.  We pull the no-matching events from the microservice database and create a report in our BI tool with some basic info about the events (name of the product, nº of sellers who has tried to sell it etc). Then, the catalog team takes a look at that report and creates the products manually.

For making their job more efficient, we have integrated into our database 4 external providers of products info. So that our BI report is highly enriched and basically now the catalog team just have to copy the information that we provide in our product management tool. 

From my point of view, this is operational and not analytical, but I don't know if the scope of a data team should be only analytical or everything related to data. Is this something that data teams usually do?",True,False,False,dataengineering,t5_36en4,45642,public,self,Should Data Engineers work on operational (kind of) flows?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r2rvxe/should_data_engineers_work_on_operational_kind_of/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,RstarPhoneix,,,[],,,,text,t2_57e44nxs,False,False,False,[],False,False,1637945559,i.redd.it,https://www.reddit.com/r/dataengineering/comments/r2rlo7/is_there_a_way_by_which_we_can_get_column_lineage/,{},r2rlo7,False,True,False,False,True,True,False,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,16,0,False,all_ads,/r/dataengineering/comments/r2rlo7/is_there_a_way_by_which_we_can_get_column_lineage/,False,image,"{'enabled': True, 'images': [{'id': 'ZA5ZEyWjGzH8sA_5wL2VuFACPwzF99X4lClvOS41lpc', 'resolutions': [{'height': 68, 'url': 'https://preview.redd.it/zbwie8lkzy181.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8edfe279fbdf296acee7509865366faa3bb84e8b', 'width': 108}, {'height': 137, 'url': 'https://preview.redd.it/zbwie8lkzy181.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4cf86b3972ce6a7fb163f58c58c9fa2a85a188eb', 'width': 216}, {'height': 203, 'url': 'https://preview.redd.it/zbwie8lkzy181.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4508992a99b26728ac162453d8f323a8e3d734e3', 'width': 320}, {'height': 406, 'url': 'https://preview.redd.it/zbwie8lkzy181.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4ec09cccc80af54d8ab1006e88e1a91185857e98', 'width': 640}, {'height': 610, 'url': 'https://preview.redd.it/zbwie8lkzy181.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9336ba66930f7e2e1881fa08006461a8c1985601', 'width': 960}, {'height': 686, 'url': 'https://preview.redd.it/zbwie8lkzy181.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e2278726b176f9b2b99f13cf50a65d620293f7ed', 'width': 1080}], 'source': {'height': 779, 'url': 'https://preview.redd.it/zbwie8lkzy181.png?auto=webp&amp;s=ca497111d418e521b35e6ed5fb15dae6c1cedf8d', 'width': 1225}, 'variants': {}}]}",6,1637945570,1,,True,False,False,dataengineering,t5_36en4,45642,public,https://b.thumbs.redditmedia.com/Ja-YgwQCSMo_FZQ4Bt3vgtybrGqVbnhtXA2waVfzUGE.jpg,Is there a way by which we can get column lineage diagram from SQL query ? Any ideas ?,0,[],1.0,https://i.redd.it/zbwie8lkzy181.png,all_ads,6,,,,,,89.0,140.0,https://i.redd.it/zbwie8lkzy181.png,,,,,,,,,,
[],False,user19911506,,,[],,,,text,t2_6ys5mu5,False,False,False,[],False,False,1637945387,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r2rjmo/any_des_from_india/,{},r2rjmo,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,5,0,False,all_ads,/r/dataengineering/comments/r2rjmo/any_des_from_india/,False,,,6,1637945398,1,Any fellow Indians here? I wanted to know what is the tech stack you are working on and what is the compensation scene here.,True,False,False,dataengineering,t5_36en4,45642,public,self,Any DE's from India?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r2rjmo/any_des_from_india/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,mister_patience,,,[],,,,text,t2_xt5zb,False,False,False,[],False,False,1637935576,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r2o7oj/path_to_dp_203/,{},r2o7oj,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,7,0,False,all_ads,/r/dataengineering/comments/r2o7oj/path_to_dp_203/,False,,,6,1637935587,1,"Hi all,

I am trying to help a team go from zero to DP-203. What would be the best microsoft courses to get me there?  


What is the difference between 200, 201, 202?   


Should I use DP900 as a base? Anything people can do to help me would be great.",True,False,False,dataengineering,t5_36en4,45635,public,self,Path to DP 203,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r2o7oj/path_to_dp_203/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Comprehensive-Set-77,,,[],,,,text,t2_8j06kn2a,False,False,False,[],False,False,1637926130,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r2l4ww/what_should_i_learn_next/,{},r2l4ww,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,8,0,False,all_ads,/r/dataengineering/comments/r2l4ww/what_should_i_learn_next/,False,,,6,1637926141,1,"I work as a data engineer and I know all the basic stuff. I am looking for something that is not that domain specific. Something that makes me improve overall fundamentally.

I guess one can formulate the question:

What are some areas you think would make you a better data engineer/developer overall? If any. 

Random examples: Statistics, System architecture l, Algebra, Hardware, Economics",True,False,False,dataengineering,t5_36en4,45628,public,self,What should I learn next,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r2l4ww/what_should_i_learn_next/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,SuperUser2112,,,[],,,,text,t2_dw0fb5us,False,False,False,[],False,False,1637917164,additionalsheet.com,https://www.reddit.com/r/dataengineering/comments/r2j134/exploratory_data_analysis_with_python_an_easy_way/,{},r2j134,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r2j134/exploratory_data_analysis_with_python_an_easy_way/,False,link,"{'enabled': False, 'images': [{'id': 'iLOkYdaFHb8iEkILArfgNGv4YZ1SWrSv-_Lm1IcPKI4', 'resolutions': [{'height': 72, 'url': 'https://external-preview.redd.it/PhQFDC4mpifgfjONxwoUgs-H7eVP00fRBknUIqkltcI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ef26bfc33ff47a87dcebdeb655aed126d2f4937a', 'width': 108}, {'height': 144, 'url': 'https://external-preview.redd.it/PhQFDC4mpifgfjONxwoUgs-H7eVP00fRBknUIqkltcI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fd316875b27d7ba7c8e8f336f5a8094b34b8f4f3', 'width': 216}, {'height': 213, 'url': 'https://external-preview.redd.it/PhQFDC4mpifgfjONxwoUgs-H7eVP00fRBknUIqkltcI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9b477f6f01093a1e1a0e3d4040ec4f7556f82126', 'width': 320}, {'height': 426, 'url': 'https://external-preview.redd.it/PhQFDC4mpifgfjONxwoUgs-H7eVP00fRBknUIqkltcI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e01ec0952c2c8f00c14743445c6f5139889a2498', 'width': 640}, {'height': 640, 'url': 'https://external-preview.redd.it/PhQFDC4mpifgfjONxwoUgs-H7eVP00fRBknUIqkltcI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c0cec323ed60bed070683ddb239e3300508635e6', 'width': 960}], 'source': {'height': 683, 'url': 'https://external-preview.redd.it/PhQFDC4mpifgfjONxwoUgs-H7eVP00fRBknUIqkltcI.jpg?auto=webp&amp;s=b7d7ad52f4ad2ff5566c01e20bc62afcc8649e93', 'width': 1024}, 'variants': {}}]}",6,1637917174,1,,True,False,False,dataengineering,t5_36en4,45624,public,https://b.thumbs.redditmedia.com/aMsBZXRfjjNnQDYUUaGaAtTaflarGrAPbecdSZAGLfU.jpg,"[Exploratory Data Analysis with Python] An Easy Way to Detect Data Abnormalities, Numpy Accumulate",0,[],1.0,https://additionalsheet.com/understanding-python-numpy-ufunc-accumulate-example-automotive,all_ads,6,,,,,,93.0,140.0,https://additionalsheet.com/understanding-python-numpy-ufunc-accumulate-example-automotive,,,,,,,,,,
[],False,pych_phd,,,[],,,,text,t2_7nu3u,False,False,False,[],False,False,1637907414,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r2gl2c/azure_storage_architecturemanagement_for/,{},r2gl2c,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,3,0,False,all_ads,/r/dataengineering/comments/r2gl2c/azure_storage_architecturemanagement_for/,False,self,"{'enabled': False, 'images': [{'id': 'LVmzWMJU1UZwRubzQYJZSar-z-Rq8ntUH65yhQyfxB8', 'resolutions': [{'height': 108, 'url': 'https://external-preview.redd.it/YH8gNap4KoGcFgyFYrNZ86fXmYfRn6pa7uuwIlZkjEE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0b9526a51504048891d5e64783519fd5dc3cd83f', 'width': 108}, {'height': 216, 'url': 'https://external-preview.redd.it/YH8gNap4KoGcFgyFYrNZ86fXmYfRn6pa7uuwIlZkjEE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0150bc3ab1c6838c35ff951d69578f3d19ae4ed3', 'width': 216}, {'height': 320, 'url': 'https://external-preview.redd.it/YH8gNap4KoGcFgyFYrNZ86fXmYfRn6pa7uuwIlZkjEE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c1e830770227ae4802c5776d22f63d0f6aa71b15', 'width': 320}], 'source': {'height': 400, 'url': 'https://external-preview.redd.it/YH8gNap4KoGcFgyFYrNZ86fXmYfRn6pa7uuwIlZkjEE.jpg?auto=webp&amp;s=e62264227377a9581e2e2946169864d130fa3217', 'width': 400}, 'variants': {}}]}",6,1637907424,1,"Hi all,   


I am wanting to get other peoples opinions on the storage architecture for landing Streaming data in Azure, taking in to account storage limitations and lifecycle management.  


 P.S I have only included data lake + delta Lake as the final processed storage to simplify the design. Yes there would be a WH too if needed. 

\_\_ My thought processes \_\_

The standard Architecture I see for storing raw streaming for keep long term is to put in to Azure Datalake / Blob storage.  Combined with Batch processing and then to Delta Lake for processing, it would look like this:  


&amp;#x200B;

[Batch and stream to data lake for raw, then to Delta lake for processing](https://preview.redd.it/wg7xqj4dmv181.png?width=573&amp;format=png&amp;auto=webp&amp;s=9797bc976d17604d23bd70aab81bcb1ab1814c93)

  
However, I recently came across this comment about storing log analytics from azure monitor :

""Don't use an existing storage account that has other, non-monitoring  data stored in it to better control access to the data and prevent  reaching storage ingress rate limit, failures, and latency. "" From [https://docs.microsoft.com/en-us/azure/azure-monitor/logs/logs-data-export?tabs=portal](https://docs.microsoft.com/en-us/azure/azure-monitor/logs/logs-data-export?tabs=portal).   


It's the first time I have seen this and have been thinking about what it means. I have always kind of assumed that there should be only one data lake. This idea was first crushed when I saw the delta lake is on it's own (at least based on example I have seen). My take way is that each streaming source should have it's own blob storage/data lake sink. When I think about how this might look with multiple streams of data and life cycle management. I came up with this:  


https://preview.redd.it/t41phh6dpv181.png?width=759&amp;format=png&amp;auto=webp&amp;s=ac32cb1423bf6434e7219898c914535893ad480d

The blobs could also be Data lakes, I'm not sure on which is better for this scenario.. I also suspect that The delta lake would also have azure lifecycle management turned on too.   


Alternately, I have seen Blobs Storage referenced to as a temporary storage (I forget where). In which case the design could also be:  


https://preview.redd.it/mlabqyl3sv181.png?width=916&amp;format=png&amp;auto=webp&amp;s=24f6a262e61b1271e034510651e5d8ba6d25b387

This design weakness is that there is more data movement costs, but it's advantage is having one long term storage for raw data in stead of many.   
\----  
Your thoughts and ideas would be much appreciated.",True,False,False,dataengineering,t5_36en4,45618,public,https://b.thumbs.redditmedia.com/oLpQ30hJRpYqWDNXrDDZ3VSHVq4DiAeVI54uR7NEhxM.jpg,Azure Storage Architecture/Management for Streaming/High throughput?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r2gl2c/azure_storage_architecturemanagement_for/,all_ads,6,,,,,,76.0,140.0,,"{'mlabqyl3sv181': {'e': 'Image', 'id': 'mlabqyl3sv181', 'm': 'image/png', 'p': [{'u': 'https://preview.redd.it/mlabqyl3sv181.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=40a2e7bd99269c2e8fcfa118cdce66709229878e', 'x': 108, 'y': 83}, {'u': 'https://preview.redd.it/mlabqyl3sv181.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=44cfb6cec11bb7270647f9a6cbb9e533365c52e0', 'x': 216, 'y': 166}, {'u': 'https://preview.redd.it/mlabqyl3sv181.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8fccfc45361e5cb715dbdd902acb19c3bb05023b', 'x': 320, 'y': 246}, {'u': 'https://preview.redd.it/mlabqyl3sv181.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6455cb6b747e1d8ed576b4e2ee5e49824d39bdf9', 'x': 640, 'y': 493}], 's': {'u': 'https://preview.redd.it/mlabqyl3sv181.png?width=916&amp;format=png&amp;auto=webp&amp;s=24f6a262e61b1271e034510651e5d8ba6d25b387', 'x': 916, 'y': 707}, 'status': 'valid'}, 't41phh6dpv181': {'e': 'Image', 'id': 't41phh6dpv181', 'm': 'image/png', 'p': [{'u': 'https://preview.redd.it/t41phh6dpv181.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0801d54512157990af438bbf42d6cbd61a188435', 'x': 108, 'y': 104}, {'u': 'https://preview.redd.it/t41phh6dpv181.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=35824d01caec37c8ba3d965c005aeaae28594b85', 'x': 216, 'y': 208}, {'u': 'https://preview.redd.it/t41phh6dpv181.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3e0771952be72641810e8c17542d02876ab91a13', 'x': 320, 'y': 308}, {'u': 'https://preview.redd.it/t41phh6dpv181.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=22306cb927b0f1e6b519c4344e5c52dcf1e0dd13', 'x': 640, 'y': 616}], 's': {'u': 'https://preview.redd.it/t41phh6dpv181.png?width=759&amp;format=png&amp;auto=webp&amp;s=ac32cb1423bf6434e7219898c914535893ad480d', 'x': 759, 'y': 731}, 'status': 'valid'}, 'wg7xqj4dmv181': {'e': 'Image', 'id': 'wg7xqj4dmv181', 'm': 'image/png', 'p': [{'u': 'https://preview.redd.it/wg7xqj4dmv181.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b66992986b246edb5bbd5cd9544283e27ee98cf8', 'x': 108, 'y': 59}, {'u': 'https://preview.redd.it/wg7xqj4dmv181.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fcba5cdcf07144c9439e8f8717b8425dda5a4184', 'x': 216, 'y': 118}, {'u': 'https://preview.redd.it/wg7xqj4dmv181.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=aeb8042f5412c478f84009762bc197b270e296f5', 'x': 320, 'y': 175}], 's': {'u': 'https://preview.redd.it/wg7xqj4dmv181.png?width=573&amp;format=png&amp;auto=webp&amp;s=9797bc976d17604d23bd70aab81bcb1ab1814c93', 'x': 573, 'y': 315}, 'status': 'valid'}}",,,,,,,,,
[],False,iiyamabto,,,[],,,,text,t2_l2zu3,False,False,False,[],False,False,1637901122,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r2eu1t/what_is_your_ideal_data_team_composition/,{},r2eu1t,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,5,0,False,all_ads,/r/dataengineering/comments/r2eu1t/what_is_your_ideal_data_team_composition/,False,,,6,1637901133,1,"You have given a privilege to build your own ideal data team. How many people do you allocate for these position:


* Data Engineers
* Data Architects
* Data Analysts
* Data Scientists
* BI Developers
* Data Platform Engineers
* Tech Lead, Managers, Head


PS: Size of team can vary maybe 10, 20, 50, or even more.",True,False,False,dataengineering,t5_36en4,45610,public,self,What is your ideal data team composition?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r2eu1t/what_is_your_ideal_data_team_composition/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,ClittoryHinton,,,[],,,,text,t2_11oqv9,False,False,False,[],False,False,1637879212,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r2853m/why_is_learning_data_engineering_so_opaque/,{},r2853m,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,37,0,False,all_ads,/r/dataengineering/comments/r2853m/why_is_learning_data_engineering_so_opaque/,False,,,6,1637879223,1,"I am a full stack developer trying to learn more about data engineering, but so far everything is so damn opaque. I know front end has its own messes, but at least at this point everyone is unified under one programming language, a couple open source frameworks, and a couple open source package managers. It’s not hard to find tutorials frontend or backend that have you developing locally in minutes with commonly accepted tools on sound examples.

Meanwhile, under Azure there are so many services that seem like they should do the same thing. Synapse and Databricks. Data Explorer and Analysis Services. Data Factory and HDInsight. Iot hub and stream analytics. And learning any one of them is an annoying exercise in setting up an account and learning a user interface that will be swapped out in a couple years. There are no definitive starting places it feels like.

Where should I start? I know Python/SQL well and I’ve read Kimball and Designing Data Intensive Applications and want to start applying this stuff but can’t even begin to know what tech to choose.",True,False,False,dataengineering,t5_36en4,45596,public,self,Why is learning data engineering so opaque,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r2853m/why_is_learning_data_engineering_so_opaque/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,tmccormick92,,,[],,,,text,t2_2fpa4ubx,False,False,False,[],False,False,1637864273,githowto.com,https://www.reddit.com/r/dataengineering/comments/r22smf/this_is_the_best_handson_git_tutorial/,{},r22smf,False,True,False,False,False,True,False,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r22smf/this_is_the_best_handson_git_tutorial/,False,,,6,1637864284,1,,True,False,False,dataengineering,t5_36en4,45580,public,default,this is the best hands-on Git tutorial,0,[],1.0,https://githowto.com/,all_ads,6,,,,,,,,https://githowto.com/,,,,,,,,,,
[],False,SnowPlowOpenSource,,,[],,,,text,t2_enkpz7c3,False,False,False,[],False,False,1637858408,i.redd.it,https://www.reddit.com/r/dataengineering/comments/r20lac/snowplow_twitch_livestream_291121_2100_cet/,{},r20lac,False,True,False,False,True,True,False,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/r20lac/snowplow_twitch_livestream_291121_2100_cet/,False,image,"{'enabled': True, 'images': [{'id': 'JaWGcsrp-FYE5iKCFcw61y8O2oUGryADvTEuZ-fWa9A', 'resolutions': [{'height': 60, 'url': 'https://preview.redd.it/11r1c7lasr181.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4af573e8c3ab9c473ebee436a74f9e4562fdd505', 'width': 108}, {'height': 121, 'url': 'https://preview.redd.it/11r1c7lasr181.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0bcbc245e934a334842ad135833f5ad1c86070e1', 'width': 216}, {'height': 180, 'url': 'https://preview.redd.it/11r1c7lasr181.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7117ad1b1ccd7277e0f78fe453bd5158eebafd87', 'width': 320}, {'height': 360, 'url': 'https://preview.redd.it/11r1c7lasr181.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a2f671053402e46dd8d32efa860f286e29321891', 'width': 640}], 'source': {'height': 450, 'url': 'https://preview.redd.it/11r1c7lasr181.jpg?auto=webp&amp;s=46a5547cde1945edbdc3071879dcf5010cbfe516', 'width': 800}, 'variants': {}}]}",6,1637858420,1,,True,False,False,dataengineering,t5_36en4,45573,public,https://b.thumbs.redditmedia.com/PKhgphPcq6us64LDilo33uXRhBqfePGlnWfLHCh_FxQ.jpg,Snowplow Twitch LiveStream 29/11/21 21:00 CET,0,[],1.0,https://i.redd.it/11r1c7lasr181.jpg,all_ads,6,,,,,,78.0,140.0,https://i.redd.it/11r1c7lasr181.jpg,,,,,,,,,,
[],False,killer_unkill,,,[],,,,text,t2_18oazxf4,False,False,False,[],False,False,1637850871,missing.csail.mit.edu,https://www.reddit.com/r/dataengineering/comments/r1xvbe/the_missing_semester_of_your_cs_education/,{},r1xvbe,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,False,6,0,False,all_ads,/r/dataengineering/comments/r1xvbe/the_missing_semester_of_your_cs_education/,False,,,6,1637850882,1,,True,False,False,dataengineering,t5_36en4,45565,public,default,The Missing Semester of Your CS Education,0,[],1.0,https://missing.csail.mit.edu/,all_ads,6,,,,,,,,https://missing.csail.mit.edu/,,,,,,,,,,
[],False,skibooj,,,[],,,,text,t2_108pe9,False,False,False,[],False,False,1637849295,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r1xbux/loading_data_into_a_fact_table/,{},r1xbux,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,7,0,False,all_ads,/r/dataengineering/comments/r1xbux/loading_data_into_a_fact_table/,False,self,"{'enabled': False, 'images': [{'id': 'ZOijBhPThq1OPXTi0cT7ErgDN3E6y8sUhTS2Q-W1uFQ', 'resolutions': [{'height': 74, 'url': 'https://external-preview.redd.it/pdAGeRfJ0COgGc34s9rsBYwulWWxozunXBzKsQLBQfc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1abb9bbfa0fcc03abd62fee2c1db42c0ccf78aa9', 'width': 108}, {'height': 149, 'url': 'https://external-preview.redd.it/pdAGeRfJ0COgGc34s9rsBYwulWWxozunXBzKsQLBQfc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=31df72e473d8bf2e6edfd013dd739a75b7887b5d', 'width': 216}, {'height': 222, 'url': 'https://external-preview.redd.it/pdAGeRfJ0COgGc34s9rsBYwulWWxozunXBzKsQLBQfc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ea3dae6e423bc85996d3fcff02215874d20f271b', 'width': 320}, {'height': 444, 'url': 'https://external-preview.redd.it/pdAGeRfJ0COgGc34s9rsBYwulWWxozunXBzKsQLBQfc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=77547dafdb59ca60f07a5f09b2609359aba775b2', 'width': 640}], 'source': {'height': 444, 'url': 'https://external-preview.redd.it/pdAGeRfJ0COgGc34s9rsBYwulWWxozunXBzKsQLBQfc.jpg?auto=webp&amp;s=5624e2354c908684a6d1e1f3c336f413a0a369d7', 'width': 640}, 'variants': {}}]}",6,1637849306,1,"Hello, I am currently learning the concept of dimensional modeling as described in Kimball's book. I more or less understand the idea, but I'm wondering what the process of loading data into a fact table looks like and what the maintenance of the database should look like. I found the [following article](https://medium.com/analytics-vidhya/how-to-create-fact-and-dimension-tables-from-denormalized-raw-data-e26127df2249) that answers my questions but I am not sure if this should be a good example.",True,False,False,dataengineering,t5_36en4,45566,public,self,Loading data into a fact table,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r1xbux/loading_data_into_a_fact_table/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Complex-Stress373,,,[],,,,text,t2_9va3r6a5,False,False,False,[],False,False,1637843169,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r1vhcw/managing_data_lake_s3_layers/,{},r1vhcw,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,7,0,False,all_ads,/r/dataengineering/comments/r1vhcw/managing_data_lake_s3_layers/,False,,,6,1637843180,1,"I have an ELT extracting some data from mysql tables.

Then i have:

1. I upload csv files to S3
2. copy from s3 into snowflake
3. apply transformations in snowflake
4. create data model in snowflake

Very classic. My question are now: 

1. should I upload intermediate files to the s3 bucket and follow the rules of an s3 data lake with layers?. Does it apply this for an ELT?, What about if instead of an ELT is a ETL?.
2. should I upload to the bucket a file after each transformation?, or just one at the very end?",True,False,False,dataengineering,t5_36en4,45555,public,self,managing data lake s3 layers,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r1vhcw/managing_data_lake_s3_layers/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,LuizOAC,,,[],,,,text,t2_fomyk6q5,False,False,False,[],False,False,1637835987,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r1tnjp/data_streaming_flow/,{},r1tnjp,False,False,False,False,False,False,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r1tnjp/data_streaming_flow/,False,,,6,1637835998,1,[removed],True,False,False,dataengineering,t5_36en4,45553,public,self,Data streaming flow,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r1tnjp/data_streaming_flow/,all_ads,6,,,automod_filtered,,,,,,,,,,,,,,,
[],False,awsconsultant,,,[],,,,text,t2_5ggm5svc,False,False,False,[],False,False,1637821828,google.com,https://www.reddit.com/r/dataengineering/comments/r1q34o/coursera_plus_subscription_at_1_with_cyber_week/,{},r1q34o,False,True,False,False,False,True,False,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,4,0,False,all_ads,/r/dataengineering/comments/r1q34o/coursera_plus_subscription_at_1_with_cyber_week/,False,link,"{'enabled': False, 'images': [{'id': 'zxygX8cmUbRAPkJ14xUtC5anwWIiFXoKBJ6wMv6fxLE', 'resolutions': [{'height': 60, 'url': 'https://external-preview.redd.it/5xygftyf4I6GAIVPnHFNvxARw5iHON3JB63wmSsOaI8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=200f0ad215cba2add8e68f3b51dcb5d377ed483e', 'width': 108}, {'height': 120, 'url': 'https://external-preview.redd.it/5xygftyf4I6GAIVPnHFNvxARw5iHON3JB63wmSsOaI8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=77c8dd5dd5c895f0d7dc1872668a4251ada7d0bc', 'width': 216}, {'height': 178, 'url': 'https://external-preview.redd.it/5xygftyf4I6GAIVPnHFNvxARw5iHON3JB63wmSsOaI8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ae42bd4fb9dca0c540dd4a5b631ab4c48a43e58e', 'width': 320}, {'height': 356, 'url': 'https://external-preview.redd.it/5xygftyf4I6GAIVPnHFNvxARw5iHON3JB63wmSsOaI8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6cb81f9af3766a42e19268f01fac9708b202f50a', 'width': 640}], 'source': {'height': 445, 'url': 'https://external-preview.redd.it/5xygftyf4I6GAIVPnHFNvxARw5iHON3JB63wmSsOaI8.jpg?auto=webp&amp;s=3cbe6d5f5a741abb45ebed3c1bb7603d9924f6a4', 'width': 800}, 'variants': {}}]}",6,1637821840,1,,True,False,False,dataengineering,t5_36en4,45547,public,https://b.thumbs.redditmedia.com/4o2LqysrBvFtEknvTHDtcFuyZnvjCUnFJABm3eWx0wM.jpg,Coursera Plus Subscription at $1 with Cyber Week sale.,0,[],1.0,https://www.google.com/amp/s/topitguy.com/coursera-plus/amp/,all_ads,6,,,,,,77.0,140.0,https://www.google.com/amp/s/topitguy.com/coursera-plus/amp/,,,,,,,,,,
[],False,Oikonomiaki,,,[],,,,text,t2_3linb9th,False,False,False,[],False,False,1637807521,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r1lq59/example_of_streaming_aside_from_twitter/,{},r1lq59,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/r1lq59/example_of_streaming_aside_from_twitter/,False,,,6,1637807531,1,I have browsed many streaming exercises (for Spark Streaming) and almost all of them uses Twitter. Do you know of any public/free streaming APIs (returns JSON) that I can use aside from Twitter?,True,False,False,dataengineering,t5_36en4,45539,public,self,Example of streaming aside from Twitter,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r1lq59/example_of_streaming_aside_from_twitter/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,AndyMacht58,,,[],,,,text,t2_bc2rrdnd,False,False,False,[],False,False,1637802162,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r1k1dp/did_using_cloud_services_made_your_job_easiermore/,{},r1k1dp,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/r1k1dp/did_using_cloud_services_made_your_job_easiermore/,False,,,6,1637802173,1,"I'm currently working for a bigger digital company that was doing everything on premise so far. RDBMS DWH appliance solution + Hadoop cluster.

In the past I was often frustated because for the majority of time I was busy with infrastructure/deployment related issues. E.g. mainly kubernetes/helm, maven, ansible, sdk and jenkins related configuration problems, setting up pods for deployment services of new infrastructure etc. The dev part then comes on top of that. If I didn't create the infrastructure myself (which often wasn't 100% perfectly set up neither ofc.), I was often enforced to deliver features on badly operating infrastructure where I feel like beta testing somebody elses work since I'm often working on pionier projects, so I'm often delayed as I have to identify and clarify issues before integrating my solution. In a perfect world this shouldn't happen as such things should be tested before a sprint starts but the communication between my department and the operational support is cumbersome since there're too many people having their fingers everywhere involved and I feel like our processes and infrastructure changes faster than I'm able to catch up so I'm constantly adjusting and relearning every process.


My company is now moving on to GCP and replacing both rdbms and the hadoop cluster slowly with big query as a dwh solution and this will take several years if everything remains as planned.

So I wonderd, as most companies nowadays are cloud only. Will cloud services seriously help me to focus more on the development part, instead of troubleshooting infrastructure and deployment related issues the majority of the time or are the cloud advertisments mere marketing by promising this?

I'm not sure if using cloud services will make a difference here or if I should change the company  after several years now, as I feel that the complexity of my current job burns me out since I feel that I need to deal with too many technologies on too many different environments which are changing faster than I can keep track so I currently fee bit like the red queen of alice in wonderland:

""My dear, here we must run as fast as we can, just to stay in place. And if you wish to go anywhere you must run twice as fast as that.”

[View Poll](https://www.reddit.com/poll/r1k1dp)",True,False,False,dataengineering,t5_36en4,45537,public,self,Did using cloud services made your job easier/more convenient?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r1k1dp/did_using_cloud_services_made_your_job_easiermore/,all_ads,6,,,,,,,,,,,,,"{'is_prediction': False, 'options': [{'id': '12062085', 'text': 'Yes, using cloud services made my job easier.'}, {'id': '12062086', 'text': 'No, life was easier when working on premise.'}], 'prediction_status': None, 'resolved_option_id': None, 'total_stake_amount': None, 'total_vote_count': 0, 'tournament_id': None, 'user_selection': None, 'user_won_amount': None, 'vote_updates_remained': None, 'voting_end_timestamp': 1638406962264}",,,,,
[],False,maudlinbird,,,[],,,,text,t2_nte7wri,False,False,False,[],False,False,1637794884,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r1hnq8/successfully_pivoted_from_a_etl_bi_developer_to_a/,{},r1hnq8,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,66,0,False,all_ads,/r/dataengineering/comments/r1hnq8/successfully_pivoted_from_a_etl_bi_developer_to_a/,False,,,6,1637794895,1,"I've been posting in here occasionally about my journey to becoming a real data engineer. I would get a lot of messages about what I did in my journey/how I'm currently doing so I thought I would share with everyone. 

Background:

I worked for a Fortune 100 company in Texas. I had the Data Engineer title in my previous role but I was really an ETL / BI Developer. I realized I had an inflated job title, was getting paid a lot for what I actually did, but knew this wasn't good for my career in the long term. I had no industry programming experience, only knew GUI drag-and-drop ETL tools, and some reporting tools. I had my sights set on Seattle and fully expected to restart my career to get my foot in the door.

The Grind:

Started the grind in August.

1. Learned Python syntax through Codecademy
2. Did Easy and Medium level questions on Leetcode. Strings and Arrays only.
3. Did Easy, Medium, and Hard SQL questions on Leetcode.
4. Took a AWS Data Analytics course on Udemy
5. Took a Hadoop ecosystem course on Udemy
6. Read through the first two Chapters of Kimball's ""Data Warehouse Toolkit""
7. Watched YouTube videos on system design
8. Learned Big O on a basic level
9. Learned basic Data Structures / Algorithms

Interviewing: 

I submitted as many applications as I could, about 10 per day starting in October. My response rate was actually pretty decent (around 20%). I went into interviews being honest about my experience. I told interviewers that I wanted to pivot into something more technical. I made it obvious that learning and new challenges are what I enjoy.

It's true what everyone says. I only got asked Easy level Python questions in interviews. Even Amazon asked easy level questions. The only exception was Uber. For SQL, Leetcode hard is not enough. From my experience, SQL interview questions are on a whole other level. Expect a triple or quadruple nested query, using multiple CTEs, and less common functions like mod(). It sounds hard but if you know SQL pretty well, it should be easy for you to piece together a solution.

I was never asked about AWS or Hadoop. However, I was asked about Data Modeling and System Design so knowing about Cloud Systems and Big Data Frameworks at a high level ended up being beneficial. Expect to also be asked about DevOps, troubleshooting, and creating fault tolerant pipelines.

I wasn't asked about Big O notation specifically but the interviewer will at least expect you to know about where some efficiency improvements can be made in your code. Example: Using a two pointer/sliding window approach instead of using a double nested for loop. I guess this point can be tied to DS/A too.

Result:

I didn't get any offers from top-tier companies but for the offer I accepted, I ended up getting offered a Senior level position since I have almost 5 years industry experience! It's not FAANG, but something I'm at least satisfied with since I fully expected to restart my career. I was kind of surprised but I'm going to trust in the process here lol.

For the people who have been messaging me, good luck on your journey! I know it seems like an impossible task but just take it one day at a time. The offer will come eventually!",True,False,False,dataengineering,t5_36en4,45534,public,self,Successfully pivoted from a ETL / BI Developer to a Data Engineer (in Seattle!). Wanted to share my experience.,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r1hnq8/successfully_pivoted_from_a_etl_bi_developer_to_a/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,ronald_r3,,,[],,,,text,t2_zdh50,False,False,False,[],False,False,1637794622,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r1hkfp/dimensional_modeling_question_concerning/,{},r1hkfp,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/r1hkfp/dimensional_modeling_question_concerning/,False,,,6,1637794632,1,"Good Afternoon!

I'm looking to create this dimensional model for a personal project of mine and the issue I'm sort of having is dealing correctly with multivalued dimensional tables.

In this instance I have a business in my fact table which can be placed into multiple categories. So to prevent the many-to-many relationship I made a bridge table. Coincidentally I also have a chain dimension (i.e. restaurant chain, business chain, etc.). 

Now having looked at the data warehouse toolkit this is exactly the table structure it recommends (with the chain dimension serving as the group table) although if correct the Chain dimension could just be removed? Considering there is no m-to-m relationship between the business fact table and the bridge table.

That's a sort of question in passing. Now my real question is concerning the AcceptedPaymentsBridge dimension. This columns in here would have low cardinality and in the same book it mentions that in such a case one could use positional tables, that is a table which the different values serving as column names. Now would that be a viable option here?Or is it just fine to implement the bridge table?

The tables in question are circled in red as well if it helps to see that 😀",True,False,False,dataengineering,t5_36en4,45534,public,self,Dimensional modeling question concerning multivalued dimension tables,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r1hkfp/dimensional_modeling_question_concerning/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,lucidible,,,[],,,,text,t2_4rh5415o,False,False,False,[],False,False,1637792720,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r1gwp9/smaller_scale_data_catalog_tools_for_data_lake/,{},r1gwp9,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,4,0,False,all_ads,/r/dataengineering/comments/r1gwp9/smaller_scale_data_catalog_tools_for_data_lake/,False,self,"{'enabled': False, 'images': [{'id': 'GVhh0Q2O1POES0NHoyIpuiq1NwagzwqtMulNVi0JHVA', 'resolutions': [{'height': 56, 'url': 'https://external-preview.redd.it/0KyxvAPsDMxFUNIbSNGi-mF8er9CLFA3hbxg4Gj3ClQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4b9d8637fc9a1345a3e876658c460ee0ae06cec6', 'width': 108}, {'height': 113, 'url': 'https://external-preview.redd.it/0KyxvAPsDMxFUNIbSNGi-mF8er9CLFA3hbxg4Gj3ClQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=79e952f82ce1143b7cb57681fae3a3fbc7521081', 'width': 216}, {'height': 167, 'url': 'https://external-preview.redd.it/0KyxvAPsDMxFUNIbSNGi-mF8er9CLFA3hbxg4Gj3ClQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a5c7343f16c3365311fd7c68f14fe53b3baf826f', 'width': 320}, {'height': 335, 'url': 'https://external-preview.redd.it/0KyxvAPsDMxFUNIbSNGi-mF8er9CLFA3hbxg4Gj3ClQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=312c972aae5b61fac80841d80235a47f1a0a233f', 'width': 640}, {'height': 502, 'url': 'https://external-preview.redd.it/0KyxvAPsDMxFUNIbSNGi-mF8er9CLFA3hbxg4Gj3ClQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b65653777776647b7a9cd97e500b9090c7861d68', 'width': 960}, {'height': 565, 'url': 'https://external-preview.redd.it/0KyxvAPsDMxFUNIbSNGi-mF8er9CLFA3hbxg4Gj3ClQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=67cdd1dd69543ba6d1aecafb6a93b25157e4196d', 'width': 1080}], 'source': {'height': 629, 'url': 'https://external-preview.redd.it/0KyxvAPsDMxFUNIbSNGi-mF8er9CLFA3hbxg4Gj3ClQ.jpg?auto=webp&amp;s=917b8a137bc7af401ea6e9c3b74dbb022af78d2d', 'width': 1201}, 'variants': {}}]}",6,1637792732,1,"We are building an internal data platform for collecting data from our various teams into one place.   We're taking a hybrid data lake/data warehouse approach by ingesting raw data but still having some structure around it.  One key component is the data catalog/tagging/library layer.  As is implied by those slashes, we're not quite sure what exactly we need to build but fundamentally it is something that tags our data so that we can make sense of what we have.   

I've talked with data catalog vendors and that seems like the thing we need but most of those are way more than we need at our stage.  We only have gigabytes of relatively slowly changing data (weekly updates are probably sufficient).  Most of the data is tabular from niche SaaS solutions for our relatively specialized industry.   think csv exports of data, with the occasional JSON payload from a webhook push or API pull.  Will also have some mix of structured and unstructured data (pdfs with a standard but non tabular structure as well as just narrative docs).  

It's been suggested to me that elasticsearch or solr may be a good tool to use to tag but in looking at those it seems that both are more for humans searching data and not necessarily for extracting metadata that can be used in queries.   The use case being not merely finding the docs but doing some data warehouse type queries against data sourced from both the tabular data and the unstructured text docs.   

Rightly or wrongly, what I'm kind of envisioning (I think) is the idea of dimension tables in a data warehouse star schema.  ie extracting attributes from the various data types but instead of only having dimension tables, we also have dimension ""blobs"".   

Not sure if that makes sense but any other suggestions between elasticsearch and a heavy duty data catalog like [data.world](https://data.world) or collibra woudl be welcome.",True,False,False,dataengineering,t5_36en4,45535,public,self,smaller scale data catalog tools for data lake?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r1gwp9/smaller_scale_data_catalog_tools_for_data_lake/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,secodaHQ,,,[],,,,text,t2_aiinah9q,False,False,False,[],False,False,1637790618,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r1g5mo/step_by_step_guide_to_creating_a_data_catalog/,{},r1g5mo,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/r1g5mo/step_by_step_guide_to_creating_a_data_catalog/,False,self,"{'enabled': False, 'images': [{'id': '-o2C2VvOkTeA49zhLKf2dQIlmFVuBFwSwywUbEAJsPw', 'resolutions': [{'height': 66, 'url': 'https://external-preview.redd.it/nF_s6B6wRyjYPiQMuyZYCPU55nWsUjsvKrq_9kZYODU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=77ef36cccb3617a31e6c3735616c300271c9e265', 'width': 108}, {'height': 132, 'url': 'https://external-preview.redd.it/nF_s6B6wRyjYPiQMuyZYCPU55nWsUjsvKrq_9kZYODU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d66aa769df96e2392f8e2a87bda68b7d3e8f7d25', 'width': 216}, {'height': 195, 'url': 'https://external-preview.redd.it/nF_s6B6wRyjYPiQMuyZYCPU55nWsUjsvKrq_9kZYODU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3073d419dbc66018d86ee1a15e18812f90e6b3f8', 'width': 320}, {'height': 391, 'url': 'https://external-preview.redd.it/nF_s6B6wRyjYPiQMuyZYCPU55nWsUjsvKrq_9kZYODU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3cf2ed40d3b9ba5c596cc4ead2736f3a045f60b9', 'width': 640}, {'height': 587, 'url': 'https://external-preview.redd.it/nF_s6B6wRyjYPiQMuyZYCPU55nWsUjsvKrq_9kZYODU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=42d20f249e19f7e01e086086a0c3c35ef96fe1ad', 'width': 960}, {'height': 660, 'url': 'https://external-preview.redd.it/nF_s6B6wRyjYPiQMuyZYCPU55nWsUjsvKrq_9kZYODU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bf9a147927596f8bfd643cb2173c8fa0c17c368f', 'width': 1080}], 'source': {'height': 744, 'url': 'https://external-preview.redd.it/nF_s6B6wRyjYPiQMuyZYCPU55nWsUjsvKrq_9kZYODU.jpg?auto=webp&amp;s=76ff84b6dcf8cfff7e56e73e6d0d31f46917ca78', 'width': 1216}, 'variants': {}}]}",6,1637790629,1,"Simple data cataloging starts with a great organization. A data catalog is a collection of metadata and documentation that helps make sense of the data sprawl that exists in most growing companies. Getting together and starting to use a data catalog is a simple process, but starting to get adoption and having the dictionary exist as part of your workflow is a little bit more difficult. 

Even though it may seem like an easy task, getting different stakeholders to change their routines and start using a new tool can be very challenging. An example of the data catalog problems shared by one of the delivery companies we spoke with. At this company, it was difficult to get aligned on which tables were commonly used, joined, how they were used together and what columns meant. Similarly, it’s difficult to monitor the number of data assets that exist across different departments, especially when the number of resources grows at a faster rate than people. Why is this the case? 

Data is becoming more decentralized through concepts like the data mesh. As more teams outside of the data function start to use data in their day-to-day, different tables, dashboards and definitions are being created at an almost exponential rate. Data catalogs are important because they help you organize your data whether you are working with structured or unstructured data. They help you identify what kind of data you have, how it is related to each other and what the best means to store it is so that you can quickly find it when needed.

Below are the steps that teams need to take when creating a data catalog:

**1. Gather sources from across the organization** 

The first step data teams need to take is to collect the different resources that are scattered across different tools in the origination. This may require multiple meetings and stakeholders to come together and figure out which resources need to be in the catalog. Today, this collection could be done in a spreadsheet with an ongoing list of all resources and how they connect.

**2. Give each resource an owner**

After data teams have identified all the resources from across the company that they would like to include in their data catalog, we recommend assigning ownership to each resource. Teams that we’ve worked within the past have assigned ownership based on the source, schema or even domain. Teams that start assigning ownership should look for people who are familiar with the data knowledge they are responsible for managing and are willing to help others who want to learn how to use it. 

**3. Get support and sign off**

Once these meetings conclude and owners are on the same page, have the owners sign off on their responsibilities. The owners should be in alignment with the documentation and feel like the data team worked collaboratively with them to come to this ownership structure. One effective strategy is to involve the leadership team in the exercise early to make sure that their team leads are signing off on the owners of data. This way, leadership can see how widespread the understanding of data is across the company. If the team leadership team sees the value of a data catalog, this can move at a much faster pace.

**4. Integrate the catalog base into your workflow**

After data teams have received support for their data documentation process, they should look for ways to integrate this tool into their workflow. This step is critical for maintenance and upkeep. Without a tool that allows teammates to receive notifications on Slack, it will likely be forgotten. By creating a process around the data catalog, teams can ensure that it is not left behind as the team grows

**5. Upkeep the data catalog**

Although the documentation should be stable, it may need to change over time. One instance that might require documentation to change is when a new revenue stream is introduced or when the pricing of an existing revenue line changes. These changes traditionally come from the business team and might require the data team to implement the changes into the data catalog.

Teams that invest the time to get alignment using a data catalog can see major benefits in the long term as they make faster decisions as a team. Creating a data catalog is not a small undertaking. Tou can read the full step-by-step guide here if you found this post useful: [https://www.secoda.co/blog/how-to-create-a-data-catalog-a-step-by-step-guide](https://www.secoda.co/blog/how-to-create-a-data-catalog-a-step-by-step-guide)",True,False,False,dataengineering,t5_36en4,45532,public,self,Step by step guide to creating a data catalog,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r1g5mo/step_by_step_guide_to_creating_a_data_catalog/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,marshr9523,transparent,,[],19bba012-ac9d-11eb-b77b-0eec37c01719,Data Analyst,dark,text,t2_y3x7xvk,False,False,False,[],False,False,1637785236,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r1e8gv/resume_critique_data_analyst_transitioning_to_de/,{},r1e8gv,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,8,0,False,all_ads,/r/dataengineering/comments/r1e8gv/resume_critique_data_analyst_transitioning_to_de/,False,,,6,1637785247,1,"Hey guys!

I joined this sub about 4-5 months back, when I was almost clueless about what Data Engineering is. Since then, I've asked a lot of questions here, received a lot of answers, and learned a lot. And as my biggest step since then (at least according to me) -- I passed the Microsoft Azure Data Engineer certification this week! A big thanks to everyone on this sub, for the regular informative content and a pretty encouraging environment all around. 10/10 would recommend :)

So now, I feel that I can start applying for jobs with the knowledge I've gained and the certification as well added to my resume. I've created a draft version of my resume, it would be great if you guys could critique it and provide some feedback on it. I should specify that I've taken inspiration very heavily from this post - [please\_critique\_my\_resume\_data\_analyst](https://www.reddit.com/r/dataengineering/comments/q0alnb/please_critique_my_resume_data_analyst/). Got some really good pointers from there.

Tl;dr -  Was clueless about DE 5 months ago, now feel ready to start applying to DE jobs. Please critique my resume! Thanks everyone!

https://preview.redd.it/j0szi24iql181.png?width=707&amp;format=png&amp;auto=webp&amp;s=a33a3268ec3d8af4ad2ab4e6017cec89e1c19dd7",True,False,False,dataengineering,t5_36en4,45527,public,https://b.thumbs.redditmedia.com/g8REUK813XcRGtUQ0pMXkCqauDpqVMR9vD9wZ_GJGhM.jpg,Resume Critique: Data Analyst Transitioning to DE!,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r1e8gv/resume_critique_data_analyst_transitioning_to_de/,all_ads,6,,,,,,140.0,140.0,,"{'j0szi24iql181': {'e': 'Image', 'id': 'j0szi24iql181', 'm': 'image/png', 'p': [{'u': 'https://preview.redd.it/j0szi24iql181.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2586464d042b5342bfe4d289172e5362c1225e5d', 'x': 108, 'y': 152}, {'u': 'https://preview.redd.it/j0szi24iql181.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1cbeae5e78065e4eb4630c7acc72366a397a9dc7', 'x': 216, 'y': 305}, {'u': 'https://preview.redd.it/j0szi24iql181.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c0b42e7050178ea2e751adb159c46e0d19f73253', 'x': 320, 'y': 452}, {'u': 'https://preview.redd.it/j0szi24iql181.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=32e177d8737a44f5d9fb77e88686b12a767c5d78', 'x': 640, 'y': 905}], 's': {'u': 'https://preview.redd.it/j0szi24iql181.png?width=707&amp;format=png&amp;auto=webp&amp;s=a33a3268ec3d8af4ad2ab4e6017cec89e1c19dd7', 'x': 707, 'y': 1000}, 'status': 'valid'}}",,,,,,,,,
[],False,Complete_Solution_83,,,[],,,,text,t2_7q0rla1e,False,False,False,[],False,False,1637783464,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r1dl0n/what_should_i_choose_to_join/,{},r1dl0n,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,16,0,False,all_ads,/r/dataengineering/comments/r1dl0n/what_should_i_choose_to_join/,False,,,6,1637783475,1,"I got an offer from an analytics company for DE role. Now my current firm is tending to retain me. Help me decide what should I do. 

Pros if I stay with current company:
1. Same company but matching the offered compensation
2. Providing resources and mentorship to learn DE skills ( i do not work as a DE as of now) (Asked me to learn about data warehousing fundamentals, metallion, aws fundamentals, apache spark, databricks)
3. I can leave again if I don't like the learning
4. Smooth transition to DE profile; I can test the waters
5. Services company; probability to see the variety in work

Cons if I stay with current company:
1. Opportunity cost: I may not get a good job offer again

Pros if I join the analytics company - 
1. Part of founding member of data Engineering team; i can create impact early
2. They will train on the required skill sets by people from US
3. Smaller company (1000 employees globally), so greater visibility
4. Product based organization - they make analytical products
5. T shaped exposure
6. Diversity exposure (people from more than 70 countries)

Cons if I join Analytics company:
1. Might have to work on limited technologies which they use ( right now what i know is they work with tech like MSSQLSERVER, hadoop, snowflake and aws)
2. I am not sure if I should join a place as a starting data engineer where I have may have minimal peer guidance.

My decision:
Stay if they pay well &amp; opportunities to learn and work as DE. Otherwise, join the analytics company.

I can reveal company names on request.",True,False,False,dataengineering,t5_36en4,45526,public,self,What should I choose to join?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r1dl0n/what_should_i_choose_to_join/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Cool_Telephone,,,[],,,,text,t2_87zfi59a,False,False,False,[],False,False,1637782651,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r1da3s/tracking_an_application_pipeline/,{},r1da3s,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,8,0,False,all_ads,/r/dataengineering/comments/r1da3s/tracking_an_application_pipeline/,False,,,6,1637782662,1,"Hi all! Hoping someone out there can get their heads around this.

We have a business process with non-standard flow. A simplified example would be that there are 5 step. An application/object can go 1&gt;2&gt;4&gt;3&gt;5 or 2&gt;4&gt;3&gt;4&gt;2&gt;5. It can hit the same step multiple times. There are about 50 steps.

We’ve built two Kimball model fact tables:

An accumulating snapshot at the application grain which tracks the first and last time for each status; and

A standard transactional fact with columns for application, status, date time, and a metric for ‘days from account creation’.

The second of these allows us to take any application at any two status/date time points, take one ‘days from account creation’ away from the other and get the days between the two.

What the business wants is a ‘velocity chart’. They want to select a start and end process step and based on a start month see the percentage of applications that got to the end process step in 1 day, 2 days, 3 days —&gt; n days. The aim is to see that over a period of months, we have increased the average speed between the two given steps.

I’m having a hard time wrapping my head around the SQL, primarily based on the challenge that a application can go through the same step multiple times.

I’m not looking for someone to work this out/send me the SQL. But I am hoping for some advice/pointers to head me in the right direction.",True,False,False,dataengineering,t5_36en4,45526,public,self,Tracking an Application Pipeline,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r1da3s/tracking_an_application_pipeline/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Method0,,,[],,,,text,t2_h9uf236,False,False,False,[],False,False,1637778742,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r1btfn/how_valuable_are_certifications/,{},r1btfn,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,6,0,False,all_ads,/r/dataengineering/comments/r1btfn/how_valuable_are_certifications/,False,,,6,1637778753,1,And if they are which ones are best to get?,True,False,False,dataengineering,t5_36en4,45519,public,self,How valuable are certifications?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r1btfn/how_valuable_are_certifications/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,dumbledore__,,,[],,,,text,t2_2qbyuqm,False,False,False,[],False,False,1637776460,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r1ayzf/development_environment_for_emr/,{},r1ayzf,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,5,0,False,all_ads,/r/dataengineering/comments/r1ayzf/development_environment_for_emr/,False,,,6,1637776472,1,"I have previously developed in apache spark using Databricks.  The notebook experience there makes developing and debugging functions very straightforward.  

I have recently joined a new organization that uses AWS EMR.  The devs here are developing and submitting their entire script to the cluster each time they test a script.  This is clearly not ideal, because it's time consuming and puts stress on data sources which are being hit many times for testing purposes.  For example, most of these scripts are hitting a RedShift Cluster or Hive Tables in S3.

EMR Developers out there.  What kind of development environment would you recommend?  Should I  develop locally in pyspark and replace environment variables when I push code to EMR? Use EMR Studio?  Something else?",True,False,False,dataengineering,t5_36en4,45519,public,self,Development Environment for EMR,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r1ayzf/development_environment_for_emr/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Delicious_Attempt_99,,,[],,,,text,t2_ci308gob,False,False,False,[],False,False,1637775962,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r1as97/spark_collect_method/,{},r1as97,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/r1as97/spark_collect_method/,False,,,6,1637775973,1,"In my current project,  I have spark integrated with livy in an interactive mode. Query is our input, when we fire a query from livy, it executes on presto and the presto results needs to be sent back to livy. We are creating data frame from the presto results. How can I get the results back to Livy without using the df.collect() method. At present we are using collect for development, but I worry about its impact when we have huge data.
Can anyone suggest me a good approach? Thanks in advance",True,False,False,dataengineering,t5_36en4,45519,public,self,Spark collect method.,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r1as97/spark_collect_method/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,BoiElroy,,,[],,,,text,t2_r8dyi,False,False,True,[],False,False,1637772899,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r19lyx/has_anyone_actually_used_palantir_products/,{},r19lyx,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,14,0,False,all_ads,/r/dataengineering/comments/r19lyx/has_anyone_actually_used_palantir_products/,False,,,6,1637772909,1,"A job post I looked at mentioned it. I can't find a lot of information about it apart from some YouTube demos.

Has anyone here used it? And what are your thoughts?",True,False,False,dataengineering,t5_36en4,45517,public,self,Has anyone actually used Palantir products?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r19lyx/has_anyone_actually_used_palantir_products/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,JY-DataMechanics,,,[],,,,text,t2_4yd2q7g5,False,False,False,[],False,False,1637770708,datamechanics.co,https://www.reddit.com/r/dataengineering/comments/r18t8r/tutorial_run_your_r_sparklyr_workloads_at_scale/,{},r18t8r,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r18t8r/tutorial_run_your_r_sparklyr_workloads_at_scale/,False,link,"{'enabled': False, 'images': [{'id': 'SZtEFnRXNfLjH8mO79QiL2DuWyCGnxuNfuVZZGC1PHU', 'resolutions': [{'height': 60, 'url': 'https://external-preview.redd.it/M_w1OVwqXXePMcX4ylysQDGrBZ6gAA4a4MKzDCkLlZw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f30d1f540522c1d640b288c378fd7b016e781c1f', 'width': 108}, {'height': 121, 'url': 'https://external-preview.redd.it/M_w1OVwqXXePMcX4ylysQDGrBZ6gAA4a4MKzDCkLlZw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f57dba262efadfc0284961486403e07e228b8d2c', 'width': 216}, {'height': 180, 'url': 'https://external-preview.redd.it/M_w1OVwqXXePMcX4ylysQDGrBZ6gAA4a4MKzDCkLlZw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5589e65552fc7b2ea1a4805f30e7c0f00f03618c', 'width': 320}, {'height': 360, 'url': 'https://external-preview.redd.it/M_w1OVwqXXePMcX4ylysQDGrBZ6gAA4a4MKzDCkLlZw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f5134f34ef9e13efa94eb2c1756ff7c8a1b5d1dd', 'width': 640}, {'height': 540, 'url': 'https://external-preview.redd.it/M_w1OVwqXXePMcX4ylysQDGrBZ6gAA4a4MKzDCkLlZw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1bdd54a5683d9ef3a94aea9f5d491a3a15ff5956', 'width': 960}], 'source': {'height': 540, 'url': 'https://external-preview.redd.it/M_w1OVwqXXePMcX4ylysQDGrBZ6gAA4a4MKzDCkLlZw.jpg?auto=webp&amp;s=d962b8b3eb09852cd56295277ae29509f92b2492', 'width': 960}, 'variants': {}}]}",6,1637770719,1,,True,False,False,dataengineering,t5_36en4,45513,public,https://a.thumbs.redditmedia.com/A-a0lJhHv_KeUshWFGnbV4gPrIToKXHWggHuOdAJNM0.jpg,Tutorial: Run your R (SparklyR) workloads at scale with Spark-on-Kubernetes - Data Mechanics Blog,0,[],1.0,https://www.datamechanics.co/blog-post/tutorial-run-your-r-sparklyr-workloads-at-scale-with-spark-on-kubernetes,all_ads,6,,,,,,78.0,140.0,https://www.datamechanics.co/blog-post/tutorial-run-your-r-sparklyr-workloads-at-scale-with-spark-on-kubernetes,,,,,,,,,,
[],False,unity-dino,,,[],,,,text,t2_e6f08552,False,False,False,[],False,False,1637765070,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r16rxy/speeding_up_power_bi_etl_using_python/,{},r16rxy,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/r16rxy/speeding_up_power_bi_etl_using_python/,False,,,6,1637765081,1,I have a Power BI dashboard that takes too long to run for business needs. How do I speed up the process? I think the data is too large and the SQL queries take too long to run. How can I use Python to improve the ETL process. Wouldn’t python still have to read the SQL data to transform it? Can the process of transformations and loading take place simultaneously then go to Power BI?,True,False,False,dataengineering,t5_36en4,45507,public,self,Speeding up Power BI ETL using Python,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r16rxy/speeding_up_power_bi_etl_using_python/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Benoitale,,,[],,,,text,t2_12aeue,False,False,False,[],False,False,1637759937,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r1505v/oss_vs_proprietary/,{},r1505v,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,14,0,False,all_ads,/r/dataengineering/comments/r1505v/oss_vs_proprietary/,False,,,6,1637759947,1,"There are so many tooling options, how does everyone decide between OSS vs Prop?

Do you have a criteria or factors you evaluate on?",True,False,False,dataengineering,t5_36en4,45500,public,self,OSS vs Proprietary,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r1505v/oss_vs_proprietary/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Born-Comment3359,,,[],,,,text,t2_5t56uq7x,False,False,False,[],False,False,1637758460,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r14k34/which_tech_skills_are_the_most_marketable_in_the/,{},r14k34,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,55,0,False,all_ads,/r/dataengineering/comments/r14k34/which_tech_skills_are_the_most_marketable_in_the/,False,,,6,1637758471,1,"I heard Scala/Spark is really marketable and pays a lot and has a high demand with low supply ot talent. Is that true? 
What other tech skills are in high demand and pay well?",True,False,False,dataengineering,t5_36en4,45500,public,self,Which tech skills are the most marketable in the data engineering job market?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r14k34/which_tech_skills_are_the_most_marketable_in_the/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,rafinirovannoe,,,[],,,,text,t2_1nv4lffu,False,False,False,[],False,False,1637744945,medium.com,https://www.reddit.com/r/dataengineering/comments/r110cr/landing_data_on_s3_the_good_the_bad_and_the_ugly/,{},r110cr,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r110cr/landing_data_on_s3_the_good_the_bad_and_the_ugly/,False,link,"{'enabled': False, 'images': [{'id': 'Xz1f3oFtB6enwlW4Bg1ueIviJH6NHXGwOD-KJVNRnzs', 'resolutions': [{'height': 56, 'url': 'https://external-preview.redd.it/hH5CqGpd1h29nWCefZjFYtFJYx0kwFfq1nJ72i-jLxo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b808a3aa6e609a58dde7550e05a11d25f88527c1', 'width': 108}, {'height': 112, 'url': 'https://external-preview.redd.it/hH5CqGpd1h29nWCefZjFYtFJYx0kwFfq1nJ72i-jLxo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=653d15ec08c315f15dfe87f20b11f122f78d4b0a', 'width': 216}, {'height': 166, 'url': 'https://external-preview.redd.it/hH5CqGpd1h29nWCefZjFYtFJYx0kwFfq1nJ72i-jLxo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=368c8f551b0d0100d6aa17a0901b05ab52cb0ebd', 'width': 320}, {'height': 332, 'url': 'https://external-preview.redd.it/hH5CqGpd1h29nWCefZjFYtFJYx0kwFfq1nJ72i-jLxo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=797f96c9cf8d43c38bdbce2d96c9a78644b06a79', 'width': 640}, {'height': 498, 'url': 'https://external-preview.redd.it/hH5CqGpd1h29nWCefZjFYtFJYx0kwFfq1nJ72i-jLxo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dc5cb6c964bbab9432aead8398d529890eb85920', 'width': 960}, {'height': 560, 'url': 'https://external-preview.redd.it/hH5CqGpd1h29nWCefZjFYtFJYx0kwFfq1nJ72i-jLxo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=899726bd5be00ffa075ab3ee67425266a873d4c4', 'width': 1080}], 'source': {'height': 623, 'url': 'https://external-preview.redd.it/hH5CqGpd1h29nWCefZjFYtFJYx0kwFfq1nJ72i-jLxo.jpg?auto=webp&amp;s=0949170749d2d32b83171c160822566b126a425f', 'width': 1200}, 'variants': {}}]}",6,1637744955,1,,True,False,False,dataengineering,t5_36en4,45500,public,https://b.thumbs.redditmedia.com/sTPJcZ3tsu_CV0LQd36VqHSFzM3egPC78JkKa5XpMVE.jpg,"Landing data on S3: the good, the bad and the ugly",0,[],1.0,https://medium.com/joom/landing-data-on-s3-the-good-the-bad-and-the-ugly-ca42a1d4408d,all_ads,6,,,,,,72.0,140.0,https://medium.com/joom/landing-data-on-s3-the-good-the-bad-and-the-ugly-ca42a1d4408d,,,,,,,,,,
[],False,soobrosa,,,[],,,,text,t2_4v8u9xg7,False,False,False,[],False,False,1637744229,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r10txy/parttime_data_engineering_bootcamp_for_the_us/,{},r10txy,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,16,0,False,all_ads,/r/dataengineering/comments/r10txy/parttime_data_engineering_bootcamp_for_the_us/,False,self,"{'enabled': False, 'images': [{'id': 'QV55KXidu7y1Lqkwrjt9JNearespeP_ZMUK_mHGdSbQ', 'resolutions': [{'height': 108, 'url': 'https://external-preview.redd.it/i1hkhOsywqjCKIdXVz0mzOTsDkNyPqCTiXiGiM7ekXU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=464fb116aa276630a48951b50f8eeba58a5fc0b4', 'width': 108}, {'height': 216, 'url': 'https://external-preview.redd.it/i1hkhOsywqjCKIdXVz0mzOTsDkNyPqCTiXiGiM7ekXU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fd713e623f84fd5333db9f84e0d416cf4ee991e4', 'width': 216}, {'height': 320, 'url': 'https://external-preview.redd.it/i1hkhOsywqjCKIdXVz0mzOTsDkNyPqCTiXiGiM7ekXU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=65f648658ce2835097d919ea64e3c572b6ce4b6a', 'width': 320}, {'height': 640, 'url': 'https://external-preview.redd.it/i1hkhOsywqjCKIdXVz0mzOTsDkNyPqCTiXiGiM7ekXU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b8d87196bced31cb712d2ad41184c9e3b78ada99', 'width': 640}, {'height': 960, 'url': 'https://external-preview.redd.it/i1hkhOsywqjCKIdXVz0mzOTsDkNyPqCTiXiGiM7ekXU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3fc7335b5a35d162e3fa6bb32bb235d80924076e', 'width': 960}, {'height': 1080, 'url': 'https://external-preview.redd.it/i1hkhOsywqjCKIdXVz0mzOTsDkNyPqCTiXiGiM7ekXU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2ad85ab32027646311fc1d71f8d6c0e796f31f17', 'width': 1080}], 'source': {'height': 1500, 'url': 'https://external-preview.redd.it/i1hkhOsywqjCKIdXVz0mzOTsDkNyPqCTiXiGiM7ekXU.jpg?auto=webp&amp;s=2faa84eb7d97ebf07c9b40c6642c7356009cbfee', 'width': 1500}, 'variants': {}}]}",6,1637744239,1,"Out of stealth: are you located in the United States?

Join our new part-time course launching January 2022! Book a call with us!  
[https://www.dataengineering.academy](https://www.dataengineering.academy)",True,False,False,dataengineering,t5_36en4,45500,public,self,Part-time data engineering bootcamp for the US,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r10txy/parttime_data_engineering_bootcamp_for_the_us/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,jonnyshitknuckles,,,[],,,,text,t2_16w366,False,False,False,[],False,False,1637729878,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r0wwvl/ideas_on_how_to_use_my_data_skills_for_a_good/,{},r0wwvl,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,22,0,False,all_ads,/r/dataengineering/comments/r0wwvl/ideas_on_how_to_use_my_data_skills_for_a_good/,False,,,6,1637729889,1,"Hey y'all, I've been trying to think of ways to do something positive with my job skills. Python, SQL,  automation, ML. Any of that. 

Ideally I'd like to build a project that helps people in need. Not thinking too much about mentoring or tutoring as I already do a bit and would like to do something else.

Any ideas or experience is appreciated!",True,False,False,dataengineering,t5_36en4,45492,public,self,Ideas on how to use my data skills for a good cause?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r0wwvl/ideas_on_how_to_use_my_data_skills_for_a_good/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,BoiElroy,,,[],,,,text,t2_r8dyi,False,False,True,[],False,False,1637728674,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r0wk78/is_it_just_me_or_is_this_job_description_asking/,{},r0wk78,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,9,0,False,all_ads,/r/dataengineering/comments/r0wk78/is_it_just_me_or_is_this_job_description_asking/,False,,,6,1637728686,1,[https://jobsearcher.com/j/sr-data-engineer-at-supernal-in-irvine-ca-OpjAbEV](https://jobsearcher.com/j/sr-data-engineer-at-supernal-in-irvine-ca-OpjAbEV),True,False,False,dataengineering,t5_36en4,45491,public,self,Is it just me or is this job description asking for a lot...,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r0wk78/is_it_just_me_or_is_this_job_description_asking/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,BoiElroy,,,[],,,,text,t2_r8dyi,False,False,True,[],False,False,1637728103,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r0wdsx/lakefs_and_the_problem_of_data_lake_management/,{},r0wdsx,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,3,0,False,all_ads,/r/dataengineering/comments/r0wdsx/lakefs_and_the_problem_of_data_lake_management/,False,,,6,1637728114,1,"I've been looking through LakeFS's documentation and am quite sold on the data as code idea. Branching, merging, commits and references. Seems like it could get messy the way git does at times, but if you know what you're doing could be quite powerful.

I don't see a whole lot out there about LakeFS though. It's a new-ish product I guess, but was curious what other people in this community are doing today for data lake versioning, and what they wish they were doing lol.

Specifically, apart from roll backs and atomic transactions, the part that escapes me most is the use case of needing to tie different versions of ML models (differentiated by git commit ids) to specific snapshots of data. The data can both have been appended to as well as have been updated. Gets messy for reproducibility. But is an important factor as the AI ethics regulations are starting to loom.",True,False,False,dataengineering,t5_36en4,45491,public,self,LakeFS and the problem of data lake management,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r0wdsx/lakefs_and_the_problem_of_data_lake_management/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,International-Life17,,,[],,,,text,t2_8xxbnh8b,False,False,False,[],False,False,1637727691,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r0w9bg/dealing_with_schema_changes_with_an_existing/,{},r0w9bg,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,12,0,False,all_ads,/r/dataengineering/comments/r0w9bg/dealing_with_schema_changes_with_an_existing/,False,,,6,1637727702,1,"Let’s say my pipeline is very simple batch pipeline scheduled by airflow. 

I stage data to s3 from rest apis using a python program. 

And then I process the data, my business logic here 

And finally store the data in an un managed hive table for querying which acts like a clean table for analysis. It is stored in parquet format in s3 

Suppose if the data changes at source. Like few columns added and I want to have these columns added in my table as well.

How would I handle this? Make changes in the airflow day to create another branch called v2 and create another table and store the results or migrate the existing hive table?

Ideally I want to have this with minimal code changes. 

Please let me know your thoughts. Thanks.",True,False,False,dataengineering,t5_36en4,45491,public,self,Dealing with schema changes with an existing pipeline,0,[],0.99,https://www.reddit.com/r/dataengineering/comments/r0w9bg/dealing_with_schema_changes_with_an_existing/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Ad_Fun123,,,[],,,,text,t2_guyybq99,False,False,False,[],False,False,1637717455,books.google.com,https://www.reddit.com/r/dataengineering/comments/r0sw2m/the_text_mining_handbook_advanced_approaches_in/,{},r0sw2m,False,False,False,False,False,False,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r0sw2m/the_text_mining_handbook_advanced_approaches_in/,False,link,"{'enabled': False, 'images': [{'id': 'mMF7-uPqGIidk4WbZ_gcIDybaUeFpfU3LhfPz8KUMVc', 'resolutions': [{'height': 161, 'url': 'https://external-preview.redd.it/UADK4a9jpskIJIEz4obPyO0ByVK01y7_JT4GheEXY2U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0aaa69ad6c99fc896b288d043bef8f2e5d76f8e6', 'width': 108}], 'source': {'height': 191, 'url': 'https://external-preview.redd.it/UADK4a9jpskIJIEz4obPyO0ByVK01y7_JT4GheEXY2U.jpg?auto=webp&amp;s=3f6c04d56d9020bcee3c0c98eb5305e7b03eacbd', 'width': 128}, 'variants': {}}]}",6,1637717466,1,,True,False,False,dataengineering,t5_36en4,45480,public,https://b.thumbs.redditmedia.com/QkW0QVYfYJ_08mDf6f_F0ctKDjt7XcbWNQvOsx4vVoM.jpg,"The Text Mining Handbook: Advanced Approaches in Analyzing Unstructured Data - Ronen Feldman, James Sanger",0,[],1.0,https://books.google.com/books?hl=en&amp;lr=&amp;id=U3EA_zX3ZwEC&amp;oi=fnd&amp;pg=PR1&amp;dq=data+science+research&amp;ots=2OzJOaDyNG&amp;sig=DB5vEatINqz3qERInIdfQS2MoXM#v=onepage&amp;q=data%20science%20research&amp;f=false,all_ads,6,,,reddit,,,70.0,70.0,https://books.google.com/books?hl=en&amp;lr=&amp;id=U3EA_zX3ZwEC&amp;oi=fnd&amp;pg=PR1&amp;dq=data+science+research&amp;ots=2OzJOaDyNG&amp;sig=DB5vEatINqz3qERInIdfQS2MoXM#v=onepage&amp;q=data%20science%20research&amp;f=false,,,,,,,,,,
[],False,coding_up_a_storm,,,[],,,,text,t2_2ts0nofm,False,False,False,[],False,False,1637714972,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r0s1r8/software_engineering_interview_course_by_pramp/,{},r0s1r8,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/r0s1r8/software_engineering_interview_course_by_pramp/,False,self,"{'enabled': False, 'images': [{'id': '2QRS2d3sLshQwA7TBjtYPTGixJiH65Fy7eZBVpK4TBE', 'resolutions': [{'height': 63, 'url': 'https://external-preview.redd.it/ibgwP4jCr_gp0v7RVCOzKklLdvcnvuExRrJFQZa-p-k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=deae7cc101dd9066e8910636efd8ce174ed400c2', 'width': 108}, {'height': 126, 'url': 'https://external-preview.redd.it/ibgwP4jCr_gp0v7RVCOzKklLdvcnvuExRrJFQZa-p-k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=da02864ae16f7169ef6d3b1dfd10ce921d021050', 'width': 216}, {'height': 186, 'url': 'https://external-preview.redd.it/ibgwP4jCr_gp0v7RVCOzKklLdvcnvuExRrJFQZa-p-k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ed1857b6d8bce22d22d33080870d9a3c520bae75', 'width': 320}, {'height': 373, 'url': 'https://external-preview.redd.it/ibgwP4jCr_gp0v7RVCOzKklLdvcnvuExRrJFQZa-p-k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6f8968ed8b211631331adb95d88b0a082b470835', 'width': 640}], 'source': {'height': 420, 'url': 'https://external-preview.redd.it/ibgwP4jCr_gp0v7RVCOzKklLdvcnvuExRrJFQZa-p-k.jpg?auto=webp&amp;s=2ae199f398b33180c727687cc1eb6b201ccb948a', 'width': 720}, 'variants': {}}]}",6,1637714982,1,"[Course link](https://www.tryexponent.com/courses/software-engineering?ref=pramp&amp;promo_code=BLACKFRIDAY&amp;utm_source=drip&amp;utm_medium=email&amp;utm_campaign=Just+launched%3A+Software+engineering+interview+course)

I just received this promotion via email. Do you feel this course is beneficial for preparing for DE jobs in NYC area fintech and healthcare? I specifically wonder about the content under the ""System Design Questions"" heading where it discusses building Netflix and Twitter. Is this material relevant to what I am doing?

My job targets:
Location: NYC area
Industry: Finance (primary), Healthcare (secondary)
Title: DE
Will learn: Big data stack",True,False,False,dataengineering,t5_36en4,45478,public,self,Software Engineering Interview Course (by Pramp),0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r0s1r8/software_engineering_interview_course_by_pramp/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,_Niwubo,,,[],,,,text,t2_7q001ke0,False,False,False,[],False,False,1637709908,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r0qajn/data_mesh_old_idea_new_label/,{},r0qajn,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,16,0,False,all_ads,/r/dataengineering/comments/r0qajn/data_mesh_old_idea_new_label/,False,self,"{'enabled': False, 'images': [{'id': 'cV-ntZDeLLeFvILYftCPF745xYUz4lDtniShO4TtrL0', 'resolutions': [{'height': 54, 'url': 'https://external-preview.redd.it/QC2_FlC0bhiMFzRvLbV4IC2aCyToKi32l0OP66NGs60.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2c63d996825147e0333359406ae252c45e33bb01', 'width': 108}, {'height': 109, 'url': 'https://external-preview.redd.it/QC2_FlC0bhiMFzRvLbV4IC2aCyToKi32l0OP66NGs60.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=481133144d5967227873ed2031c4b5a59e42b237', 'width': 216}, {'height': 162, 'url': 'https://external-preview.redd.it/QC2_FlC0bhiMFzRvLbV4IC2aCyToKi32l0OP66NGs60.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e76ddfe995b3c6126c1ad815c343ef9904d536f8', 'width': 320}, {'height': 324, 'url': 'https://external-preview.redd.it/QC2_FlC0bhiMFzRvLbV4IC2aCyToKi32l0OP66NGs60.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2d5100f3c8fe806f61eb354f58cba25436e639f5', 'width': 640}, {'height': 486, 'url': 'https://external-preview.redd.it/QC2_FlC0bhiMFzRvLbV4IC2aCyToKi32l0OP66NGs60.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c5e88991a6c2ccbf8d67415ea2cd13aeffc12539', 'width': 960}, {'height': 547, 'url': 'https://external-preview.redd.it/QC2_FlC0bhiMFzRvLbV4IC2aCyToKi32l0OP66NGs60.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ed42b5d8d06ce94c23a0015a214636462493831d', 'width': 1080}], 'source': {'height': 608, 'url': 'https://external-preview.redd.it/QC2_FlC0bhiMFzRvLbV4IC2aCyToKi32l0OP66NGs60.jpg?auto=webp&amp;s=5103d146cf9e6a802c95d543401a80cb7353b3e8', 'width': 1200}, 'variants': {}}]}",6,1637709919,1,"I cant completely understand how the Data Mesh is different from the idea of dimensional modelling and Data marts (kimball called the concept Independent data marts) .

&amp;#x200B;

suggested read with inspiration: [https://towardsdatascience.com/data-mesh-pain-points-b4bebca37357](https://towardsdatascience.com/data-mesh-pain-points-b4bebca37357) (all credit to the author  Andriy Zabavskyy)

&amp;#x200B;

My current understanding of the data mesh

The idea is that you govern and develop data products based on Domain (the exact same idea with data marts specific on deparment). Some Dimension (kimball terminology) are crossing domains (Shared dimensions). however instead of having a Data Engineering team working on a central Data Layer (DataWarehouse/Data Lake/Data Lakehouse), they are split into the specific domain: e.g. HR/Finance and being responsible for all the data logic and glossary of the domain. Therefore the Data Engineer will be closer to the business and understand their data needs better than any central team will ever be able to. However the new decentral nature of domain specific Data Engineers requres them to work with strict rules to ensure that 1) You keep one source of truth and 2) maintain the same tooling and 3) Follow best practice ETL-rules 4) work into a single Data Layer architecture (meaning all data from various sources are fetched into a single place - be that a data lake or a database based on the nature of the data. so that all the integrations from systems are done centrally, before cleasing, manipulation and enrichment work are carried out in relation to the data domains). 

&amp;#x200B;

Am i on the right track or is there something im missing? All your ideas and thoughts on the subject will be greatly appreaciated!",True,False,False,dataengineering,t5_36en4,45475,public,self,Data Mesh - Old Idea New Label?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r0qajn/data_mesh_old_idea_new_label/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,jmhcodes,,,[],,,,text,t2_5hwkkgrt,False,False,False,[],False,False,1637705711,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r0osxp/how_to_replicate_production_mysql_data_for_devs/,{},r0osxp,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,12,0,False,all_ads,/r/dataengineering/comments/r0osxp/how_to_replicate_production_mysql_data_for_devs/,False,,,6,1637705722,1,"We have a *large (\~200gb)* AWS RDS MySQL production database that runs our web app. 

Previously when a dev needed to create new features or alter tables, they would run a mysql dump from our production database and load the data into their own local instance of MySQL then begin development. But the mysql dump process anonymizes/scrubs the data and takes about \~4-5 hours each time.   
Instead of doing the dump which takes a lot of time, we created a docker image that fills a MySQL database with a prepared seeding dataset. But this seeding dataset will need to be manually up kept to mimic the production database, i.e. when a column is added to production, that column will need to be manually added to the seeding dataset. So the seeding dataset will need to be maintained by someone to ensure it is robust enough for dev work in the future.

Our data is customer oriented, so we thought about taking a subset of customers and using that subset to seed the rest of our database but were unsure how we would enforce all the foreign key constraints associated for each customer and the related tables within the subset.

What methods do you all use to replicate production servers so that devs can have their own, independent development database?",True,False,False,dataengineering,t5_36en4,45472,public,self,How to replicate Production MySQL data for Devs to use in local Docker,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r0osxp/how_to_replicate_production_mysql_data_for_devs/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,fn-mlengineer,,,[],,,,text,t2_24y1aelz,False,False,False,[],False,False,1637702914,medium.com,https://www.reddit.com/r/dataengineering/comments/r0nrtz/how_a_data_lakehouse_helps_with_ma_integration/,{},r0nrtz,False,False,False,False,False,False,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r0nrtz/how_a_data_lakehouse_helps_with_ma_integration/,False,link,"{'enabled': False, 'images': [{'id': 'sUch_xKsINHWuWGGs9FBn4MvwW1HjjXV3VNhe79WwJ8', 'resolutions': [{'height': 33, 'url': 'https://external-preview.redd.it/wEa_oxHjx3qI78fS4L0HWKKvRWNP0qQrf-dY0AjzXOE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c6a832f36463a2244f0180fb3b4bfbf67d2cdf9a', 'width': 108}, {'height': 66, 'url': 'https://external-preview.redd.it/wEa_oxHjx3qI78fS4L0HWKKvRWNP0qQrf-dY0AjzXOE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6739da4ac90b7626006a7bf742f87a62bc08df12', 'width': 216}, {'height': 98, 'url': 'https://external-preview.redd.it/wEa_oxHjx3qI78fS4L0HWKKvRWNP0qQrf-dY0AjzXOE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=51a5a83b4b6662916df50f01769c78674f7ce301', 'width': 320}, {'height': 196, 'url': 'https://external-preview.redd.it/wEa_oxHjx3qI78fS4L0HWKKvRWNP0qQrf-dY0AjzXOE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=947f6ec24dff8ff0d3edc7711b6f09c5219c2c06', 'width': 640}, {'height': 294, 'url': 'https://external-preview.redd.it/wEa_oxHjx3qI78fS4L0HWKKvRWNP0qQrf-dY0AjzXOE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=26c76ae05dff47df675285b835131bda297dc761', 'width': 960}, {'height': 331, 'url': 'https://external-preview.redd.it/wEa_oxHjx3qI78fS4L0HWKKvRWNP0qQrf-dY0AjzXOE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c4827b3424c50376afbf91bb19a21916ac3d00ab', 'width': 1080}], 'source': {'height': 368, 'url': 'https://external-preview.redd.it/wEa_oxHjx3qI78fS4L0HWKKvRWNP0qQrf-dY0AjzXOE.jpg?auto=webp&amp;s=2dc3214fb71aa07bd1153a3d2b3d6c802a30c067', 'width': 1200}, 'variants': {}}]}",6,1637702925,1,,True,False,False,dataengineering,t5_36en4,45470,public,https://b.thumbs.redditmedia.com/UbOwFDLsB7_RkrZCA_kh_tbIaGT0yP6T02WoLxe-8UM.jpg,How a data lakehouse helps with M&amp;A integration,0,[],1.0,https://medium.com/fiscalnoteworthy/how-fiscalnote-is-leveraging-a-data-lakehouse-to-accelerate-integration-from-m-a-eb4fbaf0eb5f,all_ads,6,,,automod_filtered,,,42.0,140.0,https://medium.com/fiscalnoteworthy/how-fiscalnote-is-leveraging-a-data-lakehouse-to-accelerate-integration-from-m-a-eb4fbaf0eb5f,,,,,,,,,,
[],False,ronald_r3,,,[],,,,text,t2_zdh50,False,False,False,[],False,False,1637698251,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r0m16z/guidance_on_whether_to_store_data_using_temporal/,{},r0m16z,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,9,0,False,all_ads,/r/dataengineering/comments/r0m16z/guidance_on_whether_to_store_data_using_temporal/,False,self,"{'enabled': False, 'images': [{'id': 'LVmzWMJU1UZwRubzQYJZSar-z-Rq8ntUH65yhQyfxB8', 'resolutions': [{'height': 108, 'url': 'https://external-preview.redd.it/YH8gNap4KoGcFgyFYrNZ86fXmYfRn6pa7uuwIlZkjEE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0b9526a51504048891d5e64783519fd5dc3cd83f', 'width': 108}, {'height': 216, 'url': 'https://external-preview.redd.it/YH8gNap4KoGcFgyFYrNZ86fXmYfRn6pa7uuwIlZkjEE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0150bc3ab1c6838c35ff951d69578f3d19ae4ed3', 'width': 216}, {'height': 320, 'url': 'https://external-preview.redd.it/YH8gNap4KoGcFgyFYrNZ86fXmYfRn6pa7uuwIlZkjEE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c1e830770227ae4802c5776d22f63d0f6aa71b15', 'width': 320}], 'source': {'height': 400, 'url': 'https://external-preview.redd.it/YH8gNap4KoGcFgyFYrNZ86fXmYfRn6pa7uuwIlZkjEE.jpg?auto=webp&amp;s=e62264227377a9581e2e2946169864d130fa3217', 'width': 400}, 'variants': {}}]}",6,1637698262,1,"Good Afternoon,

I was just looking for some advice on how to store my data as I know I have multiple alternatives and not really sure which one is the best.

Some background first... I have a project where I'm extracting data from Yelp's rest API which I'm looking to store in a regular SQL server database and then from there ingest the data into a DWH and after integrate an API so people can query the data.This API might even implement a ML algo I have to see.

Now the JSON data has this structure approximately (see the image below 😀) and there's also other endpoints from which I plan on ingesting data but this here is the focus of my question.

With that in mind I know making decisions about design always should have the end goal in mind so the SQL server database would serve as a way for users to do time series analysis on a businesses rating and review count and see how that changes over time.

So because the JSON shows the current review count of a business I decided to treat the data as one would treat stock data and just add new records at a set time each day (at least initially as I would like to change this later on)  so technically this would involve incremental data loads. However with this in mind I came across this feature of temporal tables and in the [Microsoft documentation](https://docs.microsoft.com/en-us/sql/relational-databases/tables/temporal-table-usage-scenarios?view=sql-server-ver15) specifically in the section titled ***OLTP with auto-generated data history*** they have an usage scenario regarding inventory management which is technically the same as my usage scenario. For example they are maintaining the inventory count of a product and in my case I'm doing the same thing but with a business' review count.

So now I'm not sure whether I should go ahead with the temporal tables because I'm not sure if there could be unforeseen negative consequences in the future or if just setting up 2 datetime columns apart from a temporal table would be better? Maybe they're accomplishing the same thing and temporal tables are more simple to use? Maybe I'm coming at this from a completely erroneous perspective?

I would like to add as well that I plan on using temporal tales regardless on other tables in my db as I would like to implement SCD 2 in my DWH for the potential changes in the dimensions.

And also if it helps I've thought about changing the way I represent changes in a businesses review count by just programming a way for a difference in the review count between the businesses current state,let's say today, and a previous state ,let's say it's last place in the table, and like so creating a sort of ""transaction based table"" that just adds reviews into the table so kind of like purchases but not quite immutable data.

So I hope I explained myself and I would be willing to provide a lot more information. If you made it this far I'm already grateful haha. Any suggestions/help is welcome. Thanks in advance 😎👌

https://preview.redd.it/lh4w0btmfe181.png?width=1600&amp;format=png&amp;auto=webp&amp;s=cb7c080f99b0c6dd154d0c3bf24b5bc248def600",True,False,False,dataengineering,t5_36en4,45462,public,https://a.thumbs.redditmedia.com/GvIbBRWVxxy5FQSjCZHKkK6hXl73OcklJVC9eamH358.jpg,Guidance on whether to store data using temporal tables or just using 2 columns for representing time in MSSQL Server,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r0m16z/guidance_on_whether_to_store_data_using_temporal/,all_ads,6,,,,,,78.0,140.0,,"{'lh4w0btmfe181': {'e': 'Image', 'id': 'lh4w0btmfe181', 'm': 'image/png', 'p': [{'u': 'https://preview.redd.it/lh4w0btmfe181.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1135145565f3466c87d38cce25526f2dd01bfa22', 'x': 108, 'y': 60}, {'u': 'https://preview.redd.it/lh4w0btmfe181.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4c0a2693cdb9189f95e031cf676a960c69cec7e8', 'x': 216, 'y': 121}, {'u': 'https://preview.redd.it/lh4w0btmfe181.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b744b62f1de35821c30a73484fd514b6a85ce262', 'x': 320, 'y': 180}, {'u': 'https://preview.redd.it/lh4w0btmfe181.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c2fbb5abb603e7f7b3611668162fd0706431f1e4', 'x': 640, 'y': 360}, {'u': 'https://preview.redd.it/lh4w0btmfe181.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d7edee5059a969314bac65f5f58d4baa058a9f14', 'x': 960, 'y': 540}, {'u': 'https://preview.redd.it/lh4w0btmfe181.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=467839fa32c5d475668574de3a1a2490854b910b', 'x': 1080, 'y': 607}], 's': {'u': 'https://preview.redd.it/lh4w0btmfe181.png?width=1600&amp;format=png&amp;auto=webp&amp;s=cb7c080f99b0c6dd154d0c3bf24b5bc248def600', 'x': 1600, 'y': 900}, 'status': 'valid'}}",,,,,,,,,
[],False,authentichooman,,,[],,,,text,t2_dgq3lsfa,False,False,False,[],False,False,1637685964,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r0hejg/eu_folks_frankfurt_or_zurich/,{},r0hejg,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/r0hejg/eu_folks_frankfurt_or_zurich/,False,,,6,1637685974,1,"I have an offer from both Frankfurt and Zurich based org , same role and responsibilities 

Frankurt - 80k euro , Zurich - 115 chf

Zurich seems to have high cost of living . Which option you think would be better ?
I have 9 years of experience with 4 years in Data Engineering 

Any good input is appreciated :)",True,False,False,dataengineering,t5_36en4,45443,public,self,[Eu folks ] Frankfurt or Zurich?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r0hejg/eu_folks_frankfurt_or_zurich/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Tender_Figs,,,[],,,,text,t2_3uoce3bn,False,False,False,[],False,False,1637683980,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r0go80/so_data_engineering_jobs_are_starting_to_ask_for/,{},r0go80,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,38,0,False,all_ads,/r/dataengineering/comments/r0go80/so_data_engineering_jobs_are_starting_to_ask_for/,False,,,6,1637683991,1,Have a job notification alert set on linkedin for data engineering and have noticed over the past several days that there's an increase in statistics/ML knowledge combined with the ETL/ELT/DWH experience. Is this something that we should expect to rise in the future?,True,False,False,dataengineering,t5_36en4,45438,public,self,So data engineering jobs are starting to ask for statistics and ML knowledge too?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r0go80/so_data_engineering_jobs_are_starting_to_ask_for/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,DataStackAcademy,,,[],,,,text,t2_gsfor44i,False,False,False,[],False,False,1637680071,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r0f98o/data_engineering_career_path_free_webinar/,{},r0f98o,False,False,False,False,False,False,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r0f98o/data_engineering_career_path_free_webinar/,False,self,"{'enabled': False, 'images': [{'id': '3zN-zlrSORO15NzAxYCuoNwnNsiMlOtxJFhpg5VYFmY', 'resolutions': [{'height': 54, 'url': 'https://external-preview.redd.it/93vhpicc8rrwdYhXIY6nzHXJiio6w73qN19omglLesk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cfb6c6f7058fd14a3807f398908eaeeabf30eeb1', 'width': 108}, {'height': 108, 'url': 'https://external-preview.redd.it/93vhpicc8rrwdYhXIY6nzHXJiio6w73qN19omglLesk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0526e9650990ae64b51f60a2e8210e0d71999ccf', 'width': 216}, {'height': 160, 'url': 'https://external-preview.redd.it/93vhpicc8rrwdYhXIY6nzHXJiio6w73qN19omglLesk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=716d04eb3e69ec5b424396b3bfb81531458c2f6e', 'width': 320}, {'height': 320, 'url': 'https://external-preview.redd.it/93vhpicc8rrwdYhXIY6nzHXJiio6w73qN19omglLesk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=de15cb89be48ad34bba3b11859bb718b33954406', 'width': 640}, {'height': 480, 'url': 'https://external-preview.redd.it/93vhpicc8rrwdYhXIY6nzHXJiio6w73qN19omglLesk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=16355d28a731758f3d6a74eaa412f8051e752eca', 'width': 960}], 'source': {'height': 500, 'url': 'https://external-preview.redd.it/93vhpicc8rrwdYhXIY6nzHXJiio6w73qN19omglLesk.jpg?auto=webp&amp;s=891dbc158194b0599ccc3efd2956598eb1373312', 'width': 1000}, 'variants': {}}]}",6,1637680082,1,[removed],True,False,False,dataengineering,t5_36en4,45436,public,self,Data Engineering Career Path (FREE WEBINAR),0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r0f98o/data_engineering_career_path_free_webinar/,all_ads,6,,,automod_filtered,,,,,,,,,,,,,,,
[],False,AndreSionek,,,[],,,,text,t2_sgepl,False,False,False,[],False,False,1637679750,medium.com,https://www.reddit.com/r/dataengineering/comments/r0f54g/how_to_implement_aws_cdk_to_manage_a_data/,{},r0f54g,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/r0f54g/how_to_implement_aws_cdk_to_manage_a_data/,False,link,"{'enabled': False, 'images': [{'id': 'KcSW5QtKcdxY6o4bo9Q1pZ3nB9sjxHoHkytBWRDTB8w', 'resolutions': [{'height': 62, 'url': 'https://external-preview.redd.it/D3mYdUfDIjj9q3dP-toCOrJTpzFyTzGlivVHYT0qTJs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0be7ed3b06acefee625e396c1c84ad7be5f9dc6b', 'width': 108}, {'height': 124, 'url': 'https://external-preview.redd.it/D3mYdUfDIjj9q3dP-toCOrJTpzFyTzGlivVHYT0qTJs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=55cbdad4a337e0d30f7e0209b7441ae3f931a0e0', 'width': 216}, {'height': 184, 'url': 'https://external-preview.redd.it/D3mYdUfDIjj9q3dP-toCOrJTpzFyTzGlivVHYT0qTJs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0e41a34df14a7e5c972b210fb1faefd78c2ce148', 'width': 320}, {'height': 369, 'url': 'https://external-preview.redd.it/D3mYdUfDIjj9q3dP-toCOrJTpzFyTzGlivVHYT0qTJs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d03361f07153ab33ca369aabba0bad2f599a94cf', 'width': 640}, {'height': 553, 'url': 'https://external-preview.redd.it/D3mYdUfDIjj9q3dP-toCOrJTpzFyTzGlivVHYT0qTJs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7822c613244e4d4226694be9c49f5cf30cee411b', 'width': 960}, {'height': 622, 'url': 'https://external-preview.redd.it/D3mYdUfDIjj9q3dP-toCOrJTpzFyTzGlivVHYT0qTJs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fb77d0d9170e369f6154a7a98d3b92cf5b769aa4', 'width': 1080}], 'source': {'height': 692, 'url': 'https://external-preview.redd.it/D3mYdUfDIjj9q3dP-toCOrJTpzFyTzGlivVHYT0qTJs.jpg?auto=webp&amp;s=5cf8f8a83af4d231c7fd960a3566c0d69b5dd5cb', 'width': 1200}, 'variants': {}}]}",6,1637679761,1,,True,False,False,dataengineering,t5_36en4,45434,public,https://b.thumbs.redditmedia.com/NcVOg60vAJOwiCb2AH7a-JF-WqOZMHXgRkFXJ_5AtAo.jpg,How to implement AWS CDK to manage a Data Engineering Stack,0,[],1.0,https://medium.com/gousto-engineering-techbrunch/managing-a-data-engineering-stack-with-aws-cdk-2b86f4082b84,all_ads,6,,,,,,80.0,140.0,https://medium.com/gousto-engineering-techbrunch/managing-a-data-engineering-stack-with-aws-cdk-2b86f4082b84,,,,,,,,,,
[],False,parthetic_insaan_99,,,[],,,,text,t2_93uxwpj6,False,False,False,[],False,False,1637678299,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r0emnv/gcp_dataflow_to_bigquery_batch_loading_in_java/,{},r0emnv,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,4,0,False,all_ads,/r/dataengineering/comments/r0emnv/gcp_dataflow_to_bigquery_batch_loading_in_java/,False,,,6,1637678310,1,"Hey , everyone. I am absolute beginner in Data engineering, trying to work out some basic flows on managed services like dataflow + Beam. 
1. Taking files from GCS bucket 
2. Transformation / cleaning &amp; quality checks
3. Putting to bigquery for analysing

I have done this thing in python but now I came to know that java is actually well loaded compared to python Beam. So I am trying to accomplish this. 
I am stuck at how to go about this. I searched on google and found many ready projects but all including Maven for building and maintening the project. I tried to compile the sample projects (project no. 545,1743,1744 in maven archetype selection) only, it gave me a lot of tough time to solve some dependency errors in pom.xml &amp; yet not compiled.
I am using intelliJ for it.
Is it possible to create a standalone (without maven) Java program to do this? Can I do it without maven? 
Or java+maven is the only way?
Are there any working maven projects around this specifically for beam + java combination ? 
Any suggestions, help would be great.",True,False,False,dataengineering,t5_36en4,45434,public,self,GCP dataflow to Bigquery batch loading in java,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r0emnv/gcp_dataflow_to_bigquery_batch_loading_in_java/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,TGEL0,,,[],,,,text,t2_b65jjra,False,False,False,[],False,False,1637677419,app.mailbrew.com,https://www.reddit.com/r/dataengineering/comments/r0ec0e/hi_rdataengineering_the_45th_issue_of_my/,{},r0ec0e,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/r0ec0e/hi_rdataengineering_the_45th_issue_of_my/,False,link,"{'enabled': False, 'images': [{'id': 'SQ2j7kMkIomQFa-rnr7DhV7C_EPwAXYzcTyBCo7fZ6w', 'resolutions': [{'height': 56, 'url': 'https://external-preview.redd.it/Th5kuWDMRBBCa7kBlW4B5-7RIfNuPPD7zy0vDWUZd7U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6b92a2253fa6e2bef32dc19790fb564737dd6d00', 'width': 108}, {'height': 112, 'url': 'https://external-preview.redd.it/Th5kuWDMRBBCa7kBlW4B5-7RIfNuPPD7zy0vDWUZd7U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=788b7dd35fb1c00c39f29bcd0c0742d38af040b8', 'width': 216}, {'height': 166, 'url': 'https://external-preview.redd.it/Th5kuWDMRBBCa7kBlW4B5-7RIfNuPPD7zy0vDWUZd7U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=51112ae5adb2c418146d52cf3f2d62f46d4eb81c', 'width': 320}, {'height': 332, 'url': 'https://external-preview.redd.it/Th5kuWDMRBBCa7kBlW4B5-7RIfNuPPD7zy0vDWUZd7U.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=42aa26802265411c456e7ee8a26f9548853cc9aa', 'width': 640}], 'source': {'height': 416, 'url': 'https://external-preview.redd.it/Th5kuWDMRBBCa7kBlW4B5-7RIfNuPPD7zy0vDWUZd7U.jpg?auto=webp&amp;s=66fe03d9e684358d53ac276169d34d945dfde071', 'width': 800}, 'variants': {}}]}",6,1637677429,1,,True,False,False,dataengineering,t5_36en4,45432,public,https://b.thumbs.redditmedia.com/Am2Jj2yUzNxGYuBfKopr9TozqLXZTwfvvtiCS3H-yrQ.jpg,"Hi r/dataengineering, the 45th issue of my unofficial BigQuery newsletter is out - this is a special one as it includes news about Snowflake, Redshift and even this sub!",0,[],1.0,https://app.mailbrew.com/tgel0/not-so-bigquery-9HLtpmj0Pl8B,all_ads,6,,,,,,72.0,140.0,https://app.mailbrew.com/tgel0/not-so-bigquery-9HLtpmj0Pl8B,,,,,,,,,,
[],False,RainyPufferfish,,,[],,,,text,t2_5gttulkz,False,False,False,[],False,False,1637670080,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r0c0cy/how_many_days_of_paid_training_to_you_get_per_year/,{},r0c0cy,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,4,0,False,all_ads,/r/dataengineering/comments/r0c0cy/how_many_days_of_paid_training_to_you_get_per_year/,False,,,6,1637670092,1,"My current company offers 6 fully paid days per year which we can take for extra training, eg online courses (not completely off topic but within a broader range of technical interests). What about you?

[View Poll](https://www.reddit.com/poll/r0c0cy)",True,False,False,dataengineering,t5_36en4,45430,public,self,How many days of paid training to you get per year?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r0c0cy/how_many_days_of_paid_training_to_you_get_per_year/,all_ads,6,,,,,,,,,,,,,"{'is_prediction': False, 'options': [{'id': '12029175', 'text': '0'}, {'id': '12029176', 'text': '1-6'}, {'id': '12029177', 'text': '7-12'}, {'id': '12029178', 'text': '13+'}], 'prediction_status': None, 'resolved_option_id': None, 'total_stake_amount': None, 'total_vote_count': 0, 'tournament_id': None, 'user_selection': None, 'user_won_amount': None, 'vote_updates_remained': None, 'voting_end_timestamp': 1637929280783}",,,,,
[],False,DarkestKnight_23,,,[],,,,text,t2_d1v0zccf,False,False,False,[],False,False,1637665149,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r0arzn/gcp_bq_vs_snowflake_vs_databricks_deltalake/,{},r0arzn,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,48,0,False,all_ads,/r/dataengineering/comments/r0arzn/gcp_bq_vs_snowflake_vs_databricks_deltalake/,False,,,6,1637665162,1,Deleted earlier post cuz it wasn't worded right. What are the main advantages/disadvantages that factor into choosing one of these options over the other?,True,False,False,dataengineering,t5_36en4,45427,public,self,GCP BQ vs Snowflake vs Databricks Deltalake,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r0arzn/gcp_bq_vs_snowflake_vs_databricks_deltalake/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,DarkestKnight_23,,,[],,,,text,t2_d1v0zccf,False,False,False,[],False,False,1637664764,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r0aoul/gcp_vs_snowflake_vs_databricks_deltalake/,{},r0aoul,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,1,0,False,all_ads,/r/dataengineering/comments/r0aoul/gcp_vs_snowflake_vs_databricks_deltalake/,False,,,6,1637664775,1,What are the main advantages/disadvantages that factor into choosing one of these options over the other?,True,False,False,dataengineering,t5_36en4,45427,public,self,GCP vs Snowflake vs Databricks Deltalake,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r0aoul/gcp_vs_snowflake_vs_databricks_deltalake/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,heyitscactusjack,,,[],,,,text,t2_86p3j05x,False,False,False,[],False,False,1637661203,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r09v0a/am_i_in_any_sort_of_position_to_ask_for_a_pay_rise/,{},r09v0a,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,14,0,False,all_ads,/r/dataengineering/comments/r09v0a/am_i_in_any_sort_of_position_to_ask_for_a_pay_rise/,False,,,6,1637661214,1,"Long story short. I’m in my first job in the field as an ETL developer/Data Analyst. Another junior and myself were hired to supplement the two ETL devs (been there 5 years each) for a few months while they focus on getting on top of their work, then we were going be placed on the data insights team doing analysis. 
1-2 months in, both etl devs resigned, meaning that my colleague and I have become responsible their workload, which is extremely critical to the business’s function. We are expected to maintain this workload until the processes are automated, which we are involved in too. This is estimated to take months. 

Just to be clear, I am aware that I may come off as entitled and ungrateful, but I truly am not, and I am extremely grateful for the challenges and opportunities to prove myself. However, I can’t help but feel a bit under appreciated, especially while knowing we are being paid and considered as juniors, even though we have taken on work that mid-senior developers were probably paid twice as much to do. 

If I was to ask for compensation, would it be too early considering I’ve only been there for 3 months?",True,False,False,dataengineering,t5_36en4,45422,public,self,Am I in any sort of position to ask for a pay rise?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r09v0a/am_i_in_any_sort_of_position_to_ask_for_a_pay_rise/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Georgehwp,,,[],,,,text,t2_5okt4lzw,False,False,False,[],False,False,1637658203,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r096gh/managing_schema_changes_when_using_a_draganddrop/,{},r096gh,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,6,0,False,all_ads,/r/dataengineering/comments/r096gh/managing_schema_changes_when_using_a_draganddrop/,False,,,6,1637658214,1,"I'm about to set-up a DB for Superset Analytics. Consider it as running on top of a data warehouse. The only function of that DB is to be the storage for visualization. I'm used to operating in situations where you have a  DB -&gt; Flask API  (with SQLAlchemy) -&gt; React front-end, this gives you enough opportunities for abstraction to smoothly manage schema changes. How do you manage a schema change when your viz tool is linked directly to the DB?",True,False,False,dataengineering,t5_36en4,45421,public,self,Managing schema changes when using a drag-and-drop Dashboarding tool,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r096gh/managing_schema_changes_when_using_a_draganddrop/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Travolt,,,[],,,,text,t2_upfxac9,False,False,False,[],False,False,1637645278,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r05w1c/please_review_my_resume_data_engineer_wanna_be/,{},r05w1c,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,8,0,False,all_ads,/r/dataengineering/comments/r05w1c/please_review_my_resume_data_engineer_wanna_be/,False,,,6,1637645289,1,"Hi guys, 

Recently, I have been trying to transition into Data Engineering from Data Analytics. 

I have 1.5 YOE working as data scientist and data analyst. And in my current data analyst role, I'm building ETL pipelines, data warehousing and BI reporting mainly with Python, MS SQL and Power BI. 

I've learned some other data engineering techs in my spare time and also listed them on my resume. 

I'd appreciate your time in reviewing my resume! I'm based in Australia btw.",True,False,False,dataengineering,t5_36en4,45408,public,self,Please review my resume! Data Engineer wanna be with 1.5 YOE in Analytics,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r05w1c/please_review_my_resume_data_engineer_wanna_be/,all_ads,6,,,,,,,,,,,,True,,,,,,
[],False,abhi5025,,,[],,,,text,t2_j1zb3,False,False,False,[],False,False,1637636729,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/r03ava/aws_dms_for_cdc_from_sourcelikely_postgres_rds/,{},r03ava,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,7,0,False,all_ads,/r/dataengineering/comments/r03ava/aws_dms_for_cdc_from_sourcelikely_postgres_rds/,False,,,6,1637636740,1,"Hello,

Has anyone implemented CDC using DMS in Production-scale systems. I designed a pipeline using DMS but now realize there is too much dependency on the source db setup i.e. changing internal params, additional load on source db for WAL logs etc. I am also realizing too many limitations on what the tool can replicate to targets. [https://docs.aws.amazon.com/dms/latest/userguide/CHAP\_Source.PostgreSQL.html](https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.PostgreSQL.html)

Going by all of this, I am second guessing if DMS can be a scalable choice considering I might want to use the similar approach for multiple objects(tables) in the future.

Wondering if anyone can share their experience in real-time.

Thank you.",True,False,False,dataengineering,t5_36en4,45398,public,self,AWS DMS for CDC from Source(likely Postgres RDS),0,[],1.0,https://www.reddit.com/r/dataengineering/comments/r03ava/aws_dms_for_cdc_from_sourcelikely_postgres_rds/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,tmccormick92,,,[],,,,text,t2_2fpa4ubx,False,False,False,[],False,False,1637621622,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qzyab8/post_your_data_engineering_black_friday_resources/,{},qzyab8,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,36,0,False,all_ads,/r/dataengineering/comments/qzyab8/post_your_data_engineering_black_friday_resources/,False,,,6,1637621633,1,"This is just a thread where you can drop a link or a discount code for the community to use to learn new things or brush up on old skills. I'll start.

[**Leetcode Premium [Annual]**](https://leetcode.com/subscribe/) -- **THANKS20201** (~~$159~~ $129)  

[**Pluralsight [Annual]**](https://www.pluralsight.com/offer/2021/bf-cm-40-off) -- 40% off (Standard ~~$299~~$179 and Premium ~~$449~~ $269). 

[**A Cloud Guru [Annual]**](https://acloudguru.com/content/bf40-2021?utm_source=site&amp;utm_medium=hmepgtkovrp&amp;utm_campaign=2021_blackfriday) -- 40% off (Basic ~~$348~~ $209 and Plus ~~$468~~ $279).  

[**Dataquest**](https://www.dataquest.io/) -- not sure yet -- stay tuned (Vik tweeted about discounts to a monthly plan, but the only discount I see is 50% off annual)


I have personally used all four resources. Leetcode Premium is great to build SQL skills and learn things like algorithms. Pluralsight actually acquired ACG recently, and I used ACG to pass a few AWS exams. I used Dataquest when they first launched (probably over 5 years ago) so I'm not 100% confident in this resource, but hoping others can chime in.  

Thanks and happy holidays!",True,False,False,dataengineering,t5_36en4,45386,public,self,Post your Data Engineering Black Friday resources here!,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qzyab8/post_your_data_engineering_black_friday_resources/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Nevizki,,,[],,,,text,t2_9nxl0m5,False,False,False,[],False,False,1637613497,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qzv6gs/what_are_some_tasks_you_wish_to_delegate/,{},qzv6gs,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,4,0,False,all_ads,/r/dataengineering/comments/qzv6gs/what_are_some_tasks_you_wish_to_delegate/,False,,,6,1637613511,1,"As title says - what kind of tasks you face constantly, wishing that there was maybe some junior in the team who could even benefit from doing it",True,False,False,dataengineering,t5_36en4,45382,public,self,What are some tasks you wish to delegate?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qzv6gs/what_are_some_tasks_you_wish_to_delegate/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Born-Comment3359,,,[],,,,text,t2_5t56uq7x,False,False,False,[],False,False,1637607443,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qzsrt6/which_languagesframeworks_is_a_must_to_knowlearn/,{},qzsrt6,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,77,0,False,all_ads,/r/dataengineering/comments/qzsrt6/which_languagesframeworks_is_a_must_to_knowlearn/,False,,,6,1637607454,1,If you're a Hire Manager or an HR what's most important for you?,True,False,False,dataengineering,t5_36en4,45378,public,self,Which languages/frameworks is a must to know/learn for a six figure salary DE job?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qzsrt6/which_languagesframeworks_is_a_must_to_knowlearn/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,twoy519,,,[],,,,text,t2_11akn1,False,False,False,[],False,False,1637601771,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qzqjyq/building_api_templates_for_customers/,{},qzqjyq,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/qzqjyq/building_api_templates_for_customers/,False,,,6,1637601782,1,"Hi DE folks!  


Here is my problem statement:  
\* The company I work for has an API which our customers can use to get information about how they are using our service.  
\* Our customers should be considered to fall into two categories:  
  1. Some customers will be semi-sophisticated. They will be able to or have access to someone who is able to understand and use cloud services.

  2. Some customers will be unsophisticated. Picture someone who knows how to export to csv and pivot in excel.  
\* We would like to enable as many of our customers to access our API, store the results, and use SQL to join and aggregate the API responses together to answer their own business problems.  


Here is my question:

Are there any products or solutions out there I should know about which would allow us to templatize example API queries and subsequent data operations (eg SQL)?  


My best solution right now is to document the process of setting up a flow in Google Cloud:  
1. Write python script(s) hosted on a Google Cloud Instance to query our API

2. Store the results using Google Storage

3. Port the data over to be accessible in BigQuery

4. Write SQL

5. Profit

Are there better solutions? That process is going to be heavy or unrealistic for most of our customers.",True,False,False,dataengineering,t5_36en4,45374,public,self,Building API templates for customers,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qzqjyq/building_api_templates_for_customers/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,shittyfuckdick,,,[],,,,text,t2_1kset4fg,False,False,False,[],False,False,1637595690,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qzo8q8/need_help_with_designing_infrastructure_for_new/,{},qzo8q8,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,30,0,False,all_ads,/r/dataengineering/comments/qzo8q8/need_help_with_designing_infrastructure_for_new/,False,,,6,1637595702,1,"I am lone DE tasked with building new infrastructure for a startup. Here are there needs and my ideas:

Raw data is sourced from different hospitals delivered via SFTP or email. Format is csv, excel, etc. Data is delivered on a monthly or daily basis and can range from a few gigs to hundreds of gigs. 

*my idea here is use Airflow/Prefect for orchestration. Spark for heavy data processing and loading. And use Snowflake for our warehouse. Kubernetes maybe but seems like a lot for one dude running the show. *

All raw data must be transformed into a standard data model the company uses. Data Scientists must be able to access data via SQL to run ML models on. Some heavy processing must be done such deduplication and record merging. 

*again I think snowflake is a good choice here. Can handle large amounts of data that is readily accessible.*

The company also wants the data to be versioned. 

*I honestly have no experience here but first thing that comes to mind is routine backups and audit tables. But I’m sure there are other tools out there for this? Versioning is DS can reproduce ML output and FDA requirements.*

Lastly, we need data quality checks. Checks for completeness and expected distributions. 

*I have some experience with Great Expectations and seems like the right tool for the job here*

How does all this sounds? Any other recommendations for infrastructure?",True,False,False,dataengineering,t5_36en4,45371,public,self,Need Help With Designing Infrastructure For New Company,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qzo8q8/need_help_with_designing_infrastructure_for_new/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,unskilledexplorer,,,[],,,,text,t2_404pimvo,False,False,False,[],False,False,1637590501,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qzmcd7/how_to_design_data_architecture_for_fast_adhoc/,{},qzmcd7,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,12,0,False,all_ads,/r/dataengineering/comments/qzmcd7/how_to_design_data_architecture_for_fast_adhoc/,False,,,6,1637590512,1,"We have several clients for whom we need to create a BI tool. Looks like good opportunity to look ahead and try to build a SaaS tool, since we already have the clients to pilot with. 

The idea at its simplest: a client connects their datasource and our tool shows dashboard with insights. We already have a MVP which our clients like, but works only with manually uploaded CSV files with small amounts of data. Our next step is to design a scalable data architecture. We have very little hands-on experience with this, currently we are just doing some research.

The problem we see is that we do not know data schemas a priori. Each client has different dataset with a different schema. So we cannot simply design a normalized data model for fast ad-hoc queries which will work with everybody. Or can we? What are some good approaches to this?

Datasources may be:

* log of HTTP requests / event tracking (JSON)
* relational database
* csv file
* ...

Challenges we identified so far:

* a client's data schema may change in time (change of datatypes, renaming/adding/removing columns, etc...)
* we need to keep track of changes in time (eg. changes of attributes for an entity)
* ... ?

Forgive me if I use incorrect terminology, still learning.",True,False,False,dataengineering,t5_36en4,45368,public,self,How to design data architecture for fast ad-hoc analytics if the data model is not known a priori?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qzmcd7/how_to_design_data_architecture_for_fast_adhoc/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,SnowPlowOpenSource,,,[],,,,text,t2_enkpz7c3,False,False,False,[],False,False,1637589245,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qzlwnc/snowplow_meet_up_festive_gathering_snowplow/,{},qzlwnc,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/qzlwnc/snowplow_meet_up_festive_gathering_snowplow/,False,,,6,1637589256,1,"Hi, join us for a festive meet up in London on Wed 15th December, 2021. Interesting talks, drinks, food and networking, a raffle, and crazy golf! 

&amp;#x200B;

**Date**: 15/12/2021

**Time**: 5 - 8pm (GMT)

**Venue**: Shoreditch Balls, 333 Old Street, London.

**Registration**: [Here](https://snowplowanalytics.com/events/snowplow-meet-up-festive-gathering/?utm_source=reddit&amp;utm_medium=post&amp;utm_campaign=xmas-meetup&amp;utm_content=)",True,False,False,dataengineering,t5_36en4,45366,public,self,Snowplow Meet Up - Festive Gathering | Snowplow,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qzlwnc/snowplow_meet_up_festive_gathering_snowplow/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,getafterit123,,,[],,,,text,t2_puuzgu6,False,False,False,[],False,False,1637586034,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qzkwhd/pipeline_documenting/,{},qzkwhd,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,8,0,False,all_ads,/r/dataengineering/comments/qzkwhd/pipeline_documenting/,False,,,6,1637586048,1,"Curious how the everyone handles pipeline documentation. In this context I’m referring to documenting the pipeline itself (use case, source, where data is stored during its lifecycle, transformation specs, etc…) as opposed to data validation/ data quality checks on the data itself.",True,False,False,dataengineering,t5_36en4,45366,public,self,Pipeline documenting,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qzkwhd/pipeline_documenting/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,favoritecolorisbrown,,,[],,,,text,t2_rf8o1,False,False,False,[],False,False,1637585899,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qzkusr/strategies_have_you_used_to_sync_a_postgresql/,{},qzkusr,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,18,0,False,all_ads,/r/dataengineering/comments/qzkusr/strategies_have_you_used_to_sync_a_postgresql/,False,,,6,1637585910,1,"What strategies have y'all used in the past to sync data between a postgres instance into a Redshift cluster?

&amp;#x200B;

Context: at my job we have a postgres instance that serves as an application backend, and a Redshift cluster for warehousing - both of these are hosted on AWS. We would like to be able to replicate data from postgres to redshift and model it in our warehouse.

In the past I have used Presto (or now Trino) as a data lake solution, which conveniently has a [built-in connector](https://trino.io/docs/current/connector/postgresql.html) that solved our syncing issues.

I've done a little research and seems that [AWS DMS](https://aws.amazon.com/dms/) could be a solution, but I wonder how well it handles on-going replication and am curious if anyone here as used it.

Another option would be to leverage the \`COPY\` command from postgres, but I fell like that approach will not scale well for our amount of data.

&amp;#x200B;

What systems have you built in the past that have worked well (or not) for you?",True,False,False,dataengineering,t5_36en4,45366,public,self,Strategies have you used to sync a PostgreSQL database with Redshift?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qzkusr/strategies_have_you_used_to_sync_a_postgresql/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,prakharjain17,,,[],,,,text,t2_yox5x,False,False,False,[],False,False,1637585687,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qzkseq/does_anyone_know_about_test_questions_in_the_book/,{},qzkseq,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,3,0,False,all_ads,/r/dataengineering/comments/qzkseq/does_anyone_know_about_test_questions_in_the_book/,False,,,6,1637585698,1,"Hi,

I am looking for textbook like questions in the book Designing Data-Intensive Applications by Martin Kleppmann. This is a very popular book. I feel someone must have prepared the questions on the concepts inside the book. I am looking for some questions or some tests on the same.",True,False,False,dataengineering,t5_36en4,45365,public,self,Does anyone know about test questions in the book Designing Data-Intensive Applications by Martin Kleppmann (DDIA) to revise concepts learned,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qzkseq/does_anyone_know_about_test_questions_in_the_book/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,ppnimkar,,,[],,,,text,t2_6g6ggfmr,False,False,False,[],False,False,1637583626,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qzk7yv/data_engineering_salaries_in_sweden/,{},qzk7yv,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,22,0,False,all_ads,/r/dataengineering/comments/qzk7yv/data_engineering_salaries_in_sweden/,False,self,"{'enabled': False, 'images': [{'id': 'SSPn5EoN2yuq1imN3thcEHiUrm6gbQxBB9h70mM_M3U', 'resolutions': [{'height': 20, 'url': 'https://external-preview.redd.it/4aJ7X0QuW3eG51ESS1AUQDoGVTRTZVB-FERLr8UfzkQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9346c9caa32f75e42877fec53606bdd2efb52125', 'width': 108}], 'source': {'height': 40, 'url': 'https://external-preview.redd.it/4aJ7X0QuW3eG51ESS1AUQDoGVTRTZVB-FERLr8UfzkQ.jpg?auto=webp&amp;s=4d80b18c3eef9b81d61ae4349353d029f8404bce', 'width': 209}, 'variants': {}}]}",6,1637583637,1,"I've finished my tech and managerial/leadership rounds at a Swedish tech/product mid-size startup and I have my salary discussion coming up soon. I'm looking for a starting point/range for the same. I've checked out Glassdoor and [www.lonestatistik.se](https://www.lonestatistik.se), without much helps. Any/all pointers will be very appreciated, thanks :)

About me:  
I'm a Data Engineer (Senior) with 9 years of work-ex with European + US based tech companies. I work specifically in data engineering, data modeling, and business intelligence.",True,False,False,dataengineering,t5_36en4,45362,public,self,Data Engineering salaries in Sweden,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qzk7yv/data_engineering_salaries_in_sweden/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,The_M_G_G,,,[],,,,text,t2_9l1sejva,False,False,False,[],False,False,1637583096,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qzk2i9/how_to_build_a_onpremise_data_lake_using/,{},qzk2i9,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,18,0,False,all_ads,/r/dataengineering/comments/qzk2i9/how_to_build_a_onpremise_data_lake_using/,False,,,6,1637583107,1,"Hello folks, 

I follow this community for quite a while now and I found it as an incredible helpful source to get started with data engineering. However, I am recently a bit stuck on the topic on how to build a data  lake on-premise using open source tools. 

Let me give you some context:

I am currently working part-time at a young company while also studying in a graduate data science program. Currently, I am the only one working on this task and my knowledge of Data Engineering is limited to these topics so far:

* I know how to build and use relational databases. Recently I used PostgreSQL for this. 
* I experimented with ETL orchestration frameworks such as Airflow and Argo (Workflows + Events)
* At a former company I worked at, I got into touch with GraphQL where I queried the data I needed which was stored at AWS. 
* I experimented recently with Object Storage such as MinIO.
* Recently, I tried to combine MinIO with Trino to query the data in the Object Storage. With rather limited success. 

The company I am working for right now is operating in a domain where it is not possible to upload the data to one of the common cloud providers. My task is to find a data management solution. 

The constraints are that it should take in any kind of data. That could be binary data, image data, csv, json ets. Currently, the data is stored in files on a server where it is used to build PoCs by other team members. There is no Senior Data Engineer on the team. Ideally, everything should work in Kubernetes.

I know this could be a better basis to start with but this is what I have right now. I try to find as much information as possible on the internet or books like these:

* The Enterprise Big Data Lake
* Trino: The Definitive Guide
* Designing Data-Intensive Applications
* Data Pipelines Pocket Reference
* Building Machine Learning Pipelines

But I am at a stage where many authors (may it be online or offline, in texts or videos) either just explain the very basics (What is an object storage?) or they talk about very sophisticated Architectures which can be found at AirBnb, Uber and a like often at one of the big cloud providers. What I need is the step in the middle and on-premise. 

How do I build a data lake, that holds data of different levels of transformations, and make this available to the team members? 

Currently I would do the following (Using tools that work in Kubernetes but ignore this for now. I am building a PoC which runs locally):

* Setup MinIO and create different zones such as raw, refined, trusted etc. With these different zones I could manage the access rights and make data available at different levels of processing available for the respective users. 
* Setup the Injestion ETLs using either Argo or Kubeflow. The data from these pipelines will land in the raw zone. 
* Transform data from the raw zone to the subsequent zones using Argo or Kubeflow ETLs.
* Make the data in the different zones searchable/queryable - How?

The last point is my biggest blocking point. I read that you need to catalogue your data using metadata to make it queryable. But what is the best practice for this? Would I add some Metadata during the Injestion period and store this and the file path to the object in object store in an Relational DB to make it queryable? Maybe also an indicator of a level of transformation so the user knows whether it is available in a subsequent bucket?

I would be immensely happy if some of you could provide me with some insights on how to build such a thing or at least how to make my current setup queryable. My online research is already pretty exhausted and the only thing I find now is a new tool x that promises to give you all but isn't open source. 

Thanks to all of you and looking forward to the discussion!",True,False,False,dataengineering,t5_36en4,45362,public,self,How to build a on-premise data lake using open-source tools?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qzk2i9/how_to_build_a_onpremise_data_lake_using/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,RstarPhoneix,,,[],,,,text,t2_57e44nxs,False,False,False,[],False,False,1637575863,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qzi9cs/how_to_handle_gdpr_requests_for_data_stored_in_s3/,{},qzi9cs,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,29,0,False,all_ads,/r/dataengineering/comments/qzi9cs/how_to_handle_gdpr_requests_for_data_stored_in_s3/,False,,,6,1637575873,1,Is there any way to delete or update records stored in S3 file on AWS ? We do get request to remove some data (records) from our entire data lake. It's easy to remove from database but how to remove it from file based things ? Any idea ? Any help is highly appreciated. Thanks in advanced.,True,False,False,dataengineering,t5_36en4,45354,public,self,How to handle GDPR requests for data stored in S3 ?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qzi9cs/how_to_handle_gdpr_requests_for_data_stored_in_s3/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,tallbrownglasses,,,[],,,,text,t2_f3w5u04o,False,False,False,[],False,False,1637573990,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qzht31/the_us_dream/,{},qzht31,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,False,15,0,False,all_ads,/r/dataengineering/comments/qzht31/the_us_dream/,False,,,6,1637574001,1,I am from India. I want to move to US and settle there. Can I get a International job offer in Data Engineer role from India. Has anyone done that?,True,False,False,dataengineering,t5_36en4,45353,public,self,The US Dream,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qzht31/the_us_dream/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,an_tonova,,,[],,,,text,t2_d834g,False,False,False,[],False,False,1637569447,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qzgqgv/data_transformation_tools_like_dbt_but_visual/,{},qzgqgv,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,7,0,False,all_ads,/r/dataengineering/comments/qzgqgv/data_transformation_tools_like_dbt_but_visual/,False,,,6,1637569458,1,"Hi y'all, please recommend (if any) visual transformation tools for data warehouses in ELT space similar to dbt but visual",True,False,False,dataengineering,t5_36en4,45349,public,self,Data transformation tools like dbt but visual,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qzgqgv/data_transformation_tools_like_dbt_but_visual/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,sweetaskate,,,[],,,,text,t2_63zqpn0s,False,False,False,[],False,False,1637564698,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qzfk74/building_a_personalized_product_recommender/,{},qzfk74,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/qzfk74/building_a_personalized_product_recommender/,False,self,"{'enabled': False, 'images': [{'id': 'rE0r6vWrSoWZMhvJ7syMCaP7ANYcy8TN9nQsBzrrtbI', 'resolutions': [{'height': 56, 'url': 'https://external-preview.redd.it/546XSBPKXHza0S8ZZNqfLz-RgOn9GGqh--HVQdFM8Lg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dffa9718414b8f7d8cbea7f4d239ae093d49123f', 'width': 108}, {'height': 113, 'url': 'https://external-preview.redd.it/546XSBPKXHza0S8ZZNqfLz-RgOn9GGqh--HVQdFM8Lg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1ca6839b49a20a27bb9129e3cfbcf19c1292b124', 'width': 216}, {'height': 168, 'url': 'https://external-preview.redd.it/546XSBPKXHza0S8ZZNqfLz-RgOn9GGqh--HVQdFM8Lg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bbd215bbdb6eb5e5e1ec448371d9d3a1b6aec4bb', 'width': 320}, {'height': 337, 'url': 'https://external-preview.redd.it/546XSBPKXHza0S8ZZNqfLz-RgOn9GGqh--HVQdFM8Lg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f428fcd75b5b8569e541357ae4eccdc819e51efc', 'width': 640}, {'height': 505, 'url': 'https://external-preview.redd.it/546XSBPKXHza0S8ZZNqfLz-RgOn9GGqh--HVQdFM8Lg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=abbc73ed7c1048f478c8ea538e2791a5758da1cb', 'width': 960}, {'height': 568, 'url': 'https://external-preview.redd.it/546XSBPKXHza0S8ZZNqfLz-RgOn9GGqh--HVQdFM8Lg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=627d153edea8a8f1390d325bf7cee4655f41b80a', 'width': 1080}], 'source': {'height': 628, 'url': 'https://external-preview.redd.it/546XSBPKXHza0S8ZZNqfLz-RgOn9GGqh--HVQdFM8Lg.jpg?auto=webp&amp;s=02571bafb70489b2a99dcf67ad38ccb70cf74113', 'width': 1192}, 'variants': {}}]}",6,1637564709,1,"How are vector databases applied to product recommender systems? For anyone interested, this is a use case from an e-commerce company: https://blog.milvus.io/building-a-personalized-product-recommender-system-with-vipshop-and-milvus-94e8b8b49168?source=friends\_link&amp;sk=0ea2fbd59333da4b5b3994d5f7083032

The overall architecture consists of two main parts:

* Write process: the item feature vectors generated by the deep learning model are normalized and written into MySQL. MySQL then reads the processed item feature vectors using the data synchronization tool (ETL) and imports them into the vector database Milvus.  
 
* Read process: The search service obtains user preference feature vectors based on user query keywords and user portraits, queries similar vectors in Milvus and recalls TopK item vectors.",True,False,False,dataengineering,t5_36en4,45345,public,self,Building a Personalized Product Recommender System with Vector Database,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qzfk74/building_a_personalized_product_recommender/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,alexshom123,,,[],,,,text,t2_25xd1obo,False,False,False,[],False,False,1637553636,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qzch3l/sql_server_set_up_linked_server/,{},qzch3l,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,8,0,False,all_ads,/r/dataengineering/comments/qzch3l/sql_server_set_up_linked_server/,False,self,"{'enabled': False, 'images': [{'id': 'bE4f-_gWmOppZmlJqpdPYzq30TuLzokrQW01jrbTnFE', 'resolutions': [{'height': 39, 'url': 'https://external-preview.redd.it/ao1wtGnRCzU0Vwpq9c6X65ckk7hgVwzpR-1nVHzDZnM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7de87a004e7f6d29632a98169579cb18d8a90226', 'width': 108}, {'height': 79, 'url': 'https://external-preview.redd.it/ao1wtGnRCzU0Vwpq9c6X65ckk7hgVwzpR-1nVHzDZnM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ee12ba97d085d7b9ec8db978b6f8b143adee879a', 'width': 216}, {'height': 117, 'url': 'https://external-preview.redd.it/ao1wtGnRCzU0Vwpq9c6X65ckk7hgVwzpR-1nVHzDZnM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f4d3a44355dea6176ac4a000db9dec7b65499d1d', 'width': 320}, {'height': 235, 'url': 'https://external-preview.redd.it/ao1wtGnRCzU0Vwpq9c6X65ckk7hgVwzpR-1nVHzDZnM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=21e2faf6d212041b58148bf0a792bb6c2e9db32f', 'width': 640}, {'height': 353, 'url': 'https://external-preview.redd.it/ao1wtGnRCzU0Vwpq9c6X65ckk7hgVwzpR-1nVHzDZnM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e5d7e5dc646a8160ed07b478ccdceaaef489b531', 'width': 960}, {'height': 397, 'url': 'https://external-preview.redd.it/ao1wtGnRCzU0Vwpq9c6X65ckk7hgVwzpR-1nVHzDZnM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a47757d6ee30632dcbd81fd91f09ff1fc2d4e3f6', 'width': 1080}], 'source': {'height': 644, 'url': 'https://external-preview.redd.it/ao1wtGnRCzU0Vwpq9c6X65ckk7hgVwzpR-1nVHzDZnM.jpg?auto=webp&amp;s=2141ac5b85ec4b92db28add9992b04271fd9f938', 'width': 1750}, 'variants': {}}]}",6,1637553647,1,"Hi sorry for a relatively beginner question.

I am trying to set up a linked server on a localhost db to an sql managed instance on azure.

I can connect from the object explored to the managed instance, however, I cannot set up a linked server using the exact same server name and login info.

Below is an image to show the problem https://imgur.com/a/voNqYqK

Any help would be good thanks",True,False,False,dataengineering,t5_36en4,45339,public,self,SQL server set up linked server,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qzch3l/sql_server_set_up_linked_server/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Minimum-Membership-8,,,[],,,,text,t2_a0qsnkph,False,False,False,[],False,False,1637525645,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qz3clj/best_learning_resources/,{},qz3clj,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,2,0,False,all_ads,/r/dataengineering/comments/qz3clj/best_learning_resources/,False,,,6,1637525656,1,"I want to learn kafka, airflow, pyspark, and git. Any recommendations?",True,False,False,dataengineering,t5_36en4,45323,public,self,Best learning resources,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qz3clj/best_learning_resources/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,buhabali,,,[],,,,text,t2_qmlcf,False,False,False,[],False,False,1637518931,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qz0yy1/fbmeta_data_engineering_virtual_onsite_interviews/,{},qz0yy1,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,False,4,0,False,all_ads,/r/dataengineering/comments/qz0yy1/fbmeta_data_engineering_virtual_onsite_interviews/,False,,,6,1637518943,1,"Looking for tips and advice for FB/Meta onsite interviews.

I am preparing leetcode mediums and lots of SQL, but not sure about the way it's conducted. Some posts mentioned that they will ask to build data models etc, can anyone shed light on how it goes? Any resources that I can refer to prepare.

Any insights from someone who recently attended will be very helpful, other general tips are welcome too!

Thanks!",True,False,False,dataengineering,t5_36en4,45320,public,self,FB/Meta data Engineering Virtual onsite interviews,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qz0yy1/fbmeta_data_engineering_virtual_onsite_interviews/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Fabulous-Bridge-7330,,,[],,,,text,t2_9gtn1xps,False,False,False,[],False,False,1637502272,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qyv9kr/is_a_conformedcuratedharmonized_layer_necessary/,{},qyv9kr,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,17,0,False,all_ads,/r/dataengineering/comments/qyv9kr/is_a_conformedcuratedharmonized_layer_necessary/,False,,,6,1637502283,1,"Just started at a company in the process of building their analytics platform using dbt + Delta Lake to be used for reporting and analyst queries. While there are multiple data layers in Delta Lake, there is no one layer like a conformed/curated/harmonized layer (a layer containing the all the data organized for analytical purposes rather than transactional/raw tables using for example star, snowflake schema or denormalization, etc.). All of my previous work on data lakes stressed heavy importance on this layer, however the current team I'm in is somewhat against it (the thinking is that it will be too much to maintain and is not necessary). Instead, analyst tables/views are built directly off of raw layer tables. 

My gut is telling me that as the platform grows, the data duplication/disorganization downstream of raw will come back to haunt us, but right now the architecture is working fine...anyone have experience/thoughts on this?",True,False,False,dataengineering,t5_36en4,45313,public,self,Is a conformed/curated/harmonized layer necessary in lakehouse architecture?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qyv9kr/is_a_conformedcuratedharmonized_layer_necessary/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,MlTut,,,[],,,,text,t2_265t3i5h,False,False,False,[],False,False,1637500501,mltut.com,https://www.reddit.com/r/dataengineering/comments/qyuqog/udacity_data_engineering_nanodegree_review_should/,{},qyuqog,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,6,0,False,all_ads,/r/dataengineering/comments/qyuqog/udacity_data_engineering_nanodegree_review_should/,False,link,"{'enabled': False, 'images': [{'id': 'Pja_rOw8XCS3U8ib6A6pEpZ26ZubhnkE_XfvOgwqBBw', 'resolutions': [{'height': 60, 'url': 'https://external-preview.redd.it/dqe_DKPBx0xMvYMqAeoxSzX3FO7FaVle53E5KIlVyEQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d9c6796711c5093258b1f5a083ea881dafb9d64b', 'width': 108}, {'height': 121, 'url': 'https://external-preview.redd.it/dqe_DKPBx0xMvYMqAeoxSzX3FO7FaVle53E5KIlVyEQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c3fc6968766479b12c7972531f26a365aa637995', 'width': 216}, {'height': 180, 'url': 'https://external-preview.redd.it/dqe_DKPBx0xMvYMqAeoxSzX3FO7FaVle53E5KIlVyEQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=946f281b303fdaaf7e2027d9ce2c09010b16a893', 'width': 320}, {'height': 360, 'url': 'https://external-preview.redd.it/dqe_DKPBx0xMvYMqAeoxSzX3FO7FaVle53E5KIlVyEQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7275ab1a0b5b9549a46523515fdb4d3332012754', 'width': 640}, {'height': 540, 'url': 'https://external-preview.redd.it/dqe_DKPBx0xMvYMqAeoxSzX3FO7FaVle53E5KIlVyEQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=981078bf9dbd40acff84fa10287d805b13ffa1db', 'width': 960}, {'height': 607, 'url': 'https://external-preview.redd.it/dqe_DKPBx0xMvYMqAeoxSzX3FO7FaVle53E5KIlVyEQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1243b38ed73b6529575117935d955910f3794e35', 'width': 1080}], 'source': {'height': 1260, 'url': 'https://external-preview.redd.it/dqe_DKPBx0xMvYMqAeoxSzX3FO7FaVle53E5KIlVyEQ.jpg?auto=webp&amp;s=9925d6495c4480199739c51f0232a60c0e16d226', 'width': 2240}, 'variants': {}}]}",6,1637500513,1,,True,False,False,dataengineering,t5_36en4,45313,public,https://b.thumbs.redditmedia.com/XXquNxm0yI8cdPopRYLky7j-AA8M857gkfTIpWEzsRU.jpg,Udacity Data Engineering Nanodegree Review- Should You Enroll?,0,[],1.0,https://www.mltut.com/udacity-data-engineering-nanodegree-review/,all_ads,6,,,,,,78.0,140.0,https://www.mltut.com/udacity-data-engineering-nanodegree-review/,,,,,,,,,,
[],False,FunDirt541,,,[],,,,text,t2_8l3jnbxe,False,False,False,[],False,False,1637488823,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qyrw4f/designing_a_database_as_a_beginner/,{},qyrw4f,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,2,0,False,all_ads,/r/dataengineering/comments/qyrw4f/designing_a_database_as_a_beginner/,False,,,6,1637488842,1,"TL;DR : Need tips, learning ressources with SQL, where 70% of the data would be manually inputed

&amp;#x200B;

I've decided to build a CRUD application for a family printing business. Where employers would input information :

\- Customers details  (name/company name, phone number, adress...)

\- Place order(date, item, amount, price...)

\- stock/new merchandise (amount left would get calculated based on order)

I believe we would get at most 120 x 5 daily new data points. (4 'small' companies)

 I have 'some' knowledge in python, and just diving into SQL (no choice I guess, starting on LeetCode). I am still trying to figure out the best approach to design such a structure. 

I've just bought 'Designing data-intensive applications', so hopefully it will help.

Mainly want to hear about good practices, tips and learning ressources.

Do I need to think thoroughly about the data modeling or a general overview is fine? (I can't possibly think of every single columns in advance)

Would adding the data daily be a storage problem, or slow down the CRUD app ? 

How do I go on about keeping the customers' data secure ?

Is a cloud platform needed ? Or only good to have

Thanks for reading !",True,False,False,dataengineering,t5_36en4,45305,public,self,Designing a database (as a beginner),0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qyrw4f/designing_a_database_as_a_beginner/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,LSTMeow,,,[],,,,text,t2_1asxtsqs,False,False,False,[],False,False,1637468968,i.redd.it,https://www.reddit.com/r/dataengineering/comments/qyn24c/lesson_learned_meme_good_watermark_bad_heres/,{},qyn24c,False,True,False,False,True,True,False,False,#ff66ac,[],dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1,Meme,light,text,False,False,True,20,0,False,all_ads,/r/dataengineering/comments/qyn24c/lesson_learned_meme_good_watermark_bad_heres/,False,image,"{'enabled': True, 'images': [{'id': 'kMr9Pme7zmXguIRAShwLy0KuJ0189A6DV2w3bWiiiRY', 'resolutions': [{'height': 79, 'url': 'https://preview.redd.it/gnv05d5fmv081.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=42acd38e0f0aa519f35d5ee16a66c86772f77d38', 'width': 108}, {'height': 159, 'url': 'https://preview.redd.it/gnv05d5fmv081.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=daab6fdba6368f47edae9be94488e9caeeca0b65', 'width': 216}, {'height': 237, 'url': 'https://preview.redd.it/gnv05d5fmv081.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c6c5481daf1f4a4ee3fa31c62b8ddf947421d68e', 'width': 320}, {'height': 474, 'url': 'https://preview.redd.it/gnv05d5fmv081.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cc02c99d109c3dbb4b0f0a237c3c04a9205236d3', 'width': 640}, {'height': 711, 'url': 'https://preview.redd.it/gnv05d5fmv081.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dcfe33755c41215e7bf8d947c271c0285584e0c0', 'width': 960}, {'height': 799, 'url': 'https://preview.redd.it/gnv05d5fmv081.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=efbd86ec140acb7427ab6221529529e1016dac92', 'width': 1080}], 'source': {'height': 948, 'url': 'https://preview.redd.it/gnv05d5fmv081.jpg?auto=webp&amp;s=48115f44db843a3e5f5e0893522deaea45c132b5', 'width': 1280}, 'variants': {}}]}",6,1637468978,1,,True,False,False,dataengineering,t5_36en4,45289,public,https://b.thumbs.redditmedia.com/q9G3y_CJT357x5F8KVSJA0YMsZ4bO3rDru1p8iVWVlw.jpg,"Lesson learned: meme good, watermark bad. Here's another DE-flavored meme as compensation.",0,[],1.0,https://i.redd.it/gnv05d5fmv081.jpg,all_ads,6,,,,,,103.0,140.0,https://i.redd.it/gnv05d5fmv081.jpg,,,,,,,,,,
[],False,ronald_r3,,,[],,,,text,t2_zdh50,False,False,False,[],False,False,1637457660,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qyjqf6/using_2_different_schemas_in_dbt_for_a_model/,{},qyjqf6,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,4,0,False,all_ads,/r/dataengineering/comments/qyjqf6/using_2_different_schemas_in_dbt_for_a_model/,False,,,6,1637457672,1,"Good evening
I was look to see if someone could assist me with solving an issue where I'm looking to use a model which contains tables from 2 different schemas. The problem is that when compiled all the tables are queried as if they were in the same schema.

I checked the documentation but It didn't really assist much.",True,False,False,dataengineering,t5_36en4,45283,public,self,Using 2 different schemas in dbt for a model,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qyjqf6/using_2_different_schemas_in_dbt_for_a_model/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,ChrolloLucilfer69,,,[],,,,text,t2_7dzk44j0,False,False,False,[],False,False,1637454519,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qyis3l/wlb_as_fb_data_engineer/,{},qyis3l,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,17,0,False,all_ads,/r/dataengineering/comments/qyis3l/wlb_as_fb_data_engineer/,False,,,6,1637454530,1,I'm a consultant that'll be working as a data engineer at FB. Was wondering if anyone with knowledge of the role can comment on the Work Life Balance?,True,False,False,dataengineering,t5_36en4,45281,public,self,WLB as FB Data Engineer?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qyis3l/wlb_as_fb_data_engineer/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Griff70709,,,[],,,,text,t2_ca104qez,False,False,False,[],False,False,1637435225,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qycdte/transitioning_from_software_engineer_to_data/,{},qycdte,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,19,0,False,all_ads,/r/dataengineering/comments/qycdte/transitioning_from_software_engineer_to_data/,False,,,6,1637435236,1,"I want to work towards a fully remote job as soon as possible. Maybe a contracting job. I'm aware that might not be possible for my first foray. Just want some advice to see if I'm working in the right direction and whether there's anything I can be doing better.

I'm currently working part time as a fully remote software developer. I've been doing this for close to two years now. I'm set to graduate with a bachelors in Computer Science in January. 

To prepare for a transition to Data Engineering I've been completing DataQuest's course, which has given me a good basis in SQL and should start me off in building a data pipeline. Once I finish with DataQuest I plan to start working on my own Data Engineering related github projects. I'm also working on getting an AWS cloud practitioner certification. I figure this might help me stand out as an entry level applicant. What else can I do to land that first job? Any good places for Data-Engineer related interview questions? What job titles should I be looking for? ETL developer?

&amp;#x200B;

TL;DR: Soon to have BS in Comp Sci, 2 years remote software dev experience. Preparing for data engineering job with Data Quest, AWS certificate, and personal projects. How else can I get ahead?",True,False,False,dataengineering,t5_36en4,45272,public,self,Transitioning from Software Engineer to Data Engineer,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qycdte/transitioning_from_software_engineer_to_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,justanothersnek,,,[],,,,text,t2_8eu8dnvj,False,False,False,[],False,False,1637422185,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qy7vic/how_to_efficiently_bulk_analyze_json_data_from/,{},qy7vic,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,16,0,False,all_ads,/r/dataengineering/comments/qy7vic/how_to_efficiently_bulk_analyze_json_data_from/,False,,,6,1637422197,1,"Hello!  I am tasked with analyzing data from REST API, JSON data basically.  I am currently normalizing the data into tabular format and saving as CSV files with the intent to later load them into a database to then do aggregations or summarizations.  But I've been wondering if it would be better to leave the data as is (JSON), then load the data into a NoSQL database?  The normalization process can be tedious as I have to study the structure of the raw data and then I utilize Python pandas to normalize.  The thing is I have no experience with NoSQL databases and wondering if I can do aggregations and summarizations with them?  I'm assuming so.  The company does have a Snowflake environment but havent looked at its JSON support.  I am familiar with PostgreSQL which I think its recent versions does support JSON/JSONB, but not sure if I will have access to a Postgres environment.  Should I just bite the bullet and pursue dumping the JSON files into a NoSQL database or continue with normalizing the data into tabular format or pursue something else?",True,False,False,dataengineering,t5_36en4,45264,public,self,How to efficiently bulk analyze JSON data from REST API?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qy7vic/how_to_efficiently_bulk_analyze_json_data_from/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,alonisser,,,[],,,,text,t2_4poptfjv,False,False,False,[],False,False,1637412789,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qy5072/the_snapshot_that_couldnt_cdc_kafka_connect_and/,{},qy5072,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/qy5072/the_snapshot_that_couldnt_cdc_kafka_connect_and/,False,self,"{'enabled': False, 'images': [{'id': 'OAXFOlKerytnlqOOeO_kkxqADwARwgJXUPH6BdHEluw', 'resolutions': [{'height': 74, 'url': 'https://external-preview.redd.it/1pR5Yd_h-GI6NjKHDSqT_poUvWzWEwzw6lUcGW8iYX8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e3fb389dfc8007f60b7674271c3621c383265339', 'width': 108}, {'height': 149, 'url': 'https://external-preview.redd.it/1pR5Yd_h-GI6NjKHDSqT_poUvWzWEwzw6lUcGW8iYX8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ce6dcfde577e1b951cd27e1683f6a8b4165b75a0', 'width': 216}, {'height': 221, 'url': 'https://external-preview.redd.it/1pR5Yd_h-GI6NjKHDSqT_poUvWzWEwzw6lUcGW8iYX8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c587fbde042b5cd54200f5d462eaa7bb39560b0a', 'width': 320}, {'height': 443, 'url': 'https://external-preview.redd.it/1pR5Yd_h-GI6NjKHDSqT_poUvWzWEwzw6lUcGW8iYX8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=20e3bb8d1d49544c1762ac4cc34077a2c438a738', 'width': 640}], 'source': {'height': 554, 'url': 'https://external-preview.redd.it/1pR5Yd_h-GI6NjKHDSqT_poUvWzWEwzw6lUcGW8iYX8.jpg?auto=webp&amp;s=0c5365b2c4fc68e9cf729726f3f1e9b99e9b29a3', 'width': 800}, 'variants': {}}]}",6,1637412799,1,"[https://medium.com/zencity-engineering/the-snapshot-that-couldnt-939b729a7a5b](https://medium.com/zencity-engineering/the-snapshot-that-couldnt-939b729a7a5b)

Might be of interest here. A story from the data-engineering trenches. one that taught me some lessons about debugging, root cause analysis, and Kafka-connect.",True,False,False,dataengineering,t5_36en4,45253,public,self,"The snapshot that couldn't - CDC, kafka connect and debugging",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qy5072/the_snapshot_that_couldnt_cdc_kafka_connect_and/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,-segmentationfault-,,,[],,,,text,t2_7idw1yfm,False,False,False,[],False,False,1637406367,amazon.com,https://www.reddit.com/r/dataengineering/comments/qy3f4t/practical_eventdriven_microservices_architecture/,{},qy3f4t,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,3,0,False,all_ads,/r/dataengineering/comments/qy3f4t/practical_eventdriven_microservices_architecture/,False,,,6,1637406378,1,,True,False,False,dataengineering,t5_36en4,45249,public,default,Practical Event-Driven Microservices Architecture,0,[],1.0,https://www.amazon.com/Practical-Event-Driven-Microservices-Architecture-Sustainable/dp/1484274679/ref=sr_1_4?dchild=1&amp;qid=1635357838&amp;refinements=p_28%3ABuilding+Event-Driven+Microservices&amp;s=books&amp;sr=1-4,all_ads,6,,,,,,,,https://www.amazon.com/Practical-Event-Driven-Microservices-Architecture-Sustainable/dp/1484274679/ref=sr_1_4?dchild=1&amp;qid=1635357838&amp;refinements=p_28%3ABuilding+Event-Driven+Microservices&amp;s=books&amp;sr=1-4,,,,,,,,,,
[],False,pavan____----,,,[],,,,text,t2_3ez695c5,False,False,False,[],False,False,1637405281,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qy35zt/dimensional_modelling_on_redshift/,{},qy35zt,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,11,0,False,all_ads,/r/dataengineering/comments/qy35zt/dimensional_modelling_on_redshift/,False,,,6,1637405292,1,"Guys, I have a sql server(target system) on which we stored the data previously. Also we used to do ETL using ssis packages. Our source system is migrating and we need to rebuild the pipeline for our ETL and also the Dimensional model. I understood we could do ETL on redshift as well as on Glue and Dwh on Redshift . I just wanted some opinion on this matter 1. what should i prefer cost wise and also we have never used python in the team before and just sql for data transformation
2. I have researched on Redshift and it does not enforce PK-FK constraints for our dimensional model let me know how to fill in fact-table and dimensional table in this scenario

Any opinions and resources will be highly appreciated!",True,False,False,dataengineering,t5_36en4,45248,public,self,Dimensional modelling on redshift,0,[],0.99,https://www.reddit.com/r/dataengineering/comments/qy35zt/dimensional_modelling_on_redshift/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,gato_felix_br,,,[],,,,text,t2_go7cn9fz,False,False,False,[],False,False,1637392562,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qy0aqy/how_does_dbt_compare_with_pandas/,{},qy0aqy,False,False,False,False,False,False,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,5,0,False,all_ads,/r/dataengineering/comments/qy0aqy/how_does_dbt_compare_with_pandas/,False,,,6,1637392573,1,[removed],True,False,False,dataengineering,t5_36en4,45241,public,self,How does dbt compare with pandas?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qy0aqy/how_does_dbt_compare_with_pandas/,all_ads,6,,,automod_filtered,,,,,,,,,,,,,,,
[],False,cookiethunderstorm30,,,[],,,,text,t2_6jzfhr15,False,False,False,[],False,False,1637391117,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qxzymc/how_do_big_companies_like_facebook_handle_their/,{},qxzymc,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,14,0,False,all_ads,/r/dataengineering/comments/qxzymc/how_do_big_companies_like_facebook_handle_their/,False,,,6,1637391128,1,"Popular apps like FB, IG, twitter obviously have huge amounts of user activity data generated every second. How do they set up their pipelines to handle this? Do their application servers just send data directly to an insanely huge distributed message system like kafka? or is there more clever ways to do this?",True,False,False,dataengineering,t5_36en4,45241,public,self,How do big companies like Facebook handle their enormous user activity data?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qxzymc/how_do_big_companies_like_facebook_handle_their/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,ncafrey,,,[],,,,text,t2_s2hg9,False,False,False,[],False,False,1637352846,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qxoirm/how_to_build_a_pipeline_for_analytics_if_the/,{},qxoirm,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,12,0,False,all_ads,/r/dataengineering/comments/qxoirm/how_to_build_a_pipeline_for_analytics_if_the/,False,,,6,1637352857,1,"We are in the process of setting up a pipeline that is expected to transform the data available in Excel files and make it available for analytics purpose using SAP Analytics Cloud. There are around 10 Excel files (maximum 5MB each) that we receive every week from different departments which we would like to process. 

We already have access to Snowflake and Alteryx that could be used as a part of the pipeline. My questions are as follows:

* Is there any other tool that we could use in between to create this pipeline?
* Considering the data size, do we really need to have a DWH or could we instead use Python to process the data and then feed it as an input to SAP Analytics cloud eliminating both Snowflake and Alteryx in between?
* I would be also happy to hear any other suggestions based on your experience of how you handled Excel files to build a pipeline.",True,False,False,dataengineering,t5_36en4,45220,public,self,How to build a pipeline for analytics if the source data is made available only in Excel files?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qxoirm/how_to_build_a_pipeline_for_analytics_if_the/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,technicolor_paint,,,[],,,,text,t2_dt8oufta,False,False,False,[],False,False,1637351832,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qxo66d/data_pipelines_to_monitor_defi/,{},qxo66d,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,False,0,0,False,all_ads,/r/dataengineering/comments/qxo66d/data_pipelines_to_monitor_defi/,False,self,"{'enabled': False, 'images': [{'id': 'd8l06xf6vz7-OmkrbnF454fUd11JdyQV9zga5XDQM_o', 'resolutions': [{'height': 26, 'url': 'https://external-preview.redd.it/LThVzcgvXAzmxEB8SIB5_-1GvfmGlhAZl5lu5uEwUSc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c89fb09216be02e0d543f972b23a81502a2552a0', 'width': 108}, {'height': 53, 'url': 'https://external-preview.redd.it/LThVzcgvXAzmxEB8SIB5_-1GvfmGlhAZl5lu5uEwUSc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=05ae2b17c388e3ab61959a2d1cb5f68e99e09d9b', 'width': 216}, {'height': 78, 'url': 'https://external-preview.redd.it/LThVzcgvXAzmxEB8SIB5_-1GvfmGlhAZl5lu5uEwUSc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c172b60bd6591d5de6ee574dda2598b98d395938', 'width': 320}, {'height': 157, 'url': 'https://external-preview.redd.it/LThVzcgvXAzmxEB8SIB5_-1GvfmGlhAZl5lu5uEwUSc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6272446db3909f889096aa34260db62673da6ec8', 'width': 640}, {'height': 236, 'url': 'https://external-preview.redd.it/LThVzcgvXAzmxEB8SIB5_-1GvfmGlhAZl5lu5uEwUSc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ba2b9127862c4a6bd22d853414cf3c7b6936d87e', 'width': 960}, {'height': 265, 'url': 'https://external-preview.redd.it/LThVzcgvXAzmxEB8SIB5_-1GvfmGlhAZl5lu5uEwUSc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2942507ffa3cf4d86f425462fc71ce7820b9b872', 'width': 1080}], 'source': {'height': 295, 'url': 'https://external-preview.redd.it/LThVzcgvXAzmxEB8SIB5_-1GvfmGlhAZl5lu5uEwUSc.jpg?auto=webp&amp;s=1b8e15510e75e378f3d1d6ed9d5aceb599cb979d', 'width': 1200}, 'variants': {}}]}",6,1637351843,1,"Hi everyone\~ Here's a great article on how to use open source tools to create data pipelines for monitoring DeFi. They used a Python API with Dagster, a data pipeline framework:

[https://cloudwall.medium.com/the-geek-out-riding-moonstreams-7dbe10ac9956](https://cloudwall.medium.com/the-geek-out-riding-moonstreams-7dbe10ac9956)",True,False,False,dataengineering,t5_36en4,45218,public,self,Data pipelines to monitor DeFi,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qxo66d/data_pipelines_to_monitor_defi/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,user19911506,,,[],,,,text,t2_6ys5mu5,False,False,False,[],False,False,1637350602,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qxnrq6/transition_from_a_bi_to_data_engineer_job/,{},qxnrq6,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,6,0,False,all_ads,/r/dataengineering/comments/qxnrq6/transition_from_a_bi_to_data_engineer_job/,False,,,6,1637350613,1,"Hi All, 

I hope this is the correct sub to post this question and I am hoping I will get some clarity from experienced folks here. 

I am currently experiencing a career stagnation in my role as BI consultant, I had actually started as Analytics consulting company where I used to talk to clients and the DS in my team, there was some dashboard building stuff which I did, along with SQl writing. 

Couple of years passed by I landed a role with my client with a substantial pay hike, but the work was again mostly insights for sales, writing SQl and dashboard, I feel like I have these golden handcuffs,I constantly feel out of touch with DS world and with very little modeling work recently I am not sure if I can even qualify for a DS role anymore.I am sticking to the job just because the pay is good. 

I am at a cross roads to decide where to go from now, I done feel confidant enough to pursue pure DS role, that leaves me with traditional BI and DE work, the place where I am. I only see DS positions and no BI position at all (no EU and non US) , or feels like all traditional BI jave become DS roles suddenly. 

So was thinking of transitioning  into DE full time, since I know SQL, hadoop, python, Airflow, lol bit Dockwr as well. 
But again the job market is little low in my area. 

I wanted to know from the folks in this sub, do you feel DE role slowly being absorbed into MLE roles ? Are the lines being blurrred between DE and DS? Is Devops also now part of DE? 

Also because of my anxiousness I decided to ourselves AWS solution architect certificate to gain cloud knowledge, is this something you would expect from a DE? 

I am sorry if this post comes across as naive bit I done have any mentor at my workplace and reddit seemed the only place where I can get some honest feedback",True,False,False,dataengineering,t5_36en4,45216,public,self,"Transition from a BI to Data Engineer, Job prospects and career growth.",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qxnrq6/transition_from_a_bi_to_data_engineer_job/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,gorkemyurt,,,[],,,,text,t2_y15lw,False,False,False,[],False,False,1637347488,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qxmpc2/fal_run_python_scripts_directly_from_your_dbt/,{},qxmpc2,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/qxmpc2/fal_run_python_scripts_directly_from_your_dbt/,False,self,"{'enabled': False, 'images': [{'id': 'V_H76hMCNpleXxuBxBLaFFNzqJAiDcA_ZfOHp2Mdv2E', 'resolutions': [{'height': 54, 'url': 'https://external-preview.redd.it/pPTemWpd-JyzZhh3wqurhisI1mmGRusbL8-RdmlCO4Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5c259489a002cf7fb24d1b859df017cd80a2fa56', 'width': 108}, {'height': 108, 'url': 'https://external-preview.redd.it/pPTemWpd-JyzZhh3wqurhisI1mmGRusbL8-RdmlCO4Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b8e3541bfff15b3d5e1fd8ec0c4a92295de85d1d', 'width': 216}, {'height': 160, 'url': 'https://external-preview.redd.it/pPTemWpd-JyzZhh3wqurhisI1mmGRusbL8-RdmlCO4Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8135fa2f4f2048c4f80e9eba68ceba0b977cd76b', 'width': 320}, {'height': 320, 'url': 'https://external-preview.redd.it/pPTemWpd-JyzZhh3wqurhisI1mmGRusbL8-RdmlCO4Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ca1f9fdadfbbcf1eacbbbfe97e418ac681764a92', 'width': 640}, {'height': 480, 'url': 'https://external-preview.redd.it/pPTemWpd-JyzZhh3wqurhisI1mmGRusbL8-RdmlCO4Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a1f7078b0f7928694ddd537cecd03fec31e91420', 'width': 960}, {'height': 540, 'url': 'https://external-preview.redd.it/pPTemWpd-JyzZhh3wqurhisI1mmGRusbL8-RdmlCO4Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=74aef2a0b4422e9373e1e6c33306c248f0066d20', 'width': 1080}], 'source': {'height': 600, 'url': 'https://external-preview.redd.it/pPTemWpd-JyzZhh3wqurhisI1mmGRusbL8-RdmlCO4Q.jpg?auto=webp&amp;s=89a16e4f0df2f22e8be4bc97521b5774917045c8', 'width': 1200}, 'variants': {}}]}",6,1637347499,1,"Hi friends! Wanted to show you an open-source project that we have been working on: [https://github.com/fal-ai/fal](https://github.com/fal-ai/fal).

We love dbt **and** we want to do more with it. Specifically we want to use it for our Data Science workloads and run python on our dbt models. So we built fal to help us do that.Here is a quick demo of it: [https://www.loom.com/share/bb49fffaa6f74e90b91d26c77f35ecdc](https://www.loom.com/share/bb49fffaa6f74e90b91d26c77f35ecdc) Some examples of what you can do with fal:

* download dbt models using a familiar ref  
 syntax as pandas dataframes
* send slack messages on model runs
* do forecasts on metrics
* do sentiment analysis on support tickets

Let us know if you have any feedback or want to see more examples!",True,False,False,dataengineering,t5_36en4,45211,public,self,Fal: run python scripts directly from your dbt project,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qxmpc2/fal_run_python_scripts_directly_from_your_dbt/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,hank1321,,,[],,,,text,t2_wddyl,False,False,False,[],False,False,1637339526,betterprogramming.pub,https://www.reddit.com/r/dataengineering/comments/qxjw4j/10_common_mistakes_when_building_analytical_data/,{},qxjw4j,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/qxjw4j/10_common_mistakes_when_building_analytical_data/,False,link,"{'enabled': False, 'images': [{'id': 'kdlS_GPkce6E-nEnf-ler_fxREz1VBq0jTJ60Qa2GMc', 'resolutions': [{'height': 72, 'url': 'https://external-preview.redd.it/gqtAzb9mPdmGSZV7RIwuK-sayNfkguTbSoqv-nyLZbg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c5219d15f0ca96ce43fb0d15c4e9125db6e2a3e5', 'width': 108}, {'height': 144, 'url': 'https://external-preview.redd.it/gqtAzb9mPdmGSZV7RIwuK-sayNfkguTbSoqv-nyLZbg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=64fe102feb00f86759c4459b025c0e54064963ab', 'width': 216}, {'height': 213, 'url': 'https://external-preview.redd.it/gqtAzb9mPdmGSZV7RIwuK-sayNfkguTbSoqv-nyLZbg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=da968860d56b3564d2658b2d915707502849e691', 'width': 320}, {'height': 426, 'url': 'https://external-preview.redd.it/gqtAzb9mPdmGSZV7RIwuK-sayNfkguTbSoqv-nyLZbg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d43e528c104f919f58efe1b2b3e17582c7fdfdb4', 'width': 640}, {'height': 640, 'url': 'https://external-preview.redd.it/gqtAzb9mPdmGSZV7RIwuK-sayNfkguTbSoqv-nyLZbg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ef8b1c7398f5ef18862f8b1dc3a9b8b16273869f', 'width': 960}, {'height': 720, 'url': 'https://external-preview.redd.it/gqtAzb9mPdmGSZV7RIwuK-sayNfkguTbSoqv-nyLZbg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=407cb9e708fcc4a956e784d6c5ee4dfd1b470a73', 'width': 1080}], 'source': {'height': 800, 'url': 'https://external-preview.redd.it/gqtAzb9mPdmGSZV7RIwuK-sayNfkguTbSoqv-nyLZbg.jpg?auto=webp&amp;s=a06fd05f3f98f89f4c3dae203ac809efc2392906', 'width': 1200}, 'variants': {}}]}",6,1637339537,1,,True,False,False,dataengineering,t5_36en4,45202,public,https://b.thumbs.redditmedia.com/gE_7e_aCDZ7bVOcyByF9Ztit0wuL4b4jL7vJivxFl4E.jpg,10 Common Mistakes When Building Analytical Data Models — Designing a data model for analytics is not the same as doing it for transactional processing. You optimize for access patterns that are very different from row-level data retrieval used in OLTP systems.,0,[],1.0,https://betterprogramming.pub/10-common-mistakes-when-building-analytical-data-models-814c763d1b70,all_ads,6,,,,,,93.0,140.0,https://betterprogramming.pub/10-common-mistakes-when-building-analytical-data-models-814c763d1b70,,,,,,,,,,
[],False,jredrose,,,[],,,,text,t2_3imshnks,False,False,False,[],False,False,1637337269,youtube.com,https://www.reddit.com/r/dataengineering/comments/qxj47a/kedro_projects_and_kedro_starters_for_data/,{},qxj47a,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,4,0,False,all_ads,/r/dataengineering/comments/qxj47a/kedro_projects_and_kedro_starters_for_data/,False,rich:video,"{'enabled': False, 'images': [{'id': 'JKTBtUe5PSZ-URCxdz3wwm-xLuNhjGbUL-TsD_8e_Ck', 'resolutions': [{'height': 81, 'url': 'https://external-preview.redd.it/idseTGGEAQcYLwolxwoMZQsApKH7wTZBuimaV1BIuAg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4eabfa086101c9df917e8cd62447a1b71b36ea0d', 'width': 108}, {'height': 162, 'url': 'https://external-preview.redd.it/idseTGGEAQcYLwolxwoMZQsApKH7wTZBuimaV1BIuAg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8c657521c34b45ca3189c6cc1f8db05d96c08fb2', 'width': 216}, {'height': 240, 'url': 'https://external-preview.redd.it/idseTGGEAQcYLwolxwoMZQsApKH7wTZBuimaV1BIuAg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7e5ecbe35d9f2a3f384b007c49e350834e38f8fc', 'width': 320}], 'source': {'height': 360, 'url': 'https://external-preview.redd.it/idseTGGEAQcYLwolxwoMZQsApKH7wTZBuimaV1BIuAg.jpg?auto=webp&amp;s=657ecfdda048028ddadc8acac3a4018c170695ca', 'width': 480}, 'variants': {}}]}",6,1637337280,1,,True,False,False,dataengineering,t5_36en4,45199,public,https://b.thumbs.redditmedia.com/HtNOi6RbZa4k3r6fVoclyiMGBC27DSKr_Sr-w12Mjgs.jpg,Kedro Projects and Kedro Starters for Data Engineering and Data Science projects,0,[],1.0,https://www.youtube.com/watch?v=tG9Ksv5s50k,all_ads,6,"{'oembed': {'author_name': 'The Data Science Channel', 'author_url': 'https://www.youtube.com/channel/UCkp0ctv0vCNfh7i7D9GnHhw', 'height': 200, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/tG9Ksv5s50k?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/tG9Ksv5s50k/hqdefault.jpg', 'thumbnail_width': 480, 'title': 'Kedro Projects and Iris Dataset Starter example', 'type': 'video', 'version': '1.0', 'width': 356}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/tG9Ksv5s50k?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'scrolling': False, 'width': 356}",,"{'oembed': {'author_name': 'The Data Science Channel', 'author_url': 'https://www.youtube.com/channel/UCkp0ctv0vCNfh7i7D9GnHhw', 'height': 200, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/tG9Ksv5s50k?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/tG9Ksv5s50k/hqdefault.jpg', 'thumbnail_width': 480, 'title': 'Kedro Projects and Iris Dataset Starter example', 'type': 'video', 'version': '1.0', 'width': 356}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/tG9Ksv5s50k?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/qxj47a', 'scrolling': False, 'width': 356}",105.0,140.0,https://www.youtube.com/watch?v=tG9Ksv5s50k,,,,,,,,,,
[],False,RstarPhoneix,,,[],,,,text,t2_57e44nxs,False,False,False,[],False,False,1637336181,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qxir62/needed_advice_on_data_governance_in_etl/,{},qxir62,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,14,0,False,all_ads,/r/dataengineering/comments/qxir62/needed_advice_on_data_governance_in_etl/,False,,,6,1637336192,1,"I am making an ETL completely based in AWS. The data is stored in multiple S3 buckets and Redshift. What all data governance rules should I focus on or comply to (like data masking , roles, catalog etc) ? Any help is highly appreciated. Thanks in advance",True,False,False,dataengineering,t5_36en4,45198,public,self,Needed advice on Data governance in ETL,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qxir62/needed_advice_on_data_governance_in_etl/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Vorskl,,,[],,,,text,t2_2lvmxsuq,False,False,False,[],False,False,1637335354,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qxihc4/decent_de_courses_at_udemy/,{},qxihc4,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,3,0,False,all_ads,/r/dataengineering/comments/qxihc4/decent_de_courses_at_udemy/,False,,,6,1637335365,1,"Hi, the famous discount event started at Udemy.

What are the DE courses you're looking for to stockpile?",True,False,False,dataengineering,t5_36en4,45198,public,self,decent DE courses at Udemy?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qxihc4/decent_de_courses_at_udemy/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,rainbowenough,,,[],,,,text,t2_1bw5hrkk,False,False,False,[],False,False,1637333414,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qxhsl5/data_engineering_or_data_analyst_without_prior/,{},qxhsl5,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,False,20,0,False,all_ads,/r/dataengineering/comments/qxhsl5/data_engineering_or_data_analyst_without_prior/,False,,,6,1637333432,1,"Hello, i might be asking something that has been asked before here but let me try :) 

I have a bachelor in CIS (concentrated around information systems security) graduated in 2017 and didnt like it much so i worked in low-skills labor jobs like data clerk and cashier. 

Two months ago i got exposed to data engineering and did my research to see what they do and got me really interested. 

I subbed to DataCamp and im studied SQL (i love sql and database in general, it always made sense to me than any other programming languages). And now im studying python and it has been going well with me (also doing projects using pandas, numpy and sqlite) so i can say that i went from zero in sql and python to helping my baby brother in his python/sql highschool courses. 

Now, im not from US and Europe. I live in UAE in Dubai and going through Linkedin i didnt find internship or junior data engineering and now hesitating wether i should continue my studies in data engineering or should i study data analyst?

If i get to continue studying data engineering, i would want to connect with teams that need data engineering and i can do it for free as long as i get to use that experience in building my CV. Any idea where i can connect with people globally?",True,False,False,dataengineering,t5_36en4,45198,public,self,Data engineering or data analyst without prior experience?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qxhsl5/data_engineering_or_data_analyst_without_prior/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,order_chaos_in,,,[],,,,text,t2_17d2xtzb,False,False,False,[],False,False,1637310105,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qxbq1w/migrating_onprem_spark_jobs_to_google_dataproc/,{},qxbq1w,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/qxbq1w/migrating_onprem_spark_jobs_to_google_dataproc/,False,self,"{'enabled': False, 'images': [{'id': 'DsiOIzUSicS_9zIKwMDQbNT2LOE1o29sSYs49HAmO_k', 'resolutions': [{'height': 56, 'url': 'https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=05af26b5ec35c95ed95b5c40dbde3c1cc04dce06', 'width': 108}, {'height': 113, 'url': 'https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f36f0de65cdcbed3b705b8446710c7c83e0475e4', 'width': 216}, {'height': 168, 'url': 'https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4067aebaadaec227b271d9c19c7af833c230fd32', 'width': 320}, {'height': 336, 'url': 'https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ec46bff13e5c5bc616be11b375484d9d2a7fcbe8', 'width': 640}, {'height': 504, 'url': 'https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5db472c15d5717384ab8f8f64e9fd89efe70fa59', 'width': 960}, {'height': 567, 'url': 'https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fab49c1958487e16d598ede6d81140177c5c9a31', 'width': 1080}], 'source': {'height': 630, 'url': 'https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?auto=webp&amp;s=ada93f0d146c09556ac26cc3fa125a0aa7102150', 'width': 1200}, 'variants': {}}]}",6,1637310116,1," 

Hello,

I am looking for some advice. 

I am researching on how to migrating spark jobs to Dataproc. What are the changes like code refactoring, cluster config.... required for it. The [google documentation](https://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc) only talks about changing references from hdfs:// to gs:// assuming data has been migrated to cloud storage. I could think about creating Dataproc VM images similar to on prem hadoop with dependencies.

what are other changes required in your opinion? Does anyone has experienced difficulty on doing this? any tips and advice would be great. Thanks",True,False,False,dataengineering,t5_36en4,45185,public,self,Migrating on-prem Spark Jobs to Google Dataproc,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qxbq1w/migrating_onprem_spark_jobs_to_google_dataproc/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,icchipcompanyWu,,,[],,,,text,t2_fdv3qhwj,False,False,False,[],False,False,1637300982,12chip.com,https://www.reddit.com/r/dataengineering/comments/qx9j6g/renesas_electronics_enters_fpga_market_with_new/,{},qx9j6g,False,False,False,False,False,False,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/qx9j6g/renesas_electronics_enters_fpga_market_with_new/,False,link,"{'enabled': False, 'images': [{'id': '49SCUjj2NsyZ0dG6rmhAr7OwqSK7bzqj5PMVxl9cNxU', 'resolutions': [{'height': 60, 'url': 'https://external-preview.redd.it/VeWhG2ciNV8OYTuJPqXKSMAGxCOQhESUXPqrFRGEwMI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=966d35ca256daf6eb838fd967e0ace66ae2d1001', 'width': 108}, {'height': 121, 'url': 'https://external-preview.redd.it/VeWhG2ciNV8OYTuJPqXKSMAGxCOQhESUXPqrFRGEwMI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8297cef839df62a67864b70c0864ebb2a3ee076d', 'width': 216}, {'height': 180, 'url': 'https://external-preview.redd.it/VeWhG2ciNV8OYTuJPqXKSMAGxCOQhESUXPqrFRGEwMI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f91b63e6484497710acecf33b7eb085ff47cb34e', 'width': 320}], 'source': {'height': 288, 'url': 'https://external-preview.redd.it/VeWhG2ciNV8OYTuJPqXKSMAGxCOQhESUXPqrFRGEwMI.jpg?auto=webp&amp;s=d29a2e35453056600107c1c4fccade10a051793b', 'width': 512}, 'variants': {}}]}",6,1637300992,1,,True,False,False,dataengineering,t5_36en4,45180,public,https://b.thumbs.redditmedia.com/7_iCdqI7uRiG_6BDQiu8Bjiqa8A_9PrUeqUOamb8V6U.jpg,Renesas Electronics Enters FPGA Market with New ForgeFPGAs Focusing on Ultra-low Power and Low Cost,0,[],1.0,https://www.12chip.com/article/company-news/renesas-electronics-enters-fpga-market-with-new-forgefpgas-focusing-on-ultra-low-power-and-low-cost.html?lang=en-us,all_ads,6,,,reddit,,,78.0,140.0,https://www.12chip.com/article/company-news/renesas-electronics-enters-fpga-market-with-new-forgefpgas-focusing-on-ultra-low-power-and-low-cost.html?lang=en-us,,,,,,,,,,
[],False,BigBoatThrowaway,,,[],,,,text,t2_6x7d7af4,False,False,False,[],False,False,1637299924,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qx991g/asking_for_raise_in_dec_currently_unicorn_data/,{},qx991g,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,30,0,False,all_ads,/r/dataengineering/comments/qx991g/asking_for_raise_in_dec_currently_unicorn_data/,False,,,6,1637299935,1,"**Key Points:**

\- Small Sized Non Tech related company in DFW 

\- YTD Gross Earning of 10 Million $. 

\- Only 3 members in IT Department, Boss is a Part-Time Consultant , 1 is a SysAdmin (makes 70K Salary FT, No degree) and myself (50K Salary FT)

\- New Executive Team that is starting to become more Data-Driven instead of Intuition Driven. 

\- Responsible for retrieving Data/Building Reports for the Executive Team, Maintaining the SQL Server &amp; ETL. 

\- Also Responsible for rebuilding the Schema for a the first Proper Data Warehouse with PostgreSQL &amp; Migrating out Data Operations from On-Prem to Cloud 

\-Don't deal with any big data. 

\- Currently still doing my Bachelors in Mathematics with a minor in Data Science. Currently a Sophomore.

\- Had no Prior Data Experience. 

**- Boss has been trying to convince CFO to pay me what I'm worth (Was initiated himself not by me)**

\- All The Executives really appreciate what I do. Haven't received a single complaint from any of them

\- Boss has been expressing interest in wanting me to be sent for conferences to further career growth

\- I intend not to consider leaving the company until perhaps year 2 just so I'm a competitive candidate in the market

 

**Proposal**

**-**  ask for 75K Salary (with listed accomplishments)

or 

\- ask for 70K and be have 5K be budgeted for  Conferences 

&amp;#x200B;

Am I asking too much, too little, just right or not asking the right things?",True,False,False,dataengineering,t5_36en4,45178,public,self,"Asking for raise in Dec, currently Unicorn Data Engineer with &lt;1 year experience. DFW Area. Need Second-Opinion if am asking too much/too little for what I do.",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qx991g/asking_for_raise_in_dec_currently_unicorn_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Toriaru,,,[],,,,text,t2_6y4c6tx9,False,False,False,[],False,False,1637299031,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qx90dj/data_observability_pains_real/,{},qx90dj,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,4,0,False,all_ads,/r/dataengineering/comments/qx90dj/data_observability_pains_real/,False,,,6,1637299053,1,"Recently joined a startup in the data observability space (on the much hated sales side), but don’t have a background in data engineering, and spent the last 2 months painfully trudging through SQL courses anywhere I could find them.

Knowing that I’ll never fully understand the potential challenges that data engineering teams face, I wanted to humbly (and embarrassingly) ask if solutions like Data Diff which Datafold has, which our team will be launching soon alongside our main focus on alerting, solve an issue that data teams actually care about in the proactive QA space.

This is probably not the most relevant for the sub, but would rather not miss a shot (or info) because of not asking  instead of being embarrassed.

Thanks in advance, and mods, if this isn’t relevant enough you can take it down.",True,False,False,dataengineering,t5_36en4,45177,public,self,Data Observability Pains Real?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qx90dj/data_observability_pains_real/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,saoirsenov,,,[],,,,text,t2_3hij4m18,False,False,False,[],False,False,1637285410,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qx4x7z/building_a_data_warehouse_from_scratch_for_a/,{},qx4x7z,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,31,0,False,all_ads,/r/dataengineering/comments/qx4x7z/building_a_data_warehouse_from_scratch_for_a/,False,,,6,1637286756,1,"I will be creating a datawarehouse from ground up. Kind of lost on where to begin since I've thought I'm only be doing the maintenance for the current infrastructure.

Setup:
I am working for a medium sized (?) manufacturing company with 500 customers currently. Our sales department manually generates daily/weekly/monthly/yearly reports and they get these info from different data sources.
They want to do analytics and slice and dice type and we already bought a Tableau for the reporting.

Q:

Is MySQL + Cloud SQL good for this DWH setup.  If no, what's your recommendation?
What's the best practices?
How to begin with integrating an ETL process to fetch all the historical data to a staging layer? What are the things we should install for these?

Appreciate all your assistance!",True,False,False,dataengineering,t5_36en4,45167,public,self,Building a Data warehouse from scratch for a manufacturing company,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qx4x7z/building_a_data_warehouse_from_scratch_for_a/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Background_Claim7907,,,[],,,,text,t2_812p5csg,False,False,False,[],False,False,1637282986,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qx4685/how_expensive_are_certain_easy_computations_for/,{},qx4685,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,16,0,False,all_ads,/r/dataengineering/comments/qx4685/how_expensive_are_certain_easy_computations_for/,False,,,6,1637284414,2,"Hi, in summary here's the situation:
A large enterprise with terabytes of data on Teradata (heh). But as this is batch-run, it cannot be used for real-time information from many source databases to web applications. There's ongoing work to CDC this by building an intermediate layer, converting it and making it match with all of the different sources warehouses (some of which predate Star Wars) which ultimately ends up as a DB2 on z/OS (mainframe). *An actual* approach for data quality is pretty much nonexistant in this organization, so the general idea now (on the very short term) is to perform simple data profiling calculations on this DB2 (which is now the new 'single source of truth') and spitting it out to some reporting screen. Down the road, management is going to look at the whole circus (Apache Griffin etc). Now I was told you cannot run these calculations every 5 minutes or so because it would use too much resources (Average, median, length(field), min/max, quantile, group by and then all of the above, timediff (of 2 colomns), unique, frequency, count NULL and perhaps some more). There are about 7-8 tables, 4-5 columns each and let's say about a billion records. On my personal computer I've done these simple calculations in Python (on a single processor) on large datasets (&gt;2gb) in a few sec at most, albeit it was just a pandas dataframe (or 2). So how would this clog up a data warehouse that is built for these kinds of tasks. How would I know if performing these calculations would be too much (without actually performing them, I don't have access). I just want to have a *general idea* whether these commands cause most systems to clog up with the amount of records, or whether it really doesn't matter still. 

Me: A recently employed data scientist who was put on a data engineering task and has never seen his colleagues in real life.",True,False,False,dataengineering,t5_36en4,45166,public,self,How expensive are certain (easy) computations for mainframe data warehouses?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qx4685/how_expensive_are_certain_easy_computations_for/,all_ads,6,,,,,,,,,,,,,,,,1637283369.0,,
[],False,Whimsicalpants,,,[],,,,text,t2_6wcq1hgd,False,False,False,[],False,False,1637278754,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qx2tge/inter_cloud_streaming_with_kafka/,{},qx2tge,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,3,0,False,all_ads,/r/dataengineering/comments/qx2tge/inter_cloud_streaming_with_kafka/,False,,,6,1637279486,1,"I have a need to provide some kind of push service from my org that other organisations can subscribe to.  The data load can get quite heavy (1000000 messages per org per day) and currently we are forcing the other orgs to subscribe to our kinesis stream (ergo they also need AWS).  We now want to expand this so that orgs with other clouds (GCP, AZURE) can subscribe to our stream.

My thought is that we could use managed Kafka, but what I'm not able to determine is whether or not Kafka running on other cloud providers can subscribe to a Kafka stream from AWS?  I'm assuming it can (confluent for example)?  Any org that wanted to subscribe can use whatever Kafka service they like to connect regardless of their cloud provider.

Assuming that as a concept Kafka makes sense for intercloud streaming, what would the implications be around things like latency? 

I have exactly zero experience with Kafka (and minimal with kinesis to be honest) so I'm keen to hear if Kafka makes sense for this use case.",True,False,False,dataengineering,t5_36en4,45164,public,self,Inter cloud streaming with Kafka?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qx2tge/inter_cloud_streaming_with_kafka/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,infinitegraph,,,[],,,,text,t2_4bui6,False,False,False,[],False,False,1637276874,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qx26lq/whats_the_big_deal_with_graph_databases/,{},qx26lq,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/qx26lq/whats_the_big_deal_with_graph_databases/,False,,,6,1637277339,1,Why graph databases are so hot and why you should check them out.,True,False,False,dataengineering,t5_36en4,45160,public,self,What's the big deal with graph databases?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qx26lq/whats_the_big_deal_with_graph_databases/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,infinitegraph,,,[],,,,text,t2_4bui6,False,False,False,[],False,False,1637273822,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qx14qb/why_should_you_use_a_graph_database/,{},qx14qb,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/qx14qb/why_should_you_use_a_graph_database/,False,self,"{'enabled': False, 'images': [{'id': 'mhOYtlYYRNcq4EsLxR3-sSc7z0cSmJ2ZGB13irXlp7U', 'resolutions': [{'height': 56, 'url': 'https://external-preview.redd.it/n0qXWPSIVYk9Mst_pFnkTSWL7pwM3Y_yLSFlB21C71I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c6accccfdeda25aaba618dcfdd8babddcf1b95f4', 'width': 108}, {'height': 112, 'url': 'https://external-preview.redd.it/n0qXWPSIVYk9Mst_pFnkTSWL7pwM3Y_yLSFlB21C71I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a936b42aad2e6b1d9edcddb82a314eb60e1fb3ed', 'width': 216}, {'height': 167, 'url': 'https://external-preview.redd.it/n0qXWPSIVYk9Mst_pFnkTSWL7pwM3Y_yLSFlB21C71I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4dce15daa56fbbeaf599095bc6b6a679ebda3fa1', 'width': 320}, {'height': 334, 'url': 'https://external-preview.redd.it/n0qXWPSIVYk9Mst_pFnkTSWL7pwM3Y_yLSFlB21C71I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7eb1b6bc266fd850250168a0287bd5e615242544', 'width': 640}, {'height': 501, 'url': 'https://external-preview.redd.it/n0qXWPSIVYk9Mst_pFnkTSWL7pwM3Y_yLSFlB21C71I.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3e20a302b2e7e2263f4492fae634591ef0fa3cd6', 'width': 960}, {'height': 564, 'url': 'https://external-preview.redd.it/n0qXWPSIVYk9Mst_pFnkTSWL7pwM3Y_yLSFlB21C71I.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1c1c09ff379fe84f414403a314a8622b1d6e86d0', 'width': 1080}], 'source': {'height': 627, 'url': 'https://external-preview.redd.it/n0qXWPSIVYk9Mst_pFnkTSWL7pwM3Y_yLSFlB21C71I.jpg?auto=webp&amp;s=2b9ce31673f575a35fb157f45fd62e1db950263e', 'width': 1200}, 'variants': {}}]}",6,1637273833,1,"

**What is a Graph Database?**

What's all the ""buzz"" about graph databases? Well, it's not just buzz. The graph database market is exploding and so is the usage of graph databases in operational systems.

The limitations of the Relational Database Management System (RDBMS) model, such as strict schemas and strong consistency, along with the explosion of data and data types has led software developers and data architects to explore alternative data models for the past couple decades. Relational databases were not designed to traverse relationships between things. They were intended to catalog and retrieve structured data. Also, as data volumes increase, relational databases require much more complex engineering and administration, and their performance and reliability degrade significantly.

The “Not Only SQL” or “NoSQL” database movement emerged in the early 2000’s. It gave developers and architects greater freedom to store and query data in new and innovative ways versus dealing with the restrictions and limitations of RDBMS and SQL.

The database types within NoSQL include Key-Value Stores, Wide Column Stores, Document Databases and Graph Databases. 

One of the key advantages of using a graph database is the ability to model real world situations where there is lots of data and there are many complex connections between the data, not only reflected in the graph model, but also as new connections are discovered.

**The foundation of modern data.**

***“Graph forms the foundation of modern data and analytics with capabilities to enhance and improve user collaboration, machine learning models and explainable AI.”***

(Source:[ ](https://www.gartner.com/smarterwithgartner/gartner-top-10-data-and-analytics-trends-for-2021)[https://www.gartner.com/smarterwithgartner/gartner-top-10-data-and-analytics-trends-for-2021](https://www.gartner.com/smarterwithgartner/gartner-top-10-data-and-analytics-trends-for-2021))

Graph databases enable the traversal of relationships and connections, or nodes and edges, within complex data structures often referred to as graph data. The technical model of a graph database is the focus on relationships and ability to store the graph. Querying relationships is fast because they are stored within the database itself. Moreover, relationships can be easily visualized using graph databases.

For an application, if the connections between data are as important or more important than the data itself to the dominant queries, then you need a database that natively supports relationships. This particularly applies to navigational and pathfinding queries. Graph databases have an inherent advantage in cutting through big data clutter and finding the information that really matters. 

Graph analysis enables the use of multiple, simultaneous interpretive models, in parallel, over the same data, allowing business users to describe dynamic and complex domains more naturally and efficiently.

**Why Should I Care?**

**Graph is the Present and Future**

We are in the midst of the decade of data, a period where our world will produce an unfathomable amount of machine data — metrics, measurements, and telemetry data that is emitted from everything from servers to robots and satellites. And the pace of that data generation is increasing exponentially. Today this data is a virtually untapped source of extraordinary business insights — data so dense and rich it will dwarf traditional business intelligence in terms of its potential value.

Companies that learn to harness and leverage this dense and rich data will be the clear winners in their respective industries. 

The 2020s will be the era of data companies boosting massive markets. Database startups, data movement startups, data quality startups, data lineage startups, machine learning startups will be the zeitgeist of the decade as they shape the next wave of massive innovation.

Graph has the capability of becoming the de facto and leading model for NoSQL databases.

**Data is becoming more connected.**

As data becomes more and more connected or linked, information will be increasingly derived from the links between data instead of the data itself. As new use cases emerge, data-models need to be very flexible. The popular graph databases are those that store data using flexible models to extract information out of linked data.

If the data contains a lot of many-to-many relationships and the primary objective is quickly finding connections, patterns and relationships, then graph databases are superior to other technologies including relational databases, key-value, column or document databases.

According to Gartner, Inc., 80% of analytics will be graph based by 2025. The graph database is one of the top technologies used to detect fraud, uncover fraud rings, and identify sophisticated scams, including eCommerce fraud, corruption, and money laundering. Graph databases use pattern recognition, classification, statistical analysis and machine learning models to identify fraud from massive amounts of data. In addition to BFSI (Banking, Financial Services and Insurance) other operational use cases include recommendation engines, network analysis, knowledge graphs, traceability, logistics, healthcare intelligence systems and enterprise AI applications.

People. Places. Things. Events. Locations. Graph databases find relationships between them across diverse data assets in an instant for new intelligence.

[https://infinitegraph.com/why-graphdb/](https://infinitegraph.com/why-graphdb/)",True,False,False,dataengineering,t5_36en4,45157,public,self,Why Should You Use a Graph Database?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qx14qb/why_should_you_use_a_graph_database/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Complex-Stress373,,,[],,,,text,t2_9va3r6a5,False,False,False,[],False,False,1637272087,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qx0ifg/why_copying_a_parquet_file_into_snowflake_when/,{},qx0ifg,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/qx0ifg/why_copying_a_parquet_file_into_snowflake_when/,False,,,6,1637272098,1,"I feel confuse about the next scenario:

1. I have a parquet file into S3
2. I copy the parquet file into Snowflake (specifying compression Snappy and format Parquet).
3. The file is copied in my table and I can see the raw value. HOWEVER, this value is json, the parquet schema is los.

So Im wondering. I know the benefits of using Parquet, but indeed I don't know why copying parquet files into Snowflake is good when you miss an important bit of information, like the schema.

Am I doing something wrong?, is a logic question/doubt?",True,False,False,dataengineering,t5_36en4,45155,public,self,Why copying a parquet file into snowflake when the parquet schema is missed?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qx0ifg/why_copying_a_parquet_file_into_snowflake_when/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,demince,,,[],,,,text,t2_umfhx4d,False,False,False,[],False,False,1637271163,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qx06kd/efficient_data_processing/,{},qx06kd,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/qx06kd/efficient_data_processing/,False,self,"{'enabled': False, 'images': [{'id': 'bqzmvJkidPgfaXRiQd2-KECtmLqnSuoJkWq_0WhTxHg', 'resolutions': [{'height': 60, 'url': 'https://external-preview.redd.it/IwEkWlb1PpOkAXqF2ze0UC6LMetDkIrnjtCz0gWAssM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cfedb2c91dcba65597389f3f6b2d987c52ffe1d5', 'width': 108}, {'height': 121, 'url': 'https://external-preview.redd.it/IwEkWlb1PpOkAXqF2ze0UC6LMetDkIrnjtCz0gWAssM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=33059dc2b535970d2650528baf69988c814985c9', 'width': 216}, {'height': 180, 'url': 'https://external-preview.redd.it/IwEkWlb1PpOkAXqF2ze0UC6LMetDkIrnjtCz0gWAssM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=75989f95df101e4dd840e48d4798691a84a38847', 'width': 320}, {'height': 360, 'url': 'https://external-preview.redd.it/IwEkWlb1PpOkAXqF2ze0UC6LMetDkIrnjtCz0gWAssM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=25bfa4538aae86d119428083c8085a94d4677eb8', 'width': 640}, {'height': 540, 'url': 'https://external-preview.redd.it/IwEkWlb1PpOkAXqF2ze0UC6LMetDkIrnjtCz0gWAssM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=903496708f235d15f2a2d5ac6325418da271b1b5', 'width': 960}], 'source': {'height': 540, 'url': 'https://external-preview.redd.it/IwEkWlb1PpOkAXqF2ze0UC6LMetDkIrnjtCz0gWAssM.jpg?auto=webp&amp;s=8570fc1d2d800286a6b1e5848546a751caa5c973', 'width': 960}, 'variants': {}}]}",6,1637271174,1,"Hi all, I would like to bring your attention to an open source solution Versatile Data Kit. There is a new article that was just published on the topic. You can have a look here: [https://towardsdatascience.com/an-overview-of-versatile-data-kit-a812cfb26de7](https://towardsdatascience.com/an-overview-of-versatile-data-kit-a812cfb26de7) The tool can help anybody who is working with databases by allowing you to create Data Pipelines (either SQL, Python or both):

\- Data engineers: data ingestions, data transformations, provides out-of-the-box templates for building dimensional models, publishing data jobs

\- Data Analysts: who are performing complex data transformations and are looking for ways on automating them

\- Data Scientists: who without realising are already doing sophisticated data processing or are already benefiting from data structured in a well-known data models as the Kimball's dimensional model.

Hope this would also help some of you. Let me know if you need any help.",True,False,False,dataengineering,t5_36en4,45156,public,self,Efficient Data Processing,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qx06kd/efficient_data_processing/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Sliquid4,,,[],,,,text,t2_16bwux,False,False,False,[],False,False,1637268780,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qwzbt9/discussion_for_prospective_data_engineer/,{},qwzbt9,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,10,0,False,all_ads,/r/dataengineering/comments/qwzbt9/discussion_for_prospective_data_engineer/,False,,,6,1637268790,1,"Hi all, I am thinking about enrolling in a data engineer training program as a possible career switch. I have worked as a field engineer in the high voltage industry for several years. I was hoping to discuss with someone the day to day aspects of work life and what to expect in this industry. Any help is appreciated!",True,False,False,dataengineering,t5_36en4,45156,public,self,Discussion for prospective data engineer,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qwzbt9/discussion_for_prospective_data_engineer/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,NorthForNights,,,[],,,,text,t2_ks527,False,False,False,[],False,False,1637255266,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qwud6t/post_files_to_rest_api_vs_uploading_files_to_sftp/,{},qwud6t,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,12,0,False,all_ads,/r/dataengineering/comments/qwud6t/post_files_to_rest_api_vs_uploading_files_to_sftp/,False,,,6,1637255277,1,"Hey all,

Need to setup a monthly delivery of files made up of JPEG's, mp3's, mp4's, etc. It would typically be about \~1500-2000 files per delivery for about 5GB to 7.5GB worth of data.

Have both options available, just trying to figure our what would make the most sense, or be the easier/more efficient option. I think I have an idea, but would like to hear the pros and cons if possible.

Thanks",True,False,False,dataengineering,t5_36en4,45135,public,self,POST Files to REST API vs. Uploading Files to SFTP -- Which Would Make More Sense for Large Quantities of Files?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qwud6t/post_files_to_rest_api_vs_uploading_files_to_sftp/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,DigBick616,,,[],,,,text,t2_161ye8,False,False,False,[],False,False,1637252308,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qwt9s2/at_my_wits_end_with_ssis/,{},qwt9s2,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,37,0,False,all_ads,/r/dataengineering/comments/qwt9s2/at_my_wits_end_with_ssis/,False,,,6,1637252319,1,"Venting more than anything, but does anybody actually use SSIS and enjoy it? I get that it’s supposed to be a low code solution but I find that creating packages eats up so much more time with all the “gotchas” that come with the tool. 

Like today I’m trying to create a simple pipeline importing daily sales information via text file and it’s fighting me on converting a pricing column for a potential loss of data. I’ve cleaned out every issue with conditional split already and even loaded the dataset to DB with the field as text, shouldn’t be any issues from what I see and yet the import still bombs with necessary conversions to numeric. 

It’s just so frustrating when you get hung up on petty bullshit like this. I’m thinking future state I’ll be doing ELT with transformations done in SQL, at least until I progress further in my learning with python. 

Any other SSIS users feel this way?",True,False,False,dataengineering,t5_36en4,45129,public,self,At my wit’s end with SSIS,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qwt9s2/at_my_wits_end_with_ssis/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,igorlukanin,,,[],,,,text,t2_6qpanfe,False,False,False,[],False,False,1637251970,cube.dev,https://www.reddit.com/r/dataengineering/comments/qwt5ae/opensource_metrics_store_with_cube_building_a/,{},qwt5ae,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/qwt5ae/opensource_metrics_store_with_cube_building_a/,False,link,"{'enabled': False, 'images': [{'id': 'TKQXGvQvMGAFW-CBhRBClh_sC2dIPBug1fF1FjxElI8', 'resolutions': [{'height': 54, 'url': 'https://external-preview.redd.it/ipY5rq3YFalJej3Yubzfne5CAikOzE0WDlZqDDcjBpY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9863e08785cc61730686e1d19e725646febedb40', 'width': 108}, {'height': 108, 'url': 'https://external-preview.redd.it/ipY5rq3YFalJej3Yubzfne5CAikOzE0WDlZqDDcjBpY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c4200a9803ad409872abc3dfa2e195d87f3cffa1', 'width': 216}, {'height': 160, 'url': 'https://external-preview.redd.it/ipY5rq3YFalJej3Yubzfne5CAikOzE0WDlZqDDcjBpY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=270564faa261f597634b5c06552be987b0e6ae59', 'width': 320}, {'height': 320, 'url': 'https://external-preview.redd.it/ipY5rq3YFalJej3Yubzfne5CAikOzE0WDlZqDDcjBpY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=85b2a80885aad04f5259fd98ad7d3b33d94a9e90', 'width': 640}, {'height': 480, 'url': 'https://external-preview.redd.it/ipY5rq3YFalJej3Yubzfne5CAikOzE0WDlZqDDcjBpY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5979b1e0fcf9c63f78c8e73caf55acc115b9aac9', 'width': 960}, {'height': 540, 'url': 'https://external-preview.redd.it/ipY5rq3YFalJej3Yubzfne5CAikOzE0WDlZqDDcjBpY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f461285b7691dc580d54dff5f31120614c19956f', 'width': 1080}], 'source': {'height': 960, 'url': 'https://external-preview.redd.it/ipY5rq3YFalJej3Yubzfne5CAikOzE0WDlZqDDcjBpY.jpg?auto=webp&amp;s=ed7f38e12cc2c0650860af1485f2853e9d00cdbd', 'width': 1920}, 'variants': {}}]}",6,1637251981,1,,True,False,False,dataengineering,t5_36en4,45128,public,https://b.thumbs.redditmedia.com/l_JLHIJ0PnV2vJvJD8iUJMzGrOy92_0xDvIIGwO0wKo.jpg,Open-Source Metrics Store with Cube — building a single source of truth for metrics that integrates with Superset/Tableau AND your custom front-end app at the same time,0,[],1.0,https://cube.dev/blog/introducing-cube-sql/,all_ads,6,,,,,,70.0,140.0,https://cube.dev/blog/introducing-cube-sql/,,,,,,,,,,
[],False,cmstrump,,,[],,,,text,t2_wlpva,False,False,False,[],False,False,1637241772,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qwpmch/data_models_give_companies_the_good_oil_for_data/,{},qwpmch,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/qwpmch/data_models_give_companies_the_good_oil_for_data/,False,self,"{'enabled': False, 'images': [{'id': '826g_advtLsG9_RlccaZqdSdr-o50ZOMIUaFjzklmyI', 'resolutions': [{'height': 67, 'url': 'https://external-preview.redd.it/pUQgb8FEHEfdWpJiAQg6gGbQ_D3MxQasmEDgrarCAEY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cfb82d72eb11f31368353faf6140a172f4489710', 'width': 108}, {'height': 134, 'url': 'https://external-preview.redd.it/pUQgb8FEHEfdWpJiAQg6gGbQ_D3MxQasmEDgrarCAEY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=10bc4531f71e071ad3e0aca01c4a3bcfa13ef244', 'width': 216}, {'height': 199, 'url': 'https://external-preview.redd.it/pUQgb8FEHEfdWpJiAQg6gGbQ_D3MxQasmEDgrarCAEY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=97ba29e9665efff9cc8faf89ddfe0fabc617382c', 'width': 320}, {'height': 399, 'url': 'https://external-preview.redd.it/pUQgb8FEHEfdWpJiAQg6gGbQ_D3MxQasmEDgrarCAEY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=180d243274153004dce6752a7b881e3553daa5f4', 'width': 640}, {'height': 598, 'url': 'https://external-preview.redd.it/pUQgb8FEHEfdWpJiAQg6gGbQ_D3MxQasmEDgrarCAEY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=94d969140624b94a9868346f05c0fdab83b05aa6', 'width': 960}, {'height': 673, 'url': 'https://external-preview.redd.it/pUQgb8FEHEfdWpJiAQg6gGbQ_D3MxQasmEDgrarCAEY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=75204eca89dfe93d4f71d24781e06a3cdd880532', 'width': 1080}], 'source': {'height': 1204, 'url': 'https://external-preview.redd.it/pUQgb8FEHEfdWpJiAQg6gGbQ_D3MxQasmEDgrarCAEY.jpg?auto=webp&amp;s=8e101768f49f69ca5b78538b63800f284b3b85a2', 'width': 1930}, 'variants': {}}]}",6,1637241783,1,"The following guide shows how you can craft a data governance practice to align with your company’s overall business goals for establishing the processes that guard the data throughout its lifecycle and defining the policies for accessing data with the help of well-articulated roles and metrics: [Data Models Give Companies the Good Oil for Data Governance](https://bipp.io/blog/bipp-built-a-better-bi-tool-with-modern-software-architecture/)

The approach represented in more details in the guide above could be called the four pillars of data model governance. These will help you gauge the effectiveness of data models to connect data management and data definition:

1. Data Coherence
2. Data Consistency
3. Data Compatibility
4. Data Compliance",True,False,False,dataengineering,t5_36en4,45122,public,self,Data Models Give Companies the Good Oil for Data Governance,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qwpmch/data_models_give_companies_the_good_oil_for_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,LarsBearjew,,,[],,,,text,t2_z6uokpp,False,False,False,[],False,False,1637232141,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qwn30j/oracle_consultant/,{},qwn30j,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,False,3,0,False,all_ads,/r/dataengineering/comments/qwn30j/oracle_consultant/,False,,,6,1637232152,1,"**Hi all!** I recently graduated in CS and now looking for work. I found two opportunities in line with my possibilities (for now, I don't want to relocate, and the offers are nearby and/or remote-friendly). 

One is a full-stack developer position: mainly Python and JS programming, streaming applications focused. I'm ""specialized"" in Python, and I like it, but I'm ignorant of JS. The full-stack part it's not so much in line with my aspirations. 

The other is an Oracle consultant position: I will work on ELT and Datawarehouse implementation for a prominent local company using [ODI](https://www.oracle.com/middleware/technologies/data-integrator.html). Probably some BI project soon after.

My academic background is AI / DS. I liked it, but soon I found out that there is no ""Titanic dataset"" in the real world. Actual data tend to be awful. I recently discovered DE, and now I'm in love with it. I want to become a hybrid figure: half DS, half DE (maybe the term is Analytics Engineer?). I would like to work with big data techs like Kafka, Spark, and many more in the long term. **The main question is**: which of the two is more coherent with my perspectives? The first one is more generic, and programming skills are always helpful. The second one is the most consistent, but I'm afraid I will ""fossilize"" my career opportunities by specializing in Oracle. 

Any advice/opinion is welcome!",True,False,False,dataengineering,t5_36en4,45111,public,self,Oracle Consultant,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qwn30j/oracle_consultant/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,unsupervised_cluster,,,[],,,,text,t2_d630taql,False,False,False,[],False,False,1637231821,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qwn0ey/etl_vs_elt_broken_down_in_generations/,{},qwn0ey,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/qwn0ey/etl_vs_elt_broken_down_in_generations/,False,self,"{'enabled': False, 'images': [{'id': 'QqSY3F9i2BgB-OdT_JpQr1vBqr2oq4spYNzkghHXwCM', 'resolutions': [], 'source': {'height': 64, 'url': 'https://external-preview.redd.it/5ZI7oL3JTPPt59G0vTfOaQMHvka17QCAdFnF87leUeA.jpg?auto=webp&amp;s=52cc36e047bdca039326e84d3b7ce7aabaf12be6', 'width': 64}, 'variants': {}}]}",6,1637231832,1,"ETL Tools have been around for a while, but they have considerably evolved in the past few years, as part of their efforts to keep up with the development of data infrastructures.

We distinguish three generations of ETL/ELT tools:

**1st generation**: Standard ETL (Extract-transform-Load) tools. They follow processes dictated by stringent storage, bandwidth, and computation constraints that characterized the 1990's.

**2nd generation**: ELT (Extract-Load-Transform) processes result from the arrival of cloud data warehouses and the lifting of storage and bandwidth constraints.

**3rd generation:** Third-generation ETL tools provide a greater number of connectors thanks to their ability to standardize connectors and leverage their community.

What are your thoughts?

Here's a great analysis of the evolution of ETL / ELT. It says we're entering a new ETL 3.0 era, with Airbyte's [\#opensource](https://www.linkedin.com/feed/hashtag/?keywords=opensource&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6866487535854583808) approach leading the charge. Link to [article](https://www.castordoc.com/blog/etl-benchmark-for-mid-market-companies)",True,False,False,dataengineering,t5_36en4,45111,public,self,ETL vs EL-T broken down in generations,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qwn0ey/etl_vs_elt_broken_down_in_generations/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,crazyb14,,,[],,,,text,t2_wjclo,False,False,False,[],False,False,1637231544,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qwmy5b/how_to_calculate_excel_formulas_efficiently/,{},qwmy5b,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,4,0,False,all_ads,/r/dataengineering/comments/qwmy5b/how_to_calculate_excel_formulas_efficiently/,False,,,6,1637231560,1,"We are using Libreoffice's soffice headless to calculate excel formulas but it is not scalable.


Is there a better way to do this? We use Python, AWS.


Thank you.",False,False,False,dataengineering,t5_36en4,45111,public,self,How to calculate excel formulas efficiently,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qwmy5b/how_to_calculate_excel_formulas_efficiently/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,gogoladzetedo,,,[],,,,text,t2_3usdjt0x,False,False,False,[],False,False,1637230757,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qwmroi/gcp_professional_data_engineer_certification/,{},qwmroi,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,3,0,False,all_ads,/r/dataengineering/comments/qwmroi/gcp_professional_data_engineer_certification/,False,,,6,1637230768,1,"Hi DE community,

I'm looking for the materials for the GCP Professional Data Engineer exam preparation, has anybody gone through this process, does anybody have the suggestions?

So far what I've found, is the course by Janani Ravi, which is quite outdated, and Google's official course track on Coursera. Both are very broad in my opinion, and explain the basic topics as well - I'm looking for something that Azure DE / AWS Certified Architect would need, the course that is focused solely on the Data Engineering aspects of GCP without explaining the basics of cloud computing and architecture. 

Thanks!",True,False,False,dataengineering,t5_36en4,45111,public,self,GCP Professional Data Engineer certification course materials,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qwmroi/gcp_professional_data_engineer_certification/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,morningmotherlover,,,[],,,,text,t2_12neis,False,False,False,[],False,False,1637226519,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qwlu1x/anyone_else_attending_sap_teched/,{},qwlu1x,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/qwlu1x/anyone_else_attending_sap_teched/,False,,,6,1637226530,1,I am so underwhelmed with what they've shown. I felt like it was roughly a 2016 random big data products showcase. Anyone else got the same feeling?,True,False,False,dataengineering,t5_36en4,45110,public,self,Anyone else attending SAP TechEd?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qwlu1x/anyone_else_attending_sap_teched/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,BoiElroy,,,[],,,,text,t2_r8dyi,False,False,True,[],False,False,1637219231,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qwk6c1/what_type_of_testing_or_validation_do_you_do_with/,{},qwk6c1,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/qwk6c1/what_type_of_testing_or_validation_do_you_do_with/,False,,,6,1637219242,1,"Just want to hear some thoughts from the community as to what you use to test your data pipelines. 

I'm familiar with great expectations, but curious if there were any best practices around testing for pipeline code? Does anyone do extensive unit testing? If so what kind of tests do you run?",True,False,False,dataengineering,t5_36en4,45102,public,self,What type of testing or validation do you do with data pipelines?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qwk6c1/what_type_of_testing_or_validation_do_you_do_with/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,LSTMeow,,,[],,,,text,t2_1asxtsqs,False,False,False,[],False,False,1637216294,i.redd.it,https://www.reddit.com/r/dataengineering/comments/qwjgcj/hi_rdataengineering_your_memes_are_pretty_good_i/,{},qwjgcj,False,True,False,False,True,True,False,False,#ff66ac,[],dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1,Meme,light,text,False,False,True,16,0,False,all_ads,/r/dataengineering/comments/qwjgcj/hi_rdataengineering_your_memes_are_pretty_good_i/,False,image,"{'enabled': True, 'images': [{'id': 'GRN1-s4k_JwAtVYayhNKfAOFDvYtDhkKPPDqpGdQbl0', 'resolutions': [{'height': 80, 'url': 'https://preview.redd.it/oegs7773ra081.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=efc2e46d1ce562a571c05616de9766b7e4805aa5', 'width': 108}, {'height': 161, 'url': 'https://preview.redd.it/oegs7773ra081.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d0c2fdf783f17a0bc0ccf460ca79c1e68b557376', 'width': 216}, {'height': 238, 'url': 'https://preview.redd.it/oegs7773ra081.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4ead1a4bf4b639f4c4f8f6a8507ec6e08613e2a2', 'width': 320}, {'height': 477, 'url': 'https://preview.redd.it/oegs7773ra081.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=587a9b61851a18e98ac7262bb9d0583f04bba508', 'width': 640}, {'height': 716, 'url': 'https://preview.redd.it/oegs7773ra081.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d3565b0219f8d84f067c07ee451a7a013ee465b2', 'width': 960}, {'height': 806, 'url': 'https://preview.redd.it/oegs7773ra081.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e7c7dd68f1d689b291512539caf4508584a77c99', 'width': 1080}], 'source': {'height': 896, 'url': 'https://preview.redd.it/oegs7773ra081.jpg?auto=webp&amp;s=689808b7d6ee898a415f18133a7ca8e7d4bf5ad3', 'width': 1200}, 'variants': {}}]}",6,1637216313,1,,True,False,False,dataengineering,t5_36en4,45103,public,https://b.thumbs.redditmedia.com/f-dZirvNm3ZpEh-szsyqijMUhtvGzkcbZAmglUBwoVM.jpg,"Hi r/dataengineering, Your memes are pretty good. I want to give back to your community so here's an MLOps meme involving DEs.",0,[],1.0,https://i.redd.it/oegs7773ra081.jpg,all_ads,6,,,,,,104.0,140.0,https://i.redd.it/oegs7773ra081.jpg,,,,,,,,,,
[],False,dragonachu117,,,[],,,,text,t2_a7vbf8fe,False,False,False,[],False,False,1637214101,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qwivls/data_ingestion_best_practises/,{},qwivls,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,20,0,False,all_ads,/r/dataengineering/comments/qwivls/data_ingestion_best_practises/,False,,,6,1637214112,1,"Hello All, 

I am very new to data ingestion. I have to ingest data from various sources like SAP, salesforce etc using azure data factory. 

Can someone let me know or comment links about how should I build an ingestion framework that can be reused across the whole enterprise? Your comments/experiences are also welcome.",True,False,False,dataengineering,t5_36en4,45104,public,self,Data Ingestion best practises,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qwivls/data_ingestion_best_practises/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,dontmakemeplaypool,,,[],,,,text,t2_ssh4888,False,False,False,[],False,False,1637213487,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qwipnr/bridging_the_gap_dba_to_de/,{},qwipnr,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/qwipnr/bridging_the_gap_dba_to_de/,False,,,6,1637213499,1,"So, I've been a DBA for the last 6+ years, split pretty evenly between Oracle and SQL Server (current role is in SQL Server, and I prefer it to Oracle as a career path, but I have fiddled with Postgres, MySQL/MariaDB, \~6 months with Hadoop years ago, etc). My current employer is invested in Azure, but it's still pretty new overall.

I'd like to start learning more about and gaining skills in the DE field, and it is tough to decide where to begin.

I've done Python a decent bit for API and data collection tasks, and lots of PowerShell in my current role for gathering data for monitoring/tuning purposes, but I figure python is probably a bit more in the DE side?

Any tips on how to organize my learning so that I don't feel like I'm just randomly jumping between topics?

Thanks!",True,False,False,dataengineering,t5_36en4,45104,public,self,Bridging the gap - DBA to DE,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qwipnr/bridging_the_gap_dba_to_de/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,SeattleDataGuy,,,[],,,,text,t2_b003dzgv,False,False,False,[],False,False,1637208832,youtube.com,https://www.reddit.com/r/dataengineering/comments/qwhdyw/how_i_got_my_first_data_engineering_job/,{},qwhdyw,False,True,False,False,False,True,False,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/qwhdyw/how_i_got_my_first_data_engineering_job/,False,rich:video,"{'enabled': False, 'images': [{'id': 'FQHKa5pqaX13aHSze5sGpbIY8hZ8w4x-4xFs7fnBpSw', 'resolutions': [{'height': 81, 'url': 'https://external-preview.redd.it/O2JKWDUESCPTyh5kz5uUbld0Dz49_brYcOkvkDrhymk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6cf74fd9d3aea2d5222b499e54a89017230d90c4', 'width': 108}, {'height': 162, 'url': 'https://external-preview.redd.it/O2JKWDUESCPTyh5kz5uUbld0Dz49_brYcOkvkDrhymk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=46f94261c1626f197cf07cdf9098f2078aeb6264', 'width': 216}, {'height': 240, 'url': 'https://external-preview.redd.it/O2JKWDUESCPTyh5kz5uUbld0Dz49_brYcOkvkDrhymk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4cc02e2b366c88f1fdea1bf771b2c9c9615e7ddb', 'width': 320}], 'source': {'height': 360, 'url': 'https://external-preview.redd.it/O2JKWDUESCPTyh5kz5uUbld0Dz49_brYcOkvkDrhymk.jpg?auto=webp&amp;s=efbe47d79902d73f09c42d5d4f42f805e7c90ca2', 'width': 480}, 'variants': {}}]}",6,1637208843,1,,True,False,False,dataengineering,t5_36en4,45103,public,https://b.thumbs.redditmedia.com/FsE1rQ0h3aOy8zmoD960rhGXNAv36NDDEt-jhiRUAEI.jpg,How I Got My First Data Engineering Job,0,[],1.0,https://www.youtube.com/watch?v=FgPKHzXRhcE,all_ads,6,"{'oembed': {'author_name': 'Seattle Data Guy', 'author_url': 'https://www.youtube.com/c/SeattleDataGuy', 'height': 200, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/FgPKHzXRhcE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/FgPKHzXRhcE/hqdefault.jpg', 'thumbnail_width': 480, 'title': 'How I Got My First Data Engineering Job', 'type': 'video', 'version': '1.0', 'width': 356}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/FgPKHzXRhcE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'scrolling': False, 'width': 356}",,"{'oembed': {'author_name': 'Seattle Data Guy', 'author_url': 'https://www.youtube.com/c/SeattleDataGuy', 'height': 200, 'html': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/FgPKHzXRhcE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_url': 'https://i.ytimg.com/vi/FgPKHzXRhcE/hqdefault.jpg', 'thumbnail_width': 480, 'title': 'How I Got My First Data Engineering Job', 'type': 'video', 'version': '1.0', 'width': 356}, 'type': 'youtube.com'}","{'content': '&lt;iframe width=""356"" height=""200"" src=""https://www.youtube.com/embed/FgPKHzXRhcE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'height': 200, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/qwhdyw', 'scrolling': False, 'width': 356}",105.0,140.0,https://www.youtube.com/watch?v=FgPKHzXRhcE,,,,,,,,,,
[],False,badraptor73,,,[],,,,text,t2_13bczj,False,False,False,[],False,False,1637199049,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qwebc5/looking_for_kubeflow_tutorials/,{},qwebc5,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/qwebc5/looking_for_kubeflow_tutorials/,False,,,6,1637199059,1,Does any one have any suggestions on good Kubeflow tutorials?,True,False,False,dataengineering,t5_36en4,45090,public,self,Looking for Kubeflow tutorials,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qwebc5/looking_for_kubeflow_tutorials/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,OSSkubopanda,,,[],,,,text,t2_gnbvoh13,False,False,False,[],False,False,1637196586,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qwdizk/prestocon_december_9th_free_virtual_conference/,{},qwdizk,False,False,False,False,False,False,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/qwdizk/prestocon_december_9th_free_virtual_conference/,False,,,6,1637196597,1,[removed],True,False,False,dataengineering,t5_36en4,45089,public,https://b.thumbs.redditmedia.com/q0yoy0UHRmAaYTf7Qui42UCD6eYSJg4poMkUjcjfeak.jpg,"PrestoCon: December 9th! Free Virtual Conference about Presto, the popular open source software",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qwdizk/prestocon_december_9th_free_virtual_conference/,all_ads,6,,,automod_filtered,,,78.0,140.0,,,,,,,,,,,
[],False,nobel-001,,,[],,,,text,t2_a3lkw6g4,False,False,False,[],False,False,1637190637,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qwbjmq/oreilly_learning/,{},qwbjmq,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,4,0,False,all_ads,/r/dataengineering/comments/qwbjmq/oreilly_learning/,False,,,6,1637190648,1,"Does Oreilly Learning subscription worth it?
Is it good learning investment for data engineers?

https://learning.oreilly.com",True,False,False,dataengineering,t5_36en4,45085,public,self,Oreilly Learning,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qwbjmq/oreilly_learning/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,OlderWhiskey,,,[],,,,text,t2_3co3592k,False,False,False,[],False,False,1637185201,i.redd.it,https://www.reddit.com/r/dataengineering/comments/qw9oec/airflow_be_like/,{},qw9oec,False,True,False,False,True,True,False,False,#ff66ac,[],dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1,Meme,light,text,False,False,False,10,0,False,all_ads,/r/dataengineering/comments/qw9oec/airflow_be_like/,False,image,"{'enabled': True, 'images': [{'id': 'vLr_525l8qNtgbv67Et0EYWLEImc7zsQ8VadsKJ_LmE', 'resolutions': [{'height': 58, 'url': 'https://preview.redd.it/f1dyok3n68081.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0419f4f85570b240b42f239bda6808c2439e3eab', 'width': 108}, {'height': 117, 'url': 'https://preview.redd.it/f1dyok3n68081.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f8421859b2808b26752948175a43f69d7bee459c', 'width': 216}, {'height': 174, 'url': 'https://preview.redd.it/f1dyok3n68081.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8588f89983fdeab8639f283bd0b25659ecd88162', 'width': 320}, {'height': 348, 'url': 'https://preview.redd.it/f1dyok3n68081.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9c1e4ab9d5e3e9308837441bb6e2df9e2a1b2435', 'width': 640}], 'source': {'height': 500, 'url': 'https://preview.redd.it/f1dyok3n68081.jpg?auto=webp&amp;s=de63ba03af9ff1e9149fa051d3737474129f8452', 'width': 918}, 'variants': {}}]}",6,1637185212,1,,True,False,False,dataengineering,t5_36en4,45079,public,https://b.thumbs.redditmedia.com/WFrZqb0RNJ5tujAhyeIklzUL3iIqy8EbpIF4GtPy6IU.jpg,Airflow be like…,0,[],1.0,https://i.redd.it/f1dyok3n68081.jpg,all_ads,6,,,,,,76.0,140.0,https://i.redd.it/f1dyok3n68081.jpg,,,,,,,,,,
[],False,Illustrious_Ad4259,,,[],,,,text,t2_9px155tg,False,False,False,[],False,False,1637182559,linkedin.com,https://www.reddit.com/r/dataengineering/comments/qw8ry0/i_have_had_lot_of_people_asking_me_for_advice_on/,{},qw8ry0,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/qw8ry0/i_have_had_lot_of_people_asking_me_for_advice_on/,False,link,"{'enabled': False, 'images': [{'id': 'XHvN_PSINm2OJpI96TJWrKT7f2gD79Cazp2jnsmfZpQ', 'resolutions': [{'height': 60, 'url': 'https://external-preview.redd.it/GS97m0Pmb4UjY9-JH4COttwACVl9jNBjgnoXOyMqGEA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bdab0735ef0c29edd5433d7033eac4cdf9e31841', 'width': 108}, {'height': 121, 'url': 'https://external-preview.redd.it/GS97m0Pmb4UjY9-JH4COttwACVl9jNBjgnoXOyMqGEA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7eb7623ee8973823c09c49832fa3cc193b0dcb8a', 'width': 216}, {'height': 180, 'url': 'https://external-preview.redd.it/GS97m0Pmb4UjY9-JH4COttwACVl9jNBjgnoXOyMqGEA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c441ef369f355d7a1d37490aa9acda5814c82ea9', 'width': 320}, {'height': 360, 'url': 'https://external-preview.redd.it/GS97m0Pmb4UjY9-JH4COttwACVl9jNBjgnoXOyMqGEA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=180c91f97b9900e800d5f23a2dd6003dea98992e', 'width': 640}, {'height': 540, 'url': 'https://external-preview.redd.it/GS97m0Pmb4UjY9-JH4COttwACVl9jNBjgnoXOyMqGEA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=83b798bc6e6b579619ed46ec64900d8e43c59649', 'width': 960}, {'height': 607, 'url': 'https://external-preview.redd.it/GS97m0Pmb4UjY9-JH4COttwACVl9jNBjgnoXOyMqGEA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dd94cb89de8f3df6c1282888e682870cad6419d8', 'width': 1080}], 'source': {'height': 720, 'url': 'https://external-preview.redd.it/GS97m0Pmb4UjY9-JH4COttwACVl9jNBjgnoXOyMqGEA.jpg?auto=webp&amp;s=1e5bf2a6b01a8ba7de3ffe213192b4a5384dd95e', 'width': 1280}, 'variants': {}}]}",6,1637182570,1,,True,False,False,dataengineering,t5_36en4,45078,public,https://b.thumbs.redditmedia.com/LZhZzPe1DJV2wVe-CEDwdktWzqX8FHza_7AJ5RnPCIE.jpg,I have had lot of people asking me for advice on passing GCP data engineering certification. Hope this blog helps the community and for those wanting a solid DE certification under their belt.,0,[],1.0,https://www.linkedin.com/pulse/google-cloud-platform-professional-data-engineer-roadmap-nakul-gowdra,all_ads,6,,,,,,78.0,140.0,https://www.linkedin.com/pulse/google-cloud-platform-professional-data-engineer-roadmap-nakul-gowdra,,,,,,,,,,
[],False,chmod-rwxrwx,,,[],,,,text,t2_fayfgi7s,False,False,False,[],False,False,1637178902,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qw7hc4/where_can_i_learn_snowflake/,{},qw7hc4,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/qw7hc4/where_can_i_learn_snowflake/,False,,,6,1637178913,1,The text,True,False,False,dataengineering,t5_36en4,45073,public,self,Where can I learn Snowflake?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qw7hc4/where_can_i_learn_snowflake/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Gavt13,,,[],,,,text,t2_p31md,False,False,False,[],False,False,1637177610,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qw713k/best_practices_for_mongodb_key_value_arrays/,{},qw713k,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,5,0,False,all_ads,/r/dataengineering/comments/qw713k/best_practices_for_mongodb_key_value_arrays/,False,,,6,1637177621,1,"Currently moving mongoDB dumps from Google cloud storage to BigQuery as part of an ELT architecture. Python API is super and defining the schema is really easy.

Problem I have is some collections have nested key value arrays e.g.:

{person: {axy: {id: 1}, bty:{id: 2}}}

bigQuery can't handle this as the keys to access the people are random IDs so don't conform to a schema. 

I can convert it to a list and move the IDs into the nested object but some of these files are 500+ GBs so the transformation step takes quite a bit of time to stream transform and then write.

Any suggestions appreciated",True,False,False,dataengineering,t5_36en4,45072,public,self,Best practices for mongoDB key value arrays,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qw713k/best_practices_for_mongodb_key_value_arrays/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,noNSFWcontent,,,[],,,,text,t2_yagno,False,False,False,[],False,False,1637176100,i.imgur.com,https://www.reddit.com/r/dataengineering/comments/qw6htq/after_exactly_one_month_and_three_days_of/,{},qw6htq,False,True,False,False,False,True,False,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,False,37,0,False,all_ads,/r/dataengineering/comments/qw6htq/after_exactly_one_month_and_three_days_of/,False,image,"{'enabled': True, 'images': [{'id': 'cDM77NrlT1yFVYYjJ04MamlEwP2Wc1-_gNHuwhGLVfU', 'resolutions': [{'height': 108, 'url': 'https://external-preview.redd.it/6UBsXRCrDQE8WuMw2EJqeom5wkULr-wqMqPbq-MSELo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2be144990f4b6729f643ad507a9ed745885759aa', 'width': 108}, {'height': 216, 'url': 'https://external-preview.redd.it/6UBsXRCrDQE8WuMw2EJqeom5wkULr-wqMqPbq-MSELo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c951a986638fa849cb66426fff6f1872b1526832', 'width': 216}, {'height': 320, 'url': 'https://external-preview.redd.it/6UBsXRCrDQE8WuMw2EJqeom5wkULr-wqMqPbq-MSELo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=25de2e8a4b7bb480ce235e7bd74616158fc11c19', 'width': 320}], 'source': {'height': 500, 'url': 'https://external-preview.redd.it/6UBsXRCrDQE8WuMw2EJqeom5wkULr-wqMqPbq-MSELo.jpg?auto=webp&amp;s=6263f120a50e2aacfbb84028f81c9615d0c7ffbd', 'width': 500}, 'variants': {}}]}",6,1637176111,1,,True,False,False,dataengineering,t5_36en4,45069,public,https://b.thumbs.redditmedia.com/3wmIdL7DcBg5kUP0TivF8pfprlWUsSGl2SblYmCr-gk.jpg,"After exactly one month and three days of starting my Azure Data Engineer preparation, I have good news!",0,[],1.0,https://i.imgur.com/CsLFR9q.jpg,all_ads,6,,,,,,140.0,140.0,https://i.imgur.com/CsLFR9q.jpg,,,,,,,,,,
[],False,BerghainInMyVeins,,,[],,,,text,t2_91oo85c8,False,False,False,[],False,False,1637175585,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qw6avd/when_should_i_use_a_beamspark_over_sql_and_vice/,{},qw6avd,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/qw6avd/when_should_i_use_a_beamspark_over_sql_and_vice/,False,,,6,1637175596,1,"I am loading some data every morning from CSV into a big query data warehouse. The data comes in daily and needs to be inserted as new rows into the warehouse. The schema is always the same. 

-I load the csv into cloud storage. 
-I delete the rows that contain null values with a filter.  
-I transform a string: eg: “John Doe, Ron Swanson, Tim Turner” into a list: “John Doe”, “Ron Swanson”, “Tim Turner” 

I’m wondering if I should use Apache beam for this with an etl style approach , or if I could do an elt style approach with sql, or an etl style with sql. 

For the experienced DE’s in here:

How would you go about this? Do you do most of your transforms in sql? When should I use beam over sql? Or sql over beam? 

Thank you.",True,False,False,dataengineering,t5_36en4,45069,public,self,When should I use a beam/spark over SQL? And vice versa for transforming data,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qw6avd/when_should_i_use_a_beamspark_over_sql_and_vice/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Guitar_player87,,,[],,,,text,t2_6axss,False,False,False,[],False,False,1637174379,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qw5vfw/dbt_cloud_any_way_to_obtain_updated_manifestjson/,{},qw5vfw,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/qw5vfw/dbt_cloud_any_way_to_obtain_updated_manifestjson/,False,,,6,1637174390,1,Is there any way to obtain an updated manifest.json file directly from dbt-cloud after a project has been run?,True,False,False,dataengineering,t5_36en4,45068,public,self,dbt Cloud - Any way to obtain updated manifest.json file?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qw5vfw/dbt_cloud_any_way_to_obtain_updated_manifestjson/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,youderkB,,,[],,,,text,t2_2ishbuf,False,False,False,[],False,False,1637172959,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qw5d82/prefect_and_powershell_windows_shell/,{},qw5d82,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/qw5d82/prefect_and_powershell_windows_shell/,False,self,"{'enabled': False, 'images': [{'id': '9EM8wAXFXQ1RPVhHsHnLMtyeX18Dkfj1rHUIpoyZGkE', 'resolutions': [{'height': 54, 'url': 'https://external-preview.redd.it/hfPlQWSqgsxXhoJYk926ShXV05Jds3XYxr8mIEsydeY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ab3c54703e9ee7c4cc42eb6f7fc202db99b973fe', 'width': 108}, {'height': 108, 'url': 'https://external-preview.redd.it/hfPlQWSqgsxXhoJYk926ShXV05Jds3XYxr8mIEsydeY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bb4d632f509c178c3015a58502f57d4ffc9fa51f', 'width': 216}, {'height': 160, 'url': 'https://external-preview.redd.it/hfPlQWSqgsxXhoJYk926ShXV05Jds3XYxr8mIEsydeY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=41aa4050419371244c685bc0dd215d56296c79b3', 'width': 320}, {'height': 320, 'url': 'https://external-preview.redd.it/hfPlQWSqgsxXhoJYk926ShXV05Jds3XYxr8mIEsydeY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2113d3f7ae51d4ae2ed7ef60626cc3262f60b5b8', 'width': 640}, {'height': 480, 'url': 'https://external-preview.redd.it/hfPlQWSqgsxXhoJYk926ShXV05Jds3XYxr8mIEsydeY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b42cc91d86581cf54c5bddb60bf53fd1362abf54', 'width': 960}, {'height': 540, 'url': 'https://external-preview.redd.it/hfPlQWSqgsxXhoJYk926ShXV05Jds3XYxr8mIEsydeY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=23725be599dcbe28530289251ed49965df401b8e', 'width': 1080}], 'source': {'height': 600, 'url': 'https://external-preview.redd.it/hfPlQWSqgsxXhoJYk926ShXV05Jds3XYxr8mIEsydeY.jpg?auto=webp&amp;s=59c6553bb6ea3f3cb91a1a07b8768b09e8b79322', 'width': 1200}, 'variants': {}}]}",6,1637172970,1,"Hi,

has someone worked with prefect and used s task to execute a Powershell script?
I could only find a GitHub post suggesting that prefect cannot run powershell scripts.
https://github.com/PrefectHQ/prefect/issues/3375",True,False,False,dataengineering,t5_36en4,45068,public,self,Prefect and Powershell / Windows shell,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qw5d82/prefect_and_powershell_windows_shell/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Gawgba,,,[],,,,text,t2_b1494,False,False,False,[],False,False,1637171990,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qw4zun/data_sciencearchitectengineer/,{},qw4zun,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,4,0,False,all_ads,/r/dataengineering/comments/qw4zun/data_sciencearchitectengineer/,False,,,6,1637172001,1,"I've reviewed a couple variations on this topic but my question is less about what these roles do (as I think I have a reasonable understanding of job responsibilities) and more about which would provide the best compensation/career path for a mid-career transition from DBA.

Basically I've been a multi-platform DBA (but mostly Oracle) for 15 years, and I'm being given the opportunity to transition to a new (but adjacent) career path, so I'm wondering as we go into 2022 the advantages/disadvantages of these (or other data paths I havent listed)...

In addition to my DBA role I have some familiarity with python, data visualization (Tableau/SSRS), ETLs (SSIS/Boomi), and Linux admin in case these are relevant.",True,False,False,dataengineering,t5_36en4,45067,public,self,Data (Science/Architect/Engineer/?),0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qw4zun/data_sciencearchitectengineer/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,happysunshinekidd,,,[],,,,text,t2_dltif81,False,False,False,[],False,False,1637171573,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qw4uef/resources_for_going_from_de_to_the_only_de/,{},qw4uef,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,19,0,False,all_ads,/r/dataengineering/comments/qw4uef/resources_for_going_from_de_to_the_only_de/,False,,,6,1637171584,1,"Hey all,

I've been working as a DE for about 3-4 years, and I'm pretty confident with all the ""core skills"". I can ETL stuff, message stuff, log stuff, horizontally scale stuff, etc.

I recently accepted a role where I'm going to be the main DE for a fresh startup. Super excited about the role.

However, I was wondering if you had any resources on more... architecting good data systems instead of ""how to do xyz"". I'm comfortable helping them scale up to some degree, but I was self-taught and I feel a gap in the higher-level thinking of systems design and architecture.

Bonus points if you have MLOps/DE references as well, as that'll be part of it. But the main crux is just -- how to think of a current business problem, a current company, and see 3 years into the future?",True,False,False,dataengineering,t5_36en4,45066,public,self,Resources for going from DE to the only DE,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qw4uef/resources_for_going_from_de_to_the_only_de/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,123duck123,,,[],,,,text,t2_ag2bno,False,False,True,[],False,False,1637170058,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qw4aky/hey_data_engineers_were_opening_our_managed/,{},qw4aky,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,4,0,False,all_ads,/r/dataengineering/comments/qw4aky/hey_data_engineers_were_opening_our_managed/,False,,,6,1637170070,1,"We are pleased to announce our next wave of invites. The limited number of invites are now up for grabs. If you are developing on ClickHouse or Kafka and want a cloud-agnostic managed service request an invite to the all-new [https://Double.Cloud](https://double.cloud/) today.

**DoubleCloud** is a new company incorporated in the USA and the EU and part of the Yandex company group. We are the creators of the first managed ClickHouse service back in 2018 which is currently managing more than 1000 clusters in the production of ClickHouse and Kafka for the more than 10th thousand customers with hundreds of PetaBytes of total volume user’s data.

Request invite here: [https://double.cloud](https://double.cloud/)

&amp;#x200B;

https://i.redd.it/1b9egb4bx6081.gif",True,False,False,dataengineering,t5_36en4,45064,public,https://b.thumbs.redditmedia.com/8Oh9hi7-G522QMHXy4Y_qSFfSTrYqiXwjWhcfSGPj7I.jpg,"Hey Data Engineers, we're opening our Managed ClickHouse service for free preview and looking for early adopters",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qw4aky/hey_data_engineers_were_opening_our_managed/,all_ads,6,,,,,,78.0,140.0,,"{'1b9egb4bx6081': {'e': 'AnimatedImage', 'id': '1b9egb4bx6081', 'm': 'image/gif', 'p': [{'u': 'https://preview.redd.it/1b9egb4bx6081.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=686ab6bc5ac5dfbce9972cc25fb97e626a6d9fdf', 'x': 108, 'y': 60}, {'u': 'https://preview.redd.it/1b9egb4bx6081.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=f0ae05a84b285a3b369d7b0d662950db09760f3d', 'x': 216, 'y': 121}, {'u': 'https://preview.redd.it/1b9egb4bx6081.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=34be5a429da7f0085bfb74dd2cfea70282ec4a47', 'x': 320, 'y': 179}, {'u': 'https://preview.redd.it/1b9egb4bx6081.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=5fe0dc18e7f4b24ebcadad9773de6f34774ef5e4', 'x': 640, 'y': 359}, {'u': 'https://preview.redd.it/1b9egb4bx6081.gif?width=960&amp;crop=smart&amp;format=png8&amp;s=d156f343e3f2b017fea00dbfaf3726f2cb256c82', 'x': 960, 'y': 538}, {'u': 'https://preview.redd.it/1b9egb4bx6081.gif?width=1080&amp;crop=smart&amp;format=png8&amp;s=2342e6914f3581cb85b9079a06ffcc124d8c7ed8', 'x': 1080, 'y': 605}], 's': {'gif': 'https://i.redd.it/1b9egb4bx6081.gif', 'mp4': 'https://preview.redd.it/1b9egb4bx6081.gif?format=mp4&amp;s=37c0e3314adfc4abd1b64b27e4fa0e4d9df76e5a', 'x': 1854, 'y': 1040}, 'status': 'valid'}}",,,True,,,,,,
[],False,gblawal,,,[],,,,text,t2_11pi7q,False,False,False,[],False,False,1637169678,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qw45nf/how_to_integrate_a_fortran_technical_analysis/,{},qw45nf,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,2,0,False,all_ads,/r/dataengineering/comments/qw45nf/how_to_integrate_a_fortran_technical_analysis/,False,,,6,1637169706,1,"So I have been building a few trading bots for a while. Not super complex, but I get the gist of it. All of them have been in Python. I'd ingest both streaming and historical stock data from like an Alpaca or Polygon and do some transformations with pandas and some very basic technical analysis with Python libraries like ta and bta-lib then push trade signals with some combination of AWS lambda and/or TradingView and so on. Up until now, I have managed to do this almost completely in Python (maybe some JS here and there, but nothing significant).

Now, recently, I have been thinking about partnering with a guy who is absolutely great at data science and analysis. He is an older well seasoned guy who only operates in Fortran as his background was engineering. We are now trying to see how we can work with each other. He has worked with both large data sets and streamed sensors data. However, he has no knowledge of internet protocols or these fancy APIs or how to ultimately implement his analysis into actual order execution. We are in the process of exploring if and how we can work together.

So as we are currently thinking through it. I would be setting up the ingest pipeline and the storage infrastructure. He would be doing the technical analysis. I don't know if Fortran has some good http libraries for actual trade signaling and order execution, so I am guessing I would be taking care of that part as well. Also, I haven't worked with compiled languages in my pipelines.

So my ask is advice on how to set this up.

1. How can I have the Fortran technical analysis part communicate with both the ingest (especially streaming data) and the signaling and trade execution parts?
2. What sort of things and libraries should I be looking out for?
3. Would introducing microservices into my otherwise monolothic architecture help?
4. Or does pub-sub work better? Doing pub-sub with Fortran, is that a thing?
5. I couldn't find any documentation on working Fortran with AWS lambda, or exposing endpoints with Fortran, so does this mean I would have to introduce another intermediary store for generated signals and then work with those with Python?

Honestly, any criticism or advice would be greatly appreciated. I know I could try to learn Fortran, but I am trying to avoid that if possible. It takes a really long time to know the ins and outs of a language.

So even just a high level description like ""ingest with this, store like that, expose with this, analyze with that, push with this, execute with that"" would help greatly. I really want to understand how to work with different languages, especially compiled, at different parts of a pipeline. So I am guessing this question can also be applied to C++, GoLang or Java.

Thank you for taking time to read this.",True,False,False,dataengineering,t5_36en4,45064,public,self,How to integrate a Fortran technical analysis stage (or any other compiled language) into a Python pipeline for automated stock trading,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qw45nf/how_to_integrate_a_fortran_technical_analysis/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Gregeal,,,[],,,,text,t2_3n3b0bzh,False,False,False,[],False,False,1637169666,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qw45hj/need_an_etl_developerdata_engineer_mentor/,{},qw45hj,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/qw45hj/need_an_etl_developerdata_engineer_mentor/,False,,,6,1637169696,1,"Please, I would like to connect with a mentor to help me in my ETL/Data engineering journey. 
Please reach out to me as I applied for a role that I really like. I currently work as a BI developer and looking to transition to data engineering. 
Thank you.",True,False,False,dataengineering,t5_36en4,45064,public,self,Need an ETL developer/Data engineer mentor,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qw45hj/need_an_etl_developerdata_engineer_mentor/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,secodaHQ,,,[],,,,text,t2_aiinah9q,False,False,False,[],False,False,1637164041,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qw24b4/analytics_teams_need_more_than_traditional/,{},qw24b4,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/qw24b4/analytics_teams_need_more_than_traditional/,False,,,6,1637164052,1,"Hey folks. Documenting processes allows business teams to scale. Documenting code allows software engineering teams to scale. Similarly, documenting data enables the same scale for analytics teams.

But how? 🤔

Confluence doesn't support documenting technical resources, while software documentation best practices don't support procedural and business documentation easily, let alone documenting data.

So, what now? 🙋‍♀️

In our recent blog post written by [Sarah Krasnik](https://www.linkedin.com/in/ACoAABmJPzgBEV2mi4z6I1pGBXyRvY76Tr4P5mk), you'll learn why analytics teams need more than traditional documentation. You can find the full post here: [https://www.secoda.co/blog/analytics-teams-need-more-than-traditional-documentation](https://www.secoda.co/blog/analytics-teams-need-more-than-traditional-documentation)",True,False,False,dataengineering,t5_36en4,45057,public,self,Analytics Teams Need More Than Traditional Documentation,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qw24b4/analytics_teams_need_more_than_traditional/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,funkyjunkymonky,,,[],,,,text,t2_7ggm9xqe,False,False,False,[],False,False,1637163504,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qw1xk1/treatment_of_thounsands_hundred_of_xml_files/,{},qw1xk1,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,5,0,False,all_ads,/r/dataengineering/comments/qw1xk1/treatment_of_thounsands_hundred_of_xml_files/,False,,,6,1637163514,1,"Hello, I am actually trying to process more than 400k small xml files with pyspark on a dataproc cluster.  
I use databricks to read the file (which are on cloud storage) and create a spark dataframe from all the files.

My main problem is that the masternode is reading one by one the file and it is taking 5 minutes for 1k files which means 30 hours for the whole process. I want to find a way to be more efficient.

Does anyone have a suggestion?

Thank you in advance",True,False,False,dataengineering,t5_36en4,45057,public,self,Treatment of thounsands hundred of xml files,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qw1xk1/treatment_of_thounsands_hundred_of_xml_files/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,novicedataengineer,,,[],,,,text,t2_fsjwfn80,False,False,False,[],False,False,1637162832,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qw1oy8/how_relevant_is_dimensional_modeling_today/,{},qw1oy8,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,15,0,False,all_ads,/r/dataengineering/comments/qw1oy8/how_relevant_is_dimensional_modeling_today/,False,,,6,1637162844,1,"I am currently taking a course on data warehousing and the instructor introduced dimensional modeling. My understanding is that the purpose of a dimensional model is to make it easy to summarize (sum, average, etc.) facts stored in fact tables.  

With storage getting cheaper (more granular data) and columnar data stores being prevelant (faster retrieval and joins), are dimensional models becoming obsolete?",True,False,False,dataengineering,t5_36en4,45057,public,self,How relevant is dimensional modeling today?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qw1oy8/how_relevant_is_dimensional_modeling_today/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,phmark19,,,[],,,,text,t2_33924v0v,False,False,False,[],False,False,1637160313,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qw0sz9/new_to_de_got_invited_for_an_opportunity/,{},qw0sz9,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,7,0,False,all_ads,/r/dataengineering/comments/qw0sz9/new_to_de_got_invited_for_an_opportunity/,False,self,"{'enabled': False, 'images': [{'id': 'zcxtbc2oJ6aU3Oi_qaGrQRE7Yp7xzBH4hoZq-RlMgdA', 'resolutions': [{'height': 108, 'url': 'https://external-preview.redd.it/cmLH5_KhNpwuUXn0_L5DIhSm5epCL070q6Txqz5c5Ro.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a59d442517fe2a7de37b209f6f0f99373690086f', 'width': 108}], 'source': {'height': 192, 'url': 'https://external-preview.redd.it/cmLH5_KhNpwuUXn0_L5DIhSm5epCL070q6Txqz5c5Ro.jpg?auto=webp&amp;s=27cae616f896fab4438f69a43f28967881f190c7', 'width': 192}, 'variants': {}}]}",6,1637160325,1,"I'm from a software engineering background with some exposure on data analysis and ETL light jobs.  
And a recruiter reached out to me with this job, we talked about the position and stuff. Finally I got scheduled to be interviewed by the head of data team. For me it looks like this is a backdoor access in the data engineering world. For those of you who are in the data engineering field, does this job description (please check the link below) sounds like what a data engineer will do?

[https://docs.google.com/document/d/16P1\_35pLSK6venvOyDWi\_yrThxKbZA-pZBxpl\_x2Ty4](https://docs.google.com/document/d/16P1_35pLSK6venvOyDWi_yrThxKbZA-pZBxpl_x2Ty4)

(Adding to the job description above, the recruiter was asking a thing about Kafka and processing millions of rows.)  


Feel free to drop your thoughts, I'll take it into account (will also appreciate if you can quantify from 1-10 just so I have numbers to base on)",True,False,False,dataengineering,t5_36en4,45054,public,self,"New to DE, got invited for an opportunity",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qw0sz9/new_to_de_got_invited_for_an_opportunity/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,twopairisgood,,,[],,,,text,t2_d50wl,False,False,False,[],False,False,1637157831,lakefs.io,https://www.reddit.com/r/dataengineering/comments/qvzyrw/takeaways_from_the_future_of_metadata_aft/,{},qvzyrw,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/qvzyrw/takeaways_from_the_future_of_metadata_aft/,False,link,"{'enabled': False, 'images': [{'id': 'OikqqLrJ7MrDrw6WpvgicGAkFXcKQqBOczMbwYdYoaI', 'resolutions': [{'height': 61, 'url': 'https://external-preview.redd.it/wtKl8kClGCD9FSUwLhKtqhF2w4DRmL0UHekJnGB9ugc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0868a3298059daf6bb2e4f16e9d05cff71e3850a', 'width': 108}, {'height': 123, 'url': 'https://external-preview.redd.it/wtKl8kClGCD9FSUwLhKtqhF2w4DRmL0UHekJnGB9ugc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1a8274db248952c5cf2fcd6cf67e4adb4fa91295', 'width': 216}, {'height': 182, 'url': 'https://external-preview.redd.it/wtKl8kClGCD9FSUwLhKtqhF2w4DRmL0UHekJnGB9ugc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=483ad42fe015d3eedd529fd60a0104663ca38ab9', 'width': 320}, {'height': 364, 'url': 'https://external-preview.redd.it/wtKl8kClGCD9FSUwLhKtqhF2w4DRmL0UHekJnGB9ugc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4db25e694abf1ee2550e91d2acaa5cbf9935d9d7', 'width': 640}, {'height': 547, 'url': 'https://external-preview.redd.it/wtKl8kClGCD9FSUwLhKtqhF2w4DRmL0UHekJnGB9ugc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f1224942e2fa83b33bbf1a3bde14845e53beea69', 'width': 960}, {'height': 615, 'url': 'https://external-preview.redd.it/wtKl8kClGCD9FSUwLhKtqhF2w4DRmL0UHekJnGB9ugc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1b23e471dd6777904f7caa76e714e8a0b49ff20d', 'width': 1080}], 'source': {'height': 1264, 'url': 'https://external-preview.redd.it/wtKl8kClGCD9FSUwLhKtqhF2w4DRmL0UHekJnGB9ugc.jpg?auto=webp&amp;s=ff61c6790e53c9176c3562fa391e7ba2ca737df6', 'width': 2218}, 'variants': {}}]}",6,1637157842,1,,True,False,False,dataengineering,t5_36en4,45047,public,https://b.thumbs.redditmedia.com/rtlsocmhLRRf34JQZjGnFr6AhIMyYKwr8xzrZi9-AZI.jpg,Takeaways From the Future of Metadata Aft,0,[],1.0,https://lakefs.io/a-few-takeaways-from-the-future-of-metadata-after-hive-roundtable/,all_ads,6,,,,,,79.0,140.0,https://lakefs.io/a-few-takeaways-from-the-future-of-metadata-after-hive-roundtable/,,,,,,,,,,
[],False,Minimum-Membership-8,,,[],,,,text,t2_a0qsnkph,False,False,False,[],False,False,1637154322,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qvyunl/the_ideal_data_architecture/,{},qvyunl,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,25,0,False,all_ads,/r/dataengineering/comments/qvyunl/the_ideal_data_architecture/,False,,,6,1637154333,1,"If you could build a real-time data architecture from scratch, what tools would you use?

Context:
Structured data only
Real-time data
Process 1M records per day

Needs:
Data has to be sourced from DB2",True,False,False,dataengineering,t5_36en4,45043,public,self,The ideal data architecture,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qvyunl/the_ideal_data_architecture/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,RealMatchesMalonee,,,[],,,,text,t2_5xcqmvp,False,False,False,[],False,False,1637152495,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qvyboa/unable_to_use_subprocesscheck_output_inside/,{},qvyboa,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/qvyboa/unable_to_use_subprocesscheck_output_inside/,False,,,6,1637152507,1,"Hi. I need to use `gsutil` inside an Airflow DAG, and that has to be done by treating it as a command line program, ie calling via subprocess.check\_output. While I can call gsutil using the terminal and with subprocess, within a python shell, the same script gives an when called through an Airflow DAG.

&amp;#x200B;

pasting error for reference.

    Traceback (most recent call last):
      File ""/home/&lt;user-name&gt;/.local/bin/gsutil"", line 6, in &lt;module&gt;
        from pkg_resources import load_entry_point
      File ""/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 3251, in &lt;module&gt;
        @_call_aside
      File ""/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 3235, in _call_aside
        f(*args, **kwargs)
      File ""/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 3264, in _initialize_master_working_set
        working_set = WorkingSet._build_master()
      File ""/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 583, in _build_master
        ws.require(__requires__)
      File ""/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 900, in require
        needed = self.resolve(parse_requirements(requirements))
      File ""/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 786, in resolve
        raise DistributionNotFound(req, requirers)
    pkg_resources.DistributionNotFound: The 'gsutil==5.4' distribution was not found and is required by the application

Am I missing something? I understand that I have the option of using google-cloud module for python, but the higher-ups insist on using the \`gsutil\` command for this job.",True,False,False,dataengineering,t5_36en4,45042,public,self,Unable to use subprocess.check_output inside Airflow.,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qvyboa/unable_to_use_subprocesscheck_output_inside/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Practical-East1161,,,[],,,,text,t2_ap1ukptp,False,False,False,[],False,False,1637145845,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qvwltz/master_data_management/,{},qvwltz,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,6,0,False,all_ads,/r/dataengineering/comments/qvwltz/master_data_management/,False,,,6,1637145856,1,"Hello,

I want to learn about master data management. Could anyone advise me good resources about it? My current level of understanding is at definition level with no practical experience.",True,False,False,dataengineering,t5_36en4,45037,public,self,Master Data Management,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qvwltz/master_data_management/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Krypton_Rimsdim,,,[],,,,text,t2_4o8jw344,False,False,False,[],False,False,1637144205,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qvw87w/best_resources_to_learn_sql_regex_regular/,{},qvw87w,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,4,0,False,all_ads,/r/dataengineering/comments/qvw87w/best_resources_to_learn_sql_regex_regular/,False,,,6,1637144217,1,"Preparing for internship assessments, figured I should know regex for those pattern matching questions.

If you got any book or resources you can share it'd help a great deal.",True,False,False,dataengineering,t5_36en4,45036,public,self,Best resources to learn SQL Regex Regular Functions and their usages.,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qvw87w/best_resources_to_learn_sql_regex_regular/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,DeadPoetSociety1994,,,[],,,,text,t2_prknqkh,False,False,False,[],False,False,1637143286,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qvw0c5/bigquerys_equivalent_of_snowflake_dynamic_masking/,{},qvw0c5,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,11,0,False,all_ads,/r/dataengineering/comments/qvw0c5/bigquerys_equivalent_of_snowflake_dynamic_masking/,False,,,6,1637143297,1,"Hi all!

I recently moved from Snowflake to BigQuery and I'm looking for a way to mask data which, for example replace phone number with asterisks for unauthorized roles. More about Snowflake's Data Masking [here](https://docs.snowflake.com/en/sql-reference/sql/create-masking-policy.html) .

Thank you",True,False,False,dataengineering,t5_36en4,45037,public,self,Bigquery's Equivalent of Snowflake Dynamic Masking Policy,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qvw0c5/bigquerys_equivalent_of_snowflake_dynamic_masking/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Different_Eggplant97,,,[],,,,text,t2_c1uwm29g,False,False,False,[],False,False,1637141934,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qvvp50/moldy_data_and_dashboards/,{},qvvp50,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/qvvp50/moldy_data_and_dashboards/,False,self,"{'enabled': False, 'images': [{'id': '8x88D1n0qLdm1td3_P9QRYgj7yePyTPXnUnMgIhBPjM', 'resolutions': [{'height': 76, 'url': 'https://external-preview.redd.it/wmDtr1jlyVJMBK3PIyRaZxzJzYki3u8ebaWNtQ8WCTs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f601008ec7fe9b925dc27945709c615c7dc6d228', 'width': 108}, {'height': 153, 'url': 'https://external-preview.redd.it/wmDtr1jlyVJMBK3PIyRaZxzJzYki3u8ebaWNtQ8WCTs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=acdc8e52cd8bf58c683ab3e2ab7eb67bb9b2580f', 'width': 216}, {'height': 227, 'url': 'https://external-preview.redd.it/wmDtr1jlyVJMBK3PIyRaZxzJzYki3u8ebaWNtQ8WCTs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8f6e897958ae733b623e244a3014c9e56270f8ac', 'width': 320}, {'height': 454, 'url': 'https://external-preview.redd.it/wmDtr1jlyVJMBK3PIyRaZxzJzYki3u8ebaWNtQ8WCTs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=44a29999ae8bec33179ffc196412e0bc47d6a062', 'width': 640}, {'height': 682, 'url': 'https://external-preview.redd.it/wmDtr1jlyVJMBK3PIyRaZxzJzYki3u8ebaWNtQ8WCTs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=59134f9941462f194b7923d4e86ddb946a875b57', 'width': 960}, {'height': 767, 'url': 'https://external-preview.redd.it/wmDtr1jlyVJMBK3PIyRaZxzJzYki3u8ebaWNtQ8WCTs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ef89a3d674228dec395a38ce3fbdc3366958131d', 'width': 1080}], 'source': {'height': 853, 'url': 'https://external-preview.redd.it/wmDtr1jlyVJMBK3PIyRaZxzJzYki3u8ebaWNtQ8WCTs.jpg?auto=webp&amp;s=c09f1d63ac659d3c93991d0835f878e46effdd1a', 'width': 1200}, 'variants': {}}]}",6,1637141944,1,https://mikkeldengsoe.substack.com/p/moldy-data,True,False,False,dataengineering,t5_36en4,45036,public,self,Moldy data and dashboards,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qvvp50/moldy_data_and_dashboards/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,bl4ckCloudz,,,[],,,,text,t2_jk5r3,False,False,False,[],False,False,1637133803,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qvtt14/what_books_do_you_recommend_to_someone_thats_a/,{},qvtt14,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,34,0,False,all_ads,/r/dataengineering/comments/qvtt14/what_books_do_you_recommend_to_someone_thats_a/,False,,,6,1637133814,1,"I come from more of an analyst background, but I somehow managed to get a job as a DE. Yeah, idk how that happened but it's been good so far.  

I'm pretty comfortable with Python and SQL. Most of my work for now is just prepping and uploading data to Snowflake for others to use.  

Clearly, I'm going to have to do more complex stuff in the future and rather not make my manager regret sticking out for me lol. What books or resources helped you back when you were just starting off?",True,False,False,dataengineering,t5_36en4,45030,public,self,What books do you recommend to someone that's a very new DE?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qvtt14/what_books_do_you_recommend_to_someone_thats_a/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,aj4manu,,,[],,,,text,t2_7uh7q,False,False,False,[],False,False,1637127469,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qvs55j/sample_databases_or_apis_to_import_data_into/,{},qvs55j,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,1,0,False,all_ads,/r/dataengineering/comments/qvs55j/sample_databases_or_apis_to_import_data_into/,False,,,6,1637127480,1,"Hi everyone

I'm a novice and trying to explore both Amplitude and Segment for our product analytics needs. Can someone kindly guide me towards any freely available data sources or databases to pull into these tools to compare the analytics? I hope I conveyed my requirements, I'm aware of free APIs such as PokeAPI etc.  but how can I use them to pull data with HTTP APIs? Please guide.",True,False,False,dataengineering,t5_36en4,45026,public,self,Sample databases or APIs to import data into Amplitude and Segment,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qvs55j/sample_databases_or_apis_to_import_data_into/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,OrdinaryAwareness784,,,[],,,,text,t2_9oivfyix,False,False,False,[],False,False,1637125743,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qvrnkc/what_technologies_are_you_learning_or_are/,{},qvrnkc,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,14,0,False,all_ads,/r/dataengineering/comments/qvrnkc/what_technologies_are_you_learning_or_are/,False,,,6,1637125754,1,"Lately work has been slower and I'll have more time to learn some things, so I wanted to get some ideas. What technologies are you thinking on learning or are currently learning?

I'm also interested in getting opinions on what newer technologies might be important to get familiarized with.

For example: I've been hearing a lot about DBT and want to learn more, I've also been wanting to explore the data streaming side, so thinking about Kafka, Pub/Sub, etc.",True,False,False,dataengineering,t5_36en4,45020,public,self,What technologies are you learning or are interested in learning?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qvrnkc/what_technologies_are_you_learning_or_are/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,LieCheatSteaI,,,[],,,,text,t2_15z7al,False,False,False,[],False,False,1637119807,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qvpw1w/question_on_basics_of_segmentation_and_compaction/,{},qvpw1w,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/qvpw1w/question_on_basics_of_segmentation_and_compaction/,False,,,6,1637119819,1,"I'm reading through designing data intensive applications and am on chapter 3 where the author describes segmentation and compaction (the basics of it at least). 

The argument here is that we can run out of memory if we keep on appending to one segment.  I'm curious how does segmentation help here. If you have one big file of size N and divide it into some parts, isn't the total size the same? How would this help us if we run out of memory? 

Maybe this is something covered later in the book, but it's a bit confusing to me. I guess I can see how maybe you run out of memory on one disk and you use segmentation to write to another? But it's not making sense how this saves us from the issue of running out of memory. There's probably some 'fuller picture' thing I'm missing here.. Thanks in advance!",True,False,False,dataengineering,t5_36en4,45012,public,self,Question on basics of segmentation and compaction concept (from designing data intensive applications book),0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qvpw1w/question_on_basics_of_segmentation_and_compaction/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,m4329b,,,[],,,,text,t2_l1pcf,False,False,False,[],False,False,1637119215,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qvppj0/do_mdm_tools_work/,{},qvppj0,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,26,0,False,all_ads,/r/dataengineering/comments/qvppj0/do_mdm_tools_work/,False,,,6,1637119226,1,"My company is looking at implementing an MDM tool (Informatica) to help manage our customer and marketing data, generate a universal customer ID, resolve duplicates, etc.  We're a B2C company.

I have some reservations and feel like it wouldn't necessarily solve our problems. Anyone have experience here?",True,False,False,dataengineering,t5_36en4,45012,public,self,Do MDM Tools Work?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qvppj0/do_mdm_tools_work/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Melecertes,,,[],,,,text,t2_9ta5qcy9,False,False,False,[],False,False,1637118873,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qvplqb/what_is_the_best_workflowarchitecture_to/,{},qvplqb,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,4,0,False,all_ads,/r/dataengineering/comments/qvplqb/what_is_the_best_workflowarchitecture_to/,False,,,6,1637118884,1,"This is something I'm curious about, but don't have much knowledge coming from a more strictly data science/statistics background, but basically want to be able to incorporate more powerful python transformations prior to the data being loaded into Looker from the DB; how is the best way about doing that?

I know Looker has LookML with its own packages, but I'd really like to be able to just use python and potentially call-in whatever API I want. 

An option that I seem to find is to have data sent into my database into a clean/pristine table, and then extracted from there into the python workflow and redownloaded into a different table that Looker references/accesses and uses for reporting, leaving the original table with un-transformed data. Is that generally a recommended workflow?

Any other options, recommendations, or considerations?",True,False,False,dataengineering,t5_36en4,45012,public,self,What is the best workflow/architecture to incorporate python and machine learning into Looker Dashboard reports?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qvplqb/what_is_the_best_workflowarchitecture_to/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,throwawayacc6487,,,[],,,,text,t2_9f2asg58,False,False,False,[],False,False,1637105135,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qvl69e/which_is_more_promising/,{},qvl69e,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,4,0,False,all_ads,/r/dataengineering/comments/qvl69e/which_is_more_promising/,False,,,6,1637105146,1,"More generally: web development track vs. data track

[View Poll](https://www.reddit.com/poll/qvl69e)",True,False,False,dataengineering,t5_36en4,45001,public,self,Which is more promising?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qvl69e/which_is_more_promising/,all_ads,6,,,,,,,,,,,,,"{'is_prediction': False, 'options': [{'id': '11894320', 'text': 'Full Stack Developer'}, {'id': '11894321', 'text': 'Data Engineer'}, {'id': '11894322', 'text': 'Results'}], 'prediction_status': None, 'resolved_option_id': None, 'total_stake_amount': None, 'total_vote_count': 0, 'tournament_id': None, 'user_selection': None, 'user_won_amount': None, 'vote_updates_remained': None, 'voting_end_timestamp': 1637364335150}",,,,,
[],False,countlessbass,,,[],,,,text,t2_66078vh9,False,False,False,[],False,False,1637103313,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qvkj7b/informatica_alternatives_proscons/,{},qvkj7b,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,4,0,False,all_ads,/r/dataengineering/comments/qvkj7b/informatica_alternatives_proscons/,False,,,6,1637103324,1,"Finance guy getting the tech pitch from informatica and my concerns are that it seems more of a closed system than the alternatives and the cost associated with the GUI interface (slowing production). Am I off base?

What other alternatives do you prefer?",True,False,False,dataengineering,t5_36en4,45000,public,self,Informatica alternatives? Pros/cons?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qvkj7b/informatica_alternatives_proscons/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Thybrat,,,[],,,,text,t2_beueng4,False,False,False,[],False,False,1637102366,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qvk6wq/best_practices_for_logging_events/,{},qvk6wq,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,3,0,False,all_ads,/r/dataengineering/comments/qvk6wq/best_practices_for_logging_events/,False,,,6,1637102377,1,"Hi everyone!

I'm currently thinking about how to track events for a web app I created. Specifically, I would want to track button clicks for a B2C web application with up to millions of users every day that click the button up to hundreds of times per day.

For now, it will be about 2000 users with about 10-20 interactions with the buttons per user per day.

I can currently see the following three options:

&amp;#x200B;

* Hard code tracking on web app and store it in the database with POST requests
   * Pro: Gives me complete freedom and perfect mapping to users
   * Con:  Can create a lot of load and demand on the DB with each request
* Google Analytics Tag Manager implementation with data layer variable for user ids
   * Pro: Simple and good enough for current state, no demand on production DB
   * Con: I'm not sure how accurate the user id will be + would have to be joined with production DB data
* 3rd Party Tools (Segment, etc.)
   * Pro: Already solved so no ad hoc issues, no demand on production DB
   * Con: Might be too expensive + would  have to be joined with production DB data  


I'd love to get your thoughts from a data engineering perspective! Thanks a lot!",True,False,False,dataengineering,t5_36en4,45000,public,self,Best practices for logging events,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qvk6wq/best_practices_for_logging_events/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Delicious_Attempt_99,,,[],,,,text,t2_ci308gob,False,False,False,[],False,False,1637098104,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qvily5/big_data_projects/,{},qvily5,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/qvily5/big_data_projects/,False,,,6,1637098116,1,"I find very few resources/websites for working on big data side projects. Can anyone help me to come up with some good ideas for working on big data projects?
If permitted, you can also share high level architecture of your currently working projects.",True,False,False,dataengineering,t5_36en4,44999,public,self,Big data projects,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qvily5/big_data_projects/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,rob121212111,,,[],,,,text,t2_31hbmows,False,False,False,[],False,False,1637097037,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qvi7r3/best_way_to_transfer_multiple_tables_from_hadoop/,{},qvi7r3,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,4,0,False,all_ads,/r/dataengineering/comments/qvi7r3/best_way_to_transfer_multiple_tables_from_hadoop/,False,,,6,1637097049,1,I have a couple hundred tables to copy from Hadoop to Oracle. What would be the easiest way to do a one time copy from Hadoop to Oracle?,True,False,False,dataengineering,t5_36en4,44998,public,self,Best way to transfer multiple tables from Hadoop to Oracle?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qvi7r3/best_way_to_transfer_multiple_tables_from_hadoop/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Georgehwp,,,[],,,,text,t2_5okt4lzw,False,False,False,[],False,False,1637095817,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qvhr4e/how_to_refactor_slow_pipelines/,{},qvhr4e,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,8,0,False,all_ads,/r/dataengineering/comments/qvhr4e/how_to_refactor_slow_pipelines/,False,,,6,1637095829,1,"There are many slow steps of a slow pipeline I suspect I could refactor to improve performance and even clarity, but the cost of refactoring the code is high because it takes so long to properly test. I'd want to test each update in isolation but that leads to a very slow iterative process. How do people typically deal with this constraint? It's been the same everywhere I've worked.",True,False,False,dataengineering,t5_36en4,44997,public,self,How to refactor slow pipelines?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qvhr4e/how_to_refactor_slow_pipelines/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Culpgrant21,,,[],,,,text,t2_1n3qfa0v,False,False,False,[],False,False,1637091033,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qvfxi8/best_way_to_send_data_out_of_snowflake_to_rest_api/,{},qvfxi8,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,11,0,False,all_ads,/r/dataengineering/comments/qvfxi8/best_way_to_send_data_out_of_snowflake_to_rest_api/,False,,,6,1637091046,1,"Hey I am working with a team who has an api setup to receive data. I need to send snowflake daily to the API. 

The data is about 1-2 million rows and 6 columns. 
Would the best way to do this is just a python script to send the data?",True,False,False,dataengineering,t5_36en4,44992,public,self,Best way to send data out of Snowflake to REST API,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qvfxi8/best_way_to_send_data_out_of_snowflake_to_rest_api/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,theporterhaus,#46d160,mod,[],fd5b074e-239e-11e8-a28b-0e0f8d9eda5a,mod | Sr. Data Engineer,light,text,t2_2tv9i42n,False,False,False,[],False,False,1637090599,twitter.com,https://www.reddit.com/r/dataengineering/comments/qvfr2z/databricks_vs_snowflake/,{},qvfr2z,False,True,False,False,False,True,False,False,#ff66ac,[],dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1,Meme,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/qvfr2z/databricks_vs_snowflake/,False,link,"{'enabled': False, 'images': [{'id': 'Mv4Z3De3fyv0f5rDhrXKZeUyAR1VIU0fDrK6s864zs4', 'resolutions': [{'height': 61, 'url': 'https://external-preview.redd.it/g_FN8c6eJu7AxYYym1A_fX40Sp3oFXZzwiQ3lvMrm5w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7d9a1e9af1c157685c21ae7db01ba69f2c19d0fa', 'width': 108}], 'source': {'height': 80, 'url': 'https://external-preview.redd.it/g_FN8c6eJu7AxYYym1A_fX40Sp3oFXZzwiQ3lvMrm5w.jpg?auto=webp&amp;s=eaa1ccb841c25aac0dfdecb4e9206f113770eba3', 'width': 140}, 'variants': {}}]}",6,1637090612,1,,True,False,False,dataengineering,t5_36en4,44992,public,https://b.thumbs.redditmedia.com/dHgOu_dtASBV6KMEClBHeKEcu0CSbkRnYsvHI4Ei2dY.jpg,Databricks vs Snowflake,0,[],1.0,https://twitter.com/andy_pavlo/status/1460429162741645330?s=21,all_ads,6,"{'oembed': {'author_name': 'Andy Pavlo', 'author_url': 'https://twitter.com/andy_pavlo', 'cache_age': 3153600000, 'height': None, 'html': '&lt;blockquote class=""twitter-video""&gt;&lt;p lang=""en"" dir=""ltr""&gt;It continues... &lt;a href=""https://t.co/kLbMYXBJiB""&gt;https://t.co/kLbMYXBJiB&lt;/a&gt; &lt;a href=""https://t.co/aeSyNtqbpM""&gt;pic.twitter.com/aeSyNtqbpM&lt;/a&gt;&lt;/p&gt;&amp;mdash; Andy Pavlo (@andy_pavlo) &lt;a href=""https://twitter.com/andy_pavlo/status/1460429162741645330?ref_src=twsrc%5Etfw""&gt;November 16, 2021&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=""https://platform.twitter.com/widgets.js"" charset=""utf-8""&gt;&lt;/script&gt;\n', 'provider_name': 'Twitter', 'provider_url': 'https://twitter.com', 'type': 'rich', 'url': 'https://twitter.com/andy_pavlo/status/1460429162741645330', 'version': '1.0', 'width': 350}, 'type': 'twitter.com'}","{'content': '&lt;blockquote class=""twitter-video""&gt;&lt;p lang=""en"" dir=""ltr""&gt;It continues... &lt;a href=""https://t.co/kLbMYXBJiB""&gt;https://t.co/kLbMYXBJiB&lt;/a&gt; &lt;a href=""https://t.co/aeSyNtqbpM""&gt;pic.twitter.com/aeSyNtqbpM&lt;/a&gt;&lt;/p&gt;&amp;mdash; Andy Pavlo (@andy_pavlo) &lt;a href=""https://twitter.com/andy_pavlo/status/1460429162741645330?ref_src=twsrc%5Etfw""&gt;November 16, 2021&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=""https://platform.twitter.com/widgets.js"" charset=""utf-8""&gt;&lt;/script&gt;\n', 'height': 200, 'scrolling': False, 'width': 350}",,"{'oembed': {'author_name': 'Andy Pavlo', 'author_url': 'https://twitter.com/andy_pavlo', 'cache_age': 3153600000, 'height': None, 'html': '&lt;blockquote class=""twitter-video""&gt;&lt;p lang=""en"" dir=""ltr""&gt;It continues... &lt;a href=""https://t.co/kLbMYXBJiB""&gt;https://t.co/kLbMYXBJiB&lt;/a&gt; &lt;a href=""https://t.co/aeSyNtqbpM""&gt;pic.twitter.com/aeSyNtqbpM&lt;/a&gt;&lt;/p&gt;&amp;mdash; Andy Pavlo (@andy_pavlo) &lt;a href=""https://twitter.com/andy_pavlo/status/1460429162741645330?ref_src=twsrc%5Etfw""&gt;November 16, 2021&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=""https://platform.twitter.com/widgets.js"" charset=""utf-8""&gt;&lt;/script&gt;\n', 'provider_name': 'Twitter', 'provider_url': 'https://twitter.com', 'type': 'rich', 'url': 'https://twitter.com/andy_pavlo/status/1460429162741645330', 'version': '1.0', 'width': 350}, 'type': 'twitter.com'}","{'content': '&lt;blockquote class=""twitter-video""&gt;&lt;p lang=""en"" dir=""ltr""&gt;It continues... &lt;a href=""https://t.co/kLbMYXBJiB""&gt;https://t.co/kLbMYXBJiB&lt;/a&gt; &lt;a href=""https://t.co/aeSyNtqbpM""&gt;pic.twitter.com/aeSyNtqbpM&lt;/a&gt;&lt;/p&gt;&amp;mdash; Andy Pavlo (@andy_pavlo) &lt;a href=""https://twitter.com/andy_pavlo/status/1460429162741645330?ref_src=twsrc%5Etfw""&gt;November 16, 2021&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=""https://platform.twitter.com/widgets.js"" charset=""utf-8""&gt;&lt;/script&gt;\n', 'height': 200, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/qvfr2z', 'scrolling': False, 'width': 350}",80.0,140.0,https://twitter.com/andy_pavlo/status/1460429162741645330?s=21,,,,,,,,,,
[],False,flyoverstat,,,[],,,,text,t2_18xxmwj2,False,False,False,[],False,False,1637084937,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qvdjgm/is_talend_still_in_business/,{},qvdjgm,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,54,0,False,all_ads,/r/dataengineering/comments/qvdjgm/is_talend_still_in_business/,False,,,6,1637084948,1,"Anybody in regular contact with Talend? I'm working on a AWS DMS replacement project and Talend was on my short list. Four ""call me"" forms and an email direct to the Chief Revenue Officer and I haven't heard anything. It's been 10 days... All other vendors responded within hours. Anyone have a clue what's going on?",True,False,False,dataengineering,t5_36en4,44984,public,self,Is Talend still in business?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qvdjgm/is_talend_still_in_business/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,therealtibblesnbits,,,[],,,,text,t2_42gtu2c3,False,False,False,[],False,False,1637078545,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qvb1pz/is_de_just_all_metrics_all_the_time/,{},qvb1pz,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,6,0,False,all_ads,/r/dataengineering/comments/qvb1pz/is_de_just_all_metrics_all_the_time/,False,,,6,1637078557,1,"When I was first looking into DE as a formal career path, I thought it would be more like work I had done previously: writing parsers to extract data from PDFs, developing code to connect two pieces of software together via APIs, automating reports, building tools to make interacting with the data easier, optimizing SQL queries to run faster, writing a wrapper to connect to multiple databases so an analyst can get the data they need, etc. 

Instead, it's just all metrics. At least in my day to day. It's things like: why has this metric been stuck at 100% for the last week, let's ideate on possible metrics for the new product roll out, what are some ways we can reduce this metric, etc.

Either that or it's maintenance: we're changing the value of a field in a table, so let's make sure that doesn't break any of our existing pipelines. 

Does this match your experience? Is this reflective of what DE is?",True,False,False,dataengineering,t5_36en4,44974,public,self,Is DE just all metrics all the time?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qvb1pz/is_de_just_all_metrics_all_the_time/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Complex-Stress373,,,[],,,,text,t2_9va3r6a5,False,False,False,[],False,False,1637074287,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qv9h5u/how_would_you_manage_the_schema_evolution_in_this/,{},qv9h5u,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/qv9h5u/how_would_you_manage_the_schema_evolution_in_this/,False,,,6,1637074301,1,"I'm receiving daily three files: one parquet, one json and a txt file.

The information they contain are related. I have a pipeline that can ingest this data ""without problems"". However the challenge for me is trying to figure out how to handle any schema change on these files.

My concerns are mostly:

\- if they change a column name field. Will I need to change code all around the pipeline?

\- how can I detect in advance this schema change?

How would you manage this stuff?",True,False,False,dataengineering,t5_36en4,44973,public,self,how would you manage the schema evolution in this case?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qv9h5u/how_would_you_manage_the_schema_evolution_in_this/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,novicedataengineer,,,[],,,,text,t2_fsjwfn80,False,False,False,[],False,False,1637072140,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qv8q8g/how_are_olap_cubes_made/,{},qv8q8g,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,11,0,False,all_ads,/r/dataengineering/comments/qv8q8g/how_are_olap_cubes_made/,False,,,6,1637072151,1,"I am taking a course to learn the basics of data warehousing and the instructor introduced OLAP cubes. The assignment requires connecting to an OLAP server and using a predefined cube to create pivot tables. While I am able to work with cubes to create pivot tables, I do not understand how cubes are made as it was not covered in the course.

Coming from a SQL background, I understand how tables are made on a server and how they get populated either via application logic or through ETL processes.  

Can someone explain, like I am five, the end to end process of how an OLAP cube is created on an OLAP server?",True,False,False,dataengineering,t5_36en4,44972,public,self,How are OLAP cubes made?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qv8q8g/how_are_olap_cubes_made/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,RstarPhoneix,,,[],,,,text,t2_57e44nxs,False,False,False,[],False,False,1637063054,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qv60i5/column_order_in_aws_redshift/,{},qv60i5,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,5,0,False,all_ads,/r/dataengineering/comments/qv60i5/column_order_in_aws_redshift/,False,,,6,1637063065,1,Are there any easy ways / methods to reorder a column in AWS Redshift ? I know that the alter table .. after col is not supported but there should be some or the other way via which we can reorder columns in Redshift. Any help is highly appreciated. Thanks in advance.,True,False,False,dataengineering,t5_36en4,44970,public,self,Column order in AWS Redshift,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qv60i5/column_order_in_aws_redshift/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Born-Comment3359,,,[],,,,text,t2_5t56uq7x,False,False,False,[],False,False,1637055154,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qv43ad/why_are_there_no_entrylevel_data_engineering_jobs/,{},qv43ad,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,47,0,False,all_ads,/r/dataengineering/comments/qv43ad/why_are_there_no_entrylevel_data_engineering_jobs/,False,,,6,1637055165,1,"I used to have a great passion to learn data engineering but a lot of people adviced not to do it and waste my because there is not such a thing as entry level data engineer, so I will not find a DE job. They adviced me to learn Java or Golang instead because there are more entry level jobs for those technologies.
So my question is to data engineers and people who have been in the DE field for a long time: why are there no entry level roles in DE job market?",True,False,False,dataengineering,t5_36en4,44962,public,self,Why are there no entry-level data engineering jobs?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qv43ad/why_are_there_no_entrylevel_data_engineering_jobs/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,gato_felix_69,,,[],,,,text,t2_ayt31tct,False,False,False,[],False,False,1637050868,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qv33sx/best_sources_to_learn_snowflake_and_get_certified/,{},qv33sx,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,9,0,False,all_ads,/r/dataengineering/comments/qv33sx/best_sources_to_learn_snowflake_and_get_certified/,False,,,6,1637050879,1,"After joining this forum and finding out that I was [living under a rock](https://www.reddit.com/r/dataengineering/comments/qs4f4w/comment/hkaubjl/?utm_source=share&amp;utm_medium=web2x&amp;context=3), I´ve decided to give Sowflake a try. Would you guys recommend any good reference for someone with a good background in DE (mostly Python, SQL, Spark, Cloudera and GCP)? I am particularly interested in anything focused on preparing for the certifications. Something like a voucher for their online training catalog (like the one they have in databricks) would be great as well.

Thanks!",True,False,False,dataengineering,t5_36en4,44961,public,self,Best sources to learn Snowflake and get certified,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qv33sx/best_sources_to_learn_snowflake_and_get_certified/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,cookiethunderstorm30,,,[],,,,text,t2_6jzfhr15,False,False,False,[],False,False,1637049867,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qv2vav/how_to_simulate_realtime_big_data_source_for/,{},qv2vav,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,18,0,False,all_ads,/r/dataengineering/comments/qv2vav/how_to_simulate_realtime_big_data_source_for/,False,,,6,1637049878,1,"Hi, all. Currently learning kafka, and I want to build a personal project to use kafka to ingest ""big data"" in real-time. I am thinking 100MB+ per second (continuously so it would be like 2GB+ per hour), because I want to learn processing big data with kafka. My problem is, how do I simulate a real-time data source like that? I am guessing there is no open-source real-time data with that kind of volume. Any advice? Thank you.",True,False,False,dataengineering,t5_36en4,44960,public,self,"How to simulate real-time ""big data"" source for personal project",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qv2vav/how_to_simulate_realtime_big_data_source_for/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,noNSFWcontent,,,[],,,,text,t2_yagno,False,False,False,[],False,False,1637043998,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qv1ew5/booked_my_azure_dp203_data_engineer_certification/,{},qv1ew5,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,8,0,False,all_ads,/r/dataengineering/comments/qv1ew5/booked_my_azure_dp203_data_engineer_certification/,False,,,6,1637044009,1,"I am starting my new job from Monday and since I had been studying Azure Data Engineer certification for a month, I thought I might as well get it over with. 

Those of you who have successfully completed the exam what topics do you recommend I should be fully prepared with?",True,False,False,dataengineering,t5_36en4,44959,public,self,Booked my Azure DP-203 Data Engineer Certification exam for tomorrow. Any advice you would have is welcome.,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qv1ew5/booked_my_azure_dp203_data_engineer_certification/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,theferalmonkey,,,[],,,,text,t2_6b0ljzqy,False,False,False,[],False,False,1637043645,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qv1bpx/project_hamilton_a_microframework_for_creating/,{},qv1bpx,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,4,0,False,all_ads,/r/dataengineering/comments/qv1bpx/project_hamilton_a_microframework_for_creating/,False,self,"{'enabled': False, 'images': [{'id': 'nCAHzMgYa2ZNgHHZWDNtF-I5Jq5-_KbkpVwFubHl9Xk', 'resolutions': [{'height': 54, 'url': 'https://external-preview.redd.it/n9AT13A_LsD5joBpem7q--sPSjc6sARvhG_s8FQNLp8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=67f613760833e45925367f17bc75de3b5cc59a24', 'width': 108}, {'height': 108, 'url': 'https://external-preview.redd.it/n9AT13A_LsD5joBpem7q--sPSjc6sARvhG_s8FQNLp8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5e92a13ab3b74c6348ff02986af8633d103aab2a', 'width': 216}, {'height': 160, 'url': 'https://external-preview.redd.it/n9AT13A_LsD5joBpem7q--sPSjc6sARvhG_s8FQNLp8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=942bb7209d3335c64de6410c4aa60697934adfff', 'width': 320}, {'height': 320, 'url': 'https://external-preview.redd.it/n9AT13A_LsD5joBpem7q--sPSjc6sARvhG_s8FQNLp8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5488f35647cb6bb45d36822bd6b68e5d0f5d7be5', 'width': 640}, {'height': 480, 'url': 'https://external-preview.redd.it/n9AT13A_LsD5joBpem7q--sPSjc6sARvhG_s8FQNLp8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0e9958dd5926af23a5c06bde9fe52d018e6d9fbb', 'width': 960}, {'height': 540, 'url': 'https://external-preview.redd.it/n9AT13A_LsD5joBpem7q--sPSjc6sARvhG_s8FQNLp8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cdd35a70c4448b04a0f5595e4d870727836fdca1', 'width': 1080}], 'source': {'height': 600, 'url': 'https://external-preview.redd.it/n9AT13A_LsD5joBpem7q--sPSjc6sARvhG_s8FQNLp8.jpg?auto=webp&amp;s=bac00357eb3bbe4918830846d0368703c3b08a01', 'width': 1200}, 'variants': {}}]}",6,1637043656,1,"Hi, I'm one of authors that created - [https://github.com/stitchfix/hamilton](https://github.com/stitchfix/hamilton).

In terms of why I'm posting? I'm after some feedback on the project itself, in addition to where to take this project. Would this be a useful tool/of interest to people in data engineering? Why/Why not? 

What is Hamilton? It's an open source library for easily creating \[pandas\] dataframes. It was birthed to help a team tame their code-base. It is production grade code and has been running at Stitch Fix for \~2 years.

The core of the idea is that people write functions that look like this:

    def column_c(column_a: pd.Series, column_b: pd.Series) -&gt; pd.Series:
        """"""Some doc string""""""
        return column_a + column_b  

instead of:

    df['column_c'] = df['column_a'] + df['column_b'] 

I think it's a cool paradigm, and helps simplify and empower a team to do more without worrying about how it's all glued together. For the backstory on how it came to be, please see our blog post on it [https://multithreaded.stitchfix.com/blog/2021/10/14/functions-dags-hamilton/](https://multithreaded.stitchfix.com/blog/2021/10/14/functions-dags-hamilton/).

If you have thoughts or suggestions, I'm all ears. Thanks in advance.

If this isn't content that's appropriate for this subreddit -- apologies -- I'll happily take suggestions.",True,False,False,dataengineering,t5_36en4,44959,public,self,[Project] Hamilton: a micro-framework for creating dataframes -- looking for feedback,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qv1bpx/project_hamilton_a_microframework_for_creating/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Sifu_Breeze,,,[],,,,text,t2_39diqq11,False,False,False,[],False,False,1637034100,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/quyjcq/are_small_freelance_projects_a_good_way_to_get/,{},quyjcq,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,3,0,False,all_ads,/r/dataengineering/comments/quyjcq/are_small_freelance_projects_a_good_way_to_get/,False,,,6,1637034111,1,"Hey there! I’m currently honing my skills from an education/practice perspective on Coursera and free online data sets. However, I’m wondering if it is feasible to get started on small freelance work from gig sites like Upwork or Fiverr to have paid work experience to talk about? Has anyone has success in finding part time work as a Data Engineer as well?",True,False,False,dataengineering,t5_36en4,44950,public,self,Are small freelance projects a good way to get experience while working in another role full time?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/quyjcq/are_small_freelance_projects_a_good_way_to_get/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Ok-Message1053,,,[],,,,text,t2_a4jh6m70,False,False,False,[],False,False,1637026957,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/quw75a/debugging_puppeteer_on_a_vm/,{},quw75a,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,4,0,False,all_ads,/r/dataengineering/comments/quw75a/debugging_puppeteer_on_a_vm/,False,,,6,1637026968,1,"I'm using puppeteer to crawl a website and my program runs perfectly on my own computer, however when I run it using Docker on a VM, it's throwing an error. I want to be able to run my code on the VM but somehow see the browser on my own computer so I can try to debug what's going on. Can someone help me figure out where to get started? I know how to SSH into the VM, but it's the browser part that I'm unsure of. If relevant, the VM is a GCE instance with linux and I'm on a mac.",True,False,False,dataengineering,t5_36en4,44943,public,self,debugging puppeteer on a VM,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/quw75a/debugging_puppeteer_on_a_vm/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,BigMakondo,,,[],,,,text,t2_8k7mo,False,False,False,[],False,False,1637018589,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qutf7y/what_is_the_workflow_of_adding_new_variables_to/,{},qutf7y,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,4,0,False,all_ads,/r/dataengineering/comments/qutf7y/what_is_the_workflow_of_adding_new_variables_to/,False,,,6,1637018601,1,"Imagine I have a bunch of JSON files that I want to add to a MongoDB collection. A JSON file could look like this:

    JSON file:

    {
      ""name"": ""Tom"",
      ""height"": ""175"",
      ""weight"": 70
    }

Also, for every document in the collection I would like to create a new variable, for example the BMI. So that the final collection would look something like this:

    MongoDB collection:

    {
      ""name"": ""Tom"",
      ""height"": ""175"",
      ""weight"": 70,
      ""bmi"": 22.9
    }
    {
      ""name"": ""Sara"",
      ""height"": ""180"",
      ""weight"": 75,
      ""bmi"": 23.1
    }
    ...

I could do this in a for loop, and for every JSON file, I could add it to the collection and create the new variable:

    Pseudocode:

    for file in json_files:
        add_to_collection()
        create_bmi()

No issues until this point.

Now, imagine that after adding multiple JSON files, I want to create another variable, say the BMR (for the sake of the example, let's assume that I have all the required variables to calculate it). Plus, other JSON files will arrive in the future. What are the best practices to do this?

**Is it fine to create an additional process that loops through the whole collection and adds the new variable?**

**Or, should I create a new collection instead?**

Or, something else that I am missing?

Also, should the `create_bmi()`method be called **after** all JSONs have been added, so that I may reuse some code when adding new variables? Or **is it fine to have two different processes: one to add already known variables and another one to add possible variables in the future?**

I understand this might be quite basic for this subreddit, but I'm not very familiar with best practices in this context.

Thank you.

PS: I'm doing all this in python but I'm looking for generic guidelines and best practices.",True,False,False,dataengineering,t5_36en4,44938,public,self,What is the workflow of adding new variables to an existing database (e.g. a MongoDB collection)?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qutf7y/what_is_the_workflow_of_adding_new_variables_to/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,noNSFWcontent,,,[],,,,text,t2_yagno,False,False,False,[],False,False,1637015621,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/quscyt/final_part_9_preparing_for_the_azure_data/,{},quscyt,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/quscyt/final_part_9_preparing_for_the_azure_data/,False,self,"{'enabled': False, 'images': [{'id': 'Qlb8PTi66y4vTnC_VkS48movu5W5oxf_xm-Tk5Z_ZzU', 'resolutions': [{'height': 54, 'url': 'https://external-preview.redd.it/Qux0qTUVnk0uK2opgeR2hK79w_NRE-UpTKENyC326FU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=302d5b2ddc62297be7508b24d0f6332b0017dfd1', 'width': 108}, {'height': 108, 'url': 'https://external-preview.redd.it/Qux0qTUVnk0uK2opgeR2hK79w_NRE-UpTKENyC326FU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8196f7b4568425b5a17aa6b70be9ba2e40c64cff', 'width': 216}, {'height': 160, 'url': 'https://external-preview.redd.it/Qux0qTUVnk0uK2opgeR2hK79w_NRE-UpTKENyC326FU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0738829eaaf3bfaaf7ff3d07c06ea1c05c42a1da', 'width': 320}, {'height': 320, 'url': 'https://external-preview.redd.it/Qux0qTUVnk0uK2opgeR2hK79w_NRE-UpTKENyC326FU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c6d93e2c4502b8acd6aadfdda002bf2da4cd3b30', 'width': 640}, {'height': 480, 'url': 'https://external-preview.redd.it/Qux0qTUVnk0uK2opgeR2hK79w_NRE-UpTKENyC326FU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a84710c16fde574080df5f37484111bfcd4c4e83', 'width': 960}, {'height': 540, 'url': 'https://external-preview.redd.it/Qux0qTUVnk0uK2opgeR2hK79w_NRE-UpTKENyC326FU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3f1a810f0985f46c782e48fb13f35ccada1d9ff1', 'width': 1080}], 'source': {'height': 600, 'url': 'https://external-preview.redd.it/Qux0qTUVnk0uK2opgeR2hK79w_NRE-UpTKENyC326FU.jpg?auto=webp&amp;s=02bbf4fd122d479abf9033f191084f19439b83a6', 'width': 1200}, 'variants': {}}]}",6,1637015633,1,"Hello everyone!

For those of you who are seeing this post for the first time, I'm am preparing to for the DP203 using a Udemy course and am also documenting my learning. So I thought might as well share with the community. I've already posted eight parts: [Part 1](https://www.reddit.com/r/dataengineering/comments/q9i4s9/part_1_preparing_for_the_azure_data_engineer/), [Part 2](https://www.reddit.com/r/dataengineering/comments/q9oxug/part_2_preparing_for_the_azure_data_engineer/), [Part 3\(a\)](https://www.reddit.com/r/dataengineering/comments/qccahx/part_3a_preparing_for_the_azure_data_engineer/), [Part 3\(b\)](https://www.reddit.com/r/dataengineering/comments/qdltri/part_3b_preparing_for_the_azure_data_engineer/), [Part 4](https://www.reddit.com/r/dataengineering/comments/qh2icg/part_4_preparing_for_the_azure_data_engineer/), [Part 5](https://www.reddit.com/r/dataengineering/comments/qlcsy8/part_5_preparing_for_the_azure_data_engineer/), [Part 6](https://www.reddit.com/r/dataengineering/comments/qmslnv/part_6_preparing_for_the_azure_data_engineer/), [Part 7](https://www.reddit.com/r/dataengineering/comments/qpk31l/part_7_preparing_for_the_azure_data_engineer/) and [Par 8](https://www.reddit.com/r/dataengineering/comments/qrstbf/part_8_preparing_for_the_azure_data_engineer/).

I kind of found the topics a little less interesting and may have not documented certain things without going into too much detail so probably use other resources as well to learn about these services. 

The larger topics covered are from a variety of services- 

* Learning about the Monitor Service on Azure
* Cache feature in your dedicated sql pool
* Monitoring in Data Factory 
* Metrics that are available in Stream Analytics
* Monitoring a Stream job
* Partitions in Event Hubs

[Here](https://github.com/rayaroun/Azure-DP203-Preparation/blob/master/9.%20Monitoring%20and%20optimizing%20data%20storage%20and%20processing.pdf) is the link to the pdf.

Well, this is it. This was the final part in this series. I hope whoever goes through the series leaves with a better understanding of the Azure services covered in the series. 

Thank you everyone!",True,False,False,dataengineering,t5_36en4,44937,public,self,FINAL Part 9 - Preparing for the Azure Data Engineer Certificate DP203 - Notes for optimizing data storage and processing,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/quscyt/final_part_9_preparing_for_the_azure_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,nobel-001,,,[],,,,text,t2_a3lkw6g4,False,False,False,[],False,False,1637008689,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qupszt/query_log_files_from_azure_blob_storage/,{},qupszt,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,1,0,False,all_ads,/r/dataengineering/comments/qupszt/query_log_files_from_azure_blob_storage/,False,,,6,1637008700,1,"I have application that saves its log files as json file in the Azure blob storage (around 40k files until now)

I want to allow the team to be able query these files.

What is the best practice to do this?

I am thinking to periodically load the log files into Snowflake table where the team can query it, but I feel this may not be the best solution.",True,False,False,dataengineering,t5_36en4,44928,public,self,Query log files from Azure blob storage,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qupszt/query_log_files_from_azure_blob_storage/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Blayzovich,,,[],,,,text,t2_8uxk9,False,False,False,[],False,False,1637007326,databricks.com,https://www.reddit.com/r/dataengineering/comments/qupb5w/databricks_responds_to_snowflake_tpcds/,{},qupb5w,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,False,65,0,False,all_ads,/r/dataengineering/comments/qupb5w/databricks_responds_to_snowflake_tpcds/,False,link,"{'enabled': False, 'images': [{'id': 'IPk-DFLmGw1u3q75PxxNsWSVeLN1cvKpdl_ooZUBUp0', 'resolutions': [{'height': 56, 'url': 'https://external-preview.redd.it/KKj1s87osQHGE_8uSUxnuZAUS5DlsIRm6xtXjLSolAA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d3d59d74abc7c174afe464db384591002c0024a5', 'width': 108}, {'height': 113, 'url': 'https://external-preview.redd.it/KKj1s87osQHGE_8uSUxnuZAUS5DlsIRm6xtXjLSolAA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a003f7f12c9e34a6922120c0e85cfebd14efcabc', 'width': 216}, {'height': 167, 'url': 'https://external-preview.redd.it/KKj1s87osQHGE_8uSUxnuZAUS5DlsIRm6xtXjLSolAA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fe86d1039c9302c90047890728985548f79e314c', 'width': 320}, {'height': 334, 'url': 'https://external-preview.redd.it/KKj1s87osQHGE_8uSUxnuZAUS5DlsIRm6xtXjLSolAA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9b9289beb2cbef034d24ef4b1abe8d2353823ca9', 'width': 640}, {'height': 502, 'url': 'https://external-preview.redd.it/KKj1s87osQHGE_8uSUxnuZAUS5DlsIRm6xtXjLSolAA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9687dbd90f81051545d8890a3f1f7ad74b71572a', 'width': 960}, {'height': 565, 'url': 'https://external-preview.redd.it/KKj1s87osQHGE_8uSUxnuZAUS5DlsIRm6xtXjLSolAA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e97b954d7c33c6553d4dba82e0a861d0a67a1e8a', 'width': 1080}], 'source': {'height': 1884, 'url': 'https://external-preview.redd.it/KKj1s87osQHGE_8uSUxnuZAUS5DlsIRm6xtXjLSolAA.jpg?auto=webp&amp;s=bf2eb1a4f6a76fbf7ac2e789ab6d1d4f29056aeb', 'width': 3600}, 'variants': {}}]}",6,1637007337,1,,True,False,False,dataengineering,t5_36en4,44928,public,https://a.thumbs.redditmedia.com/HDY-wIj5Jcp_oFR8c1VKWglh4XpFRIFxAZkImm6fEU4.jpg,Databricks responds to Snowflake (TPC-DS),0,[],1.0,https://databricks.com/blog/2021/11/15/snowflake-claims-similar-price-performance-to-databricks-but-not-so-fast.html,all_ads,6,,,,,,73.0,140.0,https://databricks.com/blog/2021/11/15/snowflake-claims-similar-price-performance-to-databricks-but-not-so-fast.html,,,,,,,,,,
[],False,fcd12,,,[],,,,text,t2_16463ehl,False,False,False,[],False,False,1637005458,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/quomkk/big_picture_hadoop_hdfs_spark_yarn_very/,{},quomkk,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,7,0,False,all_ads,/r/dataengineering/comments/quomkk/big_picture_hadoop_hdfs_spark_yarn_very/,False,,,6,1637005469,1,"Hi guys, I am a relatively new data engineer and my primary stack at work is using Spark. When google searching and trying to debug solutions and queries I usually come across these terms Hadoop, HDFS, Yarn, HBase and it gets quite overwhelming trying to understand the bigger picture of this ecosystem.

Could someone please provide a history of how all of these things relate together to provide a bigger picture so I can build a stronger mental model.",True,False,False,dataengineering,t5_36en4,44924,public,self,"Big Picture: Hadoop, HDFS, Spark, Yarn. Very overwhelmed",0,[],1.0,https://www.reddit.com/r/dataengineering/comments/quomkk/big_picture_hadoop_hdfs_spark_yarn_very/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,koastro,,,[],,,,text,t2_ffubi,False,False,False,[],False,False,1637001786,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qun9ly/suggested_bootcamps_with_no_upfront_cost/,{},qun9ly,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,3,0,False,all_ads,/r/dataengineering/comments/qun9ly/suggested_bootcamps_with_no_upfront_cost/,False,,,6,1637001811,1,"I’ve heard of bootcamps that have little to no upfront cost but instead will take a cut of your salary for the first X months/year as a payment. Anyone have any recommendations?

Currently I have a background in analytics in telecom, along with an AS in Comp Sci. However it’s hard to move into data eng when my role doesn’t use too many of the tools that are required of data eng positions.",True,False,False,dataengineering,t5_36en4,44922,public,self,Suggested bootcamps with no upfront cost?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qun9ly/suggested_bootcamps_with_no_upfront_cost/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,soujoshi,,,[],,,,text,t2_6kmo2ecy,False,False,False,[],False,False,1636995905,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qul2wa/airflow_s3_trigger/,{},qul2wa,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,10,0,False,all_ads,/r/dataengineering/comments/qul2wa/airflow_s3_trigger/,False,,,6,1636995916,1,"Have a requirement to process files that arrive on S3 as and when it arrives. The file may arrive anytime between 8am-4pm everyday. There are around 80 such feeds everyday. 
Which is the better solution?

AWS services
S3 triggers lambda that sends a message to SQS. Have a single DAG that keeps running and polling SQS and triggers corresponding processing DAG.

Airflow Sensor
Use S3 sensor for all 80 DAG's, start processing once file arrive?

Has someone tried this? Which is feasible?",True,False,False,dataengineering,t5_36en4,44915,public,self,Airflow S3 trigger,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qul2wa/airflow_s3_trigger/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,ps2931,,,[],,,,text,t2_gzyg3,False,False,False,[],False,False,1636995545,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qukxnx/reading_orc_file_from_http/,{},qukxnx,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/qukxnx/reading_orc_file_from_http/,False,,,6,1636995556,1,"Hi

Can anyone help me to figure out how can I read ORC files from http? The ORC files are on HDFS storage (an entirely  different system) which I can access only through http endpoint. Application is in Java/Spring.

Since http response is a like an input stream I am not able to figure out how to read records from that stream. 

Thanks",True,False,False,dataengineering,t5_36en4,44915,public,self,Reading ORC file from http,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qukxnx/reading_orc_file_from_http/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,shinigamisannn,,,[],,,,text,t2_duuua8e8,False,False,False,[],False,False,1636990206,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/quiynf/get_sucked_in_coursera_assignment/,{},quiynf,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,4,0,False,all_ads,/r/dataengineering/comments/quiynf/get_sucked_in_coursera_assignment/,False,,,6,1636990219,1,"hello guys, I'm currently in middle of a course in Coursera which is ""Data Mining Pipeline"" and I need to complete a simple assignment but they keep gave me 12/20 in every trial, I have been trying to solve it for about 2 days. I would be very grateful If someone could help me fix the problem.

https://preview.redd.it/q1x6twy72sz71.png?width=1332&amp;format=png&amp;auto=webp&amp;s=aa0ea630947b29911330c746fcbc864bde123fa1

https://preview.redd.it/trpvdzx72sz71.png?width=1398&amp;format=png&amp;auto=webp&amp;s=dd902620df7dff619cffa886b0b554a4a9c0a902",True,False,False,dataengineering,t5_36en4,44911,public,https://b.thumbs.redditmedia.com/SyE3KFSCqnOGx3b0gsICj6Qu1WIADlno-ddC2lsvAUI.jpg,Get sucked in Coursera assignment,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/quiynf/get_sucked_in_coursera_assignment/,all_ads,6,,,,,,33.0,140.0,,"{'q1x6twy72sz71': {'e': 'Image', 'id': 'q1x6twy72sz71', 'm': 'image/png', 'p': [{'u': 'https://preview.redd.it/q1x6twy72sz71.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9ec2afcf4d85face40d4279cb755ab6ac46083ef', 'x': 108, 'y': 26}, {'u': 'https://preview.redd.it/q1x6twy72sz71.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0fd0c6ebadb4ae912f188e3cf7ba3ae2f9848c64', 'x': 216, 'y': 52}, {'u': 'https://preview.redd.it/q1x6twy72sz71.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=01856cf16dd9b466ab9880a17823af06c0a2f430', 'x': 320, 'y': 77}, {'u': 'https://preview.redd.it/q1x6twy72sz71.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=830c270b34592f66be4cfa8b57e8af60c47a5180', 'x': 640, 'y': 154}, {'u': 'https://preview.redd.it/q1x6twy72sz71.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7c260bd5ce1e7dc2cb5bc880dac73b53015be9f2', 'x': 960, 'y': 232}, {'u': 'https://preview.redd.it/q1x6twy72sz71.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=239c82a3e73f7684f4bbbe4455bf0436610216e9', 'x': 1080, 'y': 261}], 's': {'u': 'https://preview.redd.it/q1x6twy72sz71.png?width=1332&amp;format=png&amp;auto=webp&amp;s=aa0ea630947b29911330c746fcbc864bde123fa1', 'x': 1332, 'y': 322}, 'status': 'valid'}, 'trpvdzx72sz71': {'e': 'Image', 'id': 'trpvdzx72sz71', 'm': 'image/png', 'p': [{'u': 'https://preview.redd.it/trpvdzx72sz71.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7fe1c6dd91f634dc820c9134b5cc7b3d0b99c157', 'x': 108, 'y': 46}, {'u': 'https://preview.redd.it/trpvdzx72sz71.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=043e301980a2e3d625eaa7b23c4b0a952f181c3b', 'x': 216, 'y': 93}, {'u': 'https://preview.redd.it/trpvdzx72sz71.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c37c474b52f8dd259c86ca8d5afdd938a8a94455', 'x': 320, 'y': 139}, {'u': 'https://preview.redd.it/trpvdzx72sz71.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=aa44739d970559797b13201fefa5def156aefba3', 'x': 640, 'y': 278}, {'u': 'https://preview.redd.it/trpvdzx72sz71.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e92242d9edd0af69d6e7bbb17aa41f58c896b863', 'x': 960, 'y': 417}, {'u': 'https://preview.redd.it/trpvdzx72sz71.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cf5a4164ddd9868b9cf6dcd9da78c0a13ada52ff', 'x': 1080, 'y': 469}], 's': {'u': 'https://preview.redd.it/trpvdzx72sz71.png?width=1398&amp;format=png&amp;auto=webp&amp;s=dd902620df7dff619cffa886b0b554a4a9c0a902', 'x': 1398, 'y': 608}, 'status': 'valid'}}",,,,,,,,,
[],False,dataengineerdude,,,[],,,,text,t2_4wrevs7q,False,False,False,[],False,False,1636988125,confessionsofadataguy.com,https://www.reddit.com/r/dataengineering/comments/qui7z3/orms_are_the_cigarettes_of_the_data_engineering/,{},qui7z3,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,8,0,False,all_ads,/r/dataengineering/comments/qui7z3/orms_are_the_cigarettes_of_the_data_engineering/,False,link,"{'enabled': False, 'images': [{'id': 'yUNsGcjekOzD809N4To0SG8B1vmO82evKIk2NtRqBpw', 'resolutions': [{'height': 72, 'url': 'https://external-preview.redd.it/pS5eQ97maXYuJkCpkOQneH_Tt5NGGXOXMvaP4UX2SYs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ec305f61ed97513a7009b4bfbcf6b4c0504f4bde', 'width': 108}, {'height': 144, 'url': 'https://external-preview.redd.it/pS5eQ97maXYuJkCpkOQneH_Tt5NGGXOXMvaP4UX2SYs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f181aa6094ea53447257ef463378d52e0169e0b2', 'width': 216}, {'height': 213, 'url': 'https://external-preview.redd.it/pS5eQ97maXYuJkCpkOQneH_Tt5NGGXOXMvaP4UX2SYs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8ba599faf8ffaa1d0dc194e837aa88e4f50732cc', 'width': 320}, {'height': 426, 'url': 'https://external-preview.redd.it/pS5eQ97maXYuJkCpkOQneH_Tt5NGGXOXMvaP4UX2SYs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=22ae3384217e43c7157a8c43971a48582f254f4a', 'width': 640}, {'height': 640, 'url': 'https://external-preview.redd.it/pS5eQ97maXYuJkCpkOQneH_Tt5NGGXOXMvaP4UX2SYs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=acbbe7d7a0930e1f373ba20b8262c7e9d3f15b09', 'width': 960}], 'source': {'height': 687, 'url': 'https://external-preview.redd.it/pS5eQ97maXYuJkCpkOQneH_Tt5NGGXOXMvaP4UX2SYs.jpg?auto=webp&amp;s=2a09f8ff62f52650cac6b6322577d25f4ca8a002', 'width': 1030}, 'variants': {}}]}",6,1636988145,1,,True,False,False,dataengineering,t5_36en4,44905,public,https://b.thumbs.redditmedia.com/KYsauZhuquiAKcilJm4-oVtpMRokBPPec2HYUQqKM4Q.jpg,ORM’s are the Cigarettes of the Data Engineering World.,0,[],1.0,https://www.confessionsofadataguy.com/orms-are-the-cigarettes-of-the-data-engineering-world/,all_ads,6,,,,,,93.0,140.0,https://www.confessionsofadataguy.com/orms-are-the-cigarettes-of-the-data-engineering-world/,,,,,,,,,,
[],False,DarkestKnight_23,,,[],,,,text,t2_d1v0zccf,False,False,False,[],False,False,1636986488,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/quhnqt/online_resources_on_apache_beam_and_dataflow/,{},quhnqt,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,11,0,False,all_ads,/r/dataengineering/comments/quhnqt/online_resources_on_apache_beam_and_dataflow/,False,,,6,1636986499,1,"I'll be working soon as a DE in my company, and the work is going to be focused on building Dataflow pipelines using Apache Beam. Are there any particularly useful resources online that I can use to quickly scale up my knowledge on Beam and Terraform? I'm fine going through the mountain that is the official Beam documentation but would prefer something much more concise",True,False,False,dataengineering,t5_36en4,44903,public,self,Online resources on Apache Beam and Dataflow,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/quhnqt/online_resources_on_apache_beam_and_dataflow/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,priyasweety1,,,[],,,,text,t2_3sqs3uub,False,False,False,[],False,False,1636986082,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/quhili/learn_pyspark_or_scala_without_any_prior/,{},quhili,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,10,0,False,all_ads,/r/dataengineering/comments/quhili/learn_pyspark_or_scala_without_any_prior/,False,self,"{'enabled': False, 'images': [{'id': 'MQJuUnHYnHoGKYme-9sNqyrjptj00RIKMOndH6hIfRc', 'resolutions': [{'height': 60, 'url': 'https://external-preview.redd.it/P6DbBXPinT2ad0PtyOKBgfkCNm1Jz22oWjlwSdZRnLQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1c781cd642384a9f19ec74f95a367bdc4e4317ff', 'width': 108}, {'height': 121, 'url': 'https://external-preview.redd.it/P6DbBXPinT2ad0PtyOKBgfkCNm1Jz22oWjlwSdZRnLQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a4a9d9c0937267a67be59e937b178efb7885075e', 'width': 216}, {'height': 180, 'url': 'https://external-preview.redd.it/P6DbBXPinT2ad0PtyOKBgfkCNm1Jz22oWjlwSdZRnLQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=09cedb7f152d8aacc355455223ee3c0724589242', 'width': 320}], 'source': {'height': 270, 'url': 'https://external-preview.redd.it/P6DbBXPinT2ad0PtyOKBgfkCNm1Jz22oWjlwSdZRnLQ.jpg?auto=webp&amp;s=588789d64801d7735c339165a322fdb3025ab6e1', 'width': 480}, 'variants': {}}]}",6,1636986093,1,"The title sums it up. Is it ok to do it. [https://www.udemy.com/course/rock-the-jvm-scala-for-beginners/](https://www.udemy.com/course/rock-the-jvm-scala-for-beginners/) 

I'm totally new to programming and SQL. Can anyone give me 90 days plan or practice to become a solid data engineer?",True,False,False,dataengineering,t5_36en4,44903,public,self,Learn PySpark or Scala without any prior programming experience,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/quhili/learn_pyspark_or_scala_without_any_prior/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,rotterdamn8,,,[],,,,text,t2_2o0q5m4h,False,False,False,[],False,False,1636985761,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/quheit/anyone_here_using_aws_data_pipeline/,{},quheit,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,11,0,False,all_ads,/r/dataengineering/comments/quheit/anyone_here_using_aws_data_pipeline/,False,,,6,1636985772,1,"I'm reading up on it now to automate some processes. We're using loading data into Redshift and eventually saving the output in S3. 

Data Pipeline seems to be useful; curious to hear your thoughts, how you're using it. Is there some overlap with tools like Airflow? 

Thanks.",True,False,False,dataengineering,t5_36en4,44903,public,self,anyone here using AWS Data Pipeline?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/quheit/anyone_here_using_aws_data_pipeline/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,fella85,,,[],,,,text,t2_67b69ife,False,False,False,[],False,False,1636980054,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qufjio/d_generating_documentation_of_data_fields/,{},qufjio,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,5,0,False,all_ads,/r/dataengineering/comments/qufjio/d_generating_documentation_of_data_fields/,False,,,6,1636980065,1,"Hi Everyone,

I'm wondering what people are using to create documentation of data fields of tables  created during the data processing steps.   


We want to avoid documenting the data.frame by manually annotating each field of the resulting data.frame by a different person from the person who wrote the code. This person needs to read/understand the code and understand the calculation that created the fields.  

Ideally, the programmer would add comments directly to the processing code that later would be parsed to generate the documentation automatically.  A bit how Doxygen creates documentation for code.   


Does something like that exist?

&amp;#x200B;

We have been looking at Datedo  but I'm not convinced that people will be keep the documentation up to date as new versions of the data.frames are created because of bug fixes, new ideas and calculation behind field creation or changes in the input data. 

Thanks.",True,False,False,dataengineering,t5_36en4,44900,public,self,[D] Generating documentation of data fields,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qufjio/d_generating_documentation_of_data_fields/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,an_tonova,,,[],,,,text,t2_d834g,False,False,False,[],False,False,1636969618,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qucvkd/my_team_is_looking_for_a_data_pro_who_could_help/,{},qucvkd,False,True,False,False,False,True,True,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/qucvkd/my_team_is_looking_for_a_data_pro_who_could_help/,False,,,6,1636969630,1,"Hi all, I'm a data startup founder. My team is thinking of releasing good quality analytics articles or even research on topics about modern data stack and tools. Obviously, such content shouldn't be created by copywriters or marketers.   
Probably someone among you loves blogging and writing. We will never take ownership of written materials and will never hide the name of the author. No need to say that we consider it as a paid collaboration.",True,False,False,dataengineering,t5_36en4,44894,public,self,My team is looking for a data pro who could help us with writing articles,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qucvkd/my_team_is_looking_for_a_data_pro_who_could_help/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,razkaplan,,,[],,,,text,t2_nmytf,False,False,False,[],False,False,1636965955,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/quc0rx/what_is_the_size_of_your_db_and_which_fs_you_work/,{},quc0rx,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/quc0rx/what_is_the_size_of_your_db_and_which_fs_you_work/,False,,,6,1636965966,1,"I have a new Real time DB to design and wondering which File system I should use, happy to hear your opinion.

[View Poll](https://www.reddit.com/poll/quc0rx)",True,False,False,dataengineering,t5_36en4,44893,public,self,What is the size of your DB? and which FS you work with?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/quc0rx/what_is_the_size_of_your_db_and_which_fs_you_work/,all_ads,6,,,,,,,,,,,,,"{'is_prediction': False, 'options': [{'id': '11854636', 'text': '&lt; 1 TB'}, {'id': '11854637', 'text': '1- 10 TB'}, {'id': '11854638', 'text': '10-100TB'}, {'id': '11854639', 'text': '100TB-1PB'}], 'prediction_status': None, 'resolved_option_id': None, 'total_stake_amount': None, 'total_vote_count': 0, 'tournament_id': None, 'user_selection': None, 'user_won_amount': None, 'vote_updates_remained': None, 'voting_end_timestamp': 1637225155465}",,,,,
[],False,apkaus,,,[],,,,text,t2_8lf5k10c,False,False,False,[],False,False,1636961218,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/quaw42/extracting_json_schema_from_parquet_file/,{},quaw42,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,4,0,False,all_ads,/r/dataengineering/comments/quaw42/extracting_json_schema_from_parquet_file/,False,,,6,1636961228,1,"Is there any way i can get json schema from a parquet file using python code.
I want to extract schema of some parquet files, So that i can use that schema in my terraform job to create table in bigquery",True,False,False,dataengineering,t5_36en4,44889,public,self,Extracting Json schema from parquet file,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/quaw42/extracting_json_schema_from_parquet_file/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Few_Concentrate4413,,,[],,,,text,t2_ck5d6z2q,False,False,False,[],False,False,1636949784,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qu7tmv/laptop_recommendation/,{},qu7tmv,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,False,13,0,False,all_ads,/r/dataengineering/comments/qu7tmv/laptop_recommendation/,False,,,6,1636949796,1,I want to buy a new laptop...will macbook air 2020 with m1 chip is good choice over any windows laptop within same price range.,True,False,False,dataengineering,t5_36en4,44880,public,self,Laptop recommendation,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qu7tmv/laptop_recommendation/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Bumpppppp,,,[],,,,text,t2_gjudtuz9,False,False,False,[],False,False,1636931845,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qu27qt/aws_data_lake_consultanta2c_proserv_offer_and/,{},qu27qt,False,False,False,False,False,False,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/qu27qt/aws_data_lake_consultanta2c_proserv_offer_and/,False,,,6,1636931856,1,[removed],True,False,False,dataengineering,t5_36en4,44862,public,self,AWS Data Lake consultant(A2C Proserv) Offer and Negotiations,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qu27qt/aws_data_lake_consultanta2c_proserv_offer_and/,all_ads,6,,,automod_filtered,,,,,,,,,,,,,,,
[],False,BoiElroy,,,[],,,,text,t2_r8dyi,False,False,True,[],False,False,1636928837,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qu16xp/can_someone_explain_like_im_5_nvidias_rapids_to/,{},qu16xp,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/qu16xp/can_someone_explain_like_im_5_nvidias_rapids_to/,False,,,6,1636928848,1,"I've tried to do a bit of reading about this but get confused, because Coiled claim RAPIDS as if it's a victory for Dask over Spark. But from looking at Nvidia's rapids website, [rapids.ai](https://rapids.ai), it seems like Rapids is just a way for either spark or dask or other things to run on GPUs?

So spark and dask, broadly speaking follow the driver node, worker nodes split, apply, combine strategy. Rapids is powered by GPUs which inherently are very efficient at parallel computations.

So is Rapids a paradigm shift suggesting we no longer need driver-worker clusters?",True,False,False,dataengineering,t5_36en4,44859,public,self,Can someone explain (like I'm 5) Nvidia's RAPIDS to me and its relation with Spark and Dask?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qu16xp/can_someone_explain_like_im_5_nvidias_rapids_to/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,curvature_propulsion,,,[],,,,text,t2_6f2qtp3h,False,False,False,[],False,False,1636926667,snowflake.com,https://www.reddit.com/r/dataengineering/comments/qu0ew7/industry_benchmarks_and_competing_with_integrity/,{},qu0ew7,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,7,0,False,all_ads,/r/dataengineering/comments/qu0ew7/industry_benchmarks_and_competing_with_integrity/,False,link,"{'enabled': False, 'images': [{'id': 'bdUF5r9MpkvHzi2k7HZqt11rE0cjgROx7Yg3UAQ64w4', 'resolutions': [{'height': 61, 'url': 'https://external-preview.redd.it/n5Xt_sRJnr1UcKSfqNtUammmfpx8ff8o0gdKbXpzS_Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ee665f13992c03170c7a1c3adba1fccd06f740f7', 'width': 108}, {'height': 123, 'url': 'https://external-preview.redd.it/n5Xt_sRJnr1UcKSfqNtUammmfpx8ff8o0gdKbXpzS_Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b2cba50c51c72f9c7e2d2959fe482f2ed6b0f947', 'width': 216}, {'height': 182, 'url': 'https://external-preview.redd.it/n5Xt_sRJnr1UcKSfqNtUammmfpx8ff8o0gdKbXpzS_Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3ec0f53971c6b75fe7caa6f683a589aab27d9b5c', 'width': 320}, {'height': 365, 'url': 'https://external-preview.redd.it/n5Xt_sRJnr1UcKSfqNtUammmfpx8ff8o0gdKbXpzS_Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a344b7808da9a03089ca193a30e1bf9401fcfa95', 'width': 640}, {'height': 548, 'url': 'https://external-preview.redd.it/n5Xt_sRJnr1UcKSfqNtUammmfpx8ff8o0gdKbXpzS_Y.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f4acacd332d07164ad4b0de1780b31cbe8f148cf', 'width': 960}, {'height': 617, 'url': 'https://external-preview.redd.it/n5Xt_sRJnr1UcKSfqNtUammmfpx8ff8o0gdKbXpzS_Y.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=02552b1d0215b0b02dd1f91619fa376925103c2c', 'width': 1080}], 'source': {'height': 686, 'url': 'https://external-preview.redd.it/n5Xt_sRJnr1UcKSfqNtUammmfpx8ff8o0gdKbXpzS_Y.jpg?auto=webp&amp;s=fbb22f0ae247609aa7058b7b496ed199d6d78429', 'width': 1200}, 'variants': {}}]}",6,1636926678,1,,True,False,False,dataengineering,t5_36en4,44857,public,https://b.thumbs.redditmedia.com/3bln9psWAWfEI5jlfzi2Lm0SFe7LIr8HWH4Ul0UK9GI.jpg,Industry Benchmarks and Competing with Integrity - Snowflake Blog,0,[],1.0,https://www.snowflake.com/blog/industry-benchmarks-and-competing-with-integrity/,all_ads,6,,,,,,80.0,140.0,https://www.snowflake.com/blog/industry-benchmarks-and-competing-with-integrity/,,,,,,,,,,
[],False,opensp00n,,,[],,,,text,t2_1uz13inz,False,False,False,[],False,False,1636917718,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qtx6ym/advice_on_interpreting_a_file_format/,{},qtx6ym,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,22,0,False,all_ads,/r/dataengineering/comments/qtx6ym/advice_on_interpreting_a_file_format/,False,,,6,1636917729,1,"I am trying to access some data on a blood pressure recorder so that I can export the data as a common format for use in excel etc. I want to use the recorder for a study and using the trash software bundled with it will be a pain.

The file format is .awp which seems to be a proprietary format with no explanation of what it does.

The file is unencrypted and so can be opened in notedpad. Most initial values are just stored as plain text, for example:

`YearBegin=2020`

&amp;#x200B;

After this the actual data is stored in a way I cannot understand:

`1=07E4090D001E00007A004A005700430001000000000`

`C1=`

`2=07E4090D01000000850047005400430001000000000`

`C2=`

These look like hex strings but nothing I use seems to come up with anything that makes sense. Before I abandon this idea, does anyone here have any idea if there might be an easy way to decrypt this information?",True,False,False,dataengineering,t5_36en4,44850,public,self,Advice on interpreting a file format,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qtx6ym/advice_on_interpreting_a_file_format/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,shittyfuckdick,,,[],,,,text,t2_1kset4fg,False,False,False,[],False,False,1636908217,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qttpr6/what_are_your_de_related_side_hustles/,{},qttpr6,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,62,0,False,all_ads,/r/dataengineering/comments/qttpr6/what_are_your_de_related_side_hustles/,False,,,6,1636908228,1,"After reading the 4 Hour Work Week, it really made me want to quit the 9-5 and start some company of my own.

I want to leverage my DE skills to create some semi passive income. Im curious if anyone else has done this and what has been successful?",True,False,False,dataengineering,t5_36en4,44840,public,self,What Are Your DE Related Side Hustles?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qttpr6/what_are_your_de_related_side_hustles/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,lexi_the_bunny,,,[],,,,text,t2_d5kpxkru,False,False,False,[],False,False,1636897735,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qtq6lr/how_do_you_manage_output_paths/,{},qtq6lr,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,8,0,False,all_ads,/r/dataengineering/comments/qtq6lr/how_do_you_manage_output_paths/,False,,,6,1636897746,1,"This might not be relevant to everyone here, but in my pipelines, we produce a number of intermediary data paths (Pretty much all into S3), either as checkpoints within tasks, or as hand-off points between tasks. All-told, the daily pipeline I maintain (all Spark or Pyspark, btw) probably outputs 100 or so data artifacts, with 3 of these being permanent external data artifacts and the rest only existing for a couple of weeks for debugging purposes.

We don't currently have a good way to manage these output paths. If someone on my team needs to debug a branch they're developing in, they just kind of have to know which task to look in, and look through the logs to get paths it's saving to. This is tedious and is high in the ""tribal knowledge"" aspect. I'd love some sort of system that tracks outputs of paths that engineers can see into that isn't just, like, logging. 

What do you all do? Do you know of any frameworks / libraries / systems that track this for you?",True,False,False,dataengineering,t5_36en4,44834,public,self,How do you manage output paths?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qtq6lr/how_do_you_manage_output_paths/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,adi10182,,,[],,,,text,t2_1zphk3hi,False,False,False,[],False,False,1636886104,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qtn75x/how_to_deconcentrate_data_with_negative_values/,{},qtn75x,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/qtn75x/how_to_deconcentrate_data_with_negative_values/,False,,,6,1636886115,1,"Tried adding constant and then log doesnt help. Binning is an option if all hope fails,. I need it for visualizing better and if possible keep the negative signs.

 

count 1209.000000

mean 0.000115

std 0.000305

min -0.000357

25% 0.000000

50% 0.000064

75% 0.000157

max 0.007013

Name: change, dtype: float64",True,False,False,dataengineering,t5_36en4,44829,public,self,How to deconcentrate data with negative values?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qtn75x/how_to_deconcentrate_data_with_negative_values/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,gato_felix_69,,,[],,,,text,t2_ayt31tct,False,False,False,[],False,False,1636881996,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qtm9ya/how_to_create_the_technical_strategy_for_a_data/,{},qtm9ya,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,17,0,False,all_ads,/r/dataengineering/comments/qtm9ya/how_to_create_the_technical_strategy_for_a_data/,False,,,6,1636882007,1,"I´ve been asked in a few interviews about ""how to create a technical strategy for a data engineering team"". As the topic is really broad, I don´t think the interviewer wants to know only about the technical stack (i.e., ingestion with NiFi, processing with Apache Spark, etc).

How would you build such document? What aspects would you consider? What do you think is being evaluated here?",True,False,False,dataengineering,t5_36en4,44829,public,self,How to create the technical strategy for a data engineering team?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qtm9ya/how_to_create_the_technical_strategy_for_a_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,budums,,,[],,,,text,t2_hvae1ta,False,False,False,[],False,False,1636859450,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qtgps3/base_practice_or_concept_for_data_staging/,{},qtgps3,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,15,0,False,all_ads,/r/dataengineering/comments/qtgps3/base_practice_or_concept_for_data_staging/,False,,,6,1636859461,1,"hi there  
I just started make ETL project on development instance.  
but I still confused about data staging.  


1. Data Staging is a same with transaction database but is different instance ? Or Extract some data with specific column or extract data from join table ?
2. What tools or technology use ? put data to database (mysql or other rdbms) or extract to file like flat file (.csv)

because currently I extract from transaction database from join table and put data to Warehouse Database .",True,False,False,dataengineering,t5_36en4,44816,public,self,Base Practice Or Concept For Data Staging,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qtgps3/base_practice_or_concept_for_data_staging/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,ksharma510,,,[],,,,text,t2_8c9wkn8,False,False,False,[],False,False,1636844312,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qtc9ud/require_some_open_source_data_pipeline/,{},qtc9ud,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,2,0,False,all_ads,/r/dataengineering/comments/qtc9ud/require_some_open_source_data_pipeline/,False,,,6,1636844323,1,"I have an interview where I am suppose to read and understand ETL pipeline repo of the company written in Python. My job is to suggest improvement if any and do some extensions. 

The thing is that I have only used python for some home projects as script but never used it extensively or professionally. I am well versed with few other programming language such as php, Java script , plsql. 

Please suggest me few data pipeline projects that I can study and get hang of the flow.",True,False,False,dataengineering,t5_36en4,44805,public,self,Require some Open source data pipeline package/code in Python for self study.,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qtc9ud/require_some_open_source_data_pipeline/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,No_Helicopter9361,,,[],,,,text,t2_993w1aoh,False,False,False,[],False,False,1636843566,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qtc1kc/advice_on_college_major_and_minor_for_a_data/,{},qtc1kc,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,11,0,False,all_ads,/r/dataengineering/comments/qtc1kc/advice_on_college_major_and_minor_for_a_data/,False,,,6,1636843577,1,"I am currently a undergraduate sophomore at my college and I am soon going to declare my major. I want to make sure that I am picking the right degree/major/minor for becoming a business intelligence analyst/specialist and a data analyst. .The degree is called MIS (management information systems). The degree is a mixture between business and information technology. All the business classes I already finished like marketing, accounting, finance, management, etc. 

1. The soon to be technical classes I will take are python, database management systems, SQL, system analysis, data warehousing, ETL, data analytics, business intelligence, project management, and an internship.  Are those classes worth taking for the data field? I already now those are classes that are good in the data field but I just want to make sure.

2. Are any of those particular subjects difficult to learn as in are the concepts hard to understand? If you need a comparsion for something I find difficult it's something that requires tremendous hours of studying to fully understand the concept of what is being taught in class like computer engineering majors. How would you rank them from being the most difficult to being the easiest?

3.If it turns out that I don't like data analytics once I graduate, what other carrer fields can I go into that aren't a business intelligence specialist or a data analyst? 

4 .As for the minor, my college recommends I minor in marketing, applied economics, supply chain or finance. Out of those 4 options which one will be the best and most broad in the data analyst field?

5. With that certain combination of the MIS major and whatever minor you recommend I get, what other types of career fields can I go into?",True,False,False,dataengineering,t5_36en4,44805,public,self,Advice on college major and minor for a data analyst,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qtc1kc/advice_on_college_major_and_minor_for_a_data/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Zancrow03,,,[],,,,text,t2_enm015tx,False,False,False,[],False,False,1636837453,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qta49m/de_internship_at_facebook/,{},qta49m,False,False,False,False,False,False,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/qta49m/de_internship_at_facebook/,False,,,6,1636837463,1,[removed],True,False,False,dataengineering,t5_36en4,44798,public,self,DE Internship at Facebook,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qta49m/de_internship_at_facebook/,all_ads,6,,,automod_filtered,,,,,,,,,,,,,,,
[],False,madfire89,,,[],,,,text,t2_r8rfi,False,False,False,[],False,False,1636836152,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qt9p14/confluent_solution_engineer_interview/,{},qt9p14,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,5,0,False,all_ads,/r/dataengineering/comments/qt9p14/confluent_solution_engineer_interview/,False,,,6,1636836163,1,"Hi Guys, 

I have an interview for an entry-level Solution Engineer role at Confluent, a leading cloud-based event streaming platform. The role is focused on getting users to upgrade from Opensource Kafka to PAYG Confluent Cloud and guiding them through the journey to becoming happy customers. 

&amp;#x200B;

This isn't my background. I've busted my butt off doing an Amazon Cloud practitioner essentials course which took forever, but was free and clued me up quite well on cloud. 

&amp;#x200B;

I wanted to ask you guys if you have any interesting questions I can ask the Solution Engineering interviewer that would demonstrate curiosity, especially in relation to current industry trends.

&amp;#x200B;

And also what kind of questions I can expect to be asked for such a position?  


Thanks",True,False,False,dataengineering,t5_36en4,44797,public,self,Confluent Solution Engineer Interview,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qt9p14/confluent_solution_engineer_interview/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,FakeComa,,,[],,,,text,t2_izmyx,False,False,False,[],False,False,1636829184,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qt7get/do_these_tasks_seem_doable_for_a_recent_graduate/,{},qt7get,False,True,False,False,False,True,True,False,#ffb000,[],0922f6d6-a952-11eb-91e4-0e23043eebfb,Interview,light,text,False,False,True,0,0,False,all_ads,/r/dataengineering/comments/qt7get/do_these_tasks_seem_doable_for_a_recent_graduate/,False,,,6,1636829195,1,"So I recently graduated and this is the task I got given from one of my first interviews. I'm really struggling with the pandas and SQL sections. 

Python

1) Create a function fibonacci(n) to compute nth number of the Fibonacci sequence.
Fibonacci sequence: 0, 1, 1, 2, 3, 5, 8, 13, …

2) Create a function inv_fibonacci(k) to compute the inverse of fibonacci(n).
That is, inv_fibonacci( fibonacci(n) ) = n

4) Consider a pandas DataFrame, activity, with the following columns:
CustomerID (int), Date (date), DepositAmount (float), WithdrawalAmount (float)
Create a function balances(activity) which returns a pandas DataFrame containing
CustomerID, Date, OpeningBalance, ClosingBalance, where ClosingBalance =
OpeningBalance + DepositAmount – WithdrawalAmount and OpeningBalance = the pervious
day’s closing balance. For every customer in activity there should be a record for each date
from the earliest to the latest in activity. Assume an initial opening balance of 0.
Note: Customers will not have activity records on every date

SQL

There are 4 tables outlined here.

Create SQL queries to answer the following questions:
1) Create a table to show number of depositors by year-month.
(Output columns: YearMonth, Depositors)

3) Create a table to show response rate of each offer (a positive response is a communication
which results in a deposit on the day of, or the day after, the communication).
(Output columns: OfferName, PositiveResponses, NumComms, ResponseRate)",True,False,False,dataengineering,t5_36en4,44792,public,self,Do these tasks seem doable for a recent graduate?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qt7get/do_these_tasks_seem_doable_for_a_recent_graduate/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,adi10182,,,[],,,,text,t2_1zphk3hi,False,False,False,[],False,False,1636828164,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qt7447/help_with_skewed_data_with_negative_values/,{},qt7447,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,False,3,0,False,all_ads,/r/dataengineering/comments/qt7447/help_with_skewed_data_with_negative_values/,False,,,6,1636828175,1,"Hey, Newbie here, I've got a feature change which is relative and the difference between values is really small but it's really crucial as i am building a predictive model based on this feature . I want to interpolate it and wanna know if machine learning models are sensitive enough to catch these small differences.

 count    1209.000000

 mean        0.000115 

std         0.000305 

min        -0.000357 

25%         0.000000 

50%         0.000064 

75%         0.000157 

max         0.007013 

Name: change, dtype: float64",True,False,False,dataengineering,t5_36en4,44790,public,self,Help with skewed data with negative values,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qt7447/help_with_skewed_data_with_negative_values/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,usernamehere93,,,[],,,,text,t2_wld0h2i,False,False,False,[],False,False,1636819251,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qt4569/how_to_test_python_data_pipeline_functions/,{},qt4569,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,24,0,False,all_ads,/r/dataengineering/comments/qt4569/how_to_test_python_data_pipeline_functions/,False,,,6,1636819263,1,"I’m currently in the processes of writing unit and integration tests for a set of data processing functions that runs on AWS instances. 

I’m struggling to write meaningful tests for some of the more complex function that consist of pulling together, transforming and joining large Pandas DataFrames. My current strategy is to build a collection of example input DataFrame’s and expected outputs and compare them at the end, however the size of these inputs and outputs are large and so checking line by line that these are correct before committing them won’t scale. 

I was wondering what the best way to test these kinds of pipeline functions are and any tools that help to run these kinds of tests? Currently I’m just building a pytest script together to run and manage these tests like more simple unit tests, is there a better strategy for this?",True,False,False,dataengineering,t5_36en4,44787,public,self,How to test Python data pipeline functions?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qt4569/how_to_test_python_data_pipeline_functions/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,palpablepain,,,[],,,,text,t2_d6u6li6u,False,False,False,[],False,False,1636816218,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qt36ld/how_to_get_into_entry_level_data_engineering_jobs/,{},qt36ld,False,True,False,False,False,True,True,False,#349e48,[],069dd614-a7dc-11eb-8e48-0e90f49436a3,Career,light,text,False,False,True,13,0,False,all_ads,/r/dataengineering/comments/qt36ld/how_to_get_into_entry_level_data_engineering_jobs/,False,,,6,1636816229,1,I am looking for some sort of roadmap and then a good place to look for these jobs.,True,False,False,dataengineering,t5_36en4,44787,public,self,How to get into entry level data engineering jobs?,0,[],0.99,https://www.reddit.com/r/dataengineering/comments/qt36ld/how_to_get_into_entry_level_data_engineering_jobs/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,mwlon,,,[],,,,text,t2_8cr1i,False,False,False,[],False,False,1636811040,pancakedb.com,https://www.reddit.com/r/dataengineering/comments/qt1jch/i_made_pancakedb_a_new_type_of_columnar_db_that/,{},qt1jch,False,True,False,False,False,True,False,False,#0079d3,[],eb739554-a7db-11eb-95d7-0ec0f8f30313,Blog,light,text,False,False,True,13,0,False,all_ads,/r/dataengineering/comments/qt1jch/i_made_pancakedb_a_new_type_of_columnar_db_that/,False,,,6,1636811050,1,,True,False,False,dataengineering,t5_36en4,44777,public,default,"I made PancakeDB, a new type of columnar DB that uses 30-50% less storage and read time than .snappy.parquet while offering efficient incremental writes",0,[],1.0,https://pancakedb.com/,all_ads,6,,,,,,,,https://pancakedb.com/,,,,,,,,,,
[],False,adi10182,,,[],,,,text,t2_1zphk3hi,False,False,False,[],False,False,1636807404,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qt0ie2/how_to_replace_all_values_with_one_integer_and/,{},qt0ie2,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,12,0,False,all_ads,/r/dataengineering/comments/qt0ie2/how_to_replace_all_values_with_one_integer_and/,False,,,6,1636807415,1,"Basically binning 0 or other as 1 .

Thanks",True,False,False,dataengineering,t5_36en4,44776,public,self,How to replace all values with one integer and all nan values with 0 in pandas?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qt0ie2/how_to_replace_all_values_with_one_integer_and/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Revolutionary-Sea294,,,[],,,,text,t2_a1g5hjg1,False,False,False,[],False,False,1636801741,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qsz3xm/how_to_switch_from_etl_testing_role_to_de_role/,{},qsz3xm,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,3,0,False,all_ads,/r/dataengineering/comments/qsz3xm/how_to_switch_from_etl_testing_role_to_de_role/,False,,,6,1636801752,1,"I have 2~years of work experience in ETL testing and now I'm thinking to switch to Data Engineering role in Azure. I have a little experience in building pipelines in Data Factory for ∆ load and table migrations. I have intermediate SQL and Python skills. Could anyone give me roadmap for DE role so I can focus only on important things .
Note: I have an Azure subscription provided by the company so cost is not a constraint.
Thanks in advance.",True,False,False,dataengineering,t5_36en4,44772,public,self,How to switch from ETL Testing role to DE role.,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qsz3xm/how_to_switch_from_etl_testing_role_to_de_role/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,theoriginalmantooth,,,[],,,,text,t2_4gzaf8mv,False,False,False,[],False,False,1636791170,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qswoop/has_anyone_done_an_aws_live_classroom_training/,{},qswoop,False,True,False,False,False,True,True,False,#ff4500,[],92b74b58-aaca-11eb-b160-0e6181e3773f,Discussion,light,text,False,False,True,10,0,False,all_ads,/r/dataengineering/comments/qswoop/has_anyone_done_an_aws_live_classroom_training/,False,self,"{'enabled': False, 'images': [{'id': 'RUqh18uQTwuGJocqdUcC-6UfvfWS63SRDdr8AQqU3uM', 'resolutions': [{'height': 56, 'url': 'https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b015da4f990706696f7d06ac19bc75b807d90200', 'width': 108}, {'height': 113, 'url': 'https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=44756ae9c6e1724356ccaef8214086d7d0cc95da', 'width': 216}, {'height': 168, 'url': 'https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3a5696e6599c7d56b3770650b416341ba2102fd8', 'width': 320}, {'height': 336, 'url': 'https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4610f9bb7893259c61ba4fda892295f0da1a05ef', 'width': 640}, {'height': 504, 'url': 'https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9979842391099359ecaa7d0ce4c8c31f1e3bead7', 'width': 960}, {'height': 567, 'url': 'https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7fe7c4eb58196f897578137b50f669a4707c9902', 'width': 1080}], 'source': {'height': 630, 'url': 'https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?auto=webp&amp;s=8e3eb77ba905bb641af80fcf3efe1de0190ac8c2', 'width': 1200}, 'variants': {}}]}",6,1636791181,1,"Specifically asking for the ""Data Warehousing on AWS"" one: [https://aws.amazon.com/training/classroom/data-warehousing-on-aws/](https://aws.amazon.com/training/classroom/data-warehousing-on-aws/)

Summary:

* Costs around 2095 USD
* 3-day instructor-led training course
* In-person or virtual

Want to know:

* Is it worth it?
* General thoughts on it?",True,False,False,dataengineering,t5_36en4,44766,public,self,Has anyone done an AWS live classroom training before?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qswoop/has_anyone_done_an_aws_live_classroom_training/,all_ads,6,,,,,,,,,,,,,,,,,,
[],False,Ruudkin,,,[],,,,text,t2_7lmd5,False,False,False,[],False,False,1636790327,self.dataengineering,https://www.reddit.com/r/dataengineering/comments/qswhfo/options_for_client_data_retrievaletl/,{},qswhfo,False,True,False,False,False,True,True,False,#ea0027,[],2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3,Help,light,text,False,False,True,1,0,False,all_ads,/r/dataengineering/comments/qswhfo/options_for_client_data_retrievaletl/,False,,,6,1636790337,1,"We are planning to build a product to provide dashboards to clients
For context, this is a consultant-client relationship with multiple clients
- Would it be advisable to port the data into our (the consultant) environment and work with it (ETL/Presentation) or work directly from the client environment?
- If cloning the data outside the client's environment is not ideal, how would one go about performing the necessary data transformations to feed into a BI platform? Something like a cloud etl or something?

Apologies if this isn't the right place to post. I am a bit new to this",True,False,False,dataengineering,t5_36en4,44765,public,self,Options for client data retrieval/ETL?,0,[],1.0,https://www.reddit.com/r/dataengineering/comments/qswhfo/options_for_client_data_retrievaletl/,all_ads,6,,,,,,,,,,,,,,,,,,
